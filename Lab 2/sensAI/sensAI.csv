Before Bug fix,After Bug fix,Location,Bug type,Commit Message,Project URL,File Path,Fixed Commit,Buggy Commit,Test File,Coding Effort,Constructs,Lizard Features Buggy,Lizard Features Fixed,BLEU,crystalBLEU_score,BERT_score
"1 import scipy.stats
2 
3 from dcs.basic_models.util.string import objectRepr
4 
5 
6 class NormalDistribution:
7     def __init__(self, mean=0, std=1, unitMax=False):
","1 import scipy.stats
2 
3 from .string import objectRepr
4 
5 
6 class NormalDistribution:
7     def __init__(self, mean=0, std=1, unitMax=False):
","Before: 3
After: 3",remove dcs.basic_models.util import,Fixed import,https://github.com/opcode81/sensAI,src/sensai/util/math.py,af3315446bd4c9a65892b84e03b2dc2893e8a83d,b82c9fba6188eb773ef380a6e3f532eed490f39e,0,21,"{'module': 1, 'import_statement': 1, 'import': 2, 'dotted_name': 3, 'identifier': 8, '.': 4, 'import_from_statement': 1, 'from': 1, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 51, 'name': '__init__', 'long_name': '__init__( self , mean = 0 , std = 1 , unitMax = False )', 'start_line': 7, 'end_line': 16, 'full_parameters': ['self', ' mean = 0', ' std = 1', ' unitMax = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/math.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 51, 'name': '__init__', 'long_name': '__init__( self , mean = 0 , std = 1 , unitMax = False )', 'start_line': 7, 'end_line': 16, 'full_parameters': ['self', ' mean = 0', ' std = 1', ' unitMax = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/math.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9025139799587886,0.8940387317591186,"(tensor([0.9871]), tensor([0.9481]), tensor([0.9672]), tensor([0.9518]))"
"198     """"""Interface for DataUtil classes, which are used to process data for neural networks""""""
199 
200     @abstractmethod
201     def splitInputOutputPairs(self, fractionalSizeOfFirstSet):
202         """"""
203         Splits the data set
204 
205         :param fractionalSizeOfFirstSet: the desired fractional size in
206         :return: a tuple (A, B, meta) where A and B are tuples (in, out) with input and output data, and meta is a dictionary
207             containing meta-data on the split, which may contain the following keys:
208                 ""infoText"": text on the data set/the split performed;
209                 ""outputIndicesA"": output index sequence in the set A (one index for every input/output element of A);
210                 ""outputIndicesB"": output index sequence in the set A;
211         """"""
212         pass
213 
214     @abstractmethod
","198     """"""Interface for DataUtil classes, which are used to process data for neural networks""""""
199 
200     @abstractmethod
201     def splitInputOutputPairs(self, fractionalSizeOfFirstSet):
202         """"""
203         Splits the data set
204 
205         :param fractionalSizeOfFirstSet: the desired fractional size in
206 
207         :return: a tuple (A, B, meta) where A and B are tuples (in, out) with input and output data,
208             and meta is a dictionary containing meta-data on the split, which may contain the following keys:
209 
210             * ""infoText"": text on the data set/the split performed;
211             * ""outputIndicesA"": output index sequence in the set A (one index for every input/output element of A);
212             * ""outputIndicesB"": output index sequence in the set A;
213 
214         """"""
215         pass
216 
217     @abstractmethod
","Before: 206, 207, 208, 209, 210
After: 206, 207, 208, 209, 210, 211, 212, 213",fix bug in datautil.py,Made fgen registry more user friendly by enabling autocompletion for registered factories,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,345505b4f2ff1a71cbf476d263f3b2b53c3fb386,fcf3d7b4dd567c59eeefa96094e56f1b454d5fd5,0,1753,"{'module': 1, 'expression_statement': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, ')': 1, ':': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 4, 'nloc': 9, 'token_count': 85, 'name': '__init__', 'long_name': '__init__( self , vectorDataScaler : normalisation . VectorDataScaler , cuda : bool )', 'start_line': 28, 'end_line': 36, 'full_parameters': ['self', ' vectorDataScaler : normalisation . VectorDataScaler', ' cuda : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 9, 'token_count': 85, 'name': '__init__', 'long_name': '__init__( self , vectorDataScaler : normalisation . VectorDataScaler , cuda : bool )', 'start_line': 28, 'end_line': 36, 'full_parameters': ['self', ' vectorDataScaler : normalisation . VectorDataScaler', ' cuda : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8523086452585829,0.8471738706188198,"(tensor([0.9812]), tensor([0.9910]), tensor([0.9861]), tensor([0.9901]))"
"11 from functools import wraps, partial
12 from typing import Any, Callable, Iterator, List, Optional, TypeVar
13 
14 import sqlite3
15 
16 _log = logging.getLogger(__name__)
17 
18 T = TypeVar(""T"")
19 
20 
","11 from functools import wraps, partial
12 from typing import Any, Callable, Iterator, List, Optional, TypeVar
13 
14 import sqlite3
15 
16 log = logging.getLogger(__name__)
17 
18 T = TypeVar(""T"")
19 
20 
","Before: 16
After: 16",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,97,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 11, 'identifier': 17, 'import': 3, ',': 6, 'import_statement': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'call': 2, 'attribute': 1, '.': 1, 'argument_list': 2, '(': 2, ')': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9427781070492712,0.9374962291774642,"(tensor([0.9965]), tensor([0.9965]), tensor([0.9965]), tensor([0.9965]))"
"117     """"""
118     Represents a key-value cache as a dictionary which is persisted in a file using pickle
119     """"""
120     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
121         """"""
122         :param picklePath: the path of the file where the cache values are to be persisted
123         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
124             it is discarded
125         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
126             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
127             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
128         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
129             before actually storing the cache after a cache update
130         """"""
131         self.deferredSaveDelaySecs = deferredSaveDelaySecs
132         self.picklePath = picklePath
133         self.version = version
134         self.saveOnUpdate = saveOnUpdate
135         cacheFound = False
136         if os.path.exists(picklePath):
137             try:
138                 _log.info(f""Loading cache from {picklePath}"")
139                 persistedVersion, self.cache = loadPickle(picklePath)
140                 if persistedVersion == version:
141                     cacheFound = True
142             except EOFError:
143                 _log.warning(f""The cache file in {picklePath} is corrupt"")
144         if not cacheFound:
145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
","117     """"""
118     Represents a key-value cache as a dictionary which is persisted in a file using pickle
119     """"""
120     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
121         """"""
122         :param picklePath: the path of the file where the cache values are to be persisted
123         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
124             it is discarded
125         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
126             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
127             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
128         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
129             before actually storing the cache after a cache update
130         """"""
131         self.deferredSaveDelaySecs = deferredSaveDelaySecs
132         self.picklePath = picklePath
133         self.version = version
134         self.saveOnUpdate = saveOnUpdate
135         cacheFound = False
136         if os.path.exists(picklePath):
137             try:
138                 log.info(f""Loading cache from {picklePath}"")
139                 persistedVersion, self.cache = loadPickle(picklePath)
140                 if persistedVersion == version:
141                     cacheFound = True
142             except EOFError:
143                 log.warning(f""The cache file in {picklePath} is corrupt"")
144         if not cacheFound:
145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
","Before: 138
After: 138",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,760,"{'module': 1, 'expression_statement': 13, 'string': 4, 'string_start': 4, 'string_content': 5, 'string_end': 4, 'function_definition': 1, 'def': 1, 'identifier': 47, 'parameters': 1, '(': 6, ',': 6, 'default_parameter': 3, '=': 12, 'integer': 1, 'true': 2, 'float': 1, ')': 6, ':': 6, 'block': 6, 'assignment': 9, 'attribute': 12, '.': 12, 'false': 1, 'if_statement': 3, 'if': 3, 'call': 5, 'argument_list': 5, 'try_statement': 1, 'try': 1, 'interpolation': 2, '{': 3, '}': 3, 'pattern_list': 1, 'comparison_operator': 1, '==': 1, 'except_clause': 1, 'except': 1, 'not_operator': 1, 'not': 1, 'dictionary': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9821734827776265,0.9813031301277154,"(tensor([0.9972]), tensor([0.9972]), tensor([0.9972]), tensor([0.9972]))"
"117     """"""
118     Represents a key-value cache as a dictionary which is persisted in a file using pickle
119     """"""
120     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
121         """"""
122         :param picklePath: the path of the file where the cache values are to be persisted
123         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
124             it is discarded
125         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
126             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
127             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
128         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
129             before actually storing the cache after a cache update
130         """"""
131         self.deferredSaveDelaySecs = deferredSaveDelaySecs
132         self.picklePath = picklePath
133         self.version = version
134         self.saveOnUpdate = saveOnUpdate
135         cacheFound = False
136         if os.path.exists(picklePath):
137             try:
138                 _log.info(f""Loading cache from {picklePath}"")
139                 persistedVersion, self.cache = loadPickle(picklePath)
140                 if persistedVersion == version:
141                     cacheFound = True
142             except EOFError:
143                 _log.warning(f""The cache file in {picklePath} is corrupt"")
144         if not cacheFound:
145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
","117     """"""
118     Represents a key-value cache as a dictionary which is persisted in a file using pickle
119     """"""
120     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
121         """"""
122         :param picklePath: the path of the file where the cache values are to be persisted
123         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
124             it is discarded
125         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
126             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
127             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
128         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
129             before actually storing the cache after a cache update
130         """"""
131         self.deferredSaveDelaySecs = deferredSaveDelaySecs
132         self.picklePath = picklePath
133         self.version = version
134         self.saveOnUpdate = saveOnUpdate
135         cacheFound = False
136         if os.path.exists(picklePath):
137             try:
138                 log.info(f""Loading cache from {picklePath}"")
139                 persistedVersion, self.cache = loadPickle(picklePath)
140                 if persistedVersion == version:
141                     cacheFound = True
142             except EOFError:
143                 log.warning(f""The cache file in {picklePath} is corrupt"")
144         if not cacheFound:
145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
","Before: 143
After: 143",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,810,"{'module': 1, 'expression_statement': 13, 'string': 4, 'string_start': 4, 'string_content': 5, 'string_end': 4, 'function_definition': 1, 'def': 1, 'identifier': 47, 'parameters': 1, '(': 6, ',': 6, 'default_parameter': 3, '=': 12, 'integer': 1, 'true': 2, 'float': 1, ')': 6, ':': 6, 'block': 6, 'assignment': 9, 'attribute': 12, '.': 12, 'false': 1, 'if_statement': 3, 'if': 3, 'call': 5, 'argument_list': 5, 'try_statement': 1, 'try': 1, 'interpolation': 2, '{': 3, '}': 3, 'pattern_list': 1, 'comparison_operator': 1, '==': 1, 'except_clause': 1, 'except': 1, 'not_operator': 1, 'not': 1, 'dictionary': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9821734827776265,0.9813031301277154,"(tensor([0.9972]), tensor([0.9972]), tensor([0.9972]), tensor([0.9972]))"
"145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
149         """"""
150         Saves the cache in the file whose path was provided at construction
151         """"""
152         _log.info(f""Saving cache to {self.picklePath}"")
153         dumpPickle((self.version, self.cache), self.picklePath)
154 
155     def get(self, key) -> Optional[Any]:
","145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
149         """"""
150         Saves the cache in the file whose path was provided at construction
151         """"""
152         log.info(f""Saving cache to {self.picklePath}"")
153         dumpPickle((self.version, self.cache), self.picklePath)
154 
155     def get(self, key) -> Optional[Any]:
","Before: 152
After: 152",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,879,"{'module': 1, 'expression_statement': 5, 'assignment': 2, 'attribute': 8, 'identifier': 21, '.': 8, '=': 2, 'dictionary': 1, '{': 2, '}': 2, 'call': 3, 'argument_list': 3, '(': 5, ',': 3, ')': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'interpolation': 1, 'tuple': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9690278768043126,0.9669713801128973,"(tensor([0.9983]), tensor([0.9983]), tensor([0.9983]), tensor([0.9983]))"
"211         self.cacheOfSlice.append(item)
212         self.indexInSlice += 1
213 
214     def iterItems(self) -> Iterator[Any]:
215         """"""
216         Iterate over entries in the sliced cache
217         :return: iterator over all items in the cache
218         """"""
219         for filePath in self.slicedFiles:
220             _log.info(f""Loading sliced pickle list from {filePath}"")
221             cachedPickle = self._loadPickle(filePath)
222             for item in cachedPickle:
223                 yield item
224 
225     def clear(self):
","211         self.cacheOfSlice.append(item)
212         self.indexInSlice += 1
213 
214     def iterItems(self) -> Iterator[Any]:
215         """"""
216         Iterate over entries in the sliced cache
217         :return: iterator over all items in the cache
218         """"""
219         for filePath in self.slicedFiles:
220             log.info(f""Loading sliced pickle list from {filePath}"")
221             cachedPickle = self._loadPickle(filePath)
222             for item in cachedPickle:
223                 yield item
224 
225     def clear(self):
","Before: 220
After: 220",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,1308,"{'module': 1, 'expression_statement': 6, 'call': 3, 'attribute': 6, 'identifier': 23, '.': 6, 'argument_list': 3, '(': 4, ')': 4, 'augmented_assignment': 1, '+=': 1, 'integer': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ':': 3, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'for_statement': 2, 'for': 2, 'in': 2, 'interpolation': 1, '{': 1, '}': 1, 'assignment': 1, '=': 1, 'yield': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9730376770503931,0.971307825451108,"(tensor([0.9986]), tensor([0.9986]), tensor([0.9986]), tensor([0.9986]))"
"237         """"""
238         return len(self.slicedFiles) > 0
239 
240     def _setLastCacheState(self):
241         """"""
242         Sets the state such as to be able to add items to an existant cache
243         """"""
244         _log.info(""Resetting last state of cache..."")
245         self.sliceId = len(self.slicedFiles) - 1
246         self.cacheOfSlice = self._loadPickle(self._picklePath(self.sliceId))
247         self.indexInSlice = len(self.cacheOfSlice) - 1
248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
","237         """"""
238         return len(self.slicedFiles) > 0
239 
240     def _setLastCacheState(self):
241         """"""
242         Sets the state such as to be able to add items to an existant cache
243         """"""
244         log.info(""Resetting last state of cache..."")
245         self.sliceId = len(self.slicedFiles) - 1
246         self.cacheOfSlice = self._loadPickle(self._picklePath(self.sliceId))
247         self.indexInSlice = len(self.cacheOfSlice) - 1
248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
","Before: 244
After: 244",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,1436,"{'module': 1, 'expression_statement': 6, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 4, 'identifier': 38, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, ')': 6, 'assignment': 3, 'attribute': 11, '.': 11, '=': 3, 'binary_operator': 2, 'call': 5, 'argument_list': 5, '(': 5, '-': 2, 'integer': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>=': 1, ':': 1, 'block': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9751797570884221,0.9732255294269947,"(tensor([0.9985]), tensor([0.9985]), tensor([0.9985]), tensor([0.9985]))"
"248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
252         """"""
253         Dumps the current cache (if non-empty)
254         """"""
255         if len(self.cacheOfSlice) > 0:
256             picklePath = self._picklePath(str(self.sliceId))
257             _log.info(f""Saving sliced cache to {picklePath}"")
258             dumpPickle(self.cacheOfSlice, picklePath)
259             self.slicedFiles.append(picklePath)
260 
261             # Update slice number and reset indexing and cache
262             self._nextSlice()
263         else:
264             _log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
265 
266     def _nextSlice(self):
","248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
252         """"""
253         Dumps the current cache (if non-empty)
254         """"""
255         if len(self.cacheOfSlice) > 0:
256             picklePath = self._picklePath(str(self.sliceId))
257             log.info(f""Saving sliced cache to {picklePath}"")
258             dumpPickle(self.cacheOfSlice, picklePath)
259             self.slicedFiles.append(picklePath)
260 
261             # Update slice number and reset indexing and cache
262             self._nextSlice()
263         else:
264             log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
265 
266     def _nextSlice(self):
","Before: 257
After: 257",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,1592,"{'module': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'attribute': 12, 'identifier': 32, '.': 12, '>=': 1, ':': 4, 'block': 4, 'expression_statement': 8, 'call': 9, 'argument_list': 9, '(': 10, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, '>': 1, 'integer': 1, 'assignment': 1, '=': 1, 'interpolation': 1, '{': 1, '}': 1, ',': 1, 'comment': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9594997968107818,0.9564885766148181,"(tensor([0.9975]), tensor([0.9975]), tensor([0.9975]), tensor([0.9975]))"
"248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
252         """"""
253         Dumps the current cache (if non-empty)
254         """"""
255         if len(self.cacheOfSlice) > 0:
256             picklePath = self._picklePath(str(self.sliceId))
257             _log.info(f""Saving sliced cache to {picklePath}"")
258             dumpPickle(self.cacheOfSlice, picklePath)
259             self.slicedFiles.append(picklePath)
260 
261             # Update slice number and reset indexing and cache
262             self._nextSlice()
263         else:
264             _log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
265 
266     def _nextSlice(self):
","248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
252         """"""
253         Dumps the current cache (if non-empty)
254         """"""
255         if len(self.cacheOfSlice) > 0:
256             picklePath = self._picklePath(str(self.sliceId))
257             log.info(f""Saving sliced cache to {picklePath}"")
258             dumpPickle(self.cacheOfSlice, picklePath)
259             self.slicedFiles.append(picklePath)
260 
261             # Update slice number and reset indexing and cache
262             self._nextSlice()
263         else:
264             log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
265 
266     def _nextSlice(self):
","Before: 264
After: 264",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,1644,"{'module': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'attribute': 12, 'identifier': 32, '.': 12, '>=': 1, ':': 4, 'block': 4, 'expression_statement': 8, 'call': 9, 'argument_list': 9, '(': 10, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, '>': 1, 'integer': 1, 'assignment': 1, '=': 1, 'interpolation': 1, '{': 1, '}': 1, ',': 1, 'comment': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9594997968107818,0.9564885766148181,"(tensor([0.9975]), tensor([0.9975]), tensor([0.9975]), tensor([0.9975]))"
"282         listOfFileNames.sort(key=lambda f: int(re.sub('\D', '', f)))
283         return listOfFileNames
284 
285     def _loadPickle(self, picklePath: str) -> List[Any]:
286         """"""
287         Loads pickle if file path exists, and persisted version is correct.
288         :param picklePath: file path
289         :return: list with objects
290         """"""
291         cachedPickle = []
292         if os.path.exists(picklePath):
293             try:
294                 cachedPickle = loadPickle(picklePath)
295             except EOFError:
296                 _log.warning(f""The cache file in {picklePath} is corrupt"")
297         else:
298             raise Exception(f""The file {picklePath} does not exist!"")
299         return cachedPickle
300 
301     def _picklePath(self, sliceSuffix) -> str:
","282         listOfFileNames.sort(key=lambda f: int(re.sub('\D', '', f)))
283         return listOfFileNames
284 
285     def _loadPickle(self, picklePath: str) -> List[Any]:
286         """"""
287         Loads pickle if file path exists, and persisted version is correct.
288         :param picklePath: file path
289         :return: list with objects
290         """"""
291         cachedPickle = []
292         if os.path.exists(picklePath):
293             try:
294                 cachedPickle = loadPickle(picklePath)
295             except EOFError:
296                 log.warning(f""The cache file in {picklePath} is corrupt"")
297         else:
298             raise Exception(f""The file {picklePath} does not exist!"")
299         return cachedPickle
300 
301     def _picklePath(self, sliceSuffix) -> str:
","Before: 296
After: 296",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,1865,"{'module': 1, 'expression_statement': 5, 'call': 7, 'attribute': 5, 'identifier': 30, '.': 5, 'argument_list': 7, '(': 8, 'keyword_argument': 1, '=': 3, 'lambda': 2, 'lambda_parameters': 1, ':': 7, 'string': 5, 'string_start': 5, 'string_content': 6, 'string_end': 5, ',': 3, ')': 8, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 3, '->': 1, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'block': 5, 'assignment': 2, 'list': 1, 'if_statement': 1, 'if': 1, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9835236864262558,0.9824092102042711,"(tensor([0.9990]), tensor([0.9990]), tensor([0.9990]), tensor([0.9990]))"
"331         STRING = (""VARCHAR(%d)"", )
332         INTEGER = (""LONG"", )
333 
334     def __init__(self, path, tableName=""cache"", deferredCommitDelaySecs=1.0, keyType: KeyType = KeyType.STRING,
335             maxKeyLength=255):
336         """"""
337         :param path: the path to the file that is to hold the SQLite database
338         :param tableName: the name of the table to create in the database
339         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
340         :param keyType: the type to use for keys; for complex keys (i.e. tuples), use STRING (conversions to string are automatic)
341         :param maxKeyLength: the maximum key length for the case where the keyType can be parametrised (e.g. STRING)
342         """"""
343         self.path = path
344         self.conn = SqliteConnectionManager.openConnection(path)
345         self.tableName = tableName
346         self.maxKeyLength = 255
347         self.keyType = keyType
348         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs)
349         self._numEntriesToBeCommitted = 0
350         self._connMutex = threading.Lock()
351 
352         cursor = self.conn.cursor()
353         cursor.execute(f""SELECT name FROM sqlite_master WHERE type='table';"")
354         if tableName not in [r[0] for r in cursor.fetchall()]:
355             _log.info(f""Creating cache table '{self.tableName}' in {path}"")
356             keyDbType = keyType.value[0]
357             if ""%d"" in keyDbType:
358                 keyDbType = keyDbType % maxKeyLength
359             cursor.execute(f""CREATE TABLE {tableName} (cache_key {keyDbType} PRIMARY KEY, cache_value BLOB);"")
360         cursor.close()
361 
362     def _keyDbValue(self, key):
","331         STRING = (""VARCHAR(%d)"", )
332         INTEGER = (""LONG"", )
333 
334     def __init__(self, path, tableName=""cache"", deferredCommitDelaySecs=1.0, keyType: KeyType = KeyType.STRING,
335             maxKeyLength=255):
336         """"""
337         :param path: the path to the file that is to hold the SQLite database
338         :param tableName: the name of the table to create in the database
339         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
340         :param keyType: the type to use for keys; for complex keys (i.e. tuples), use STRING (conversions to string are automatic)
341         :param maxKeyLength: the maximum key length for the case where the keyType can be parametrised (e.g. STRING)
342         """"""
343         self.path = path
344         self.conn = SqliteConnectionManager.openConnection(path)
345         self.tableName = tableName
346         self.maxKeyLength = 255
347         self.keyType = keyType
348         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs)
349         self._numEntriesToBeCommitted = 0
350         self._connMutex = threading.Lock()
351 
352         cursor = self.conn.cursor()
353         cursor.execute(f""SELECT name FROM sqlite_master WHERE type='table';"")
354         if tableName not in [r[0] for r in cursor.fetchall()]:
355             log.info(f""Creating cache table '{self.tableName}' in {path}"")
356             keyDbType = keyType.value[0]
357             if ""%d"" in keyDbType:
358                 keyDbType = keyDbType % maxKeyLength
359             cursor.execute(f""CREATE TABLE {tableName} (cache_key {keyDbType} PRIMARY KEY, cache_value BLOB);"")
360         cursor.close()
361 
362     def _keyDbValue(self, key):
","Before: 355
After: 355",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,2369,"{'module': 1, 'expression_statement': 18, 'assignment': 13, 'identifier': 69, '=': 17, 'tuple': 2, '(': 12, 'string': 8, 'string_start': 8, 'string_content': 11, 'string_end': 8, ',': 8, ')': 12, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 3, 'float': 1, 'typed_default_parameter': 1, ':': 4, 'type': 1, 'attribute': 21, '.': 21, 'integer': 5, 'block': 3, 'call': 9, 'argument_list': 9, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'not in': 2, 'list_comprehension': 1, '[': 3, 'subscript': 2, ']': 3, 'for_in_clause': 1, 'for': 1, 'in': 2, 'interpolation': 4, '{': 4, '}': 4, 'binary_operator': 1, '%': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9922538753137174,0.9918112990506994,"(tensor([0.9972]), tensor([0.9972]), tensor([0.9972]), tensor([0.9972]))"
"370         else:
371             raise Exception(f""Unhandled key type {self.keyType}"")
372 
373     def _commit(self):
374         self._connMutex.acquire()
375         try:
376             _log.info(f""Committing {self._numEntriesToBeCommitted} cache entries to the SQLite database {self.path}"")
377             self.conn.commit()
378             self._numEntriesToBeCommitted = 0
379         finally:
380             self._connMutex.release()
381 
382     def set(self, key, value):
","370         else:
371             raise Exception(f""Unhandled key type {self.keyType}"")
372 
373     def _commit(self):
374         self._connMutex.acquire()
375         try:
376             log.info(f""Committing {self._numEntriesToBeCommitted} cache entries to the SQLite database {self.path}"")
377             self.conn.commit()
378             self._numEntriesToBeCommitted = 0
379         finally:
380             self._connMutex.release()
381 
382     def set(self, key, value):
","Before: 376
After: 376",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,2609,"{'module': 1, 'expression_statement': 6, 'assignment': 2, 'identifier': 24, ':': 4, 'ERROR': 1, 'type': 1, 'call': 5, 'argument_list': 5, '(': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'interpolation': 3, '{': 3, 'attribute': 11, '.': 11, '}': 3, 'string_end': 2, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'block': 3, 'try_statement': 1, 'try': 1, '=': 1, 'integer': 1, 'finally_clause': 1, 'finally': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9674122890261669,0.9646823188245206,"(tensor([0.9979]), tensor([0.9979]), tensor([0.9979]), tensor([0.9979]))"
"397 
398         self._updateHook.handleUpdate()
399 
400     def get(self, key):
401         self._connMutex.acquire()
402         try:
403             cursor = self.conn.cursor()
404             key = self._keyDbValue(key)
405             cursor.execute(f""SELECT cache_value FROM {self.tableName} WHERE cache_key=?"", (key, ))
406             row = cursor.fetchone()
407             cursor.close()
408             if row is None:
409                 return None
410             return pickle.loads(row[0])
411         finally:
412             self._connMutex.release()
413 
414     def __len__(self):
","403         except sqlite3.DatabaseError as e:
404             raise Exception(f""Error executing query for {self.path}: {e}"")
405 
406     def get(self, key):
407         self._connMutex.acquire()
408         try:
409             cursor = self.conn.cursor()
410             key = self._keyDbValue(key)
411             self._execute(cursor, f""SELECT cache_value FROM {self.tableName} WHERE cache_key=?"", (key, ))
412             row = cursor.fetchone()
413             cursor.close()
414             if row is None:
415                 return None
416             return pickle.loads(row[0])
417         finally:
418             self._connMutex.release()
419 
420     def __len__(self):
","Before: 405
After: 411",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,2951,"{'module': 1, 'expression_statement': 8, 'call': 9, 'attribute': 14, 'identifier': 34, '.': 14, 'argument_list': 9, '(': 11, ')': 11, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, ':': 4, 'block': 4, 'try_statement': 1, 'try': 1, 'assignment': 3, '=': 3, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 1, '{': 1, '}': 1, 'string_end': 1, 'tuple': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 2, 'return_statement': 2, 'return': 2, 'subscript': 1, '[': 1, 'integer': 1, ']': 1, 'finally_clause': 1, 'finally': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4891946007027354,0.4631443970422968,"(tensor([0.8937]), tensor([0.9311]), tensor([0.9120]), tensor([0.9272]))"
"502         pass
503 
504 
505 def cached(fn: Callable[[], T], picklePath) -> T:
506     if os.path.exists(picklePath):
507         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
508         return loadPickle(picklePath)
509     else:
510         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
511         result = fn()
512         _log.info(f""Saving cached result in {picklePath}"")
513         dumpPickle(result, picklePath)
514         return result
515 
516 
","508         pass
509 
510 
511 def cached(fn: Callable[[], T], picklePath) -> T:
512     if os.path.exists(picklePath):
513         log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
514         return loadPickle(picklePath)
515     else:
516         log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
517         result = fn()
518         log.info(f""Saving cached result in {picklePath}"")
519         dumpPickle(result, picklePath)
520         return result
521 
522 
","Before: 507
After: 513",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,3694,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7413720576000381,0.7262421839123931,"(tensor([0.9679]), tensor([0.9716]), tensor([0.9697]), tensor([0.9712]))"
"502         pass
503 
504 
505 def cached(fn: Callable[[], T], picklePath) -> T:
506     if os.path.exists(picklePath):
507         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
508         return loadPickle(picklePath)
509     else:
510         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
511         result = fn()
512         _log.info(f""Saving cached result in {picklePath}"")
513         dumpPickle(result, picklePath)
514         return result
515 
516 
","508         pass
509 
510 
511 def cached(fn: Callable[[], T], picklePath) -> T:
512     if os.path.exists(picklePath):
513         log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
514         return loadPickle(picklePath)
515     else:
516         log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
517         result = fn()
518         log.info(f""Saving cached result in {picklePath}"")
519         dumpPickle(result, picklePath)
520         return result
521 
522 
","Before: 510
After: 516",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,3732,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7413720576000381,0.7262421839123931,"(tensor([0.9679]), tensor([0.9716]), tensor([0.9697]), tensor([0.9712]))"
"502         pass
503 
504 
505 def cached(fn: Callable[[], T], picklePath) -> T:
506     if os.path.exists(picklePath):
507         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
508         return loadPickle(picklePath)
509     else:
510         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
511         result = fn()
512         _log.info(f""Saving cached result in {picklePath}"")
513         dumpPickle(result, picklePath)
514         return result
515 
516 
","508         pass
509 
510 
511 def cached(fn: Callable[[], T], picklePath) -> T:
512     if os.path.exists(picklePath):
513         log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
514         return loadPickle(picklePath)
515     else:
516         log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
517         result = fn()
518         log.info(f""Saving cached result in {picklePath}"")
519         dumpPickle(result, picklePath)
520         return result
521 
522 
","Before: 512
After: 518",replace _log with log in cache.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,3758,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7413720576000381,0.7262421839123931,"(tensor([0.9679]), tensor([0.9716]), tensor([0.9697]), tensor([0.9712]))"
"160     def isRegressionModel(self) -> bool:
161         pass
162 
163     def isFitted(self):
164         return self.getPredictedVariableNames() is not None
165 
166     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
","161     def isRegressionModel(self) -> bool:
162         pass
163 
164     def isFitted(self):
165         return self._isFitted
166 
167     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
","Before: 164
After: 165",fix vector_model.py for python3,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,de79645f656fefc324cf60d8ddf02bee69e7370a,ecaecacd3a6a335d4f73e45c8e03c2ddabdfe14e,0,1053,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 7, 'parameters': 2, '(': 3, ')': 3, '->': 1, 'type': 1, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'return_statement': 1, 'return': 1, 'comparison_operator': 1, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 1, 'is not': 2, 'none': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.614936376296908,0.596765478406972,"(tensor([0.9675]), tensor([0.9348]), tensor([0.9509]), tensor([0.9380]))"
"11 
12 
13 class EnsembleVectorModel(VectorModel, ABC):
14     def __init__(self, models: Sequence[VectorModel], numProcesses=1):
15         self.numProcesses = numProcesses
16         self.models = list(models)
17         super().__init__()
18 
19     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
","11 
12 
13 class EnsembleVectorModel(VectorModel, ABC):
14     def __init__(self, models: Sequence[VectorModel], numProcesses=1):
15         self.numProcesses = numProcesses
16         self.models = list(models)
17         super().__init__(checkInputColumns=False)
18 
19     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
","Before: 17
After: 17",add checkinputcolumns=false to ensemblevectormodel,Sync faz,https://github.com/opcode81/sensAI,src/sensai/ensemble/ensemble_base.py,8c052d9051042c793a21f804acf7267e100dcf48,9eed599104c4747fc7a2be7448fb88ab8fedfef3,0,158,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 18, 'argument_list': 4, '(': 5, ',': 3, ')': 5, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'default_parameter': 1, '=': 3, 'integer': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 3, '.': 3, 'call': 3}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 36, 'name': '__init__', 'long_name': '__init__( self , models : Sequence [ VectorModel ] , numProcesses = 1 )', 'start_line': 14, 'end_line': 17, 'full_parameters': ['self', ' models : Sequence [ VectorModel ]', ' numProcesses = 1'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/ensemble/ensemble_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , models : Sequence [ VectorModel ] , numProcesses = 1 )', 'start_line': 14, 'end_line': 17, 'full_parameters': ['self', ' models : Sequence [ VectorModel ]', ' numProcesses = 1'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/ensemble/ensemble_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9581282631934457,0.9532099949136581,"(tensor([0.9696]), tensor([0.9970]), tensor([0.9831]), tensor([0.9942]))"
"71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self):
75         self._featureGenerator: Optional[""FeatureGenerator""] = None
76         self._inputTransformerChain = DataFrameTransformerChain(())
77         self._outputTransformerChain = DataFrameTransformerChain(())
78         self._predictedVariableNames = None
79         self._modelInputVariableNames = None
80         self._modelOutputVariableNames = None
81         self._targetTransformer = None
82         self._name = None
83         self._isFitted = False
84 
85     @staticmethod
","71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self, checkInputColumns=True):
75         """"""
76 
77         :param checkInputColumns: Whether to check if the input column list (after feature generation) during inference coincides
78             with the input column list during fit. This should be disabled if feature generation is not performed by the model itself,
79             e.g. in ensemble models.
80         """"""
81         self._featureGenerator: Optional[""FeatureGenerator""] = None
82         self._inputTransformerChain = DataFrameTransformerChain(())
83         self._outputTransformerChain = DataFrameTransformerChain(())
84         self._predictedVariableNames = None
85         self._modelInputVariableNames = None
86         self._modelOutputVariableNames = None
87         self._targetTransformer = None
88         self._name = None
89         self._isFitted = False
90         self.checkInputColumns = checkInputColumns
91 
92     @staticmethod
","Before: 74
After: 74, 75, 76, 77, 78, 79, 80",add checkinputcolumns argument to vectormodel,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,8c052d9051042c793a21f804acf7267e100dcf48,9eed599104c4747fc7a2be7448fb88ab8fedfef3,0,545,"{'module': 1, 'expression_statement': 10, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'function_definition': 1, 'def': 1, 'identifier': 23, 'parameters': 1, '(': 5, ')': 5, ':': 2, 'block': 1, 'assignment': 9, 'attribute': 9, '.': 9, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '=': 9, 'none': 6, 'call': 2, 'argument_list': 2, 'tuple': 2, 'false': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3628079575846551,0.3378603084330782,"(tensor([0.8258]), tensor([0.9408]), tensor([0.8795]), tensor([0.9279]))"
"164     def isFitted(self):
165         return self._isFitted
166 
167     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
168         fit = y is not None
169         if self._featureGenerator is not None:
170             if fit:
171                 x = self._featureGenerator.fitGenerate(x, y, self)
172             else:
173                 x = self._featureGenerator.generate(x, self)
174         x = self._inputTransformerChain.apply(x, fit=fit)
175         if not fit:
176             if not self.isFitted():
177                 raise Exception(f""Model has not been fitted"")
178             if list(x.columns) != self._modelInputVariableNames:
179                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(x.columns)}"")
180         return x
181 
182     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","171     def isFitted(self):
172         return self._isFitted
173 
174     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
175         fit = y is not None
176         if self._featureGenerator is not None:
177             if fit:
178                 x = self._featureGenerator.fitGenerate(x, y, self)
179             else:
180                 x = self._featureGenerator.generate(x, self)
181         x = self._inputTransformerChain.apply(x, fit=fit)
182         if not fit:
183             if not self.isFitted():
184                 raise Exception(f""Model has not been fitted"")
185             if self.checkInputColumns and list(x.columns) != self._modelInputVariableNames:
186                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(x.columns)}"")
187         return x
188 
189     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 178
After: 185",add checkinputcolumns argument to vectormodel,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,8c052d9051042c793a21f804acf7267e100dcf48,9eed599104c4747fc7a2be7448fb88ab8fedfef3,0,1222,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 53, 'parameters': 2, '(': 10, ')': 10, ':': 9, 'block': 8, 'return_statement': 2, 'return': 2, 'attribute': 15, '.': 15, ',': 6, 'typed_parameter': 1, 'type': 2, 'default_parameter': 1, '=': 6, 'none': 3, '->': 1, 'expression_statement': 4, 'assignment': 4, 'comparison_operator': 3, 'is not': 4, 'if_statement': 5, 'if': 5, 'call': 8, 'argument_list': 8, 'else_clause': 1, 'else': 1, 'keyword_argument': 1, 'not_operator': 2, 'not': 2, 'raise_statement': 2, 'raise': 2, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, '!=': 1, 'interpolation': 2, '{': 2, '}': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7207096907716869,0.7106675298976003,"(tensor([0.9702]), tensor([0.9765]), tensor([0.9733]), tensor([0.9758]))"
"6 import pandas as pd
7 from sklearn import compose
8 
9 from ..vector_model import VectorRegressionModel, VectorClassificationModel
10 
11 _log = logging.getLogger(__name__)
12 
13 
14 def createSkLearnModel(modelConstructor, modelArgs, outputTransformer=None):
15     model = modelConstructor(**modelArgs)
","6 import pandas as pd
7 from sklearn import compose
8 
9 from ..vector_model import VectorRegressionModel, VectorClassificationModel
10 
11 log = logging.getLogger(__name__)
12 
13 
14 def createSkLearnModel(modelConstructor, modelArgs, outputTransformer=None):
15     model = modelConstructor(**modelArgs)
","Before: 11
After: 11",update sklearn_base.py to use log.info(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,68,"{'module': 1, 'import_statement': 1, 'import': 3, 'aliased_import': 1, 'dotted_name': 6, 'identifier': 15, 'as': 1, 'import_from_statement': 2, 'from': 2, 'relative_import': 1, 'import_prefix': 1, '.': 3, ',': 3, 'expression_statement': 1, 'assignment': 1, '=': 2, 'call': 1, 'attribute': 1, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, 'none': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9464594399631753,0.9420682842993964,"(tensor([0.9970]), tensor([0.9970]), tensor([0.9970]), tensor([0.9970]))"
"20 
21 class AbstractSkLearnVectorRegressionModel(VectorRegressionModel, ABC):
22     """"""
23     Base class for models built upon scikit-learn's model implementations
24     """"""
25     _log = _log.getChild(__qualname__)
26 
27     def __init__(self, modelConstructor, **modelArgs):
28         """"""
29         :param modelConstructor: the sklearn model constructor
","20 
21 class AbstractSkLearnVectorRegressionModel(VectorRegressionModel, ABC):
22     """"""
23     Base class for models built upon scikit-learn's model implementations
24     """"""
25     log = log.getChild(__qualname__)
26 
27     def __init__(self, modelConstructor, **modelArgs):
28         """"""
29         :param modelConstructor: the sklearn model constructor
","Before: 25
After: 25",update sklearn_base.py to use log.info(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,158,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 11, 'argument_list': 2, '(': 3, ',': 3, ')': 3, ':': 2, 'block': 2, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, '.': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'dictionary_splat_pattern': 1, '**': 1, 'ERROR': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9301041299140577,0.92259416760473,"(tensor([0.9960]), tensor([0.9960]), tensor([0.9960]), tensor([0.9960]))"
"108             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
109         return f""{self.__class__.__name__}[{modelStr}]""
110 
111     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
112         for predictedVarName in outputs.columns:
113             _log.info(f""Fitting model for output variable '{predictedVarName}'"")
114             model = createSkLearnModel(self.modelConstructor,
115                     self.modelArgs,
116                     outputTransformer=copy.deepcopy(self.sklearnOutputTransformer))
117             model.fit(inputs, outputs[predictedVarName])
118             self.models[predictedVarName] = model
119 
120     def _predictSkLearn(self, inputs: pd.DataFrame) -> pd.DataFrame:
","108             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
109         return f""{self.__class__.__name__}[{modelStr}]""
110 
111     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
112         for predictedVarName in outputs.columns:
113             log.info(f""Fitting model for output variable '{predictedVarName}'"")
114             model = createSkLearnModel(self.modelConstructor,
115                     self.modelArgs,
116                     outputTransformer=copy.deepcopy(self.sklearnOutputTransformer))
117             model.fit(inputs, outputs[predictedVarName])
118             self.models[predictedVarName] = model
119 
120     def _predictSkLearn(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 113
After: 113",update sklearn_base.py to use log.info(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,853,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 44, '=': 4, 'string': 3, 'string_start': 3, 'interpolation': 5, '{': 5, 'attribute': 15, '.': 15, '}': 5, 'string_end': 3, 'return_statement': 1, 'return': 1, 'string_content': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 5, ',': 5, 'typed_parameter': 2, ':': 4, 'type': 2, ')': 5, 'block': 2, 'for_statement': 1, 'for': 1, 'in': 1, 'call': 4, 'argument_list': 4, 'keyword_argument': 1, 'subscript': 2, '[': 2, ']': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9772160520895917,0.9753707039079267,"(tensor([0.9985]), tensor([0.9985]), tensor([0.9985]), tensor([0.9985]))"
"139             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
140         return f""{self.__class__.__name__}[{modelStr}]""
141 
142     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
143         if len(outputs.columns) > 1:
144             _log.info(f""Fitting a single multi-dimensional model for all {len(outputs.columns)} output dimensions"")
145         self.model = createSkLearnModel(self.modelConstructor, self.modelArgs, outputTransformer=self.sklearnOutputTransformer)
146         outputValues = outputs.values
147         if outputValues.shape[1] == 1:  # for 1D output, shape must be (numSamples,) rather than (numSamples, 1)
148             outputValues = np.ravel(outputValues)
149         self.model.fit(inputs, outputValues)
150 
151     def _predictSkLearn(self, inputs: pd.DataFrame) -> pd.DataFrame:
","139             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
140         return f""{self.__class__.__name__}[{modelStr}]""
141 
142     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
143         if len(outputs.columns) > 1:
144             log.info(f""Fitting a single multi-dimensional model for all {len(outputs.columns)} output dimensions"")
145         self.model = createSkLearnModel(self.modelConstructor, self.modelArgs, outputTransformer=self.sklearnOutputTransformer)
146         outputValues = outputs.values
147         if outputValues.shape[1] == 1:  # for 1D output, shape must be (numSamples,) rather than (numSamples, 1)
148             outputValues = np.ravel(outputValues)
149         self.model.fit(inputs, outputValues)
150 
151     def _predictSkLearn(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 144
After: 144",update sklearn_base.py to use log.info(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1200,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 50, '=': 5, 'string': 3, 'string_start': 3, 'interpolation': 5, '{': 5, 'attribute': 19, '.': 19, '}': 5, 'string_end': 3, 'return_statement': 1, 'return': 1, 'string_content': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 7, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 2, ')': 7, 'block': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'call': 6, 'argument_list': 6, '>': 1, 'integer': 3, 'keyword_argument': 1, 'subscript': 1, '[': 1, ']': 1, '==': 1, 'comment': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9823610456991699,0.9809842223502003,"(tensor([0.9988]), tensor([0.9988]), tensor([0.9988]), tensor([0.9988]))"
"189             strModel = str(self.model)
190         return f""{self.__class__.__name__}[{strModel}]""
191 
192     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
193         inputValues = self._transformInput(inputs, fit=True)
194         self.model = createSkLearnModel(self.modelConstructor, self.modelArgs, self.sklearnOutputTransformer)
195         _log.info(f""Fitting sklearn classifier of type {self.model.__class__.__name__}"")
196         self.model.fit(inputValues, np.ravel(outputs.values))
197 
198     def _transformInput(self, inputs: pd.DataFrame, fit=False) -> np.ndarray:
","198         """"""
199         pass
200 
201     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
202         inputValues = self._transformInput(inputs, fit=True)
203         self._updateModelArgs(inputs, outputs)
204         self.model = createSkLearnModel(self.modelConstructor, self.modelArgs, self.sklearnOutputTransformer)
205         log.info(f""Fitting sklearn classifier of type {self.model.__class__.__name__}"")
206         self.model.fit(inputValues, np.ravel(outputs.values))
207 
208     def _transformInput(self, inputs: pd.DataFrame, fit=False) -> np.ndarray:
","Before: 195
After: 205",update sklearn_base.py to use log.info(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1664,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 44, '=': 4, 'call': 6, 'argument_list': 6, '(': 7, 'attribute': 18, '.': 18, ')': 7, 'return_statement': 1, 'return': 1, 'string': 2, 'string_start': 2, 'interpolation': 3, '{': 3, '}': 3, 'string_content': 3, 'string_end': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'typed_parameter': 2, ':': 3, 'type': 2, 'block': 1, 'keyword_argument': 1, 'true': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6277806310525128,0.5982832579541951,"(tensor([0.9600]), tensor([0.9497]), tensor([0.9548]), tensor([0.9507]))"
"7 import sklearn.tree
8 
9 from .sklearn_base import AbstractSkLearnVectorClassificationModel
10 
11 
12 _log = logging.getLogger(__name__)
13 
14 
15 class SkLearnDecisionTreeVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
16     def __init__(self, min_samples_leaf=8, random_state=42, **modelArgs):
","10 import pandas as pd
11 
12 from .sklearn_base import AbstractSkLearnVectorClassificationModel
13 
14 
15 log = logging.getLogger(__name__)
16 
17 
18 class SkLearnDecisionTreeVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
19     def __init__(self, min_samples_leaf=8, random_state=42, **modelArgs):
","Before: 12
After: 15",update sklearn_classification.py to use pandas,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_classification.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,57,"{'module': 1, 'import_statement': 1, 'import': 2, 'dotted_name': 3, 'identifier': 10, '.': 3, 'import_from_statement': 1, 'from': 1, 'relative_import': 1, 'import_prefix': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 16, 'end_line': 18, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5872449213968888,0.5611643985714511,"(tensor([0.9533]), tensor([0.9650]), tensor([0.9591]), tensor([0.9638]))"
"35 
36 
37 class SkLearnLightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
38     def __init__(self, random_state=42, num_leaves=31, **modelArgs):
39         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
40 
41 
","36     def __init__(self, **modelArgs):
37         super().__init__(sklearn.naive_bayes.MultinomialNB, **modelArgs)
38 
39 
40 class SkLearnLightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
41     log = log.getChild(__qualname__)
42 
43     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
44         """"""
45         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical
","Before: 38
After: 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 59, 60, 61, 62, 63",update sklearn_classification.py to use pandas,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_classification.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,313,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 17, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'default_parameter': 2, '=': 4, 'integer': 2, 'dictionary_splat_pattern': 1, '**': 2, 'expression_statement': 1, 'call': 2, 'attribute': 3, '.': 3, 'keyword_argument': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 16, 'end_line': 18, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2909778918512841,0.25185529870010553,"(tensor([0.7811]), tensor([0.8824]), tensor([0.8286]), tensor([0.8711]))"
"10 import lightgbm
11 
12 from .sklearn_base import AbstractSkLearnMultipleOneDimVectorRegressionModel, AbstractSkLearnMultiDimVectorRegressionModel
13 
14 
15 _log = logging.getLogger(__name__)
16 
17 
18 class SkLearnRandomForestVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
19     def __init__(self, n_estimators=100, min_samples_leaf=10, random_state=42, **modelArgs):
","10 import lightgbm
11 
12 from .sklearn_base import AbstractSkLearnMultipleOneDimVectorRegressionModel, AbstractSkLearnMultiDimVectorRegressionModel
13 
14 
15 log = logging.getLogger(__name__)
16 
17 
18 class SkLearnRandomForestVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
19     def __init__(self, n_estimators=100, min_samples_leaf=10, random_state=42, **modelArgs):
","Before: 15
After: 15",fix typos in sklearn_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_regression.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,80,"{'module': 1, 'import_statement': 1, 'import': 2, 'dotted_name': 4, 'identifier': 10, 'import_from_statement': 1, 'from': 1, 'relative_import': 1, 'import_prefix': 1, '.': 2, ',': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9427781070492712,0.9378394374337681,"(tensor([0.9978]), tensor([0.9978]), tensor([0.9978]), tensor([0.9978]))"
"53     def __init__(self, **modelArgs):
54         super().__init__(sklearn.svm.LinearSVR, **modelArgs)
55 
56 
57 class SkLearnLightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
58     _log = _log.getChild(__qualname__)
59 
60     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
61         """"""
62         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
","53     def __init__(self, **modelArgs):
54         super().__init__(sklearn.svm.LinearSVR, **modelArgs)
55 
56 
57 class SkLearnLightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
58     log = log.getChild(__qualname__)
59 
60     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
61         """"""
62         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
","Before: 58
After: 58",fix typos in sklearn_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_regression.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,427,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 23, 'parameters': 2, '(': 6, ',': 6, 'dictionary_splat_pattern': 2, '**': 3, ')': 6, ':': 4, 'block': 3, 'expression_statement': 2, 'call': 3, 'attribute': 4, 'argument_list': 4, '.': 4, 'dictionary_splat': 1, 'class_definition': 1, 'class': 1, 'assignment': 1, '=': 4, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'ERROR': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9486838981800674,0.9430611013815657,"(tensor([0.9969]), tensor([0.9969]), tensor([0.9969]), tensor([0.9969]))"
"70         self.categoricalFeatureNames = categoricalFeatureNames
71         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
72 
73     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
74         if self.categoricalFeatureNames is not None:
75             cols = list(inputs.columns)
76             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
77             args = {""cat_column"": colIndices}
78             self._log.info(f""Updating model parameters with {args}"")
79             self.modelArgs.update(args)
80 
81 
","70         self.categoricalFeatureNames = categoricalFeatureNames
71         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
72 
73     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
74         if self.categoricalFeatureNames is not None:
75             cols = list(inputs.columns)
76             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
77             args = {""cat_column"": colIndices}
78             self.log.info(f""Updating model parameters with {args}"")
79             self.modelArgs.update(args)
80 
81 
","Before: 78
After: 78",fix typos in sklearn_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_regression.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,621,"{'module': 1, 'expression_statement': 7, 'assignment': 4, 'attribute': 14, 'identifier': 44, '.': 14, '=': 6, 'call': 6, 'argument_list': 6, '(': 7, ')': 7, ',': 5, 'keyword_argument': 2, 'dictionary_splat': 1, '**': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 2, 'pair': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '}': 2, 'interpolation': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9727436279095398,0.9706246779088405,"(tensor([0.9997]), tensor([0.9997]), tensor([0.9997]), tensor([0.9997]))"
"6 import pickle
7 import re
8 import threading
9 import time
10 from abc import abstractmethod, ABC
11 from functools import wraps, partial
12 from typing import Any, Callable, Iterator, List, Optional, TypeVar
13 
14 import sqlite3
15 
","10 from abc import abstractmethod, ABC
11 from typing import Any, Callable, Iterator, List, Optional, TypeVar
12 
13 import sqlite3
14 
15 from .pickle import PickleFailureDebugger
16 
17 log = logging.getLogger(__name__)
18 
19 T = TypeVar(""T"")
","Before: 11, 16
After: 15, 16, 17",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,58,"{'module': 1, 'import_statement': 5, 'import': 8, 'dotted_name': 18, 'identifier': 18, 'import_from_statement': 3, 'from': 3, ',': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4431597610709116,0.3858540006267933,"(tensor([0.8648]), tensor([0.8767]), tensor([0.8707]), tensor([0.8755]))"
"66         return pickle.load(f)
67 
68 
69 def dumpPickle(obj, picklePath):
70     dirName = os.path.dirname(picklePath)
71     if dirName != """":
72         os.makedirs(dirName, exist_ok=True)
73     with open(picklePath, ""wb"") as f:
74         pickle.dump(obj, f)
75 
76 
","67         return pickle.load(f)
68 
69 
70 def dumpPickle(obj, picklePath):
71     dirName = os.path.dirname(picklePath)
72     if dirName != """":
73         os.makedirs(dirName, exist_ok=True)
74     with open(picklePath, ""wb"") as f:
75         try:
76             pickle.dump(obj, f)
77         except AttributeError as e:
78             failingPaths = PickleFailureDebugger.debugFailure(obj)
79             raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
80 
81 
","Before: 74
After: 75, 76, 77, 78, 79",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,342,"{'module': 1, 'return_statement': 1, 'return': 1, 'call': 5, 'attribute': 5, 'identifier': 23, '.': 5, 'argument_list': 5, '(': 6, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, ':': 3, 'block': 3, 'expression_statement': 3, 'assignment': 1, '=': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'string': 2, 'string_start': 2, 'string_end': 2, 'keyword_argument': 1, 'true': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 1, 'string_content': 1, 'as': 1, 'as_pattern_target': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4014545771226857,0.3827831433250662,"(tensor([0.8481]), tensor([0.9700]), tensor([0.9050]), tensor([0.9563]))"
"117     """"""
118     Represents a key-value cache as a dictionary which is persisted in a file using pickle
119     """"""
120     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
121         """"""
122         :param picklePath: the path of the file where the cache values are to be persisted
123         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
124             it is discarded
125         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
126             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
127             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
128         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
129             before actually storing the cache after a cache update
130         """"""
131         self.deferredSaveDelaySecs = deferredSaveDelaySecs
132         self.picklePath = picklePath
133         self.version = version
134         self.saveOnUpdate = saveOnUpdate
135         cacheFound = False
136         if os.path.exists(picklePath):
137             try:
138                 _log.info(f""Loading cache from {picklePath}"")
139                 persistedVersion, self.cache = loadPickle(picklePath)
140                 if persistedVersion == version:
141                     cacheFound = True
142             except EOFError:
143                 _log.warning(f""The cache file in {picklePath} is corrupt"")
144         if not cacheFound:
145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
","122     """"""
123     Represents a key-value cache as a dictionary which is persisted in a file using pickle
124     """"""
125     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
126         """"""
127         :param picklePath: the path of the file where the cache values are to be persisted
128         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
129             it is discarded
130         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
131             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
132             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
133         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
134             before actually storing the cache after a cache update
135         """"""
136         self.deferredSaveDelaySecs = deferredSaveDelaySecs
137         self.picklePath = picklePath
138         self.version = version
139         self.saveOnUpdate = saveOnUpdate
140         cacheFound = False
141         if os.path.exists(picklePath):
142             try:
143                 log.info(f""Loading cache from {picklePath}"")
144                 persistedVersion, self.cache = loadPickle(picklePath)
145                 if persistedVersion == version:
146                     cacheFound = True
147             except EOFError:
148                 log.warning(f""The cache file in {picklePath} is corrupt"")
149         if not cacheFound:
150             self.cache = {}
151         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
152 
153     def save(self):
","Before: 138
After: 143",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,760,"{'module': 1, 'expression_statement': 13, 'string': 4, 'string_start': 4, 'string_content': 5, 'string_end': 4, 'function_definition': 1, 'def': 1, 'identifier': 47, 'parameters': 1, '(': 6, ',': 6, 'default_parameter': 3, '=': 12, 'integer': 1, 'true': 2, 'float': 1, ')': 6, ':': 6, 'block': 6, 'assignment': 9, 'attribute': 12, '.': 12, 'false': 1, 'if_statement': 3, 'if': 3, 'call': 5, 'argument_list': 5, 'try_statement': 1, 'try': 1, 'interpolation': 2, '{': 3, '}': 3, 'pattern_list': 1, 'comparison_operator': 1, '==': 1, 'except_clause': 1, 'except': 1, 'not_operator': 1, 'not': 1, 'dictionary': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7327245271443121,0.7304028575690956,"(tensor([0.9854]), tensor([0.9851]), tensor([0.9853]), tensor([0.9852]))"
"117     """"""
118     Represents a key-value cache as a dictionary which is persisted in a file using pickle
119     """"""
120     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
121         """"""
122         :param picklePath: the path of the file where the cache values are to be persisted
123         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
124             it is discarded
125         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
126             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
127             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
128         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
129             before actually storing the cache after a cache update
130         """"""
131         self.deferredSaveDelaySecs = deferredSaveDelaySecs
132         self.picklePath = picklePath
133         self.version = version
134         self.saveOnUpdate = saveOnUpdate
135         cacheFound = False
136         if os.path.exists(picklePath):
137             try:
138                 _log.info(f""Loading cache from {picklePath}"")
139                 persistedVersion, self.cache = loadPickle(picklePath)
140                 if persistedVersion == version:
141                     cacheFound = True
142             except EOFError:
143                 _log.warning(f""The cache file in {picklePath} is corrupt"")
144         if not cacheFound:
145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
","122     """"""
123     Represents a key-value cache as a dictionary which is persisted in a file using pickle
124     """"""
125     def __init__(self, picklePath, version=1, saveOnUpdate=True, deferredSaveDelaySecs=1.0):
126         """"""
127         :param picklePath: the path of the file where the cache values are to be persisted
128         :param version: the version of cache entries. If a persisted cache with a non-matching version is found, it
129             it is discarded
130         :param saveOnUpdate: whether to persist the cache after an update; the cache is saved in a deferred
131             manner and will be saved after deferredSaveDelaySecs if no new updates have arrived in the meantime,
132             i.e. it will ultimately be saved deferredSaveDelaySecs after the latest update
133         :param deferredSaveDelaySecs: the number of seconds to wait for additional data to be added to the cache
134             before actually storing the cache after a cache update
135         """"""
136         self.deferredSaveDelaySecs = deferredSaveDelaySecs
137         self.picklePath = picklePath
138         self.version = version
139         self.saveOnUpdate = saveOnUpdate
140         cacheFound = False
141         if os.path.exists(picklePath):
142             try:
143                 log.info(f""Loading cache from {picklePath}"")
144                 persistedVersion, self.cache = loadPickle(picklePath)
145                 if persistedVersion == version:
146                     cacheFound = True
147             except EOFError:
148                 log.warning(f""The cache file in {picklePath} is corrupt"")
149         if not cacheFound:
150             self.cache = {}
151         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
152 
153     def save(self):
","Before: 143
After: 148",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,810,"{'module': 1, 'expression_statement': 13, 'string': 4, 'string_start': 4, 'string_content': 5, 'string_end': 4, 'function_definition': 1, 'def': 1, 'identifier': 47, 'parameters': 1, '(': 6, ',': 6, 'default_parameter': 3, '=': 12, 'integer': 1, 'true': 2, 'float': 1, ')': 6, ':': 6, 'block': 6, 'assignment': 9, 'attribute': 12, '.': 12, 'false': 1, 'if_statement': 3, 'if': 3, 'call': 5, 'argument_list': 5, 'try_statement': 1, 'try': 1, 'interpolation': 2, '{': 3, '}': 3, 'pattern_list': 1, 'comparison_operator': 1, '==': 1, 'except_clause': 1, 'except': 1, 'not_operator': 1, 'not': 1, 'dictionary': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7327245271443121,0.7304028575690956,"(tensor([0.9854]), tensor([0.9851]), tensor([0.9853]), tensor([0.9852]))"
"145             self.cache = {}
146         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
147 
148     def save(self):
149         """"""
150         Saves the cache in the file whose path was provided at construction
151         """"""
152         _log.info(f""Saving cache to {self.picklePath}"")
153         dumpPickle((self.version, self.cache), self.picklePath)
154 
155     def get(self, key) -> Optional[Any]:
","150             self.cache = {}
151         self._updateHook = DelayedUpdateHook(self.save, deferredSaveDelaySecs)
152 
153     def save(self):
154         """"""
155         Saves the cache in the file whose path was provided at construction
156         """"""
157         log.info(f""Saving cache to {self.picklePath}"")
158         dumpPickle((self.version, self.cache), self.picklePath)
159 
160     def get(self, key) -> Optional[Any]:
","Before: 152
After: 157",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,879,"{'module': 1, 'expression_statement': 5, 'assignment': 2, 'attribute': 8, 'identifier': 21, '.': 8, '=': 2, 'dictionary': 1, '{': 2, '}': 2, 'call': 3, 'argument_list': 3, '(': 5, ',': 3, ')': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'interpolation': 1, 'tuple': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7088200644144155,0.6990382469154127,"(tensor([0.9807]), tensor([0.9796]), tensor([0.9801]), tensor([0.9797]))"
"211         self.cacheOfSlice.append(item)
212         self.indexInSlice += 1
213 
214     def iterItems(self) -> Iterator[Any]:
215         """"""
216         Iterate over entries in the sliced cache
217         :return: iterator over all items in the cache
218         """"""
219         for filePath in self.slicedFiles:
220             _log.info(f""Loading sliced pickle list from {filePath}"")
221             cachedPickle = self._loadPickle(filePath)
222             for item in cachedPickle:
223                 yield item
224 
225     def clear(self):
","216         self.cacheOfSlice.append(item)
217         self.indexInSlice += 1
218 
219     def iterItems(self) -> Iterator[Any]:
220         """"""
221         Iterate over entries in the sliced cache
222         :return: iterator over all items in the cache
223         """"""
224         for filePath in self.slicedFiles:
225             log.info(f""Loading sliced pickle list from {filePath}"")
226             cachedPickle = self._loadPickle(filePath)
227             for item in cachedPickle:
228                 yield item
229 
230     def clear(self):
","Before: 220
After: 225",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1308,"{'module': 1, 'expression_statement': 6, 'call': 3, 'attribute': 6, 'identifier': 23, '.': 6, 'argument_list': 3, '(': 4, ')': 4, 'augmented_assignment': 1, '+=': 1, 'integer': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ':': 3, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'for_statement': 2, 'for': 2, 'in': 2, 'interpolation': 1, '{': 1, '}': 1, 'assignment': 1, '=': 1, 'yield': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6417273886949154,0.6343921567568278,"(tensor([0.9782]), tensor([0.9779]), tensor([0.9781]), tensor([0.9780]))"
"237         """"""
238         return len(self.slicedFiles) > 0
239 
240     def _setLastCacheState(self):
241         """"""
242         Sets the state such as to be able to add items to an existant cache
243         """"""
244         _log.info(""Resetting last state of cache..."")
245         self.sliceId = len(self.slicedFiles) - 1
246         self.cacheOfSlice = self._loadPickle(self._picklePath(self.sliceId))
247         self.indexInSlice = len(self.cacheOfSlice) - 1
248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
","242         """"""
243         return len(self.slicedFiles) > 0
244 
245     def _setLastCacheState(self):
246         """"""
247         Sets the state such as to be able to add items to an existant cache
248         """"""
249         log.info(""Resetting last state of cache..."")
250         self.sliceId = len(self.slicedFiles) - 1
251         self.cacheOfSlice = self._loadPickle(self._picklePath(self.sliceId))
252         self.indexInSlice = len(self.cacheOfSlice) - 1
253         if self.indexInSlice >= self.numEntriesPerSlice:
254             self._nextSlice()
255 
256     def _dump(self):
","Before: 244
After: 249",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1436,"{'module': 1, 'expression_statement': 6, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 4, 'identifier': 38, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, ')': 6, 'assignment': 3, 'attribute': 11, '.': 11, '=': 3, 'binary_operator': 2, 'call': 5, 'argument_list': 5, '(': 5, '-': 2, 'integer': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>=': 1, ':': 1, 'block': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6693624181940132,0.6583625257479317,"(tensor([0.9806]), tensor([0.9812]), tensor([0.9809]), tensor([0.9811]))"
"248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
252         """"""
253         Dumps the current cache (if non-empty)
254         """"""
255         if len(self.cacheOfSlice) > 0:
256             picklePath = self._picklePath(str(self.sliceId))
257             _log.info(f""Saving sliced cache to {picklePath}"")
258             dumpPickle(self.cacheOfSlice, picklePath)
259             self.slicedFiles.append(picklePath)
260 
261             # Update slice number and reset indexing and cache
262             self._nextSlice()
263         else:
264             _log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
265 
266     def _nextSlice(self):
","253         if self.indexInSlice >= self.numEntriesPerSlice:
254             self._nextSlice()
255 
256     def _dump(self):
257         """"""
258         Dumps the current cache (if non-empty)
259         """"""
260         if len(self.cacheOfSlice) > 0:
261             picklePath = self._picklePath(str(self.sliceId))
262             log.info(f""Saving sliced cache to {picklePath}"")
263             dumpPickle(self.cacheOfSlice, picklePath)
264             self.slicedFiles.append(picklePath)
265 
266             # Update slice number and reset indexing and cache
267             self._nextSlice()
268         else:
269             log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
270 
271     def _nextSlice(self):
","Before: 257
After: 262",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1592,"{'module': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'attribute': 12, 'identifier': 32, '.': 12, '>=': 1, ':': 4, 'block': 4, 'expression_statement': 8, 'call': 9, 'argument_list': 9, '(': 10, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, '>': 1, 'integer': 1, 'assignment': 1, '=': 1, 'interpolation': 1, '{': 1, '}': 1, ',': 1, 'comment': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6668383483049439,0.6591084346615157,"(tensor([0.9789]), tensor([0.9791]), tensor([0.9790]), tensor([0.9791]))"
"248         if self.indexInSlice >= self.numEntriesPerSlice:
249             self._nextSlice()
250 
251     def _dump(self):
252         """"""
253         Dumps the current cache (if non-empty)
254         """"""
255         if len(self.cacheOfSlice) > 0:
256             picklePath = self._picklePath(str(self.sliceId))
257             _log.info(f""Saving sliced cache to {picklePath}"")
258             dumpPickle(self.cacheOfSlice, picklePath)
259             self.slicedFiles.append(picklePath)
260 
261             # Update slice number and reset indexing and cache
262             self._nextSlice()
263         else:
264             _log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
265 
266     def _nextSlice(self):
","253         if self.indexInSlice >= self.numEntriesPerSlice:
254             self._nextSlice()
255 
256     def _dump(self):
257         """"""
258         Dumps the current cache (if non-empty)
259         """"""
260         if len(self.cacheOfSlice) > 0:
261             picklePath = self._picklePath(str(self.sliceId))
262             log.info(f""Saving sliced cache to {picklePath}"")
263             dumpPickle(self.cacheOfSlice, picklePath)
264             self.slicedFiles.append(picklePath)
265 
266             # Update slice number and reset indexing and cache
267             self._nextSlice()
268         else:
269             log.warning(""Unexpected behavior: Dump was called when cache of slice is 0!"")
270 
271     def _nextSlice(self):
","Before: 264
After: 269",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1644,"{'module': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'attribute': 12, 'identifier': 32, '.': 12, '>=': 1, ':': 4, 'block': 4, 'expression_statement': 8, 'call': 9, 'argument_list': 9, '(': 10, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, '>': 1, 'integer': 1, 'assignment': 1, '=': 1, 'interpolation': 1, '{': 1, '}': 1, ',': 1, 'comment': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6668383483049439,0.6591084346615157,"(tensor([0.9789]), tensor([0.9791]), tensor([0.9790]), tensor([0.9791]))"
"282         listOfFileNames.sort(key=lambda f: int(re.sub('\D', '', f)))
283         return listOfFileNames
284 
285     def _loadPickle(self, picklePath: str) -> List[Any]:
286         """"""
287         Loads pickle if file path exists, and persisted version is correct.
288         :param picklePath: file path
289         :return: list with objects
290         """"""
291         cachedPickle = []
292         if os.path.exists(picklePath):
293             try:
294                 cachedPickle = loadPickle(picklePath)
295             except EOFError:
296                 _log.warning(f""The cache file in {picklePath} is corrupt"")
297         else:
298             raise Exception(f""The file {picklePath} does not exist!"")
299         return cachedPickle
300 
301     def _picklePath(self, sliceSuffix) -> str:
","287         listOfFileNames.sort(key=lambda f: int(re.sub('\D', '', f)))
288         return listOfFileNames
289 
290     def _loadPickle(self, picklePath: str) -> List[Any]:
291         """"""
292         Loads pickle if file path exists, and persisted version is correct.
293         :param picklePath: file path
294         :return: list with objects
295         """"""
296         cachedPickle = []
297         if os.path.exists(picklePath):
298             try:
299                 cachedPickle = loadPickle(picklePath)
300             except EOFError:
301                 log.warning(f""The cache file in {picklePath} is corrupt"")
302         else:
303             raise Exception(f""The file {picklePath} does not exist!"")
304         return cachedPickle
305 
306     def _picklePath(self, sliceSuffix) -> str:
","Before: 296
After: 301",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1865,"{'module': 1, 'expression_statement': 5, 'call': 7, 'attribute': 5, 'identifier': 30, '.': 5, 'argument_list': 7, '(': 8, 'keyword_argument': 1, '=': 3, 'lambda': 2, 'lambda_parameters': 1, ':': 7, 'string': 5, 'string_start': 5, 'string_content': 6, 'string_end': 5, ',': 3, ')': 8, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 3, '->': 1, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'block': 5, 'assignment': 2, 'list': 1, 'if_statement': 1, 'if': 1, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7110239195832825,0.7059815408620511,"(tensor([0.9797]), tensor([0.9802]), tensor([0.9800]), tensor([0.9801]))"
"331         STRING = (""VARCHAR(%d)"", )
332         INTEGER = (""LONG"", )
333 
334     def __init__(self, path, tableName=""cache"", deferredCommitDelaySecs=1.0, keyType: KeyType = KeyType.STRING,
335             maxKeyLength=255):
336         """"""
337         :param path: the path to the file that is to hold the SQLite database
338         :param tableName: the name of the table to create in the database
339         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
340         :param keyType: the type to use for keys; for complex keys (i.e. tuples), use STRING (conversions to string are automatic)
341         :param maxKeyLength: the maximum key length for the case where the keyType can be parametrised (e.g. STRING)
342         """"""
343         self.path = path
344         self.conn = SqliteConnectionManager.openConnection(path)
345         self.tableName = tableName
346         self.maxKeyLength = 255
347         self.keyType = keyType
348         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs)
349         self._numEntriesToBeCommitted = 0
350         self._connMutex = threading.Lock()
351 
352         cursor = self.conn.cursor()
353         cursor.execute(f""SELECT name FROM sqlite_master WHERE type='table';"")
354         if tableName not in [r[0] for r in cursor.fetchall()]:
355             _log.info(f""Creating cache table '{self.tableName}' in {path}"")
356             keyDbType = keyType.value[0]
357             if ""%d"" in keyDbType:
358                 keyDbType = keyDbType % maxKeyLength
359             cursor.execute(f""CREATE TABLE {tableName} (cache_key {keyDbType} PRIMARY KEY, cache_value BLOB);"")
360         cursor.close()
361 
362     def _keyDbValue(self, key):
","336         STRING = (""VARCHAR(%d)"", )
337         INTEGER = (""LONG"", )
338 
339     def __init__(self, path, tableName=""cache"", deferredCommitDelaySecs=1.0, keyType: KeyType = KeyType.STRING,
340             maxKeyLength=255):
341         """"""
342         :param path: the path to the file that is to hold the SQLite database
343         :param tableName: the name of the table to create in the database
344         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
345         :param keyType: the type to use for keys; for complex keys (i.e. tuples), use STRING (conversions to string are automatic)
346         :param maxKeyLength: the maximum key length for the case where the keyType can be parametrised (e.g. STRING)
347         """"""
348         self.path = path
349         self.conn = SqliteConnectionManager.openConnection(path)
350         self.tableName = tableName
351         self.maxKeyLength = 255
352         self.keyType = keyType
353         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs)
354         self._numEntriesToBeCommitted = 0
355         self._connMutex = threading.Lock()
356 
357         cursor = self.conn.cursor()
358         cursor.execute(f""SELECT name FROM sqlite_master WHERE type='table';"")
359         if tableName not in [r[0] for r in cursor.fetchall()]:
360             log.info(f""Creating cache table '{self.tableName}' in {path}"")
361             keyDbType = keyType.value[0]
362             if ""%d"" in keyDbType:
363                 keyDbType = keyDbType % maxKeyLength
364             cursor.execute(f""CREATE TABLE {tableName} (cache_key {keyDbType} PRIMARY KEY, cache_value BLOB);"")
365         cursor.close()
366 
367     def _keyDbValue(self, key):
","Before: 355
After: 360",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,2369,"{'module': 1, 'expression_statement': 18, 'assignment': 13, 'identifier': 69, '=': 17, 'tuple': 2, '(': 12, 'string': 8, 'string_start': 8, 'string_content': 11, 'string_end': 8, ',': 8, ')': 12, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 3, 'float': 1, 'typed_default_parameter': 1, ':': 4, 'type': 1, 'attribute': 21, '.': 21, 'integer': 5, 'block': 3, 'call': 9, 'argument_list': 9, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'not in': 2, 'list_comprehension': 1, '[': 3, 'subscript': 2, ']': 3, 'for_in_clause': 1, 'for': 1, 'in': 2, 'interpolation': 4, '{': 4, '}': 4, 'binary_operator': 1, '%': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7807982265527305,0.7786686349993763,"(tensor([0.9859]), tensor([0.9858]), tensor([0.9859]), tensor([0.9858]))"
"370         else:
371             raise Exception(f""Unhandled key type {self.keyType}"")
372 
373     def _commit(self):
374         self._connMutex.acquire()
375         try:
376             _log.info(f""Committing {self._numEntriesToBeCommitted} cache entries to the SQLite database {self.path}"")
377             self.conn.commit()
378             self._numEntriesToBeCommitted = 0
379         finally:
380             self._connMutex.release()
381 
382     def set(self, key, value):
","375         else:
376             raise Exception(f""Unhandled key type {self.keyType}"")
377 
378     def _commit(self):
379         self._connMutex.acquire()
380         try:
381             log.info(f""Committing {self._numEntriesToBeCommitted} cache entries to the SQLite database {self.path}"")
382             self.conn.commit()
383             self._numEntriesToBeCommitted = 0
384         finally:
385             self._connMutex.release()
386 
387     def set(self, key, value):
","Before: 376
After: 381",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,2609,"{'module': 1, 'expression_statement': 6, 'assignment': 2, 'identifier': 24, ':': 4, 'ERROR': 1, 'type': 1, 'call': 5, 'argument_list': 5, '(': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'interpolation': 3, '{': 3, 'attribute': 11, '.': 11, '}': 3, 'string_end': 2, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'block': 3, 'try_statement': 1, 'try': 1, '=': 1, 'integer': 1, 'finally_clause': 1, 'finally': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6509685590958059,0.6375277547812933,"(tensor([0.9761]), tensor([0.9754]), tensor([0.9758]), tensor([0.9755]))"
"397 
398         self._updateHook.handleUpdate()
399 
400     def _execute(self, cursor, *query):
401         try:
402             cursor.execute(*query)
403         except sqlite3.DatabaseError as e:
404             raise Exception(f""Error executing query for {self.path}: {e}"")
405 
406     def get(self, key):
","402 
403         self._updateHook.handleUpdate()
404 
405     def get(self, key):
406         self._connMutex.acquire()
407         try:
408             cursor = self.conn.cursor()
409             key = self._keyDbValue(key)
410             cursor.execute(f""SELECT cache_value FROM {self.tableName} WHERE cache_key=?"", (key, ))
411             row = cursor.fetchone()
412             cursor.close()
413             if row is None:
414                 return None
415             return pickle.loads(row[0])
416         finally:
417             self._connMutex.release()
418 
419     def __len__(self):
","Before: 400, 401, 402, 403, 404, 405, 411
After: 410",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,2882,"{'module': 1, 'expression_statement': 2, 'call': 3, 'attribute': 5, 'identifier': 17, '.': 5, 'argument_list': 3, '(': 4, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'list_splat_pattern': 1, '*': 2, ':': 3, 'block': 3, 'try_statement': 1, 'try': 1, 'list_splat': 1, 'except_clause': 1, 'except': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 2, '{': 2, '}': 2, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.12520226458638986,0.1037415169130253,"(tensor([0.7405]), tensor([0.7966]), tensor([0.7675]), tensor([0.7906]))"
"508         pass
509 
510 
511 def cached(fn: Callable[[], T], picklePath) -> T:
512     if os.path.exists(picklePath):
513         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
514         return loadPickle(picklePath)
515     else:
516         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
517         result = fn()
518         _log.info(f""Saving cached result in {picklePath}"")
519         dumpPickle(result, picklePath)
520         return result
521 
522 
","507         pass
508 
509 
510 def cached(fn: Callable[[], T], picklePath) -> T:
511     if os.path.exists(picklePath):
512         log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
513         return loadPickle(picklePath)
514     else:
515         log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
516         result = fn()
517         log.info(f""Saving cached result in {picklePath}"")
518         dumpPickle(result, picklePath)
519         return result
520 
521 
","Before: 513
After: 512",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,3762,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7555933017260895,0.7491592531140409,"(tensor([0.9839]), tensor([0.9840]), tensor([0.9839]), tensor([0.9840]))"
"508         pass
509 
510 
511 def cached(fn: Callable[[], T], picklePath) -> T:
512     if os.path.exists(picklePath):
513         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
514         return loadPickle(picklePath)
515     else:
516         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
517         result = fn()
518         _log.info(f""Saving cached result in {picklePath}"")
519         dumpPickle(result, picklePath)
520         return result
521 
522 
","507         pass
508 
509 
510 def cached(fn: Callable[[], T], picklePath) -> T:
511     if os.path.exists(picklePath):
512         log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
513         return loadPickle(picklePath)
514     else:
515         log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
516         result = fn()
517         log.info(f""Saving cached result in {picklePath}"")
518         dumpPickle(result, picklePath)
519         return result
520 
521 
","Before: 516
After: 515",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,3800,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7555933017260895,0.7491592531140409,"(tensor([0.9839]), tensor([0.9840]), tensor([0.9839]), tensor([0.9840]))"
"508         pass
509 
510 
511 def cached(fn: Callable[[], T], picklePath) -> T:
512     if os.path.exists(picklePath):
513         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
514         return loadPickle(picklePath)
515     else:
516         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
517         result = fn()
518         _log.info(f""Saving cached result in {picklePath}"")
519         dumpPickle(result, picklePath)
520         return result
521 
522 
","507         pass
508 
509 
510 def cached(fn: Callable[[], T], picklePath) -> T:
511     if os.path.exists(picklePath):
512         log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
513         return loadPickle(picklePath)
514     else:
515         log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
516         result = fn()
517         log.info(f""Saving cached result in {picklePath}"")
518         dumpPickle(result, picklePath)
519         return result
520 
521 
","Before: 518
After: 517",add debugger to pickles,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,3826,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 23, 'end_line': 31, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7555933017260895,0.7491592531140409,"(tensor([0.9839]), tensor([0.9840]), tensor([0.9839]), tensor([0.9840]))"
"71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self, checkInputColumns=True):
75         """"""
76 
77         :param checkInputColumns: Whether to check if the input column list (after feature generation) during inference coincides
78             with the input column list during fit. This should be disabled if feature generation is not performed by the model itself,
79             e.g. in ensemble models.
80         """"""
81         self._featureGenerator: Optional[""FeatureGenerator""] = None
82         self._inputTransformerChain = DataFrameTransformerChain(())
83         self._outputTransformerChain = DataFrameTransformerChain(())
84         self._predictedVariableNames = None
85         self._modelInputVariableNames = None
86         self._modelOutputVariableNames = None
87         self._targetTransformer = None
88         self._name = None
89         self._isFitted = False
90         self.checkInputColumns = checkInputColumns
91 
92     @staticmethod
","71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self):
75         self._featureGenerator: Optional[""FeatureGenerator""] = None
76         self._inputTransformerChain = DataFrameTransformerChain(())
77         self._outputTransformerChain = DataFrameTransformerChain(())
78         self._predictedVariableNames = None
79         self._modelInputVariableNames = None
80         self._modelOutputVariableNames = None
81         self._targetTransformer = None
82         self._name = None
83 
84     @staticmethod
","Before: 74, 75, 76, 77, 78, 79, 80
After: 74",fix vector_model.py -- a/b/c,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,550,"{'module': 1, 'expression_statement': 12, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'function_definition': 1, 'def': 1, 'identifier': 27, 'parameters': 1, '(': 5, ',': 1, 'default_parameter': 1, '=': 11, 'true': 1, ')': 5, ':': 2, 'block': 1, 'assignment': 10, 'attribute': 10, '.': 10, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 6, 'call': 2, 'argument_list': 2, 'tuple': 2, 'false': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2710495843291646,0.267695747887006,"(tensor([0.9388]), tensor([0.8134]), tensor([0.8716]), tensor([0.8244]))"
"168     def isRegressionModel(self) -> bool:
169         pass
170 
171     def isFitted(self):
172         return self._isFitted
173 
174     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
","160     def isRegressionModel(self) -> bool:
161         pass
162 
163     def isFitted(self):
164         return self.getPredictedVariableNames() is not None
165 
166     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
","Before: 172
After: 164",fix vector_model.py -- a/b/c,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1071,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 7, 'parameters': 2, '(': 2, ')': 2, '->': 1, 'type': 1, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'return_statement': 1, 'return': 1, 'attribute': 1, '.': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5907317802026425,0.512116938282846,"(tensor([0.9249]), tensor([0.9578]), tensor([0.9411]), tensor([0.9544]))"
"171     def isFitted(self):
172         return self._isFitted
173 
174     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
175         fit = y is not None
176         if self._featureGenerator is not None:
177             if fit:
178                 x = self._featureGenerator.fitGenerate(x, y, self)
179             else:
180                 x = self._featureGenerator.generate(x, self)
181         x = self._inputTransformerChain.apply(x, fit=fit)
182         if not fit:
183             if not self.isFitted():
184                 raise Exception(f""Model has not been fitted"")
185             if self.checkInputColumns and list(x.columns) != self._modelInputVariableNames:
186                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(x.columns)}"")
187         return x
188 
189     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","163     def isFitted(self):
164         return self.getPredictedVariableNames() is not None
165 
166     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
167         fit = y is not None
168         if self._featureGenerator is not None:
169             if fit:
170                 x = self._featureGenerator.fitGenerate(x, y, self)
171             else:
172                 x = self._featureGenerator.generate(x, self)
173         x = self._inputTransformerChain.apply(x, fit=fit)
174         if not fit:
175             if not self.isFitted():
176                 raise Exception(f""Model has not been fitted"")
177             if list(x.columns) != self._modelInputVariableNames:
178                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(x.columns)}"")
179         return x
180 
181     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 185
After: 177",fix vector_model.py -- a/b/c,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1246,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 55, 'parameters': 2, '(': 10, ')': 10, ':': 9, 'block': 8, 'return_statement': 2, 'return': 2, 'attribute': 16, '.': 16, ',': 6, 'typed_parameter': 1, 'type': 2, 'default_parameter': 1, '=': 6, 'none': 3, '->': 1, 'expression_statement': 4, 'assignment': 4, 'comparison_operator': 3, 'is not': 4, 'if_statement': 5, 'if': 5, 'call': 8, 'argument_list': 8, 'else_clause': 1, 'else': 1, 'keyword_argument': 1, 'not_operator': 2, 'not': 2, 'raise_statement': 2, 'raise': 2, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'boolean_operator': 1, 'and': 1, '!=': 1, 'interpolation': 2, '{': 2, '}': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7076626998940554,0.698661869063254,"(tensor([0.9617]), tensor([0.9642]), tensor([0.9629]), tensor([0.9639]))"
"205     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
206         pass
207 
208     def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
209         """"""
210         Fits the model using the given data
211 
212         :param X: a data frame containing input data
213         :param Y: a data frame containing output data
214         """"""
215         log.info(f""Training {self.__class__.__name__}"")
216         self._predictedVariableNames = list(Y.columns)
217         X = self._computeInputs(X, y=Y)
218         if self._targetTransformer is not None:
219             self._targetTransformer.fit(Y)
220             Y = self._targetTransformer.apply(Y)
221         self._modelInputVariableNames = list(X.columns)
222         self._modelOutputVariableNames = list(Y.columns)
223         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]={self._modelInputVariableNames}"")
224         self._fit(X, Y)
225         self._isFitted = True
226 
227     @abstractmethod
","197     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
198         pass
199 
200     def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
201         """"""
202         Fits the model using the given data
203 
204         :param X: a data frame containing input data
205         :param Y: a data frame containing output data
206         """"""
207         log.info(f""Training {self.__class__.__name__}"")
208         self._predictedVariableNames = list(Y.columns)
209         X = self._computeInputs(X, y=Y)
210         if self._targetTransformer is not None:
211             self._targetTransformer.fit(Y)
212             Y = self._targetTransformer.apply(Y)
213         self._modelInputVariableNames = list(X.columns)
214         self._modelOutputVariableNames = list(Y.columns)
215         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
216         self._fit(X, Y)
217 
218     @abstractmethod
","Before: 223
After: 215",fix vector_model.py -- a/b/c,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,a85b747f758ecd4f5f53204ecb48cb577d27ede0,8c052d9051042c793a21f804acf7267e100dcf48,0,1639,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 70, 'parameters': 2, '(': 13, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 4, 'attribute': 26, '.': 26, ')': 13, '->': 1, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 11, 'string': 3, 'string_start': 3, 'string_content': 6, 'string_end': 3, 'call': 11, 'argument_list': 11, 'interpolation': 5, '{': 5, '}': 5, 'assignment': 6, '=': 7, 'keyword_argument': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'true': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6414522776334889,0.6277490658643003,"(tensor([0.9522]), tensor([0.9680]), tensor([0.9600]), tensor([0.9664]))"
"35     def __init__(self, **modelArgs):
36         super().__init__(sklearn.naive_bayes.MultinomialNB, **modelArgs)
37 
38 
39 class SkLearnLightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
40     log = log.getChild(__qualname__)
41 
42     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
43         """"""
44         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical
","36     def __init__(self, **modelArgs):
37         super().__init__(sklearn.naive_bayes.MultinomialNB, **modelArgs)
38 
39 
40 class SkLearnLightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
41     _log = _log.getChild(__qualname__)
42 
43     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
44         """"""
45         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical
","Before: 40
After: 41",fix typo in sklearn_classification.py,Fixed minor errors due to refactorings,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_classification.py,159e0afbabfb56d2d53e6f4a4a14a06e2325a78d,d2e29c694b818365a051c137796f75631790adb1,0,310,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 23, 'parameters': 2, '(': 6, ',': 6, 'dictionary_splat_pattern': 2, '**': 3, ')': 6, ':': 4, 'block': 3, 'expression_statement': 2, 'call': 3, 'attribute': 4, 'argument_list': 4, '.': 4, 'dictionary_splat': 1, 'class_definition': 1, 'class': 1, 'assignment': 1, '=': 4, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'ERROR': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 18, 'end_line': 20, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7544782881321285,0.7409836797963631,"(tensor([0.9861]), tensor([0.9861]), tensor([0.9861]), tensor([0.9861]))"
"53         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
54         self.categoricalFeatureNames = categoricalFeatureNames
55 
56     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
57         if self.categoricalFeatureNames is not None:
58             cols = list(inputs.columns)
59             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
60             args = {""cat_column"": colIndices}
61             self.log.info(f""Updating model parameters with {args}"")
62             self.modelArgs.update(args)
63 
64 
","54         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
55         self.categoricalFeatureNames = categoricalFeatureNames
56 
57     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
58         if self.categoricalFeatureNames is not None:
59             cols = list(inputs.columns)
60             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
61             args = {""cat_column"": colIndices}
62             self._log.info(f""Updating model parameters with {args}"")
63             self.modelArgs.update(args)
64 
65 
","Before: 61
After: 62",fix typo in sklearn_classification.py,Fixed minor errors due to refactorings,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_classification.py,159e0afbabfb56d2d53e6f4a4a14a06e2325a78d,d2e29c694b818365a051c137796f75631790adb1,0,504,"{'module': 1, 'expression_statement': 7, 'call': 6, 'attribute': 14, 'identifier': 44, 'argument_list': 6, '(': 7, ')': 7, '.': 14, ',': 5, 'keyword_argument': 2, '=': 6, 'dictionary_splat': 1, '**': 1, 'assignment': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 2, 'pair': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '}': 2, 'interpolation': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 18, 'end_line': 20, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 19, 'end_line': 21, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7500296957179357,0.7467154381913272,"(tensor([0.9930]), tensor([0.9933]), tensor([0.9931]), tensor([0.9932]))"
"202         self._trackedExperiment = None
203 
204     @classmethod
205     def _evalParams(cls, modelFactory, evaluatorOrValidator, skipDecider: ParameterCombinationSkipDecider, **params) -> Optional[Dict[str, Any]]:
206         if skipDecider is not None:
207             if skipDecider.isSkipped(params):
208                 cls.log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
209                 return None
210         cls.log.info(f""Evaluating {params}"")
211         model = modelFactory(**params)
212         values = computeEvaluationMetricsDict(model, evaluatorOrValidator)
213         values[""str(model)""] = str(model)
214         values.update(**params)
215         if skipDecider is not None:
216             skipDecider.tell(params, values)
217         return values
218 
219     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","201         self._trackedExperiment = None
202 
203     @classmethod
204     def _evalParams(cls, modelFactory, evaluatorOrValidator, skipDecider: ParameterCombinationSkipDecider, **params) -> Optional[Dict[str, Any]]:
205         if skipDecider is not None:
206             if skipDecider.isSkipped(params):
207                 cls._log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
208                 return None
209         cls._log.info(f""Evaluating {params}"")
210         model = modelFactory(**params)
211         values = computeEvaluationMetricsDict(model, evaluatorOrValidator)
212         values[""str(model)""] = str(model)
213         values.update(**params)
214         if skipDecider is not None:
215             skipDecider.tell(params, values)
216         return values
217 
218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","Before: 208
After: 207",fix typo and add _log to hyperopt logs,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,1589,"{'module': 1, 'expression_statement': 8, 'assignment': 4, 'attribute': 8, 'identifier': 46, '.': 8, '=': 4, 'none': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 7, 'typed_parameter': 1, ':': 5, 'type': 5, 'dictionary_splat_pattern': 1, '**': 3, ')': 9, '->': 1, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 4, 'if_statement': 3, 'if': 3, 'comparison_operator': 2, 'is not': 4, 'call': 8, 'argument_list': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'interpolation': 3, '{': 3, '}': 3, 'string_end': 3, 'return_statement': 2, 'return': 2, 'dictionary_splat': 2, 'subscript': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 30, 'end_line': 43, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7571342977309923,0.7541972696339416,"(tensor([0.9922]), tensor([0.9925]), tensor([0.9924]), tensor([0.9925]))"
"202         self._trackedExperiment = None
203 
204     @classmethod
205     def _evalParams(cls, modelFactory, evaluatorOrValidator, skipDecider: ParameterCombinationSkipDecider, **params) -> Optional[Dict[str, Any]]:
206         if skipDecider is not None:
207             if skipDecider.isSkipped(params):
208                 cls.log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
209                 return None
210         cls.log.info(f""Evaluating {params}"")
211         model = modelFactory(**params)
212         values = computeEvaluationMetricsDict(model, evaluatorOrValidator)
213         values[""str(model)""] = str(model)
214         values.update(**params)
215         if skipDecider is not None:
216             skipDecider.tell(params, values)
217         return values
218 
219     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","201         self._trackedExperiment = None
202 
203     @classmethod
204     def _evalParams(cls, modelFactory, evaluatorOrValidator, skipDecider: ParameterCombinationSkipDecider, **params) -> Optional[Dict[str, Any]]:
205         if skipDecider is not None:
206             if skipDecider.isSkipped(params):
207                 cls._log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
208                 return None
209         cls._log.info(f""Evaluating {params}"")
210         model = modelFactory(**params)
211         values = computeEvaluationMetricsDict(model, evaluatorOrValidator)
212         values[""str(model)""] = str(model)
213         values.update(**params)
214         if skipDecider is not None:
215             skipDecider.tell(params, values)
216         return values
217 
218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","Before: 210
After: 209",fix typo and add _log to hyperopt logs,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,1612,"{'module': 1, 'expression_statement': 8, 'assignment': 4, 'attribute': 8, 'identifier': 46, '.': 8, '=': 4, 'none': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 7, 'typed_parameter': 1, ':': 5, 'type': 5, 'dictionary_splat_pattern': 1, '**': 3, ')': 9, '->': 1, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 4, 'if_statement': 3, 'if': 3, 'comparison_operator': 2, 'is not': 4, 'call': 8, 'argument_list': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'interpolation': 3, '{': 3, '}': 3, 'string_end': 3, 'return_statement': 2, 'return': 2, 'dictionary_splat': 2, 'subscript': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 30, 'end_line': 43, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7571342977309923,0.7541972696339416,"(tensor([0.9922]), tensor([0.9925]), tensor([0.9924]), tensor([0.9925]))"
"342         self._trackedExperiment = trackedExperiment
343 
344     @classmethod
345     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
346             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
347         metrics = None
348         if parameterCombinationEquivalenceClassValueCache is not None:
349             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
350         if metrics is not None:
351             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
352             return metrics
353         else:
354             cls.log.info(f""Evaluating parameter combination {params}"")
355             model = modelFactory(**params)
356             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
357             cls.log.info(f""Got metrics {metrics} for {params}"")
358             if trackedExperiment is not None:
359                 values = dict(metrics)
360                 values[""str(model)""] = str(model)
361                 values.update(**params)
362                 trackedExperiment.trackValues(values)
363             if parametersMetricsCollection is not None:
364                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
365                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
366             if parameterCombinationEquivalenceClassValueCache is not None:
367                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
368         return metrics
369 
370     def _computeMetric(self, params):
","341         self._trackedExperiment = trackedExperiment
342 
343     @classmethod
344     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
345             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
346         metrics = None
347         if parameterCombinationEquivalenceClassValueCache is not None:
348             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
349         if metrics is not None:
350             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
351             return metrics
352         else:
353             cls._log.info(f""Evaluating parameter combination {params}"")
354             model = modelFactory(**params)
355             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
356             cls._log.info(f""Got metrics {metrics} for {params}"")
357             if trackedExperiment is not None:
358                 values = dict(metrics)
359                 values[""str(model)""] = str(model)
360                 values.update(**params)
361                 trackedExperiment.trackValues(values)
362             if parametersMetricsCollection is not None:
363                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
364                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
365             if parameterCombinationEquivalenceClassValueCache is not None:
366                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
367         return metrics
368 
369     def _computeMetric(self, params):
","Before: 351
After: 350",fix typo and add _log to hyperopt logs,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,2900,"{'module': 1, 'expression_statement': 15, 'assignment': 7, 'attribute': 16, 'identifier': 73, '.': 16, '=': 7, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 16, ',': 10, 'typed_parameter': 1, ':': 8, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 16, 'block': 7, 'none': 6, 'if_statement': 5, 'if': 5, 'comparison_operator': 5, 'is not': 10, 'call': 15, 'argument_list': 15, 'string': 5, 'string_start': 5, 'string_content': 8, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 5, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 30, 'end_line': 43, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.726890123484825,0.7239025231269949,"(tensor([0.9946]), tensor([0.9948]), tensor([0.9947]), tensor([0.9947]))"
"342         self._trackedExperiment = trackedExperiment
343 
344     @classmethod
345     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
346             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
347         metrics = None
348         if parameterCombinationEquivalenceClassValueCache is not None:
349             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
350         if metrics is not None:
351             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
352             return metrics
353         else:
354             cls.log.info(f""Evaluating parameter combination {params}"")
355             model = modelFactory(**params)
356             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
357             cls.log.info(f""Got metrics {metrics} for {params}"")
358             if trackedExperiment is not None:
359                 values = dict(metrics)
360                 values[""str(model)""] = str(model)
361                 values.update(**params)
362                 trackedExperiment.trackValues(values)
363             if parametersMetricsCollection is not None:
364                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
365                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
366             if parameterCombinationEquivalenceClassValueCache is not None:
367                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
368         return metrics
369 
370     def _computeMetric(self, params):
","341         self._trackedExperiment = trackedExperiment
342 
343     @classmethod
344     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
345             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
346         metrics = None
347         if parameterCombinationEquivalenceClassValueCache is not None:
348             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
349         if metrics is not None:
350             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
351             return metrics
352         else:
353             cls._log.info(f""Evaluating parameter combination {params}"")
354             model = modelFactory(**params)
355             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
356             cls._log.info(f""Got metrics {metrics} for {params}"")
357             if trackedExperiment is not None:
358                 values = dict(metrics)
359                 values[""str(model)""] = str(model)
360                 values.update(**params)
361                 trackedExperiment.trackValues(values)
362             if parametersMetricsCollection is not None:
363                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
364                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
365             if parameterCombinationEquivalenceClassValueCache is not None:
366                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
367         return metrics
368 
369     def _computeMetric(self, params):
","Before: 354
After: 353",fix typo and add _log to hyperopt logs,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,2927,"{'module': 1, 'expression_statement': 15, 'assignment': 7, 'attribute': 16, 'identifier': 73, '.': 16, '=': 7, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 16, ',': 10, 'typed_parameter': 1, ':': 8, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 16, 'block': 7, 'none': 6, 'if_statement': 5, 'if': 5, 'comparison_operator': 5, 'is not': 10, 'call': 15, 'argument_list': 15, 'string': 5, 'string_start': 5, 'string_content': 8, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 5, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 30, 'end_line': 43, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.726890123484825,0.7239025231269949,"(tensor([0.9946]), tensor([0.9948]), tensor([0.9947]), tensor([0.9947]))"
"342         self._trackedExperiment = trackedExperiment
343 
344     @classmethod
345     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
346             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
347         metrics = None
348         if parameterCombinationEquivalenceClassValueCache is not None:
349             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
350         if metrics is not None:
351             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
352             return metrics
353         else:
354             cls.log.info(f""Evaluating parameter combination {params}"")
355             model = modelFactory(**params)
356             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
357             cls.log.info(f""Got metrics {metrics} for {params}"")
358             if trackedExperiment is not None:
359                 values = dict(metrics)
360                 values[""str(model)""] = str(model)
361                 values.update(**params)
362                 trackedExperiment.trackValues(values)
363             if parametersMetricsCollection is not None:
364                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
365                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
366             if parameterCombinationEquivalenceClassValueCache is not None:
367                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
368         return metrics
369 
370     def _computeMetric(self, params):
","341         self._trackedExperiment = trackedExperiment
342 
343     @classmethod
344     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
345             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
346         metrics = None
347         if parameterCombinationEquivalenceClassValueCache is not None:
348             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
349         if metrics is not None:
350             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
351             return metrics
352         else:
353             cls._log.info(f""Evaluating parameter combination {params}"")
354             model = modelFactory(**params)
355             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
356             cls._log.info(f""Got metrics {metrics} for {params}"")
357             if trackedExperiment is not None:
358                 values = dict(metrics)
359                 values[""str(model)""] = str(model)
360                 values.update(**params)
361                 trackedExperiment.trackValues(values)
362             if parametersMetricsCollection is not None:
363                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
364                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
365             if parameterCombinationEquivalenceClassValueCache is not None:
366                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
367         return metrics
368 
369     def _computeMetric(self, params):
","Before: 357
After: 356",fix typo and add _log to hyperopt logs,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,2976,"{'module': 1, 'expression_statement': 15, 'assignment': 7, 'attribute': 16, 'identifier': 73, '.': 16, '=': 7, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 16, ',': 10, 'typed_parameter': 1, ':': 8, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 16, 'block': 7, 'none': 6, 'if_statement': 5, 'if': 5, 'comparison_operator': 5, 'is not': 10, 'call': 15, 'argument_list': 15, 'string': 5, 'string_start': 5, 'string_content': 8, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 5, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 30, 'end_line': 43, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.726890123484825,0.7239025231269949,"(tensor([0.9946]), tensor([0.9948]), tensor([0.9947]), tensor([0.9947]))"
"342         self._trackedExperiment = trackedExperiment
343 
344     @classmethod
345     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
346             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
347         metrics = None
348         if parameterCombinationEquivalenceClassValueCache is not None:
349             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
350         if metrics is not None:
351             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
352             return metrics
353         else:
354             cls.log.info(f""Evaluating parameter combination {params}"")
355             model = modelFactory(**params)
356             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
357             cls.log.info(f""Got metrics {metrics} for {params}"")
358             if trackedExperiment is not None:
359                 values = dict(metrics)
360                 values[""str(model)""] = str(model)
361                 values.update(**params)
362                 trackedExperiment.trackValues(values)
363             if parametersMetricsCollection is not None:
364                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
365                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
366             if parameterCombinationEquivalenceClassValueCache is not None:
367                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
368         return metrics
369 
370     def _computeMetric(self, params):
","341         self._trackedExperiment = trackedExperiment
342 
343     @classmethod
344     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
345             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
346         metrics = None
347         if parameterCombinationEquivalenceClassValueCache is not None:
348             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
349         if metrics is not None:
350             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
351             return metrics
352         else:
353             cls._log.info(f""Evaluating parameter combination {params}"")
354             model = modelFactory(**params)
355             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
356             cls._log.info(f""Got metrics {metrics} for {params}"")
357             if trackedExperiment is not None:
358                 values = dict(metrics)
359                 values[""str(model)""] = str(model)
360                 values.update(**params)
361                 trackedExperiment.trackValues(values)
362             if parametersMetricsCollection is not None:
363                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
364                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
365             if parameterCombinationEquivalenceClassValueCache is not None:
366                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
367         return metrics
368 
369     def _computeMetric(self, params):
","Before: 365
After: 364",fix typo and add _log to hyperopt logs,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,3095,"{'module': 1, 'expression_statement': 15, 'assignment': 7, 'attribute': 16, 'identifier': 73, '.': 16, '=': 7, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 16, ',': 10, 'typed_parameter': 1, ':': 8, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 16, 'block': 7, 'none': 6, 'if_statement': 5, 'if': 5, 'comparison_operator': 5, 'is not': 10, 'call': 15, 'argument_list': 15, 'string': 5, 'string_start': 5, 'string_content': 8, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 5, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 30, 'end_line': 43, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.726890123484825,0.7239025231269949,"(tensor([0.9946]), tensor([0.9948]), tensor([0.9947]), tensor([0.9947]))"
"441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self.log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self.log.isEnabledFor(logging.DEBUG):
479             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self._log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","Before: 462
After: 462",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,3142,"{'module': 1, 'expression_statement': 23, 'assignment': 10, 'attribute': 51, 'identifier': 121, '.': 51, '=': 12, 'call': 22, 'argument_list': 22, '(': 23, ')': 23, 'augmented_assignment': 3, '+=': 3, 'integer': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, ':': 11, 'block': 11, 'comment': 1, 'subscript': 7, 'keyword_argument': 2, '[': 7, ']': 7, 'if_statement': 8, 'if': 8, 'comparison_operator': 5, 'is': 3, 'none': 3, 'else_clause': 2, 'else': 2, 'pattern_list': 2, 'list_splat': 1, '*': 1, '<': 1, 'true': 1, '<=': 1, 'string': 7, 'string_start': 7, 'string_content': 12, 'interpolation': 7, '{': 7, '}': 7, 'string_end': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9743342586890346,0.97257141505716,"(tensor([0.9997]), tensor([0.9997]), tensor([0.9997]), tensor([0.9997]))"
"441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self.log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self.log.isEnabledFor(logging.DEBUG):
479             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self._log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","Before: 478, 479
After: 478, 479",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,3369,"{'module': 1, 'expression_statement': 23, 'assignment': 10, 'attribute': 51, 'identifier': 121, '.': 51, '=': 12, 'call': 22, 'argument_list': 22, '(': 23, ')': 23, 'augmented_assignment': 3, '+=': 3, 'integer': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, ':': 11, 'block': 11, 'comment': 1, 'subscript': 7, 'keyword_argument': 2, '[': 7, ']': 7, 'if_statement': 8, 'if': 8, 'comparison_operator': 5, 'is': 3, 'none': 3, 'else_clause': 2, 'else': 2, 'pattern_list': 2, 'list_splat': 1, '*': 1, '<': 1, 'true': 1, '<=': 1, 'string': 7, 'string_start': 7, 'string_content': 12, 'interpolation': 7, '{': 7, '}': 7, 'string_end': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9743342586890346,0.97257141505716,"(tensor([0.9997]), tensor([0.9997]), tensor([0.9997]), tensor([0.9997]))"
"478         if self.log.isEnabledFor(logging.DEBUG):
479             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","Before: 495, 496
After: 495, 496",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,3761,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 21, ')': 21, ':': 7, 'block': 6, 'expression_statement': 12, 'string': 16, 'string_start': 16, 'string_content': 20, 'interpolation': 9, '{': 10, '}': 10, 'string_end': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 6, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'binary_operator': 5, '%': 3, 'tuple': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9676190562815663,0.9651005990129914,"(tensor([0.9996]), tensor([0.9996]), tensor([0.9996]), tensor([0.9996]))"
"550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self.log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self.log.info(f""Simulated annealing completed after {time.time()-startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self._log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self._log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","Before: 562
After: 562",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,4325,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'attribute': 33, 'identifier': 87, '.': 33, '=': 11, 'none': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 14, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ')': 14, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 11, 'argument_list': 11, 'keyword_argument': 3, 'interpolation': 4, '{': 4, '}': 4, 'conditional_expression': 1, 'binary_operator': 6, '%': 2, 'if': 4, 'comparison_operator': 6, 'is not': 8, 'else': 2, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 3, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>=': 2, 'or': 1, 'break_statement': 1, 'break': 1, '/': 2, 'else_clause': 1, 'format_specifier': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.970523326243623,0.9688789960039422,"(tensor([0.9994]), tensor([0.9993]), tensor([0.9994]), tensor([0.9993]))"
"550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self.log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self.log.info(f""Simulated annealing completed after {time.time()-startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self._log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self._log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","Before: 573
After: 573",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,4500,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'attribute': 33, 'identifier': 87, '.': 33, '=': 11, 'none': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 14, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ')': 14, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 11, 'argument_list': 11, 'keyword_argument': 3, 'interpolation': 4, '{': 4, '}': 4, 'conditional_expression': 1, 'binary_operator': 6, '%': 2, 'if': 4, 'comparison_operator': 6, 'is not': 8, 'else': 2, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 3, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>=': 2, 'or': 1, 'break_statement': 1, 'break': 1, '/': 2, 'else_clause': 1, 'format_specifier': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.970523326243623,0.9688789960039422,"(tensor([0.9994]), tensor([0.9993]), tensor([0.9994]), tensor([0.9993]))"
"643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self.log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","Before: 655
After: 655",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,5184,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 164, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 25, 'argument_list': 25, '(': 28, 'none': 5, ',': 13, ')': 28, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865406526547349,0.9859210877174814,"(tensor([0.9999]), tensor([0.9999]), tensor([0.9999]), tensor([0.9999]))"
"643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self.log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","Before: 661
After: 661",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,5266,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 164, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 25, 'argument_list': 25, '(': 28, 'none': 5, ',': 13, ')': 28, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865406526547349,0.9859210877174814,"(tensor([0.9999]), tensor([0.9999]), tensor([0.9999]), tensor([0.9999]))"
"643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self.log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","Before: 697
After: 697",update local_search.py to use self._log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/local_search.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,5659,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 164, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 25, 'argument_list': 25, '(': 28, 'none': 5, ',': 13, ')': 28, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865406526547349,0.9859210877174814,"(tensor([0.9999]), tensor([0.9999]), tensor([0.9999]), tensor([0.9999]))"
"4 import numpy as np
5 
6 
7 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True):
8     """"""
9     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
10     :param title: the plot's title
11     :param xticklabels: the labels for the x-axis ticks
12     :param yticklabels: the labels for the y-axis ticks
13     :param xlabel: the label for the x-axis
14     :param ylabel: the label for the y-axis
15     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
16     :return: the plot's axes object
17     """"""
18     matrix = np.transpose(matrix)
19 
20     if normalize:
21         matrix = matrix.astype('float') / matrix.sum()
22     fig, ax = plt.subplots()
23     fig.canvas.set_window_title(title)
24     # We want to show all ticks...
25     ax.set(xticks=np.arange(matrix.shape[1]),
26         yticks=np.arange(matrix.shape[0]),
27         # ... and label them with the respective list entries
28         xticklabels=xticklabels, yticklabels=yticklabels,
29         title=title,
30         xlabel=xlabel,
31         ylabel=ylabel)
32     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
33     ax.figure.colorbar(im, ax=ax)
34 
35     # Rotate the tick labels and set their alignment.
36     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
37         rotation_mode=""anchor"")
38 
39     # Loop over data dimensions and create text annotations.
40     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
41     thresh = matrix.max() / 2.
42     for i in range(matrix.shape[0]):
43         for j in range(matrix.shape[1]):
44             ax.text(j, i, format(matrix[i, j], fmt),
45                 ha=""center"", va=""center"",
46                 color=""white"" if matrix[i, j] > thresh else ""black"")
47     fig.tight_layout()
48     return ax","5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True) -> matplotlib.figure.Figure:
9     """"""
10     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
11     :param title: the plot's title
12     :param xticklabels: the labels for the x-axis ticks
13     :param yticklabels: the labels for the y-axis ticks
14     :param xlabel: the label for the x-axis
15     :param ylabel: the label for the y-axis
16     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
17     :return: the figure object
18     """"""
19     matrix = np.transpose(matrix)
20 
21     if normalize:
22         matrix = matrix.astype('float') / matrix.sum()
23     fig, ax = plt.subplots()
24     fig.canvas.set_window_title(title)
25     # We want to show all ticks...
26     ax.set(xticks=np.arange(matrix.shape[1]),
27         yticks=np.arange(matrix.shape[0]),
28         # ... and label them with the respective list entries
29         xticklabels=xticklabels, yticklabels=yticklabels,
30         title=title,
31         xlabel=xlabel,
32         ylabel=ylabel)
33     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
34     ax.figure.colorbar(im, ax=ax)
35 
36     # Rotate the tick labels and set their alignment.
37     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
38         rotation_mode=""anchor"")
39 
40     # Loop over data dimensions and create text annotations.
41     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
42     thresh = matrix.max() / 2.
43     for i in range(matrix.shape[0]):
44         for j in range(matrix.shape[1]):
45             ax.text(j, i, format(matrix[i, j], fmt),
46                 ha=""center"", va=""center"",
47                 color=""white"" if matrix[i, j] > thresh else ""black"")
48     fig.tight_layout()
49     return fig","Before: 7
After: 8",replace ax and fig in plot.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,78,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 111, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 19, ',': 27, 'typed_parameter': 4, ':': 8, 'type': 6, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 1, '=': 23, 'true': 1, ')': 19, 'block': 4, 'expression_statement': 12, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 6, 'call': 17, 'attribute': 24, '.': 24, 'argument_list': 17, 'if_statement': 1, 'if': 4, 'binary_operator': 2, '/': 2, 'pattern_list': 1, 'comment': 4, 'keyword_argument': 16, 'subscript': 6, 'integer': 5, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, 'comparison_operator': 2, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 7, 'nloc': 25, 'token_count': 287, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 7, 'end_line': 48, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 7, 'nloc': 35, 'token_count': 293, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 8, 'end_line': 49, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7743347901395509,0.7725848698253456,"(tensor([0.9810]), tensor([0.9853]), tensor([0.9831]), tensor([0.9848]))"
"4 import numpy as np
5 
6 
7 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True):
8     """"""
9     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
10     :param title: the plot's title
11     :param xticklabels: the labels for the x-axis ticks
12     :param yticklabels: the labels for the y-axis ticks
13     :param xlabel: the label for the x-axis
14     :param ylabel: the label for the y-axis
15     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
16     :return: the plot's axes object
17     """"""
18     matrix = np.transpose(matrix)
19 
20     if normalize:
21         matrix = matrix.astype('float') / matrix.sum()
22     fig, ax = plt.subplots()
23     fig.canvas.set_window_title(title)
24     # We want to show all ticks...
25     ax.set(xticks=np.arange(matrix.shape[1]),
26         yticks=np.arange(matrix.shape[0]),
27         # ... and label them with the respective list entries
28         xticklabels=xticklabels, yticklabels=yticklabels,
29         title=title,
30         xlabel=xlabel,
31         ylabel=ylabel)
32     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
33     ax.figure.colorbar(im, ax=ax)
34 
35     # Rotate the tick labels and set their alignment.
36     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
37         rotation_mode=""anchor"")
38 
39     # Loop over data dimensions and create text annotations.
40     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
41     thresh = matrix.max() / 2.
42     for i in range(matrix.shape[0]):
43         for j in range(matrix.shape[1]):
44             ax.text(j, i, format(matrix[i, j], fmt),
45                 ha=""center"", va=""center"",
46                 color=""white"" if matrix[i, j] > thresh else ""black"")
47     fig.tight_layout()
48     return ax","5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True) -> matplotlib.figure.Figure:
9     """"""
10     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
11     :param title: the plot's title
12     :param xticklabels: the labels for the x-axis ticks
13     :param yticklabels: the labels for the y-axis ticks
14     :param xlabel: the label for the x-axis
15     :param ylabel: the label for the y-axis
16     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
17     :return: the figure object
18     """"""
19     matrix = np.transpose(matrix)
20 
21     if normalize:
22         matrix = matrix.astype('float') / matrix.sum()
23     fig, ax = plt.subplots()
24     fig.canvas.set_window_title(title)
25     # We want to show all ticks...
26     ax.set(xticks=np.arange(matrix.shape[1]),
27         yticks=np.arange(matrix.shape[0]),
28         # ... and label them with the respective list entries
29         xticklabels=xticklabels, yticklabels=yticklabels,
30         title=title,
31         xlabel=xlabel,
32         ylabel=ylabel)
33     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
34     ax.figure.colorbar(im, ax=ax)
35 
36     # Rotate the tick labels and set their alignment.
37     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
38         rotation_mode=""anchor"")
39 
40     # Loop over data dimensions and create text annotations.
41     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
42     thresh = matrix.max() / 2.
43     for i in range(matrix.shape[0]):
44         for j in range(matrix.shape[1]):
45             ax.text(j, i, format(matrix[i, j], fmt),
46                 ha=""center"", va=""center"",
47                 color=""white"" if matrix[i, j] > thresh else ""black"")
48     fig.tight_layout()
49     return fig","Before: 16
After: 17",replace ax and fig in plot.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,231,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 111, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 19, ',': 27, 'typed_parameter': 4, ':': 8, 'type': 6, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 1, '=': 23, 'true': 1, ')': 19, 'block': 4, 'expression_statement': 12, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 6, 'call': 17, 'attribute': 24, '.': 24, 'argument_list': 17, 'if_statement': 1, 'if': 4, 'binary_operator': 2, '/': 2, 'pattern_list': 1, 'comment': 4, 'keyword_argument': 16, 'subscript': 6, 'integer': 5, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, 'comparison_operator': 2, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 7, 'nloc': 25, 'token_count': 287, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 7, 'end_line': 48, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 7, 'nloc': 35, 'token_count': 293, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 8, 'end_line': 49, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7743347901395509,0.7725848698253456,"(tensor([0.9810]), tensor([0.9853]), tensor([0.9831]), tensor([0.9848]))"
"4 import numpy as np
5 
6 
7 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True):
8     """"""
9     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
10     :param title: the plot's title
11     :param xticklabels: the labels for the x-axis ticks
12     :param yticklabels: the labels for the y-axis ticks
13     :param xlabel: the label for the x-axis
14     :param ylabel: the label for the y-axis
15     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
16     :return: the plot's axes object
17     """"""
18     matrix = np.transpose(matrix)
19 
20     if normalize:
21         matrix = matrix.astype('float') / matrix.sum()
22     fig, ax = plt.subplots()
23     fig.canvas.set_window_title(title)
24     # We want to show all ticks...
25     ax.set(xticks=np.arange(matrix.shape[1]),
26         yticks=np.arange(matrix.shape[0]),
27         # ... and label them with the respective list entries
28         xticklabels=xticklabels, yticklabels=yticklabels,
29         title=title,
30         xlabel=xlabel,
31         ylabel=ylabel)
32     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
33     ax.figure.colorbar(im, ax=ax)
34 
35     # Rotate the tick labels and set their alignment.
36     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
37         rotation_mode=""anchor"")
38 
39     # Loop over data dimensions and create text annotations.
40     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
41     thresh = matrix.max() / 2.
42     for i in range(matrix.shape[0]):
43         for j in range(matrix.shape[1]):
44             ax.text(j, i, format(matrix[i, j], fmt),
45                 ha=""center"", va=""center"",
46                 color=""white"" if matrix[i, j] > thresh else ""black"")
47     fig.tight_layout()
48     return ax","5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True) -> matplotlib.figure.Figure:
9     """"""
10     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
11     :param title: the plot's title
12     :param xticklabels: the labels for the x-axis ticks
13     :param yticklabels: the labels for the y-axis ticks
14     :param xlabel: the label for the x-axis
15     :param ylabel: the label for the y-axis
16     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
17     :return: the figure object
18     """"""
19     matrix = np.transpose(matrix)
20 
21     if normalize:
22         matrix = matrix.astype('float') / matrix.sum()
23     fig, ax = plt.subplots()
24     fig.canvas.set_window_title(title)
25     # We want to show all ticks...
26     ax.set(xticks=np.arange(matrix.shape[1]),
27         yticks=np.arange(matrix.shape[0]),
28         # ... and label them with the respective list entries
29         xticklabels=xticklabels, yticklabels=yticklabels,
30         title=title,
31         xlabel=xlabel,
32         ylabel=ylabel)
33     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
34     ax.figure.colorbar(im, ax=ax)
35 
36     # Rotate the tick labels and set their alignment.
37     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
38         rotation_mode=""anchor"")
39 
40     # Loop over data dimensions and create text annotations.
41     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
42     thresh = matrix.max() / 2.
43     for i in range(matrix.shape[0]):
44         for j in range(matrix.shape[1]):
45             ax.text(j, i, format(matrix[i, j], fmt),
46                 ha=""center"", va=""center"",
47                 color=""white"" if matrix[i, j] > thresh else ""black"")
48     fig.tight_layout()
49     return fig","Before: 48
After: 49",replace ax and fig in plot.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,ab3ff1f6464f699a448da0043950cda14887eb0b,29bdfbe735a9f00e33576aa286080db30c909a16,0,489,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 111, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 19, ',': 27, 'typed_parameter': 4, ':': 8, 'type': 6, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 1, '=': 23, 'true': 1, ')': 19, 'block': 4, 'expression_statement': 12, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 6, 'call': 17, 'attribute': 24, '.': 24, 'argument_list': 17, 'if_statement': 1, 'if': 4, 'binary_operator': 2, '/': 2, 'pattern_list': 1, 'comment': 4, 'keyword_argument': 16, 'subscript': 6, 'integer': 5, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, 'comparison_operator': 2, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 7, 'nloc': 25, 'token_count': 287, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 7, 'end_line': 48, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 7, 'nloc': 35, 'token_count': 293, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 8, 'end_line': 49, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7743347901395509,0.7725848698253456,"(tensor([0.9810]), tensor([0.9853]), tensor([0.9831]), tensor([0.9848]))"
"5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True) -> matplotlib.figure.Figure:
9     """"""
10     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
11     :param title: the plot's title
12     :param xticklabels: the labels for the x-axis ticks
13     :param yticklabels: the labels for the y-axis ticks
14     :param xlabel: the label for the x-axis
15     :param ylabel: the label for the y-axis
16     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
17     :return: the figure object
18     """"""
19     matrix = np.transpose(matrix)
20 
21     if normalize:
22         matrix = matrix.astype('float') / matrix.sum()
23     fig, ax = plt.subplots()
24     fig.canvas.set_window_title(title)
25     # We want to show all ticks...
26     ax.set(xticks=np.arange(matrix.shape[1]),
27         yticks=np.arange(matrix.shape[0]),
28         # ... and label them with the respective list entries
29         xticklabels=xticklabels, yticklabels=yticklabels,
30         title=title,
31         xlabel=xlabel,
32         ylabel=ylabel)
33     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
34     ax.figure.colorbar(im, ax=ax)
35 
36     # Rotate the tick labels and set their alignment.
37     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
38         rotation_mode=""anchor"")
39 
40     # Loop over data dimensions and create text annotations.
41     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
42     thresh = matrix.max() / 2.
43     for i in range(matrix.shape[0]):
44         for j in range(matrix.shape[1]):
45             ax.text(j, i, format(matrix[i, j], fmt),
46                 ha=""center"", va=""center"",
47                 color=""white"" if matrix[i, j] > thresh else ""black"")
48     fig.tight_layout()
49     return fig","5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9)) -> matplotlib.figure.Figure:
9     """"""
10     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
11     :param title: the plot's title
12     :param xticklabels: the labels for the x-axis ticks
13     :param yticklabels: the labels for the y-axis ticks
14     :param xlabel: the label for the x-axis
15     :param ylabel: the label for the y-axis
16     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
17     :return: the figure object
18     """"""
19     matrix = np.transpose(matrix)
20 
21     if normalize:
22         matrix = matrix.astype('float') / matrix.sum()
23     fig, ax = plt.subplots(figsize=figsize)
24     fig.canvas.set_window_title(title)
25     # We want to show all ticks...
26     ax.set(xticks=np.arange(matrix.shape[1]),
27         yticks=np.arange(matrix.shape[0]),
28         # ... and label them with the respective list entries
29         xticklabels=xticklabels, yticklabels=yticklabels,
30         title=title,
31         xlabel=xlabel,
32         ylabel=ylabel)
33     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
34     ax.figure.colorbar(im, ax=ax)
35 
36     # Rotate the tick labels and set their alignment.
37     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
38         rotation_mode=""anchor"")
39 
40     # Loop over data dimensions and create text annotations.
41     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
42     thresh = matrix.max() / 2.
43     for i in range(matrix.shape[0]):
44         for j in range(matrix.shape[1]):
45             ax.text(j, i, format(matrix[i, j], fmt),
46                 ha=""center"", va=""center"",
47                 color=""white"" if matrix[i, j] > thresh else ""black"")
48     fig.tight_layout()
49     return fig","Before: 8
After: 8",add missing parameter to plotmatrix,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,f1d3a283cb272bd010cdb6df9fbab6e2bc0ca9eb,0aa3756b6bd5910a6377c4a12d67a33d580fcfc4,0,93,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 114, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 19, ',': 27, 'typed_parameter': 4, ':': 8, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 1, '=': 23, 'true': 1, ')': 19, '->': 1, 'attribute': 26, '.': 26, 'block': 4, 'expression_statement': 12, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 6, 'call': 17, 'argument_list': 17, 'if_statement': 1, 'if': 4, 'binary_operator': 2, '/': 2, 'pattern_list': 1, 'comment': 4, 'keyword_argument': 16, 'subscript': 6, 'integer': 5, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, 'comparison_operator': 2, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 7, 'nloc': 35, 'token_count': 293, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 8, 'end_line': 49, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 7, 'nloc': 35, 'token_count': 304, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 8, 'end_line': 49, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9787700887157215,0.9794231933754581,"(tensor([0.9850]), tensor([0.9900]), tensor([0.9875]), tensor([0.9895]))"
"5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True) -> matplotlib.figure.Figure:
9     """"""
10     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
11     :param title: the plot's title
12     :param xticklabels: the labels for the x-axis ticks
13     :param yticklabels: the labels for the y-axis ticks
14     :param xlabel: the label for the x-axis
15     :param ylabel: the label for the y-axis
16     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
17     :return: the figure object
18     """"""
19     matrix = np.transpose(matrix)
20 
21     if normalize:
22         matrix = matrix.astype('float') / matrix.sum()
23     fig, ax = plt.subplots()
24     fig.canvas.set_window_title(title)
25     # We want to show all ticks...
26     ax.set(xticks=np.arange(matrix.shape[1]),
27         yticks=np.arange(matrix.shape[0]),
28         # ... and label them with the respective list entries
29         xticklabels=xticklabels, yticklabels=yticklabels,
30         title=title,
31         xlabel=xlabel,
32         ylabel=ylabel)
33     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
34     ax.figure.colorbar(im, ax=ax)
35 
36     # Rotate the tick labels and set their alignment.
37     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
38         rotation_mode=""anchor"")
39 
40     # Loop over data dimensions and create text annotations.
41     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
42     thresh = matrix.max() / 2.
43     for i in range(matrix.shape[0]):
44         for j in range(matrix.shape[1]):
45             ax.text(j, i, format(matrix[i, j], fmt),
46                 ha=""center"", va=""center"",
47                 color=""white"" if matrix[i, j] > thresh else ""black"")
48     fig.tight_layout()
49     return fig","5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9)) -> matplotlib.figure.Figure:
9     """"""
10     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
11     :param title: the plot's title
12     :param xticklabels: the labels for the x-axis ticks
13     :param yticklabels: the labels for the y-axis ticks
14     :param xlabel: the label for the x-axis
15     :param ylabel: the label for the y-axis
16     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
17     :return: the figure object
18     """"""
19     matrix = np.transpose(matrix)
20 
21     if normalize:
22         matrix = matrix.astype('float') / matrix.sum()
23     fig, ax = plt.subplots(figsize=figsize)
24     fig.canvas.set_window_title(title)
25     # We want to show all ticks...
26     ax.set(xticks=np.arange(matrix.shape[1]),
27         yticks=np.arange(matrix.shape[0]),
28         # ... and label them with the respective list entries
29         xticklabels=xticklabels, yticklabels=yticklabels,
30         title=title,
31         xlabel=xlabel,
32         ylabel=ylabel)
33     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
34     ax.figure.colorbar(im, ax=ax)
35 
36     # Rotate the tick labels and set their alignment.
37     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
38         rotation_mode=""anchor"")
39 
40     # Loop over data dimensions and create text annotations.
41     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
42     thresh = matrix.max() / 2.
43     for i in range(matrix.shape[0]):
44         for j in range(matrix.shape[1]):
45             ax.text(j, i, format(matrix[i, j], fmt),
46                 ha=""center"", va=""center"",
47                 color=""white"" if matrix[i, j] > thresh else ""black"")
48     fig.tight_layout()
49     return fig","Before: 23
After: 23",add missing parameter to plotmatrix,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,f1d3a283cb272bd010cdb6df9fbab6e2bc0ca9eb,0aa3756b6bd5910a6377c4a12d67a33d580fcfc4,0,157,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 114, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 19, ',': 27, 'typed_parameter': 4, ':': 8, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 1, '=': 23, 'true': 1, ')': 19, '->': 1, 'attribute': 26, '.': 26, 'block': 4, 'expression_statement': 12, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 6, 'call': 17, 'argument_list': 17, 'if_statement': 1, 'if': 4, 'binary_operator': 2, '/': 2, 'pattern_list': 1, 'comment': 4, 'keyword_argument': 16, 'subscript': 6, 'integer': 5, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, 'comparison_operator': 2, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 7, 'nloc': 35, 'token_count': 293, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True )', 'start_line': 8, 'end_line': 49, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 7, 'nloc': 35, 'token_count': 304, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 8, 'end_line': 49, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9787700887157215,0.9794231933754581,"(tensor([0.9850]), tensor([0.9900]), tensor([0.9875]), tensor([0.9895]))"
"130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         numInputTimeSlices: the number of input time slices
138         inputDimPerTimeSlice: the dimension of the input data per time slice
139         numOutputTimeSlices: the number of time slices for which to produce outputs
140         outputDimPerTimeSlice: the number of dimensions per output time slice
141         maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         hidRNN: the number of hidden output dimensions for the RNN stage
147         skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         hidSkip: the number of output dimensions of each
149         hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","Before: 137, 138, 139, 140, 141, 142
After: 137, 138, 139, 140, 141, 142",fix typos in src/sensai/torch_modules.py,Added __init__ docstrings to documentation,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,cc7165ebdaca12c5c13d428e6d36ccd98f26a1e5,dcdadc065655a3147df4ff5ba49c428765dcaa43,0,921,"{'module': 1, 'ERROR': 16, 'identifier': 234, 'expression_statement': 1, 'boolean_operator': 1, 'or': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 3, '-': 3, ')': 4, ',': 10, 'for': 3, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 14, 'attribute': 14, 'comparison_operator': 2, 'is': 5, ';': 1, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9437093040449028,0.9457177273319625,"(tensor([0.9786]), tensor([0.9824]), tensor([0.9805]), tensor([0.9820]))"
"130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         numInputTimeSlices: the number of input time slices
138         inputDimPerTimeSlice: the dimension of the input data per time slice
139         numOutputTimeSlices: the number of time slices for which to produce outputs
140         outputDimPerTimeSlice: the number of dimensions per output time slice
141         maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         hidRNN: the number of hidden output dimensions for the RNN stage
147         skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         hidSkip: the number of output dimensions of each
149         hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","Before: 144
After: 144",fix typos in src/sensai/torch_modules.py,Added __init__ docstrings to documentation,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,cc7165ebdaca12c5c13d428e6d36ccd98f26a1e5,dcdadc065655a3147df4ff5ba49c428765dcaa43,0,910,"{'module': 1, 'ERROR': 16, 'identifier': 234, 'expression_statement': 1, 'boolean_operator': 1, 'or': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 3, '-': 3, ')': 4, ',': 10, 'for': 3, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 14, 'attribute': 14, 'comparison_operator': 2, 'is': 5, ';': 1, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9437093040449028,0.9457177273319625,"(tensor([0.9786]), tensor([0.9824]), tensor([0.9805]), tensor([0.9820]))"
"130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         numInputTimeSlices: the number of input time slices
138         inputDimPerTimeSlice: the dimension of the input data per time slice
139         numOutputTimeSlices: the number of time slices for which to produce outputs
140         outputDimPerTimeSlice: the number of dimensions per output time slice
141         maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         hidRNN: the number of hidden output dimensions for the RNN stage
147         skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         hidSkip: the number of output dimensions of each
149         hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","Before: 146, 147, 148, 149
After: 146, 147, 148, 149",fix typos in src/sensai/torch_modules.py,Added __init__ docstrings to documentation,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,cc7165ebdaca12c5c13d428e6d36ccd98f26a1e5,dcdadc065655a3147df4ff5ba49c428765dcaa43,0,947,"{'module': 1, 'ERROR': 16, 'identifier': 234, 'expression_statement': 1, 'boolean_operator': 1, 'or': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 3, '-': 3, ')': 4, ',': 10, 'for': 3, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 14, 'attribute': 14, 'comparison_operator': 2, 'is': 5, ';': 1, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9437093040449028,0.9457177273319625,"(tensor([0.9786]), tensor([0.9824]), tensor([0.9805]), tensor([0.9820]))"
"130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         numInputTimeSlices: the number of input time slices
138         inputDimPerTimeSlice: the dimension of the input data per time slice
139         numOutputTimeSlices: the number of time slices for which to produce outputs
140         outputDimPerTimeSlice: the number of dimensions per output time slice
141         maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         hidRNN: the number of hidden output dimensions for the RNN stage
147         skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         hidSkip: the number of output dimensions of each
149         hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","Before: 151, 152, 153
After: 151, 152, 153",fix typos in src/sensai/torch_modules.py,Added __init__ docstrings to documentation,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,cc7165ebdaca12c5c13d428e6d36ccd98f26a1e5,dcdadc065655a3147df4ff5ba49c428765dcaa43,0,924,"{'module': 1, 'ERROR': 16, 'identifier': 234, 'expression_statement': 1, 'boolean_operator': 1, 'or': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 3, '-': 3, ')': 4, ',': 10, 'for': 3, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 14, 'attribute': 14, 'comparison_operator': 2, 'is': 5, ';': 1, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9437093040449028,0.9457177273319625,"(tensor([0.9786]), tensor([0.9824]), tensor([0.9805]), tensor([0.9820]))"
"269         return x * y
270 
271     @staticmethod
272     def _getOutputActivationFn(name):
273         output = None
274         if not name:
275             pass
276         elif name == 'sigmoid':
277             output = torch.sigmoid
278         elif name == 'tanh':
279             output = torch.tanh
280         else:
281             try:
282                 output = getattr(F, name)
283             except AttributeError:
284                 raise Exception(f'Output function ""{name}"" unknown.')
285         return output","269         return x * y
270 
271     @staticmethod
272     def _getOutputActivationFn(name):
273         output = None
274         if not name:
275             pass
276         elif name == 'sigmoid':
277             output = torch.sigmoid
278         elif name == 'tanh':
279             output = torch.tanh
280         else:
281             try:
282                 output = getattr(F, name)
283             except AttributeError:
284                 raise Exception(f'Output function ""{name}"" unknown.')
285         return output
","Before: 285
After: 285",fix typos in src/sensai/torch_modules.py,Added __init__ docstrings to documentation,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,cc7165ebdaca12c5c13d428e6d36ccd98f26a1e5,dcdadc065655a3147df4ff5ba49c428765dcaa43,0,2273,"{'module': 1, 'return_statement': 1, 'return': 1, 'binary_operator': 1, 'identifier': 20, '*': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, ':': 7, 'block': 7, 'expression_statement': 4, 'assignment': 4, '=': 4, 'none': 1, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'pass_statement': 1, 'pass': 1, 'elif_clause': 2, 'elif': 2, 'comparison_operator': 2, '==': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'attribute': 2, '.': 2, 'else_clause': 1, 'else': 1, 'try_statement': 1, 'try': 1, 'call': 1, 'argument_list': 1, ',': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",1.0,1.0,"(tensor([1.0000]), tensor([1.0000]), tensor([1.0000]), tensor([1.0000]))"
"685                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
686         return total_loss / n_samples
687 
688     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
689         """"""Evaluates the model on the given data set (a validation set)""""""
690         model.eval()
691 
692         outputShape = dataSet[1].shape[1:]  # the shape of the output of a single model application
693         self.lossEvaluator.startValidationCollection(outputShape)
694 
695         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):  # divide all applications into batches to be processed simultaneously
696             with torch.no_grad():
697                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
698             self.lossEvaluator.collectValidationResultBatch(output, groundTruth)
699 
700         return self.lossEvaluator.endValidationCollection()
701 
702     def _init_cuda(self):
","685                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
686         return total_loss / n_samples
687 
688     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
689         """"""Evaluates the model on the given data set (a validation set)""""""
690         model.eval()
691 
692         outputShape = None
693         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):
694             if outputShape is None:
695                 outputShape = Y.shape[1:]  # the shape of the output of a single model application
696                 self.lossEvaluator.startValidationCollection(outputShape)
697             with torch.no_grad():
698                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
699             self.lossEvaluator.collectValidationResultBatch(output, groundTruth)
700 
701         return self.lossEvaluator.endValidationCollection()
702 
703     def _init_cuda(self):
","Before: 692, 693, 694, 695
After: 692, 693, 694, 695, 696",fix bug in nnoptimiser.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,cb1568c7c5277a4a4904bef74f45ca2c6eed9304,093cb16423e584852b679730d58ff0e918b43194,0,6278,"{'module': 1, 'expression_statement': 7, 'augmented_assignment': 1, 'identifier': 48, '+=': 1, 'binary_operator': 2, '*': 1, 'return_statement': 2, 'return': 2, '/': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 10, 'typed_parameter': 3, ':': 7, 'type': 3, 'attribute': 13, '.': 13, ')': 8, 'block': 3, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 7, 'argument_list': 7, 'assignment': 2, '=': 3, 'subscript': 2, '[': 2, 'integer': 2, ']': 2, 'slice': 1, 'comment': 2, 'for_statement': 1, 'for': 1, 'pattern_list': 2, 'in': 1, 'keyword_argument': 1, 'false': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7637794691010898,0.7599595170015513,"(tensor([0.9642]), tensor([0.9597]), tensor([0.9620]), tensor([0.9602]))"
"1 from . import torch_modules as modules, torch_models as models
2 from .torch_base import TensorScaler, NNLossEvaluatorRegression, NNLossEvaluator, DataUtil, NNOptimiser, WrappedTorchModule, VectorDataUtil, TorchVectorRegressionModel
","1 from . import torch_modules as modules, torch_models as models
2 from .torch_data import TensorScaler, DataUtil, VectorDataUtil
3 from .torch_base import NNLossEvaluatorRegression, NNLossEvaluator, NNOptimiser, WrappedTorchModule, \
4     WrappedTorchVectorModule, TorchVectorRegressionModel, TorchVectorClassificationModel
","Before: 2
After: 2, 3, 4",add missing import,Fixed torch top-level imports,https://github.com/opcode81/sensAI,src/sensai/torch/__init__.py,c83a08ce77d792941f7054d5e306027e5098d5fd,4a8522916134b9f0f9d2d3cd5f9d0011afb4bd02,0,50,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'relative_import': 1, 'import_prefix': 1, '.': 1, 'import': 1, 'aliased_import': 2, 'dotted_name': 2, 'identifier': 4, 'as': 2, ',': 1}",{},{},0.5642315747842429,0.5356896164856721,"(tensor([0.9481]), tensor([0.9654]), tensor([0.9566]), tensor([0.9636]))"
"787     def createTorchVectorModel(self) -> WrappedTorchVectorModule:
788         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
789 
790     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
791         dataUtil = ClassificationVectorDataUtil(inputs, outputs, self.model.cuda, len(self._labels),
792             normalisationMode=self.normalisationMode)
793         return TorchDataSetProviderFromDataUtil(dataUtil)
794 
795     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","787     def createTorchVectorModel(self) -> WrappedTorchVectorModule:
788         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
789 
790     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
791         dataUtil = ClassificationVectorDataUtil(inputs, outputs, self.model.cuda, len(self._labels),
792             normalisationMode=self.normalisationMode)
793         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
794 
795     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 793
After: 793",fix typo in src/src/sensai/torch/torch_base.py,Fixed data set provider instantiation,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,91526c2282ff365a61d1b781a2f3a1315d7dc3b8,b3db894693a90738c97a1f70af18bd719c095f23,0,7396,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 33, 'parameters': 2, '(': 6, ')': 6, '->': 2, 'type': 4, ':': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 4, 'attribute': 9, '.': 9, 'argument_list': 4, 'list_splat': 1, '*': 1, ',': 7, 'dictionary_splat': 1, '**': 1, 'typed_parameter': 2, 'expression_statement': 1, 'assignment': 1, '=': 2, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9567517979328872,0.9553825325883574,"(tensor([0.9953]), tensor([0.9971]), tensor([0.9962]), tensor([0.9969]))"
"130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", isClassification=False):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
155             to suit loss functions such CrossEntropyLoss
156         """"""
157         if numConvolutions == 0 and hwWindow == 0:
158             raise ValueError(""No processing paths remain"")
159         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
160             raise Exception(""Inconsistent numbers of times slices provided"")
161 
162         super().__init__()
163         self.inputDimPerTimeSlice = inputDimPerTimeSlice
164         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
165         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
166         self.numOutputTimeSlices = numOutputTimeSlices
167         self.cuda = cuda
168         self.window = numInputTimeSlices
169         self.hidRNN = hidRNN
170         self.numConv = numConvolutions
171         self.hidSkip = hidSkip
172         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
173         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
174         self.skip = skip
175         self.hw = hwWindow
176         self.pDropout = dropout
177         self.isClassification = isClassification
178 
179         # configure CNN-RNN path
180         if self.numConv > 0:
181             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
182             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
183             if self.skip > 0:
184                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
185                 if self.skipRnnSeqLength == 0:
186                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
187                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
188                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
189             else:
190                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
191 
192         # configure highway component
193         if self.hw > 0:
194             # direct mapping from all inputs to all outputs
195             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
196             if hwCombine == 'plus':
197                 self.highwayCombine = self._plus
198             elif hwCombine == 'product':
199                 self.highwayCombine = self._product
200             elif hwCombine == 'bilinear':
201                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
202             else:
203                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
204 
205         self.output = self._getOutputActivationFn(outputActivation)
206 
207     def forward(self, x):
","Before: 135
After: 135",add isclassification parameter to lstnetwork,Sync intime,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,a552208c688fc2a7619a62d87977c18edb326353,c87c92cc36949e3aa0cdf0f736ea09126a9fe365,0,907,"{'module': 1, 'ERROR': 19, 'identifier': 247, 'expression_statement': 1, 'boolean_operator': 1, 'or': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 3, '-': 3, ')': 4, ',': 10, 'for': 4, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 28, 'attribute': 15, 'comparison_operator': 2, 'is': 5, ';': 1, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8316674394728201,0.8305412792196996,"(tensor([0.9555]), tensor([0.9661]), tensor([0.9608]), tensor([0.9650]))"
"130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid""):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         """"""
155         if numConvolutions == 0 and hwWindow == 0:
156             raise ValueError(""No processing paths remain"")
157         if numInputTimeSlices < numCnnTimeSlices or hwWindow < numInputTimeSlices:
158             raise Exception(""Inconsistent numbers of times slices provided"")
159 
160         super().__init__()
161         self.inputDimPerTimeSlice = inputDimPerTimeSlice
162         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
163         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
164         self.numOutputTimeSlices = numOutputTimeSlices
165         self.cuda = cuda
166         self.window = numInputTimeSlices
167         self.hidRNN = hidRNN
168         self.numConv = numConvolutions
169         self.hidSkip = hidSkip
170         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
171         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
172         self.skip = skip
173         self.hw = hwWindow
174         self.pDropout = dropout
175 
176         # configure CNN-RNN path
177         if self.numConv > 0:
178             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
179             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
180             if self.skip > 0:
181                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
182                 if self.skipRnnSeqLength == 0:
183                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
184                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
185                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
186             else:
187                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
188 
189         # configure highway component
190         if self.hw > 0:
191             # direct mapping from all inputs to all outputs
192             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
193             if hwCombine == 'plus':
194                 self.highwayCombine = self._plus
195             elif hwCombine == 'product':
196                 self.highwayCombine = self._product
197             elif hwCombine == 'bilinear':
198                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
199             else:
200                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
","130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", isClassification=False):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
155             to suit loss functions such CrossEntropyLoss
156         """"""
157         if numConvolutions == 0 and hwWindow == 0:
158             raise ValueError(""No processing paths remain"")
159         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
160             raise Exception(""Inconsistent numbers of times slices provided"")
161 
162         super().__init__()
163         self.inputDimPerTimeSlice = inputDimPerTimeSlice
164         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
165         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
166         self.numOutputTimeSlices = numOutputTimeSlices
167         self.cuda = cuda
168         self.window = numInputTimeSlices
169         self.hidRNN = hidRNN
170         self.numConv = numConvolutions
171         self.hidSkip = hidSkip
172         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
173         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
174         self.skip = skip
175         self.hw = hwWindow
176         self.pDropout = dropout
177         self.isClassification = isClassification
178 
179         # configure CNN-RNN path
180         if self.numConv > 0:
181             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
182             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
183             if self.skip > 0:
184                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
185                 if self.skipRnnSeqLength == 0:
186                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
187                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
188                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
189             else:
190                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
191 
192         # configure highway component
193         if self.hw > 0:
194             # direct mapping from all inputs to all outputs
195             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
196             if hwCombine == 'plus':
197                 self.highwayCombine = self._plus
198             elif hwCombine == 'product':
199                 self.highwayCombine = self._product
200             elif hwCombine == 'bilinear':
201                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
202             else:
203                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
204 
205         self.output = self._getOutputActivationFn(outputActivation)
206 
207     def forward(self, x):
","Before: 157
After: 159",add isclassification parameter to lstnetwork,Sync intime,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,a552208c688fc2a7619a62d87977c18edb326353,c87c92cc36949e3aa0cdf0f736ea09126a9fe365,0,951,"{'module': 1, 'ERROR': 19, 'identifier': 247, 'expression_statement': 1, 'boolean_operator': 1, 'or': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 3, '-': 3, ')': 4, ',': 10, 'for': 4, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 28, 'attribute': 15, 'comparison_operator': 2, 'is': 5, ';': 1, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8316674394728201,0.8305412792196996,"(tensor([0.9555]), tensor([0.9661]), tensor([0.9608]), tensor([0.9650]))"
"201 
202         self.output = self._getOutputActivationFn(outputActivation)
203 
204     def forward(self, x):
205         batch_size = x.size(0)
206         # x has size (batch_size, window=numInputTimeSlices, inputDimPerTimeSlice)
207 
208         dropout = lambda x: self._dropout(x, pTraining=self.pDropout, pInference=self.pDropout)
209 
210         res = None
211 
212         if self.numConv > 0:
213             # CNN
214             # convSeqLength = self.window - self.Ck + 1
215             # convolution produces, via numConv kernel matrices of dimension (height=Ck, width=inputDimPerTimeSlice), from an original input sequence of length window, numConv output sequences of length convSeqLength
216             c = x.view(batch_size, 1, self.window, self.inputDimPerTimeSlice)  # insert one dim of size 1 (one channel): (batch_size, 1, height=window, width=inputDimPerTimeSlice)
217             c = F.relu(self.conv1(c))  # (batch_size, channels=numConv, convSeqLength, 1)
218             c = dropout(c)
219             c = torch.squeeze(c, 3)  # drops last dimension, i.e. new size (batch_size, numConv, convSeqLength)
220 
221             # RNN
222             # It processes the numConv sequences of length convSeqLength obtained through convolution and keep the hidden state at the end, which is comprised of hidR entries
223             # Specifically, it squashes the numConv sequences of length convSeqLength to a vector of size hidS (by iterating through the sequences and applying the same model in each step, processing all batches in parallel)
224             r = c.permute(2, 0, 1).contiguous()  # (convSeqLength, batch_size, numConv)
225             self.GRU1.flatten_parameters()
226             _, r = self.GRU1(r)  # maps (seq_len=convSeqLength, batch=batch_size, input_size=numConv) -> hidden state (num_layers=1, batch=batch_size, hidden_size=hidR)
227             r = torch.squeeze(r, 0)  # (batch_size, hidR)
228             r = dropout(r)
229 
230             # Skip-RNN
231             if self.skip > 0:
232                 s = c[:, :, -(self.skipRnnSeqLength * self.skip):].contiguous()  # (batch_size, numConv, convSeqLength) -> (batch_size, numConv, skipRnnSeqLength * skip)
233                 s = s.view(batch_size, self.numConv, self.skipRnnSeqLength, self.skip)  # (batch_size, numConv, skipRnnSeqLength, skip)
234                 s = s.permute(2, 0, 3, 1).contiguous()  # (skipRnnSeqLength, batch_size, skip, numConv)
235                 s = s.view(self.skipRnnSeqLength, batch_size * self.skip, self.numConv)  # (skipRnnSeqLength, batch_size * skip, numConv)
236                 # Why the above view makes sense:
237                 # skipRnnSeqLength is the sequence length considered for the RNN, i.e. the number of steps that is taken for each sequence.
238                 # The batch_size*skip elements of the second dimension are all processed in parallel, i.e. there are batch_size*skip RNNs being applied in parallel.
239                 # By scaling the actual batch size with 'skip', we process 'skip' RNNs of each batch in parallel, such that each RNN consecutively processes entries that are 'skip' steps apart
240                 self.GRUskip.flatten_parameters()
241                 _, s = self.GRUskip(s)  # maps (seq_len=skipRnnSeqLength, batch=batch_size * skip, input_size=numConv) -> hidden state (num_layers=1, batch=batch_size * skip, hidden_size=hidS)
242                 # Because of the way the data is grouped, we obtain not one vector of size hidS but skip vectors of size hidS
243                 s = s.view(batch_size, self.skip * self.hidSkip)  # regroup by batch -> (batch_size, skip * hidS)
244                 s = dropout(s)
245                 r = torch.cat((r, s), 1)  # (batch_size, hidR + skip * hidS)
246 
247             res = self.linear1(r)  # (batch_size, totalOutputDim)
248 
249         # auto-regressive highway model
250         if self.hw > 0:
251             resHW = x[:, -self.hw:, :]  # keep only the last hw entries for each input: (batch_size, hw, inputDimPerTimeSlice)
252             resHW = resHW.view(-1, self.hw * self.inputDimPerTimeSlice)  # (batch_size, hw * inputDimPerTimeSlice)
253             resHW = self.highway(resHW)  # (batch_size, totalOutputDim)
254             if res is None:
255                 res = resHW
256             else:
257                 res = self.highwayCombine(res, resHW)  # (batch_size, totalOutputDim)
258 
259         if self.output:
260             res = self.output(res)
261         return res.view(batch_size, self.numOutputTimeSlices, self.timeSeriesDimPerTimeSlice)
262 
263     @staticmethod
","204 
205         self.output = self._getOutputActivationFn(outputActivation)
206 
207     def forward(self, x):
208         batch_size = x.size(0)
209         # x has size (batch_size, window=numInputTimeSlices, inputDimPerTimeSlice)
210 
211         dropout = lambda x: self._dropout(x, pTraining=self.pDropout, pInference=self.pDropout)
212 
213         res = None
214 
215         if self.numConv > 0:
216             # CNN
217             # convSeqLength = self.window - self.Ck + 1
218             # convolution produces, via numConv kernel matrices of dimension (height=Ck, width=inputDimPerTimeSlice), from an original input sequence of length window, numConv output sequences of length convSeqLength
219             c = x.view(batch_size, 1, self.window, self.inputDimPerTimeSlice)  # insert one dim of size 1 (one channel): (batch_size, 1, height=window, width=inputDimPerTimeSlice)
220             c = F.relu(self.conv1(c))  # (batch_size, channels=numConv, convSeqLength, 1)
221             c = dropout(c)
222             c = torch.squeeze(c, 3)  # drops last dimension, i.e. new size (batch_size, numConv, convSeqLength)
223 
224             # RNN
225             # It processes the numConv sequences of length convSeqLength obtained through convolution and keep the hidden state at the end, which is comprised of hidR entries
226             # Specifically, it squashes the numConv sequences of length convSeqLength to a vector of size hidS (by iterating through the sequences and applying the same model in each step, processing all batches in parallel)
227             r = c.permute(2, 0, 1).contiguous()  # (convSeqLength, batch_size, numConv)
228             self.GRU1.flatten_parameters()
229             _, r = self.GRU1(r)  # maps (seq_len=convSeqLength, batch=batch_size, input_size=numConv) -> hidden state (num_layers=1, batch=batch_size, hidden_size=hidR)
230             r = torch.squeeze(r, 0)  # (batch_size, hidR)
231             r = dropout(r)
232 
233             # Skip-RNN
234             if self.skip > 0:
235                 s = c[:, :, -(self.skipRnnSeqLength * self.skip):].contiguous()  # (batch_size, numConv, convSeqLength) -> (batch_size, numConv, skipRnnSeqLength * skip)
236                 s = s.view(batch_size, self.numConv, self.skipRnnSeqLength, self.skip)  # (batch_size, numConv, skipRnnSeqLength, skip)
237                 s = s.permute(2, 0, 3, 1).contiguous()  # (skipRnnSeqLength, batch_size, skip, numConv)
238                 s = s.view(self.skipRnnSeqLength, batch_size * self.skip, self.numConv)  # (skipRnnSeqLength, batch_size * skip, numConv)
239                 # Why the above view makes sense:
240                 # skipRnnSeqLength is the sequence length considered for the RNN, i.e. the number of steps that is taken for each sequence.
241                 # The batch_size*skip elements of the second dimension are all processed in parallel, i.e. there are batch_size*skip RNNs being applied in parallel.
242                 # By scaling the actual batch size with 'skip', we process 'skip' RNNs of each batch in parallel, such that each RNN consecutively processes entries that are 'skip' steps apart
243                 self.GRUskip.flatten_parameters()
244                 _, s = self.GRUskip(s)  # maps (seq_len=skipRnnSeqLength, batch=batch_size * skip, input_size=numConv) -> hidden state (num_layers=1, batch=batch_size * skip, hidden_size=hidS)
245                 # Because of the way the data is grouped, we obtain not one vector of size hidS but skip vectors of size hidS
246                 s = s.view(batch_size, self.skip * self.hidSkip)  # regroup by batch -> (batch_size, skip * hidS)
247                 s = dropout(s)
248                 r = torch.cat((r, s), 1)  # (batch_size, hidR + skip * hidS)
249 
250             res = self.linear1(r)  # (batch_size, totalOutputDim)
251 
252         # auto-regressive highway model
253         if self.hw > 0:
254             resHW = x[:, -self.hw:, :]  # keep only the last hw entries for each input: (batch_size, hw, inputDimPerTimeSlice)
255             resHW = resHW.view(-1, self.hw * self.inputDimPerTimeSlice)  # (batch_size, hw * inputDimPerTimeSlice)
256             resHW = self.highway(resHW)  # (batch_size, totalOutputDim)
257             if res is None:
258                 res = resHW
259             else:
260                 res = self.highwayCombine(res, resHW)  # (batch_size, totalOutputDim)
261 
262         if self.output:
263             res = self.output(res)
264 
265         res = res.view(batch_size, self.numOutputTimeSlices, self.timeSeriesDimPerTimeSlice)
266         if self.isClassification:
267             res = res.permute(0, 2, 1)
268         return res
269 
270     @staticmethod
","Before: 261
After: 264, 265, 266, 267, 268",add isclassification parameter to lstnetwork,Sync intime,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,a552208c688fc2a7619a62d87977c18edb326353,c87c92cc36949e3aa0cdf0f736ea09126a9fe365,0,2122,"{'module': 1, 'expression_statement': 29, 'assignment': 27, 'attribute': 53, 'identifier': 164, '.': 53, '=': 29, 'call': 30, 'argument_list': 30, '(': 33, ')': 33, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 31, ':': 14, 'block': 7, 'integer': 16, 'comment': 32, 'lambda': 2, 'lambda_parameters': 1, 'keyword_argument': 2, 'none': 2, 'if_statement': 5, 'if': 5, 'comparison_operator': 4, '>': 3, 'pattern_list': 2, 'subscript': 2, '[': 2, 'slice': 6, 'unary_operator': 3, '-': 3, 'parenthesized_expression': 1, 'binary_operator': 4, '*': 4, ']': 2, 'tuple': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8310933998794693,0.8274864902035807,"(tensor([0.9876]), tensor([0.9910]), tensor([0.9893]), tensor([0.9907]))"
"513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath) -> T:
517     if os.path.exists(picklePath):
518         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
519         return loadPickle(picklePath)
520     else:
521         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
522         result = fn()
523         _log.info(f""Saving cached result in {picklePath}"")
524         dumpPickle(result, picklePath)
525         return result
526 
527 
","513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath, functionName=None) -> T:
517     if functionName is None:
518         functionName = fn.__name__
519     if os.path.exists(picklePath):
520         _log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
521         return loadPickle(picklePath)
522     else:
523         _log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
524         result = fn()
525         _log.info(f""Saving cached result in {picklePath}"")
526         dumpPickle(result, picklePath)
527         return result
528 
529 
","Before: 516
After: 516, 517, 518",add function name to cache.cached,Sync intime,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a552208c688fc2a7619a62d87977c18edb326353,c87c92cc36949e3aa0cdf0f736ea09126a9fe365,0,3777,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7049454354780246,0.693628494813248,"(tensor([0.9483]), tensor([0.9758]), tensor([0.9619]), tensor([0.9730]))"
"513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath) -> T:
517     if os.path.exists(picklePath):
518         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
519         return loadPickle(picklePath)
520     else:
521         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
522         result = fn()
523         _log.info(f""Saving cached result in {picklePath}"")
524         dumpPickle(result, picklePath)
525         return result
526 
527 
","513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath, functionName=None) -> T:
517     if functionName is None:
518         functionName = fn.__name__
519     if os.path.exists(picklePath):
520         _log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
521         return loadPickle(picklePath)
522     else:
523         _log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
524         result = fn()
525         _log.info(f""Saving cached result in {picklePath}"")
526         dumpPickle(result, picklePath)
527         return result
528 
529 
","Before: 518
After: 520",add function name to cache.cached,Sync intime,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a552208c688fc2a7619a62d87977c18edb326353,c87c92cc36949e3aa0cdf0f736ea09126a9fe365,0,3818,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7049454354780246,0.693628494813248,"(tensor([0.9483]), tensor([0.9758]), tensor([0.9619]), tensor([0.9730]))"
"513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath) -> T:
517     if os.path.exists(picklePath):
518         _log.info(f""Loading cached result of function '{fn.__name__}' from {picklePath}"")
519         return loadPickle(picklePath)
520     else:
521         _log.info(f""No cached result found in {picklePath}, calling function '{fn.__name__}' ..."")
522         result = fn()
523         _log.info(f""Saving cached result in {picklePath}"")
524         dumpPickle(result, picklePath)
525         return result
526 
527 
","513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath, functionName=None) -> T:
517     if functionName is None:
518         functionName = fn.__name__
519     if os.path.exists(picklePath):
520         _log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
521         return loadPickle(picklePath)
522     else:
523         _log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
524         result = fn()
525         _log.info(f""Saving cached result in {picklePath}"")
526         dumpPickle(result, picklePath)
527         return result
528 
529 
","Before: 521
After: 523",add function name to cache.cached,Sync intime,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a552208c688fc2a7619a62d87977c18edb326353,c87c92cc36949e3aa0cdf0f736ea09126a9fe365,0,3856,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 8, 'typed_parameter': 1, ':': 4, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ',': 3, ')': 8, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'call': 7, 'attribute': 7, '.': 7, 'argument_list': 7, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7049454354780246,0.693628494813248,"(tensor([0.9483]), tensor([0.9758]), tensor([0.9619]), tensor([0.9730]))"
"542         else:
543             self.filenamePrefix += ""-""
544 
545     def __call__(self, fn, *args, **kwargs):
546         if self.filename is None:
547             self.filename = self.filenamePrefix + fn.__qualname__ + "".cache.pickle""
548         picklePath = os.path.join(self.cacheBasePath,  self.filename)
549         return lambda *args, **kwargs: cached(lambda: fn(*args, **kwargs), picklePath)
","544         else:
545             self.filenamePrefix += ""-""
546 
547     def __call__(self, fn, *args, **kwargs):
548         if self.filename is None:
549             self.filename = self.filenamePrefix + fn.__qualname__ + "".cache.pickle""
550         picklePath = os.path.join(self.cacheBasePath,  self.filename)
551         return lambda *args, **kwargs: cached(lambda: fn(*args, **kwargs), picklePath, functionName=fn.__name__)
","Before: 549
After: 551",add function name to cache.cached,Sync intime,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,a552208c688fc2a7619a62d87977c18edb326353,c87c92cc36949e3aa0cdf0f736ea09126a9fe365,0,4111,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'identifier': 24, ':': 3, 'type': 1, 'attribute': 9, '.': 9, 'ERROR': 1, '+': 3, '=': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 4, 'list_splat_pattern': 1, '*': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'binary_operator': 2, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7572687251657819,0.7471399446548005,"(tensor([0.9715]), tensor([0.9740]), tensor([0.9728]), tensor([0.9737]))"
"153         self.outputScaler = data.getOutputTensorScaler()
154         self.inputScaler = data.getInputTensorScaler()
155 
156     def fit(self, data: TorchDataSetProvider, **nnOptimiserParams):
157         self._extractParamsFromData(data)
158         optimiser = NNOptimiser(cuda=self.cuda, **nnOptimiserParams)
159         optimiser.fit(self, data)
160 
161 
","153         self.outputScaler = data.getOutputTensorScaler()
154         self.inputScaler = data.getInputTensorScaler()
155 
156     def fit(self, data: TorchDataSetProvider, **nnOptimiserParams):
157         self._extractParamsFromData(data)
158         if ""cuda"" not in nnOptimiserParams:
159             nnOptimiserParams[""cuda""] = self.cuda
160         optimiser = NNOptimiser(**nnOptimiserParams)
161         optimiser.fit(self, data)
162 
163 
","Before: 158
After: 158, 159, 160",improve memory usage of wrappedtorchmodule,Sync intime,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,5822731cb6f056cfc0fa73793c56baafd08e9aed,87ce970b776125d8b78b047f37b806c5a6f77792,0,1345,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'attribute': 7, 'identifier': 26, '.': 7, '=': 4, 'call': 5, 'argument_list': 5, '(': 6, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, 'typed_parameter': 1, ':': 2, 'type': 1, 'dictionary_splat_pattern': 1, '**': 2, 'block': 1, 'keyword_argument': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6282061022909765,0.5908810292930224,"(tensor([0.9537]), tensor([0.9800]), tensor([0.9666]), tensor([0.9773]))"
"260     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
261         return self.convertClassProbabilitiesToPredictions(self._predictClassProbabilities(inputs))
262 
263     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
264         return self.model.applyScaled(inputs.values, asNumpy=True)
265 
266     def _predictClassProbabilities(self, inputs: pd.DataFrame):
","262     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
263         return self.convertClassProbabilitiesToPredictions(self._predictClassProbabilities(inputs))
264 
265     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
266         results = []
267         i = 0
268         batchSize = 64
269         while i < len(inputs):
270             inputSlice = inputs.iloc[i:i+batchSize]
271             results.append(self.model.applyScaled(inputSlice.values, asNumpy=True))
272             i += batchSize
273         return np.concatenate(results)
274 
275     def _predictClassProbabilities(self, inputs: pd.DataFrame):
","Before: 264
After: 266, 267, 268, 269, 270, 271, 272, 273",improve memory usage of wrappedtorchmodule,Sync intime,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,5822731cb6f056cfc0fa73793c56baafd08e9aed,87ce970b776125d8b78b047f37b806c5a6f77792,0,2457,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 25, 'parameters': 2, '(': 5, ',': 3, 'typed_parameter': 2, ':': 4, 'type': 4, 'attribute': 9, '.': 9, ')': 5, '->': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 3, 'argument_list': 3, 'keyword_argument': 1, '=': 1, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4111497692417574,0.3572995499144215,"(tensor([0.8604]), tensor([0.9604]), tensor([0.9076]), tensor([0.9494]))"
"130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", isClassification=False):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
155             to suit loss functions such CrossEntropyLoss
156         """"""
157         if numConvolutions == 0 and hwWindow == 0:
158             raise ValueError(""No processing paths remain"")
159         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
160             raise Exception(""Inconsistent numbers of times slices provided"")
161 
162         super().__init__()
163         self.inputDimPerTimeSlice = inputDimPerTimeSlice
164         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
165         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
166         self.numOutputTimeSlices = numOutputTimeSlices
167         self.cuda = cuda
168         self.window = numInputTimeSlices
169         self.hidRNN = hidRNN
170         self.numConv = numConvolutions
171         self.hidSkip = hidSkip
172         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
173         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
174         self.skip = skip
175         self.hw = hwWindow
176         self.pDropout = dropout
177         self.isClassification = isClassification
178 
179         # configure CNN-RNN path
180         if self.numConv > 0:
181             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
182             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
183             if self.skip > 0:
184                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
185                 if self.skipRnnSeqLength == 0:
186                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
187                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
188                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
189             else:
190                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
191 
192         # configure highway component
193         if self.hw > 0:
194             # direct mapping from all inputs to all outputs
195             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
196             if hwCombine == 'plus':
197                 self.highwayCombine = self._plus
198             elif hwCombine == 'product':
199                 self.highwayCombine = self._product
200             elif hwCombine == 'bilinear':
201                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
202             else:
203                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
204 
205         self.output = self._getOutputActivationFn(outputActivation)
206 
207     def forward(self, x):
","130     The model can produce one or more (potentially multi-dimensional) outputs, where each output typically typically corresponds
131     to a time slice for which a prediction is made.
132     """"""
133     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1, cuda=True,
134             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
135             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", isClassification=False):
136         """"""
137         :param numInputTimeSlices: the number of input time slices
138         :param inputDimPerTimeSlice: the dimension of the input data per time slice
139         :param numOutputTimeSlices: the number of time slices for which to produce outputs
140         :param outputDimPerTimeSlice: the number of dimensions per output time slice
141         :param maxHorizon: the number of time steps predicted by the model (i.e. the maximum horizon)
142         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
143             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
144         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
145             if it is 0, then the entire complex processing path is not applied.
146         :param hidRNN: the number of hidden output dimensions for the RNN stage
147         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
148         :param hidSkip: the number of output dimensions of each
149         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
150             If it is 0, the highway component is not used.
151         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
152         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
153         :param outputActivation: the output activation function
154         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
155             to suit loss functions such CrossEntropyLoss
156         """"""
157         if numConvolutions == 0 and hwWindow == 0:
158             raise ValueError(""No processing paths remain"")
159         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
160             raise Exception(""Inconsistent numbers of times slices provided"")
161 
162         super().__init__()
163         self.inputDimPerTimeSlice = inputDimPerTimeSlice
164         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
165         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
166         self.numOutputTimeSlices = numOutputTimeSlices
167         self.useCuda = cuda
168         self.window = numInputTimeSlices
169         self.hidRNN = hidRNN
170         self.numConv = numConvolutions
171         self.hidSkip = hidSkip
172         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
173         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
174         self.skip = skip
175         self.hw = hwWindow
176         self.pDropout = dropout
177         self.isClassification = isClassification
178 
179         # configure CNN-RNN path
180         if self.numConv > 0:
181             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
182             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
183             if self.skip > 0:
184                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
185                 if self.skipRnnSeqLength == 0:
186                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
187                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
188                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
189             else:
190                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
191 
192         # configure highway component
193         if self.hw > 0:
194             # direct mapping from all inputs to all outputs
195             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
196             if hwCombine == 'plus':
197                 self.highwayCombine = self._plus
198             elif hwCombine == 'product':
199                 self.highwayCombine = self._product
200             elif hwCombine == 'bilinear':
201                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
202             else:
203                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
204 
205         self.output = self._getOutputActivationFn(outputActivation)
206 
207     def forward(self, x):
","Before: 167
After: 167",fix typo in src/sensai/torch/torch_modules.py,Sync intime,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,5822731cb6f056cfc0fa73793c56baafd08e9aed,87ce970b776125d8b78b047f37b806c5a6f77792,0,1035,"{'module': 1, 'ERROR': 21, 'identifier': 271, 'expression_statement': 1, 'boolean_operator': 1, 'or': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 3, '-': 3, ')': 4, ',': 11, 'for': 4, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 30, 'attribute': 15, 'comparison_operator': 2, 'is': 6, 'pattern_list': 1, ';': 1, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'in': 2, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9972759383095486,0.9971504034952576,"(tensor([0.9980]), tensor([0.9982]), tensor([0.9981]), tensor([0.9982]))"
"387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""WrappedTorchModule"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModel(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = None
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModelBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModelBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""WrappedTorchModule"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModel(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModelBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModelBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModelBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","Before: 454
After: 454",use model.getmodelbytes() instead of model.getmodelbytes(),Sync intime,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5822731cb6f056cfc0fa73793c56baafd08e9aed,87ce970b776125d8b78b047f37b806c5a6f77792,0,4075,"{'module': 1, 'expression_statement': 63, 'concatenated_string': 1, 'string': 18, 'string_start': 18, 'string_content': 28, 'interpolation': 12, '{': 12, 'attribute': 83, 'identifier': 301, '.': 83, '}': 12, 'string_end': 18, 'function_definition': 3, 'def': 3, 'parameters': 3, '(': 72, ',': 39, 'typed_parameter': 2, ':': 22, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 11, ']': 11, ')': 72, 'block': 20, 'call': 66, 'argument_list': 66, '->': 1, 'if_statement': 8, 'if': 8, 'return_statement': 2, 'return': 2, 'elif_clause': 1, 'elif': 1, 'comparison_operator': 3, '!=': 1, 'assignment': 37, '=': 43, 'list': 5, 'else_clause': 3, 'else': 3, 'list_comprehension': 3, 'for_in_clause': 3, 'for': 6, 'in': 6, 'comment': 8, 'none': 4, 'integer': 6, 'false': 1, 'true': 1, 'for_statement': 3, 'pattern_list': 3, 'binary_operator': 7, '+': 2, '%': 4, 'float': 1, 'keyword_argument': 6, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, '==': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'except_clause': 1, 'except': 1, 'tuple': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9951564996628012,0.995787756103285,"(tensor([0.9962]), tensor([0.9959]), tensor([0.9960]), tensor([0.9959]))"
"1 from . import torch_modules as modules, torch_models as models
2 from .torch_data import TensorScaler, DataUtil, VectorDataUtil
3 from .torch_opt import NNLossEvaluatorRegression, NNLossEvaluator, NNOptimiser
4 from .torch_base import WrappedTorchModule, WrappedTorchVectorModule, TorchVectorRegressionModel, \
5     TorchVectorClassificationModel
","1 from . import torch_modules as modules, torch_models as models
2 from .torch_data import TensorScaler, DataUtil, VectorDataUtil
3 from .torch_opt import NNLossEvaluatorRegression, NNLossEvaluator, NNOptimiser
4 from .torch_base import TorchModel, VectorTorchModel, TorchVectorRegressionModel, \
5     TorchVectorClassificationModel
","Before: 4
After: 4",remove unused imports,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/__init__.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,69,"{'module': 1, 'import_from_statement': 4, 'from': 4, 'relative_import': 4, 'import_prefix': 4, '.': 4, 'import': 4, 'aliased_import': 2, 'dotted_name': 14, 'identifier': 16, 'as': 2, ',': 8, 'line_continuation': 1}",{},{},0.8931440071693292,0.8778709598107581,"(tensor([0.9898]), tensor([0.9743]), tensor([0.9820]), tensor([0.9758]))"
"16 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification
17 
18 log = logging.getLogger(__name__)
19 
20 
21 class WrappedTorchModule(ABC):
22     log = log.getChild(__qualname__)
23 
24     def __init__(self, cuda=True):
25         self.cuda = cuda
","16 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification
17 
18 log = logging.getLogger(__name__)
19 
20 
21 class TorchModel(ABC):
22     """"""
23     sensAI abstraction for torch models, which supports one-line training, allows for convenient model application,
24     has basic mechanisms for data scaling, and soundly handles persistence (via pickle).
25     An instance wraps a torch.nn.Module, which is constructed on demand during training via the factory method
","Before: 21
After: 21, 22, 23, 24, 25, 26, 27",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,169,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'relative_import': 1, 'import_prefix': 1, '.': 3, 'dotted_name': 4, 'identifier': 17, 'import': 1, ',': 3, 'expression_statement': 2, 'assignment': 2, '=': 3, 'call': 2, 'attribute': 2, 'argument_list': 3, '(': 4, ')': 4, 'class_definition': 1, 'class': 1, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.29586032941995277,0.2739402539238134,"(tensor([0.7944]), tensor([0.8912]), tensor([0.8400]), tensor([0.8805]))"
"21 class WrappedTorchModule(ABC):
22     log = log.getChild(__qualname__)
23 
24     def __init__(self, cuda=True):
25         self.cuda = cuda
26         self.model = None
27         self.outputScaler: Optional[TensorScaler] = None
28         self.inputScaler: Optional[TensorScaler] = None
29 
30     def setTorchModel(self, model):
","27     """"""
28     log = log.getChild(__qualname__)
29 
30     def __init__(self, cuda=True):
31         self.cuda = cuda
32         self.module: torch.nn.Module = None
33         self.outputScaler: Optional[TensorScaler] = None
34         self.inputScaler: Optional[TensorScaler] = None
35 
36     def setTorchModule(self, module: torch.nn.Module):
","Before: 26
After: 32",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,212,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 22, 'argument_list': 2, '(': 3, ')': 3, ':': 4, 'block': 2, 'expression_statement': 5, 'assignment': 5, '=': 6, 'call': 1, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'default_parameter': 1, 'true': 1, 'none': 3, 'type': 4, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4868801210330244,0.4516268637876864,"(tensor([0.9309]), tensor([0.9232]), tensor([0.9270]), tensor([0.9239]))"
"27         self.outputScaler: Optional[TensorScaler] = None
28         self.inputScaler: Optional[TensorScaler] = None
29 
30     def setTorchModel(self, model):
31         self.model = model
32 
33     def getModelBytes(self):
","33         self.outputScaler: Optional[TensorScaler] = None
34         self.inputScaler: Optional[TensorScaler] = None
35 
36     def setTorchModule(self, module: torch.nn.Module):
37         self.module = module
38 
39     def getModuleBytes(self):
","Before: 30, 31
After: 36, 37",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,257,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'attribute': 3, 'identifier': 14, '.': 3, ':': 3, 'type': 4, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '=': 3, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, ')': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4344487460702228,0.3621354640616849,"(tensor([0.9397]), tensor([0.9527]), tensor([0.9462]), tensor([0.9514]))"
"30     def setTorchModel(self, model):
31         self.model = model
32 
33     def getModelBytes(self):
34         bytesIO = io.BytesIO()
35         torch.save(self.model, bytesIO)
36         return bytesIO.getvalue()
37 
38     def setModelBytes(self, modelBytes):
","36     def setTorchModule(self, module: torch.nn.Module):
37         self.module = module
38 
39     def getModuleBytes(self):
40         bytesIO = io.BytesIO()
41         torch.save(self.module, bytesIO)
42         return bytesIO.getvalue()
43 
44     def setModuleBytes(self, modelBytes):
","Before: 33
After: 39",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,274,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 18, 'parameters': 2, '(': 5, ',': 2, ')': 5, ':': 2, 'block': 2, 'expression_statement': 3, 'assignment': 2, 'attribute': 5, '.': 5, '=': 2, 'call': 3, 'argument_list': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3272626347359449,0.2631646132392587,"(tensor([0.9313]), tensor([0.9386]), tensor([0.9349]), tensor([0.9379]))"
"30     def setTorchModel(self, model):
31         self.model = model
32 
33     def getModelBytes(self):
34         bytesIO = io.BytesIO()
35         torch.save(self.model, bytesIO)
36         return bytesIO.getvalue()
37 
38     def setModelBytes(self, modelBytes):
","36     def setTorchModule(self, module: torch.nn.Module):
37         self.module = module
38 
39     def getModuleBytes(self):
40         bytesIO = io.BytesIO()
41         torch.save(self.module, bytesIO)
42         return bytesIO.getvalue()
43 
44     def setModuleBytes(self, modelBytes):
","Before: 35
After: 41",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,301,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 18, 'parameters': 2, '(': 5, ',': 2, ')': 5, ':': 2, 'block': 2, 'expression_statement': 3, 'assignment': 2, 'attribute': 5, '.': 5, '=': 2, 'call': 3, 'argument_list': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3272626347359449,0.2631646132392587,"(tensor([0.9313]), tensor([0.9386]), tensor([0.9349]), tensor([0.9379]))"
"35         torch.save(self.model, bytesIO)
36         return bytesIO.getvalue()
37 
38     def setModelBytes(self, modelBytes):
39         modelFile = io.BytesIO(modelBytes)
40         self._loadModel(modelFile)
41 
42     def getTorchModel(self):
","41         torch.save(self.module, bytesIO)
42         return bytesIO.getvalue()
43 
44     def setModuleBytes(self, modelBytes):
45         modelFile = io.BytesIO(modelBytes)
46         self._loadModel(modelFile)
47 
48     def getTorchModule(self):
","Before: 38
After: 44",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,322,"{'module': 1, 'expression_statement': 3, 'call': 4, 'attribute': 5, 'identifier': 17, '.': 5, 'argument_list': 4, '(': 5, ',': 2, ')': 5, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.43704701756508146,0.38090132980774855,"(tensor([0.9554]), tensor([0.9566]), tensor([0.9560]), tensor([0.9565]))"
"39         modelFile = io.BytesIO(modelBytes)
40         self._loadModel(modelFile)
41 
42     def getTorchModel(self):
43         return self.model
44 
45     def _setCudaEnabled(self, isCudaEnabled):
","45         modelFile = io.BytesIO(modelBytes)
46         self._loadModel(modelFile)
47 
48     def getTorchModule(self):
49         return self.module
50 
51     def _setCudaEnabled(self, isCudaEnabled):
","Before: 42, 43
After: 48, 49",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,354,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 11, '=': 1, 'call': 2, 'attribute': 3, '.': 3, 'argument_list': 2, '(': 3, ')': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5032669614223712,0.43291782524005995,"(tensor([0.9618]), tensor([0.9618]), tensor([0.9618]), tensor([0.9618]))"
"48     def _isCudaEnabled(self):
49         return self.cuda
50 
51     def _loadModel(self, modelFile):
52         try:
53             self.model = torch.load(modelFile)
54         except:
55             if self._isCudaEnabled():
56                 self.log.warning(""Loading of CUDA model failed, trying without CUDA..."")
57                 if type(modelFile) != str:
58                     modelFile.seek(0)
59                 self.model = torch.load(modelFile, map_location='cpu')
60                 self._setCudaEnabled(False)
61                 self.log.info(""Model successfully loaded to CPU"")
62             else:
63                 raise
64 
65     @abstractmethod
","54     def _isCudaEnabled(self):
55         return self.cuda
56 
57     def _loadModel(self, modelFile):
58         try:
59             self.module = torch.load(modelFile)
60         except:
61             if self._isCudaEnabled():
62                 self.log.warning(""Loading of CUDA model failed, trying without CUDA..."")
63                 if type(modelFile) != str:
64                     modelFile.seek(0)
65                 self.module = torch.load(modelFile, map_location='cpu')
66                 self._setCudaEnabled(False)
67                 self.log.info(""Model successfully loaded to CPU"")
68             else:
69                 raise
70 
71     @abstractmethod
","Before: 53
After: 59",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,427,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 33, 'parameters': 2, '(': 10, ')': 10, ':': 7, 'block': 7, 'return_statement': 1, 'return': 1, 'attribute': 12, '.': 12, ',': 2, 'try_statement': 1, 'try': 1, 'expression_statement': 6, 'assignment': 2, '=': 3, 'call': 8, 'argument_list': 8, 'except_clause': 1, 'except': 1, 'if_statement': 2, 'if': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'comparison_operator': 1, '!=': 1, 'integer': 1, 'keyword_argument': 1, 'false': 1, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6166981676525708,0.6025283839538736,"(tensor([0.9689]), tensor([0.9689]), tensor([0.9689]), tensor([0.9689]))"
"48     def _isCudaEnabled(self):
49         return self.cuda
50 
51     def _loadModel(self, modelFile):
52         try:
53             self.model = torch.load(modelFile)
54         except:
55             if self._isCudaEnabled():
56                 self.log.warning(""Loading of CUDA model failed, trying without CUDA..."")
57                 if type(modelFile) != str:
58                     modelFile.seek(0)
59                 self.model = torch.load(modelFile, map_location='cpu')
60                 self._setCudaEnabled(False)
61                 self.log.info(""Model successfully loaded to CPU"")
62             else:
63                 raise
64 
65     @abstractmethod
","54     def _isCudaEnabled(self):
55         return self.cuda
56 
57     def _loadModel(self, modelFile):
58         try:
59             self.module = torch.load(modelFile)
60         except:
61             if self._isCudaEnabled():
62                 self.log.warning(""Loading of CUDA model failed, trying without CUDA..."")
63                 if type(modelFile) != str:
64                     modelFile.seek(0)
65                 self.module = torch.load(modelFile, map_location='cpu')
66                 self._setCudaEnabled(False)
67                 self.log.info(""Model successfully loaded to CPU"")
68             else:
69                 raise
70 
71     @abstractmethod
","Before: 59
After: 65",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,504,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 33, 'parameters': 2, '(': 10, ')': 10, ':': 7, 'block': 7, 'return_statement': 1, 'return': 1, 'attribute': 12, '.': 12, ',': 2, 'try_statement': 1, 'try': 1, 'expression_statement': 6, 'assignment': 2, '=': 3, 'call': 8, 'argument_list': 8, 'except_clause': 1, 'except': 1, 'if_statement': 2, 'if': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'comparison_operator': 1, '!=': 1, 'integer': 1, 'keyword_argument': 1, 'false': 1, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6166981676525708,0.6025283839538736,"(tensor([0.9689]), tensor([0.9689]), tensor([0.9689]), tensor([0.9689]))"
"63                 raise
64 
65     @abstractmethod
66     def createTorchModule(self):
67         pass
68 
69     def __getstate__(self):
","69                 raise
70 
71     @abstractmethod
72     def createTorchModule(self) -> torch.nn.Module:
73         pass
74 
75     def __getstate__(self):
","Before: 66
After: 72",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,549,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ')': 1, ':': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.35885262946480795,0.259418075232376,"(tensor([0.9048]), tensor([0.9385]), tensor([0.9213]), tensor([0.9350]))"
"66     def createTorchModule(self):
67         pass
68 
69     def __getstate__(self):
70         state = dict(self.__dict__)
71         del state[""model""]
72         state[""modelBytes""] = self.getModelBytes()
73         return state
74 
75     def __setstate__(self, d):
","72     def createTorchModule(self) -> torch.nn.Module:
73         pass
74 
75     def __getstate__(self):
76         state = dict(self.__dict__)
77         del state[""model""]
78         state[""modelBytes""] = self.getModuleBytes()
79         return state
80 
81     def __setstate__(self, d):
","Before: 72
After: 78",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,602,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 13, 'parameters': 2, '(': 4, ')': 4, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'call': 2, 'argument_list': 2, 'attribute': 2, '.': 2, 'delete_statement': 1, 'del': 1, 'subscript': 2, '[': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5589308236899769,0.5258541596476439,"(tensor([0.9313]), tensor([0.9525]), tensor([0.9418]), tensor([0.9503]))"
"72         state[""modelBytes""] = self.getModelBytes()
73         return state
74 
75     def __setstate__(self, d):
76         modelBytes = None
77         if ""modelBytes"" in d:
78             modelBytes = d[""modelBytes""]
79             del d[""modelBytes""]
80         self.__dict__ = d
81         if modelBytes is not None:
82             self.setModelBytes(modelBytes)
83 
84     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy=True, createBatch=False, mcDropoutSamples=None, mcDropoutProbability=None, scaleOutput=False,
","78         state[""modelBytes""] = self.getModuleBytes()
79         return state
80 
81     def __setstate__(self, d):
82         modelBytes = None
83         if ""modelBytes"" in d:
84             modelBytes = d[""modelBytes""]
85             del d[""modelBytes""]
86         self.__dict__ = d
87         if modelBytes is not None:
88             self.setModuleBytes(modelBytes)
89 
90     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy=True, createBatch=False, mcDropoutSamples=None, mcDropoutProbability=None, scaleOutput=False,
","Before: 82
After: 88",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,681,"{'module': 1, 'expression_statement': 5, 'assignment': 4, 'subscript': 3, 'identifier': 19, '[': 3, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ']': 3, '=': 4, 'call': 2, 'attribute': 3, '.': 3, 'argument_list': 2, '(': 3, ')': 3, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 3, 'block': 3, 'none': 2, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'in': 1, 'delete_statement': 1, 'del': 1, 'is not': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6781968839406892,0.6606290439994252,"(tensor([0.9729]), tensor([0.9749]), tensor([0.9739]), tensor([0.9747]))"
"81         if modelBytes is not None:
82             self.setModelBytes(modelBytes)
83 
84     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy=True, createBatch=False, mcDropoutSamples=None, mcDropoutProbability=None, scaleOutput=False,
85             scaleInput=False) -> Union[torch.Tensor, np.ndarray, Tuple]:
86         """"""
87         Applies the model to the given input tensor and returns the result (normalized)
88 
89         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
90             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
91         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
92         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
93         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
94         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
95         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
96         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
97 
98         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
99             containing standard deviations
100         """"""
101         def extract(z):
102             if scaleOutput:
103                 z = self.scaledOutput(z)
104             if self._isCudaEnabled():
105                 z = z.cpu()
106             z = z.detach()
107             if asNumpy:
108                 z = z.numpy()
109             return z
110 
111         model = self.getTorchModel()
112         model.eval()
113 
114         if isinstance(X, TorchDataSet):
115             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
116         elif isinstance(X, np.ndarray):
117             X = toFloatArray(X)
118             X = torch.from_numpy(X).float()
119 
120         if self._isCudaEnabled():
121             X = X.cuda()
122         if scaleInput:
123             X = self.inputScaler.normalise(X)
124         if createBatch:
125             X = X.view(1, *X.size())
126 
127         maxValue = X.max().item()
128         if maxValue > 2:
129             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
130 
131         if mcDropoutSamples is None:
132             y = model(X)
133             return extract(y)
134         else:
135             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
136             return extract(y), extract(stddev)
137 
138     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","87         if modelBytes is not None:
88             self.setModuleBytes(modelBytes)
89 
90     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy=True, createBatch=False, mcDropoutSamples=None, mcDropoutProbability=None, scaleOutput=False,
91             scaleInput=False) -> Union[torch.Tensor, np.ndarray, Tuple]:
92         """"""
93         Applies the model to the given input tensor and returns the result (normalized)
94 
95         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
96             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
97         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
98         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
99         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
100         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
101         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
102         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
103 
104         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
105             containing standard deviations
106         """"""
107         def extract(z):
108             if scaleOutput:
109                 z = self.scaledOutput(z)
110             if self._isCudaEnabled():
111                 z = z.cpu()
112             z = z.detach()
113             if asNumpy:
114                 z = z.numpy()
115             return z
116 
117         model = self.getTorchModule()
118         model.eval()
119 
120         if isinstance(X, TorchDataSet):
121             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
122         elif isinstance(X, np.ndarray):
123             X = toFloatArray(X)
124             X = torch.from_numpy(X).float()
125 
126         if self._isCudaEnabled():
127             X = X.cuda()
128         if scaleInput:
129             X = self.inputScaler.normalise(X)
130         if createBatch:
131             X = X.view(1, *X.size())
132 
133         maxValue = X.max().item()
134         if maxValue > 2:
135             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
136 
137         if mcDropoutSamples is None:
138             y = model(X)
139             return extract(y)
140         else:
141             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
142             return extract(y), extract(stddev)
143 
144     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 111
After: 117",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,865,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 116, 'is not': 2, 'none': 4, ':': 15, 'block': 14, 'expression_statement': 18, 'call': 29, 'attribute': 27, '.': 27, 'argument_list': 29, '(': 31, ')': 31, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 8, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8060527487945429,0.8036621578446377,"(tensor([0.9853]), tensor([0.9855]), tensor([0.9854]), tensor([0.9855]))"
"160             nnOptimiserParams[""cuda""] = self.cuda
161         optimiser = NNOptimiser(**nnOptimiserParams)
162         optimiser.fit(self, data)
163 
164 
165 class WrappedTorchVectorModule(WrappedTorchModule, ABC):
166     """"""
167     Base class for wrapped torch modules that map vectors to vectors
168     """"""
169     def __init__(self, cuda: bool = True):
","166             nnOptimiserParams[""cuda""] = self.cuda
167         optimiser = NNOptimiser(**nnOptimiserParams)
168         optimiser.fit(self, data)
169 
170 
171 class VectorTorchModel(TorchModel, ABC):
172     """"""
173     Adds
174     """"""
175     def __init__(self, cuda: bool = True):
","Before: 165
After: 171",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,1398,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'subscript': 1, 'identifier': 13, '[': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 1, '=': 2, 'attribute': 2, '.': 2, 'call': 2, 'argument_list': 3, '(': 3, 'dictionary_splat': 1, '**': 1, ')': 3, ',': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4935262402630168,0.46685245057497343,"(tensor([0.9495]), tensor([0.8972]), tensor([0.9226]), tensor([0.9021]))"
"162         optimiser.fit(self, data)
163 
164 
165 class WrappedTorchVectorModule(WrappedTorchModule, ABC):
166     """"""
167     Base class for wrapped torch modules that map vectors to vectors
168     """"""
169     def __init__(self, cuda: bool = True):
170         super().__init__(cuda=cuda)
171         self.inputDim = None
","168         optimiser.fit(self, data)
169 
170 
171 class VectorTorchModel(TorchModel, ABC):
172     """"""
173     Adds
174     """"""
175     def __init__(self, cuda: bool = True):
176         super().__init__(cuda=cuda)
177         self.inputDim = None
","Before: 167
After: 173",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,1411,"{'module': 1, 'expression_statement': 3, 'call': 3, 'attribute': 2, 'identifier': 15, '.': 2, 'argument_list': 4, '(': 5, ',': 3, ')': 5, 'class_definition': 1, 'class': 1, ':': 3, 'block': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 1, 'type': 1, '=': 2, 'true': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.43925977002236793,0.40982217844515506,"(tensor([0.9342]), tensor([0.8692]), tensor([0.9005]), tensor([0.8753]))"
"176         self.inputDim = data.getInputDim()
177         self.outputDim = data.getModelOutputDim()
178 
179     def createTorchModule(self):
180         return self.createTorchVectorModule(self.inputDim, self.outputDim)
181 
182     @abstractmethod
","182         self.inputDim = data.getInputDim()
183         self.outputDim = data.getModelOutputDim()
184 
185     def createTorchModule(self):
186         return self.createTorchModuleForDims(self.inputDim, self.outputDim)
187 
188     @abstractmethod
","Before: 180
After: 186",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,1540,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 7, 'identifier': 16, '.': 7, '=': 2, 'call': 3, 'argument_list': 3, '(': 4, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4969833768917055,0.4388737945016326,"(tensor([0.9553]), tensor([0.9648]), tensor([0.9601]), tensor([0.9639]))"
"180         return self.createTorchVectorModule(self.inputDim, self.outputDim)
181 
182     @abstractmethod
183     def createTorchVectorModule(self, inputDim, outputDim):
184         pass
185 
186 
","186         return self.createTorchModuleForDims(self.inputDim, self.outputDim)
187 
188     @abstractmethod
189     def createTorchModuleForDims(self, inputDim, outputDim) -> torch.nn.Module:
190         pass
191 
192 
","Before: 183
After: 189",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,1557,"{'module': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'attribute': 3, 'identifier': 11, '.': 3, 'argument_list': 1, '(': 2, ',': 3, ')': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.35142820870955216,0.2945377993872971,"(tensor([0.9050]), tensor([0.9502]), tensor([0.9270]), tensor([0.9455]))"
"185 
186 
187 class TorchVectorRegressionModel(VectorRegressionModel):
188     def __init__(self, modelClass: Callable[..., WrappedTorchVectorModule], modelArgs=(), modelKwArgs=None,
189             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
190         """"""
191         :param modelClass: the constructor with which to create the wrapped torch vector model
192         :param modelArgs: the constructor argument list to pass to modelClass
193         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
194         :param normalisationMode: the normalisation mode to apply to input data frames
195         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
196         """"""
197         super().__init__()
198         if modelKwArgs is None:
199             modelKwArgs = {}
200         if nnOptimiserParams is None:
201             nnOptimiserParams = {}
202         if ""lossEvaluator"" not in nnOptimiserParams:
203             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
204         self.normalisationMode = normalisationMode
205         self.nnOptimiserParams = nnOptimiserParams
206         self.modelClass = modelClass
207         self.modelArgs = modelArgs
208         self.modelKwArgs = modelKwArgs
209         self.model: Optional[WrappedTorchVectorModule] = None
210 
211     def _createWrappedTorchVectorModule(self) -> WrappedTorchVectorModule:
","191 
192 
193 class TorchVectorRegressionModel(VectorRegressionModel):
194     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs=(), modelKwArgs=None,
195             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
196         """"""
197         :param modelClass: the constructor with which to create the wrapped torch vector model
198         :param modelArgs: the constructor argument list to pass to modelClass
199         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
200         :param normalisationMode: the normalisation mode to apply to input data frames
201         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
202         """"""
203         super().__init__()
204         if modelKwArgs is None:
205             modelKwArgs = {}
206         if nnOptimiserParams is None:
207             nnOptimiserParams = {}
208         if ""lossEvaluator"" not in nnOptimiserParams:
209             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
210         self.normalisationMode = normalisationMode
211         self.nnOptimiserParams = nnOptimiserParams
212         self.modelClass = modelClass
213         self.modelArgs = modelArgs
214         self.modelKwArgs = modelKwArgs
215         self.model: Optional[VectorTorchModel] = None
216 
217     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
","Before: 188
After: 194",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,1601,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 44, 'argument_list': 4, '(': 6, ')': 6, ':': 7, 'block': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'typed_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 3, 'ellipsis': 1, ']': 3, 'default_parameter': 4, '=': 13, 'tuple': 1, 'none': 5, 'attribute': 10, '.': 10, 'expression_statement': 11, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, 'is': 2, 'assignment': 9, 'dictionary': 2, '{': 2, '}': 2, 'not in': 2, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6767251794386121,0.6719026514964068,"(tensor([0.9790]), tensor([0.9769]), tensor([0.9780]), tensor([0.9771]))"
"185 
186 
187 class TorchVectorRegressionModel(VectorRegressionModel):
188     def __init__(self, modelClass: Callable[..., WrappedTorchVectorModule], modelArgs=(), modelKwArgs=None,
189             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
190         """"""
191         :param modelClass: the constructor with which to create the wrapped torch vector model
192         :param modelArgs: the constructor argument list to pass to modelClass
193         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
194         :param normalisationMode: the normalisation mode to apply to input data frames
195         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
196         """"""
197         super().__init__()
198         if modelKwArgs is None:
199             modelKwArgs = {}
200         if nnOptimiserParams is None:
201             nnOptimiserParams = {}
202         if ""lossEvaluator"" not in nnOptimiserParams:
203             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
204         self.normalisationMode = normalisationMode
205         self.nnOptimiserParams = nnOptimiserParams
206         self.modelClass = modelClass
207         self.modelArgs = modelArgs
208         self.modelKwArgs = modelKwArgs
209         self.model: Optional[WrappedTorchVectorModule] = None
210 
211     def _createWrappedTorchVectorModule(self) -> WrappedTorchVectorModule:
","191 
192 
193 class TorchVectorRegressionModel(VectorRegressionModel):
194     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs=(), modelKwArgs=None,
195             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
196         """"""
197         :param modelClass: the constructor with which to create the wrapped torch vector model
198         :param modelArgs: the constructor argument list to pass to modelClass
199         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
200         :param normalisationMode: the normalisation mode to apply to input data frames
201         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
202         """"""
203         super().__init__()
204         if modelKwArgs is None:
205             modelKwArgs = {}
206         if nnOptimiserParams is None:
207             nnOptimiserParams = {}
208         if ""lossEvaluator"" not in nnOptimiserParams:
209             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
210         self.normalisationMode = normalisationMode
211         self.nnOptimiserParams = nnOptimiserParams
212         self.modelClass = modelClass
213         self.modelArgs = modelArgs
214         self.modelKwArgs = modelKwArgs
215         self.model: Optional[VectorTorchModel] = None
216 
217     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
","Before: 209
After: 215",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,1757,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 44, 'argument_list': 4, '(': 6, ')': 6, ':': 7, 'block': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'typed_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 3, 'ellipsis': 1, ']': 3, 'default_parameter': 4, '=': 13, 'tuple': 1, 'none': 5, 'attribute': 10, '.': 10, 'expression_statement': 11, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, 'is': 2, 'assignment': 9, 'dictionary': 2, '{': 2, '}': 2, 'not in': 2, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6767251794386121,0.6719026514964068,"(tensor([0.9790]), tensor([0.9769]), tensor([0.9780]), tensor([0.9771]))"
"208         self.modelKwArgs = modelKwArgs
209         self.model: Optional[WrappedTorchVectorModule] = None
210 
211     def _createWrappedTorchVectorModule(self) -> WrappedTorchVectorModule:
212         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
213 
214     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","214         self.modelKwArgs = modelKwArgs
215         self.model: Optional[VectorTorchModel] = None
216 
217     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
218         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
219 
220     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 211
After: 217",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,1769,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 5, 'identifier': 16, '.': 5, '=': 2, ':': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, '->': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1, 'list_splat': 1, '*': 1, ',': 1, 'dictionary_splat': 1, '**': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6396187834117172,0.6031479059773904,"(tensor([0.9681]), tensor([0.9640]), tensor([0.9661]), tensor([0.9644]))"
"226 
227 
228 class TorchVectorClassificationModel(VectorClassificationModel):
229     def __init__(self, modelClass: Callable[..., WrappedTorchVectorModule], modelArgs=(), modelKwArgs=None,
230             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
231         """"""
232         :param modelClass: the constructor with which to create the wrapped torch vector model
233         :param modelArgs: the constructor argument list to pass to modelClass
234         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
235         :param normalisationMode: the normalisation mode to apply to input data frames
236         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
237         """"""
238         super().__init__()
239         if modelKwArgs is None:
240             modelKwArgs = {}
241         if nnOptimiserParams is None:
242             nnOptimiserParams = {}
243         if ""lossEvaluator"" not in nnOptimiserParams:
244             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
245         self.normalisationMode = normalisationMode
246         self.nnOptimiserParams = nnOptimiserParams
247         self.modelClass = modelClass
248         self.modelArgs = modelArgs
249         self.modelKwArgs = modelKwArgs
250         self.model: Optional[WrappedTorchVectorModule] = None
251 
252     def _createWrappedTorchVectorModule(self) -> WrappedTorchVectorModule:
","232 
233 
234 class TorchVectorClassificationModel(VectorClassificationModel):
235     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs=(), modelKwArgs=None,
236             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
237         """"""
238         :param modelClass: the constructor with which to create the wrapped torch vector model
239         :param modelArgs: the constructor argument list to pass to modelClass
240         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
241         :param normalisationMode: the normalisation mode to apply to input data frames
242         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
243         """"""
244         super().__init__()
245         if modelKwArgs is None:
246             modelKwArgs = {}
247         if nnOptimiserParams is None:
248             nnOptimiserParams = {}
249         if ""lossEvaluator"" not in nnOptimiserParams:
250             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
251         self.normalisationMode = normalisationMode
252         self.nnOptimiserParams = nnOptimiserParams
253         self.modelClass = modelClass
254         self.modelArgs = modelArgs
255         self.modelKwArgs = modelKwArgs
256         self.model: Optional[VectorTorchModel] = None
257 
258     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
","Before: 229
After: 235",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,2043,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 44, 'argument_list': 4, '(': 6, ')': 6, ':': 7, 'block': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'typed_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 3, 'ellipsis': 1, ']': 3, 'default_parameter': 4, '=': 13, 'tuple': 1, 'none': 5, 'attribute': 10, '.': 10, 'expression_statement': 11, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, 'is': 2, 'assignment': 9, 'dictionary': 2, '{': 2, '}': 2, 'not in': 2, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6767251794386121,0.6719026514964068,"(tensor([0.9805]), tensor([0.9782]), tensor([0.9793]), tensor([0.9784]))"
"226 
227 
228 class TorchVectorClassificationModel(VectorClassificationModel):
229     def __init__(self, modelClass: Callable[..., WrappedTorchVectorModule], modelArgs=(), modelKwArgs=None,
230             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
231         """"""
232         :param modelClass: the constructor with which to create the wrapped torch vector model
233         :param modelArgs: the constructor argument list to pass to modelClass
234         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
235         :param normalisationMode: the normalisation mode to apply to input data frames
236         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
237         """"""
238         super().__init__()
239         if modelKwArgs is None:
240             modelKwArgs = {}
241         if nnOptimiserParams is None:
242             nnOptimiserParams = {}
243         if ""lossEvaluator"" not in nnOptimiserParams:
244             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
245         self.normalisationMode = normalisationMode
246         self.nnOptimiserParams = nnOptimiserParams
247         self.modelClass = modelClass
248         self.modelArgs = modelArgs
249         self.modelKwArgs = modelKwArgs
250         self.model: Optional[WrappedTorchVectorModule] = None
251 
252     def _createWrappedTorchVectorModule(self) -> WrappedTorchVectorModule:
","232 
233 
234 class TorchVectorClassificationModel(VectorClassificationModel):
235     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs=(), modelKwArgs=None,
236             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
237         """"""
238         :param modelClass: the constructor with which to create the wrapped torch vector model
239         :param modelArgs: the constructor argument list to pass to modelClass
240         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
241         :param normalisationMode: the normalisation mode to apply to input data frames
242         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
243         """"""
244         super().__init__()
245         if modelKwArgs is None:
246             modelKwArgs = {}
247         if nnOptimiserParams is None:
248             nnOptimiserParams = {}
249         if ""lossEvaluator"" not in nnOptimiserParams:
250             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
251         self.normalisationMode = normalisationMode
252         self.nnOptimiserParams = nnOptimiserParams
253         self.modelClass = modelClass
254         self.modelArgs = modelArgs
255         self.modelKwArgs = modelKwArgs
256         self.model: Optional[VectorTorchModel] = None
257 
258     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
","Before: 250
After: 256",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,2199,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 44, 'argument_list': 4, '(': 6, ')': 6, ':': 7, 'block': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'typed_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 3, 'ellipsis': 1, ']': 3, 'default_parameter': 4, '=': 13, 'tuple': 1, 'none': 5, 'attribute': 10, '.': 10, 'expression_statement': 11, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, 'is': 2, 'assignment': 9, 'dictionary': 2, '{': 2, '}': 2, 'not in': 2, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6767251794386121,0.6719026514964068,"(tensor([0.9805]), tensor([0.9782]), tensor([0.9793]), tensor([0.9784]))"
"249         self.modelKwArgs = modelKwArgs
250         self.model: Optional[WrappedTorchVectorModule] = None
251 
252     def _createWrappedTorchVectorModule(self) -> WrappedTorchVectorModule:
253         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
254 
255     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
","255         self.modelKwArgs = modelKwArgs
256         self.model: Optional[VectorTorchModel] = None
257 
258     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
259         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
260 
261     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
","Before: 252
After: 258",add sensai abstraction for torch models,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,2211,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 5, 'identifier': 16, '.': 5, '=': 2, ':': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, '->': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1, 'list_splat': 1, '*': 1, ',': 1, 'dictionary_splat': 1, '**': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6616150890690988,0.6307566567649564,"(tensor([0.9687]), tensor([0.9652]), tensor([0.9669]), tensor([0.9655]))"
"67         mean = torch.mean(results, 0)
68         stddev = torch.std(results, 0, unbiased=False)
69         return mean, stddev
70 
71 
72 class MultiLayerPerceptronModule(MCDropoutCapableNNModule):
73     def __init__(self, inputDim, outputDim, hiddenDims, hidActivationFn=torch.sigmoid, outputActivationFn=torch.sigmoid,
74             pDropout=None):
75         super().__init__()
76         self.inputDim = inputDim
","67         mean = torch.mean(results, 0)
68         stddev = torch.std(results, 0, unbiased=False)
69         return mean, stddev
70 
71 
72 class MultiLayerPerceptron(MCDropoutCapableNNModule):
73     def __init__(self, inputDim, outputDim, hiddenDims, hidActivationFn=torch.sigmoid, outputActivationFn=torch.sigmoid,
74             pDropout=None):
75         super().__init__()
76         self.inputDim = inputDim
","Before: 72
After: 72",fix typo in src/sensai/torch/torch_modules.py,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,466,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'identifier': 27, '=': 6, 'call': 4, 'attribute': 5, '.': 5, 'argument_list': 5, '(': 6, ',': 10, 'integer': 2, ')': 6, 'keyword_argument': 1, 'false': 1, 'return_statement': 1, 'return': 1, 'expression_list': 1, 'class_definition': 1, 'class': 1, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 3, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 15, 'end_line': 18, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9601328622880726,0.9551379771098445,"(tensor([0.9980]), tensor([0.9973]), tensor([0.9976]), tensor([0.9973]))"
"13 import torch.optim as optim
14 from torch import cuda as torchcuda
15 
16 from .torch_data import TensorScaler, DataUtil, TorchDataSet, TorchDataSetProviderFromDataUtil, TorchDataSetProvider
17 if TYPE_CHECKING:
18     from .torch_base import WrappedTorchModule
19 
20 log = logging.getLogger(__name__)
21 
22 
","13 import torch.optim as optim
14 from torch import cuda as torchcuda
15 
16 from .torch_data import TensorScaler, DataUtil, TorchDataSet, TorchDataSetProviderFromDataUtil, TorchDataSetProvider
17 if TYPE_CHECKING:
18     from .torch_base import TorchModel
19 
20 log = logging.getLogger(__name__)
21 
22 
","Before: 18
After: 18",fix coding style in src/src/sensai/torch/torch_opt.py,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,137,"{'module': 1, 'import_statement': 1, 'import': 4, 'aliased_import': 2, 'dotted_name': 11, 'identifier': 19, '.': 4, 'as': 2, 'import_from_statement': 3, 'from': 3, 'relative_import': 2, 'import_prefix': 2, ',': 4, 'if_statement': 1, 'if': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9414356817218077,0.9375021646022259,"(tensor([0.9945]), tensor([0.9841]), tensor([0.9893]), tensor([0.9852]))"
"387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""WrappedTorchModule"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModel(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModelBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModelBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModelBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModule(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModuleBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModuleBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModuleBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","Before: 390
After: 390",fix coding style in src/src/sensai/torch/torch_opt.py,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,3441,"{'module': 1, 'expression_statement': 63, 'concatenated_string': 1, 'string': 18, 'string_start': 18, 'string_content': 28, 'interpolation': 12, '{': 12, 'attribute': 84, 'identifier': 303, '.': 84, '}': 12, 'string_end': 18, 'function_definition': 3, 'def': 3, 'parameters': 3, '(': 73, ',': 39, 'typed_parameter': 2, ':': 22, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 11, ']': 11, ')': 73, 'block': 20, 'call': 67, 'argument_list': 67, '->': 1, 'if_statement': 8, 'if': 8, 'return_statement': 2, 'return': 2, 'elif_clause': 1, 'elif': 1, 'comparison_operator': 3, '!=': 1, 'assignment': 37, '=': 43, 'list': 5, 'else_clause': 3, 'else': 3, 'list_comprehension': 3, 'for_in_clause': 3, 'for': 6, 'in': 6, 'comment': 8, 'none': 3, 'integer': 6, 'false': 1, 'true': 1, 'for_statement': 3, 'pattern_list': 3, 'binary_operator': 7, '+': 2, '%': 4, 'float': 1, 'keyword_argument': 6, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, '==': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'except_clause': 1, 'except': 1, 'tuple': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865262249215463,0.9856931894990255,"(tensor([0.9954]), tensor([0.9951]), tensor([0.9952]), tensor([0.9951]))"
"387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""WrappedTorchModule"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModel(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModelBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModelBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModelBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModule(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModuleBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModuleBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModuleBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","Before: 438
After: 438",fix coding style in src/src/sensai/torch/torch_opt.py,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,3908,"{'module': 1, 'expression_statement': 63, 'concatenated_string': 1, 'string': 18, 'string_start': 18, 'string_content': 28, 'interpolation': 12, '{': 12, 'attribute': 84, 'identifier': 303, '.': 84, '}': 12, 'string_end': 18, 'function_definition': 3, 'def': 3, 'parameters': 3, '(': 73, ',': 39, 'typed_parameter': 2, ':': 22, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 11, ']': 11, ')': 73, 'block': 20, 'call': 67, 'argument_list': 67, '->': 1, 'if_statement': 8, 'if': 8, 'return_statement': 2, 'return': 2, 'elif_clause': 1, 'elif': 1, 'comparison_operator': 3, '!=': 1, 'assignment': 37, '=': 43, 'list': 5, 'else_clause': 3, 'else': 3, 'list_comprehension': 3, 'for_in_clause': 3, 'for': 6, 'in': 6, 'comment': 8, 'none': 3, 'integer': 6, 'false': 1, 'true': 1, 'for_statement': 3, 'pattern_list': 3, 'binary_operator': 7, '+': 2, '%': 4, 'float': 1, 'keyword_argument': 6, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, '==': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'except_clause': 1, 'except': 1, 'tuple': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865262249215463,0.9856931894990255,"(tensor([0.9954]), tensor([0.9951]), tensor([0.9952]), tensor([0.9951]))"
"387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""WrappedTorchModule"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModel(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModelBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModelBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModelBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModule(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModuleBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModuleBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModuleBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","Before: 454
After: 454",fix coding style in src/src/sensai/torch/torch_opt.py,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,4082,"{'module': 1, 'expression_statement': 63, 'concatenated_string': 1, 'string': 18, 'string_start': 18, 'string_content': 28, 'interpolation': 12, '{': 12, 'attribute': 84, 'identifier': 303, '.': 84, '}': 12, 'string_end': 18, 'function_definition': 3, 'def': 3, 'parameters': 3, '(': 73, ',': 39, 'typed_parameter': 2, ':': 22, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 11, ']': 11, ')': 73, 'block': 20, 'call': 67, 'argument_list': 67, '->': 1, 'if_statement': 8, 'if': 8, 'return_statement': 2, 'return': 2, 'elif_clause': 1, 'elif': 1, 'comparison_operator': 3, '!=': 1, 'assignment': 37, '=': 43, 'list': 5, 'else_clause': 3, 'else': 3, 'list_comprehension': 3, 'for_in_clause': 3, 'for': 6, 'in': 6, 'comment': 8, 'none': 3, 'integer': 6, 'false': 1, 'true': 1, 'for_statement': 3, 'pattern_list': 3, 'binary_operator': 7, '+': 2, '%': 4, 'float': 1, 'keyword_argument': 6, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, '==': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'except_clause': 1, 'except': 1, 'tuple': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865262249215463,0.9856931894990255,"(tensor([0.9954]), tensor([0.9951]), tensor([0.9952]), tensor([0.9951]))"
"387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""WrappedTorchModule"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModel(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModelBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModelBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModelBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModule(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModuleBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModuleBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModuleBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","Before: 495
After: 495",fix coding style in src/src/sensai/torch/torch_opt.py,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,4535,"{'module': 1, 'expression_statement': 63, 'concatenated_string': 1, 'string': 18, 'string_start': 18, 'string_content': 28, 'interpolation': 12, '{': 12, 'attribute': 84, 'identifier': 303, '.': 84, '}': 12, 'string_end': 18, 'function_definition': 3, 'def': 3, 'parameters': 3, '(': 73, ',': 39, 'typed_parameter': 2, ':': 22, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 11, ']': 11, ')': 73, 'block': 20, 'call': 67, 'argument_list': 67, '->': 1, 'if_statement': 8, 'if': 8, 'return_statement': 2, 'return': 2, 'elif_clause': 1, 'elif': 1, 'comparison_operator': 3, '!=': 1, 'assignment': 37, '=': 43, 'list': 5, 'else_clause': 3, 'else': 3, 'list_comprehension': 3, 'for_in_clause': 3, 'for': 6, 'in': 6, 'comment': 8, 'none': 3, 'integer': 6, 'false': 1, 'true': 1, 'for_statement': 3, 'pattern_list': 3, 'binary_operator': 7, '+': 2, '%': 4, 'float': 1, 'keyword_argument': 6, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, '==': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'except_clause': 1, 'except': 1, 'tuple': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865262249215463,0.9856931894990255,"(tensor([0.9954]), tensor([0.9951]), tensor([0.9952]), tensor([0.9951]))"
"387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""WrappedTorchModule"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModel(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModelBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModelBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModelBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModule(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModuleBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModuleBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModuleBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","Before: 503
After: 503",fix coding style in src/src/sensai/torch/torch_opt.py,Changed naming of torch classes to achieve greater consistency.,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,a1897aca7c3cb1f7551adc3f01ccb0b69bb7733e,f43a2d1ec93b94354172b4accf292b827df01606,0,4597,"{'module': 1, 'expression_statement': 63, 'concatenated_string': 1, 'string': 18, 'string_start': 18, 'string_content': 28, 'interpolation': 12, '{': 12, 'attribute': 84, 'identifier': 303, '.': 84, '}': 12, 'string_end': 18, 'function_definition': 3, 'def': 3, 'parameters': 3, '(': 73, ',': 39, 'typed_parameter': 2, ':': 22, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 11, ']': 11, ')': 73, 'block': 20, 'call': 67, 'argument_list': 67, '->': 1, 'if_statement': 8, 'if': 8, 'return_statement': 2, 'return': 2, 'elif_clause': 1, 'elif': 1, 'comparison_operator': 3, '!=': 1, 'assignment': 37, '=': 43, 'list': 5, 'else_clause': 3, 'else': 3, 'list_comprehension': 3, 'for_in_clause': 3, 'for': 6, 'in': 6, 'comment': 8, 'none': 3, 'integer': 6, 'false': 1, 'true': 1, 'for_statement': 3, 'pattern_list': 3, 'binary_operator': 7, '+': 2, '%': 4, 'float': 1, 'keyword_argument': 6, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, '==': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'except_clause': 1, 'except': 1, 'tuple': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9865262249215463,0.9856931894990255,"(tensor([0.9954]), tensor([0.9951]), tensor([0.9952]), tensor([0.9951]))"
"72     def createTorchModule(self) -> torch.nn.Module:
73         pass
74 
75     def __getstate__(self):
76         state = dict(self.__dict__)
77         del state[""model""]
78         state[""modelBytes""] = self.getModuleBytes()
79         return state
80 
81     def __setstate__(self, d):
","72     def createTorchModule(self) -> torch.nn.Module:
73         pass
74 
75     def __getstate__(self):
76         state = dict(self.__dict__)
77         if ""model"" in state:
78             del state[""model""]
79         state[""modelBytes""] = self.getModuleBytes()
80         return state
81 
82     def __setstate__(self, d):
","Before: 77
After: 77, 78",fix model index,Fix in torch_base: deleting state attributes now happens safely,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,282a60895b14dc76b077a3aca66d4bdca3010966,27db3062e93a9fca56363f5f89885572254efbf5,0,616,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 16, 'parameters': 2, '(': 4, ')': 4, '->': 1, 'type': 1, 'attribute': 4, '.': 4, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'call': 2, 'argument_list': 2, 'delete_statement': 1, 'del': 1, 'subscript': 2, '[': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7375247166588506,0.7350648766570498,"(tensor([0.9719]), tensor([0.9863]), tensor([0.9790]), tensor([0.9848]))"
"1 from typing import Sequence
2 import logging
3 import lightgbm
4 import pandas as pd
5 
","1 from typing import Sequence, Union, Optional
2 import logging
3 import lightgbm
4 import pandas as pd
5 import re
","Before: 1
After: 1, 5, 7",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,9,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'dotted_name': 5, 'identifier': 6, 'import': 4, 'import_statement': 3, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6436045454907839,0.5667521832510313,"(tensor([0.9435]), tensor([0.9672]), tensor([0.9552]), tensor([0.9648]))"
"11 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
12     _log = _log.getChild(__qualname__)
13 
14     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
15         """"""
16         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
17             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
18             need not be specified (should be inferred automatically).
19             In general, passing categorical features is preferable to using one-hot encoding, for example.
20         :param random_state: the random seed to use
21         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
22         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
23         """"""
24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     _log = _log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31, **modelArgs):
17         """"""
18         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
19             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
20             need not be specified (should be inferred automatically).
21             In general, passing categorical features is preferable to using one-hot encoding, for example.
22         :param random_state: the random seed to use
23         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
24         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
25         """"""
26         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
27 
28         if type(categoricalFeatureNames) == str:
29             categoricalFeatureNameRegex = categoricalFeatureNames
30         else:
31             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
32                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
33             else:
34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 14
After: 16",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,111,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 27, 'argument_list': 4, '(': 5, ')': 5, ':': 3, 'block': 2, 'expression_statement': 4, 'assignment': 2, '=': 7, 'call': 3, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6135185733918942,0.6118695203608663,"(tensor([0.9217]), tensor([0.9703]), tensor([0.9454]), tensor([0.9652]))"
"11 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
12     _log = _log.getChild(__qualname__)
13 
14     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
15         """"""
16         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
17             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
18             need not be specified (should be inferred automatically).
19             In general, passing categorical features is preferable to using one-hot encoding, for example.
20         :param random_state: the random seed to use
21         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
22         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
23         """"""
24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     _log = _log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31, **modelArgs):
17         """"""
18         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
19             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
20             need not be specified (should be inferred automatically).
21             In general, passing categorical features is preferable to using one-hot encoding, for example.
22         :param random_state: the random seed to use
23         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
24         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
25         """"""
26         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
27 
28         if type(categoricalFeatureNames) == str:
29             categoricalFeatureNameRegex = categoricalFeatureNames
30         else:
31             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
32                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
33             else:
34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 24
After: 28, 29, 30, 31, 32, 33, 34, 35, 36",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,124,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 27, 'argument_list': 4, '(': 5, ')': 5, ':': 3, 'block': 2, 'expression_statement': 4, 'assignment': 2, '=': 7, 'call': 3, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6135185733918942,0.6118695203608663,"(tensor([0.9217]), tensor([0.9703]), tensor([0.9454]), tensor([0.9652]))"
"24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
28         if self.categoricalFeatureNames is not None:
29             cols = list(inputs.columns)
30             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
31             args = {""cat_column"": colIndices}
32             self._log.info(f""Updating model parameters with {args}"")
33             self.modelArgs.update(args)
34 
35 
","34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
38         if self._categoricalFeatureNameRegex is not None:
39             cols = list(inputs.columns)
40             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
41             colIndices = [cols.index(f) for f in categoricalFeatureNames]
42             args = {""cat_column"": colIndices}
43             self._log.info(f""Updating model parameters with {args}"")
44             self.modelArgs.update(args)
45 
46 
","Before: 28
After: 38",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,197,"{'module': 1, 'expression_statement': 7, 'assignment': 4, 'attribute': 14, 'identifier': 44, '.': 14, '=': 6, 'call': 6, 'argument_list': 6, '(': 7, ')': 7, ',': 5, 'keyword_argument': 2, 'dictionary_splat': 1, '**': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 2, 'pair': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '}': 2, 'interpolation': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5086759311424307,0.47617248354487524,"(tensor([0.9131]), tensor([0.8904]), tensor([0.9016]), tensor([0.8926]))"
"24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
28         if self.categoricalFeatureNames is not None:
29             cols = list(inputs.columns)
30             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
31             args = {""cat_column"": colIndices}
32             self._log.info(f""Updating model parameters with {args}"")
33             self.modelArgs.update(args)
34 
35 
","34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
38         if self._categoricalFeatureNameRegex is not None:
39             cols = list(inputs.columns)
40             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
41             colIndices = [cols.index(f) for f in categoricalFeatureNames]
42             args = {""cat_column"": colIndices}
43             self._log.info(f""Updating model parameters with {args}"")
44             self.modelArgs.update(args)
45 
46 
","Before: 30
After: 40, 41",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,234,"{'module': 1, 'expression_statement': 7, 'assignment': 4, 'attribute': 14, 'identifier': 44, '.': 14, '=': 6, 'call': 6, 'argument_list': 6, '(': 7, ')': 7, ',': 5, 'keyword_argument': 2, 'dictionary_splat': 1, '**': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 2, 'pair': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '}': 2, 'interpolation': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5086759311424307,0.47617248354487524,"(tensor([0.9131]), tensor([0.8904]), tensor([0.9016]), tensor([0.8926]))"
"36 class LightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
37     _log = _log.getChild(__qualname__)
38 
39     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
40         """"""
41         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical
42             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
43             need not be specified (should be inferred automatically, but we have never actually tested this behaviour
44             successfully for a classification model).
45             In general, passing categorical features may be preferable to using one-hot encoding, for example.
46         :param random_state: the random seed to use
47         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
48         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
49         """"""
50         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
51         self.categoricalFeatureNames = categoricalFeatureNames
52 
53     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","47 class LightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
48     _log = _log.getChild(__qualname__)
49 
50     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
51         """"""
52         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical
53             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
54             need not be specified (should be inferred automatically, but we have never actually tested this behaviour
55             successfully for a classification model).
56             In general, passing categorical features may be preferable to using one-hot encoding, for example.
57         :param random_state: the random seed to use
58         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
59         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
60         """"""
61         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
62 
63         if type(categoricalFeatureNames) == str:
64             categoricalFeatureNameRegex = categoricalFeatureNames
65         else:
66             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
67                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
68             else:
69                 categoricalFeatureNameRegex = None
70         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
71 
72     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 51
After: 62, 63, 64, 65, 66, 67, 68, 69, 70",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,387,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 27, 'argument_list': 4, '(': 5, ')': 5, ':': 3, 'block': 2, 'expression_statement': 4, 'assignment': 2, '=': 7, 'call': 3, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6470006300051246,0.644341048029572,"(tensor([0.9237]), tensor([0.9727]), tensor([0.9476]), tensor([0.9676]))"
"50         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
51         self.categoricalFeatureNames = categoricalFeatureNames
52 
53     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
54         if self.categoricalFeatureNames is not None:
55             cols = list(inputs.columns)
56             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
57             args = {""cat_column"": colIndices}
58             self._log.info(f""Updating model parameters with {args}"")
59             self.modelArgs.update(args)","69                 categoricalFeatureNameRegex = None
70         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
71 
72     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
73         if self._categoricalFeatureNameRegex is not None:
74             cols = list(inputs.columns)
75             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
76             colIndices = [cols.index(f) for f in categoricalFeatureNames]
77             args = {""cat_column"": colIndices}
78             self._log.info(f""Updating model parameters with {args}"")
79             self.modelArgs.update(args)","Before: 54
After: 73",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,426,"{'module': 1, 'expression_statement': 5, 'call': 4, 'attribute': 10, 'identifier': 36, 'argument_list': 4, '(': 5, ')': 5, '.': 10, ',': 5, 'keyword_argument': 2, '=': 6, 'dictionary_splat': 1, '**': 1, 'assignment': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 1, 'pair': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5153083957710737,0.47976065316646294,"(tensor([0.9127]), tensor([0.8895]), tensor([0.9009]), tensor([0.8918]))"
"50         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
51         self.categoricalFeatureNames = categoricalFeatureNames
52 
53     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
54         if self.categoricalFeatureNames is not None:
55             cols = list(inputs.columns)
56             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
57             args = {""cat_column"": colIndices}
58             self._log.info(f""Updating model parameters with {args}"")
59             self.modelArgs.update(args)","69                 categoricalFeatureNameRegex = None
70         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
71 
72     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
73         if self._categoricalFeatureNameRegex is not None:
74             cols = list(inputs.columns)
75             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
76             colIndices = [cols.index(f) for f in categoricalFeatureNames]
77             args = {""cat_column"": colIndices}
78             self._log.info(f""Updating model parameters with {args}"")
79             self.modelArgs.update(args)","Before: 56
After: 75, 76",add orregexgroup to lightgbmvectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,1,463,"{'module': 1, 'expression_statement': 5, 'call': 4, 'attribute': 10, 'identifier': 36, 'argument_list': 4, '(': 5, ')': 5, '.': 10, ',': 5, 'keyword_argument': 2, '=': 6, 'dictionary_splat': 1, '**': 1, 'assignment': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 1, 'pair': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5153083957710737,0.47976065316646294,"(tensor([0.9127]), tensor([0.8895]), tensor([0.9009]), tensor([0.8918]))"
"72     def createTorchModule(self) -> torch.nn.Module:
73         pass
74 
75     def __getstate__(self):
76         state = dict(self.__dict__)
77         if ""model"" in state:
78             del state[""model""]
79         state[""modelBytes""] = self.getModuleBytes()
80         return state
81 
82     def __setstate__(self, d):
","73     def createTorchModule(self) -> torch.nn.Module:
74         pass
75 
76     def __getstate__(self):
77         state = dict(self.__dict__)
78         del state[""module""]
79         state[""modelBytes""] = self.getModuleBytes()
80         return state
81 
82     def __setstate__(self, d):
","Before: 77, 78
After: 78",add best epoch to the model and vectortorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,617,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 4, ')': 4, '->': 1, 'type': 1, 'attribute': 4, '.': 4, ':': 3, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'call': 2, 'argument_list': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'in': 1, 'delete_statement': 1, 'del': 1, 'subscript': 2, '[': 2, ']': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.689082319864453,0.6839924817613401,"(tensor([0.9760]), tensor([0.9609]), tensor([0.9684]), tensor([0.9623]))"
"192 
193 
194 class TorchVectorRegressionModel(VectorRegressionModel):
195     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs=(), modelKwArgs=None,
196             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
197         """"""
198         :param modelClass: the constructor with which to create the wrapped torch vector model
199         :param modelArgs: the constructor argument list to pass to modelClass
200         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
201         :param normalisationMode: the normalisation mode to apply to input data frames
202         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
203         """"""
204         super().__init__()
205         if modelKwArgs is None:
206             modelKwArgs = {}
207         if nnOptimiserParams is None:
208             nnOptimiserParams = {}
209         if ""lossEvaluator"" not in nnOptimiserParams:
210             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
211         self.normalisationMode = normalisationMode
212         self.nnOptimiserParams = nnOptimiserParams
213         self.modelClass = modelClass
214         self.modelArgs = modelArgs
215         self.modelKwArgs = modelKwArgs
216         self.model: Optional[VectorTorchModel] = None
217 
218     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
","193 
194 
195 class TorchVectorRegressionModel(VectorRegressionModel):
196     def __init__(self, modelClass: Callable[..., TorchModel], modelArgs=(), modelKwArgs=None,
197             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
198         """"""
199         :param modelClass: the constructor with which to create the wrapped torch vector model
200         :param modelArgs: the constructor argument list to pass to modelClass
201         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
202         :param normalisationMode: the normalisation mode to apply to input data frames
203         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
204         """"""
205         super().__init__()
206         if modelKwArgs is None:
207             modelKwArgs = {}
208         if nnOptimiserParams is None:
209             nnOptimiserParams = {}
210         if ""lossEvaluator"" not in nnOptimiserParams:
211             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
212         self.normalisationMode = normalisationMode
213         self.nnOptimiserParams = nnOptimiserParams
214         self.modelClass = modelClass
215         self.modelArgs = modelArgs
216         self.modelKwArgs = modelKwArgs
217         self.model: Optional[TorchModel] = None
218 
219     def _createWrappedTorchVectorModule(self) -> TorchModel:
","Before: 195
After: 196",add best epoch to the model and vectortorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1654,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 44, 'argument_list': 4, '(': 6, ')': 6, ':': 7, 'block': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'typed_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 3, 'ellipsis': 1, ']': 3, 'default_parameter': 4, '=': 13, 'tuple': 1, 'none': 5, 'attribute': 10, '.': 10, 'expression_statement': 11, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, 'is': 2, 'assignment': 9, 'dictionary': 2, '{': 2, '}': 2, 'not in': 2, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6915643966211195,0.6894415056615404,"(tensor([0.9877]), tensor([0.9856]), tensor([0.9867]), tensor([0.9858]))"
"192 
193 
194 class TorchVectorRegressionModel(VectorRegressionModel):
195     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs=(), modelKwArgs=None,
196             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
197         """"""
198         :param modelClass: the constructor with which to create the wrapped torch vector model
199         :param modelArgs: the constructor argument list to pass to modelClass
200         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
201         :param normalisationMode: the normalisation mode to apply to input data frames
202         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
203         """"""
204         super().__init__()
205         if modelKwArgs is None:
206             modelKwArgs = {}
207         if nnOptimiserParams is None:
208             nnOptimiserParams = {}
209         if ""lossEvaluator"" not in nnOptimiserParams:
210             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
211         self.normalisationMode = normalisationMode
212         self.nnOptimiserParams = nnOptimiserParams
213         self.modelClass = modelClass
214         self.modelArgs = modelArgs
215         self.modelKwArgs = modelKwArgs
216         self.model: Optional[VectorTorchModel] = None
217 
218     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
","193 
194 
195 class TorchVectorRegressionModel(VectorRegressionModel):
196     def __init__(self, modelClass: Callable[..., TorchModel], modelArgs=(), modelKwArgs=None,
197             normalisationMode=NormalisationMode.NONE, nnOptimiserParams=None):
198         """"""
199         :param modelClass: the constructor with which to create the wrapped torch vector model
200         :param modelArgs: the constructor argument list to pass to modelClass
201         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
202         :param normalisationMode: the normalisation mode to apply to input data frames
203         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
204         """"""
205         super().__init__()
206         if modelKwArgs is None:
207             modelKwArgs = {}
208         if nnOptimiserParams is None:
209             nnOptimiserParams = {}
210         if ""lossEvaluator"" not in nnOptimiserParams:
211             nnOptimiserParams[""lossEvaluator""] = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
212         self.normalisationMode = normalisationMode
213         self.nnOptimiserParams = nnOptimiserParams
214         self.modelClass = modelClass
215         self.modelArgs = modelArgs
216         self.modelKwArgs = modelKwArgs
217         self.model: Optional[TorchModel] = None
218 
219     def _createWrappedTorchVectorModule(self) -> TorchModel:
","Before: 216
After: 217",add best epoch to the model and vectortorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1810,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 44, 'argument_list': 4, '(': 6, ')': 6, ':': 7, 'block': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'typed_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 3, 'ellipsis': 1, ']': 3, 'default_parameter': 4, '=': 13, 'tuple': 1, 'none': 5, 'attribute': 10, '.': 10, 'expression_statement': 11, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, 'is': 2, 'assignment': 9, 'dictionary': 2, '{': 2, '}': 2, 'not in': 2, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6915643966211195,0.6894415056615404,"(tensor([0.9877]), tensor([0.9856]), tensor([0.9867]), tensor([0.9858]))"
"215         self.modelKwArgs = modelKwArgs
216         self.model: Optional[VectorTorchModel] = None
217 
218     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
219         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
220 
221     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","216         self.modelKwArgs = modelKwArgs
217         self.model: Optional[TorchModel] = None
218 
219     def _createWrappedTorchVectorModule(self) -> TorchModel:
220         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
221 
222     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
","Before: 218
After: 219, 222, 223, 224, 225",add best epoch to the model and vectortorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1822,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 5, 'identifier': 16, '.': 5, '=': 2, ':': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, '->': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1, 'list_splat': 1, '*': 1, ',': 1, 'dictionary_splat': 1, '**': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5665679555691797,0.5441799557959847,"(tensor([0.9577]), tensor([0.9679]), tensor([0.9628]), tensor([0.9669]))"
"218     def _createWrappedTorchVectorModule(self) -> VectorTorchModel:
219         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
220 
221     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
222         self.model = self._createWrappedTorchVectorModule()
223         dataUtil = VectorDataUtil(inputs, outputs, self.model.cuda, normalisationMode=self.normalisationMode)
224         dataSetProvider = TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
225         self.model.fit(dataSetProvider, **self.nnOptimiserParams)
226 
227     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
","223         dataUtil = VectorDataUtil(inputs, outputs, self.model.cuda, normalisationMode=self.normalisationMode)
224         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
225 
226     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
227         self.model = self._createWrappedTorchVectorModule()
228         dataSetProvider = self._createDataSetProvider(inputs, outputs)
229         self.model.fit(dataSetProvider, **self.nnOptimiserParams)
230 
231     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","Before: 223, 224
After: 228, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240",add best epoch to the model and vectortorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1915,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 43, 'parameters': 2, '(': 7, ')': 7, '->': 1, 'type': 3, ':': 4, 'block': 2, 'return_statement': 1, 'return': 1, 'call': 5, 'attribute': 15, '.': 15, 'argument_list': 5, 'list_splat': 1, '*': 1, ',': 8, 'dictionary_splat': 2, '**': 2, 'typed_parameter': 2, 'expression_statement': 4, 'assignment': 3, '=': 4, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5625928959662233,0.5404218811465544,"(tensor([0.9270]), tensor([0.9191]), tensor([0.9230]), tensor([0.9199]))"
"224         dataSetProvider = TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
225         self.model.fit(dataSetProvider, **self.nnOptimiserParams)
226 
227     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
228         yArray = self.model.applyScaled(inputs.values)
229         return pd.DataFrame(yArray, columns=self.getModelOutputVariableNames())
230 
231     def __str__(self):
","238             i += batchSize
239         return np.concatenate(results)
240 
241     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
242         yArray = self._predictOutputsForInputDataFrame(inputs)
243         return pd.DataFrame(yArray, columns=self.getModelOutputVariableNames())
244 
245     def __str__(self):
","Before: 228
After: 242",add best epoch to the model and vectortorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1996,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'identifier': 31, '=': 3, 'call': 5, 'argument_list': 5, '(': 6, ',': 4, 'attribute': 12, '.': 12, ')': 6, 'dictionary_splat': 1, '**': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 2, '->': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 34, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.43330502413729466,0.35102906433108927,"(tensor([0.8738]), tensor([0.8376]), tensor([0.8553]), tensor([0.8411]))"
"269     def getInputTensorScaler(self) -> TensorScaler:
270         return self.inputTensorScaler
271 
272     def getModelOutputDim(self) -> Optional[int]:
273         """"""
274         :return: the number of output dimensions that would be required to be generated by the model to match this dataset.
275         """"""
276         return self.modelOutputDim
277 
278     def getInputDim(self) -> Optional[int]:
","269     def getInputTensorScaler(self) -> TensorScaler:
270         return self.inputTensorScaler
271 
272     def getModelOutputDim(self) -> int:
273         """"""
274         :return: the number of output dimensions that would be required to be generated by the model to match this dataset.
275         """"""
276         return self.modelOutputDim
277 
278     def getInputDim(self) -> Optional[int]:
","Before: 272
After: 272",fix typo in src/sensai/torch/torch_data.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,2269,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 11, 'parameters': 2, '(': 2, ')': 2, '->': 2, 'type': 3, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'attribute': 2, '.': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9290044244135681,0.9287157792258922,"(tensor([0.9954]), tensor([0.9943]), tensor([0.9948]), tensor([0.9944]))"
"138         pass
139 
140     @abstractmethod
141     def startTraining(self, cuda):
142         """"""Prepares for a new training process, initialising internal state as required""""""
143         pass
144 
145     @abstractmethod
","138         pass
139 
140     @abstractmethod
141     def startTraining(self, cuda) -> ""NNLossEvaluatorState"":
142         """"""Prepares for a new training process, initialising internal state as required""""""
143         pass
144 
145     def getValidationMetricName(self) -> str:
","Before: 141
After: 141",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1131,"{'module': 1, 'pass_statement': 2, 'pass': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, ')': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6918211695197932,0.6844289054610126,"(tensor([0.8778]), tensor([0.9819]), tensor([0.9269]), tensor([0.9704]))"
"140     @abstractmethod
141     def startTraining(self, cuda):
142         """"""Prepares for a new training process, initialising internal state as required""""""
143         pass
144 
145     @abstractmethod
146     def startValidationCollection(self, groundTruthShape):
147         """"""
148         Initiates validation data collection for a new epoch
149 
","142         """"""Prepares for a new training process, initialising internal state as required""""""
143         pass
144 
145     def getValidationMetricName(self) -> str:
146         """"""
147         Gets the name of the metric (key of dictionary as returned by endValidationCollection), which
148         is defining for the quality of the model
149 
150         :return: the name of the metrics that is indicated of model quality
151         """"""
152         pass
153 
154 
","Before: 145, 146
After: 145",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1142,"{'module': 1, 'decorated_definition': 2, 'decorator': 2, '@': 2, 'identifier': 15, 'function_definition': 2, 'def': 2, 'parameters': 2, '(': 2, ',': 2, ')': 2, ':': 2, 'block': 2, 'expression_statement': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'pass_statement': 1, 'pass': 1, 'ERROR': 2, 'for': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3012549490029001,0.28065497883737084,"(tensor([0.7997]), tensor([0.8434]), tensor([0.8210]), tensor([0.8388]))"
"143         pass
144 
145     @abstractmethod
146     def startValidationCollection(self, groundTruthShape):
147         """"""
148         Initiates validation data collection for a new epoch
149 
150         :param groundTruthShape: the tensor shape of a single ground truth data point
151         """"""
152         pass
153 
154     @abstractmethod
","142         """"""Prepares for a new training process, initialising internal state as required""""""
143         pass
144 
145     def getValidationMetricName(self) -> str:
146         """"""
147         Gets the name of the metric (key of dictionary as returned by endValidationCollection), which
148         is defining for the quality of the model
149 
150         :return: the name of the metrics that is indicated of model quality
151         """"""
152         pass
153 
154 
","Before: 148
After: 147, 148",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1164,"{'module': 1, 'pass_statement': 2, 'pass': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, ')': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.164590064801235,0.14716454767100157,"(tensor([0.7834]), tensor([0.8062]), tensor([0.7946]), tensor([0.8039]))"
"143         pass
144 
145     @abstractmethod
146     def startValidationCollection(self, groundTruthShape):
147         """"""
148         Initiates validation data collection for a new epoch
149 
150         :param groundTruthShape: the tensor shape of a single ground truth data point
151         """"""
152         pass
153 
154     @abstractmethod
","142         """"""Prepares for a new training process, initialising internal state as required""""""
143         pass
144 
145     def getValidationMetricName(self) -> str:
146         """"""
147         Gets the name of the metric (key of dictionary as returned by endValidationCollection), which
148         is defining for the quality of the model
149 
150         :return: the name of the metrics that is indicated of model quality
151         """"""
152         pass
153 
154 
","Before: 150
After: 150",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1178,"{'module': 1, 'pass_statement': 2, 'pass': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, ')': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.164590064801235,0.14716454767100157,"(tensor([0.7834]), tensor([0.8062]), tensor([0.7946]), tensor([0.8039]))"
"149 
150         :param groundTruthShape: the tensor shape of a single ground truth data point
151         """"""
152         pass
153 
154     @abstractmethod
155     def collectValidationResultBatch(self, output, groundTruth):
156         """"""
157         Collects, for validation, the given output and ground truth data (tensors holding data on one batch,
158         where the first dimensions is the batch)
","150         :return: the name of the metrics that is indicated of model quality
151         """"""
152         pass
153 
154 
155     class State(ABC):
156         @abstractmethod
157         def startValidationCollection(self, groundTruthShape):
158             """"""
159             Initiates validation data collection for a new epoch
","Before: 154, 155, 156, 157, 158, 160, 161, 162, 163, 164
After: 155, 156, 157, 158, 159",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1164,"{'module': 1, 'ERROR': 7, ':': 2, 'identifier': 27, 'expression_statement': 2, 'assignment': 1, 'type': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ',': 3, 'and': 1, '(': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.1785601025043808,0.1603810271442908,"(tensor([0.8326]), tensor([0.8049]), tensor([0.8185]), tensor([0.8076]))"
"161         :param groundTruth: the corresponding ground truth
162         :return:
163         """"""
164         pass
165 
166     @abstractmethod
167     def endValidationCollection(self) -> OrderedDict:
168         """"""
169         Computes validation metrics based on the data previously collected.
170 
","154 
155     class State(ABC):
156         @abstractmethod
157         def startValidationCollection(self, groundTruthShape):
158             """"""
159             Initiates validation data collection for a new epoch
160 
161             :param groundTruthShape: the tensor shape of a single ground truth data point
162             """"""
163             pass
164 
165         @abstractmethod
","Before: 166, 167, 168, 169
After: 161, 162, 163",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1188,"{'module': 1, 'ERROR': 3, ':': 4, 'identifier': 17, 'expression_statement': 3, 'assignment': 2, 'type': 4, 'constrained_type': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '.': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",3.128723715773725e-78,1.9674535800430757e-78,"(tensor([0.8177]), tensor([0.8142]), tensor([0.8160]), tensor([0.8145]))"
"164         pass
165 
166     @abstractmethod
167     def endValidationCollection(self) -> OrderedDict:
168         """"""
169         Computes validation metrics based on the data previously collected.
170 
171         :return: an ordered dictionary with validation metrics
172         """"""
173         pass
174 
175     def getValidationMetricName(self) -> str:
","160 
161             :param groundTruthShape: the tensor shape of a single ground truth data point
162             """"""
163             pass
164 
165         @abstractmethod
166         def collectValidationResultBatch(self, output, groundTruth):
167             """"""
168             Collects, for validation, the given output and ground truth data (tensors holding data on one batch,
169             where the first dimensions is the batch)
","Before: 171, 172, 173
After: 165, 166, 167, 168, 169",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1230,"{'module': 1, 'pass_statement': 2, 'pass': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ')': 1, '->': 1, 'type': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",2.160824808378145e-78,1.533024971925679e-78,"(tensor([0.7410]), tensor([0.7751]), tensor([0.7577]), tensor([0.7716]))"
"172         """"""
173         pass
174 
175     def getValidationMetricName(self) -> str:
176         """"""
177         Gets the name of the metric (key of dictionary as returned by endValidationCollection), which
178         is defining for the quality of the model
179 
180         :return: the name of the metrics that is indicated of model quality
181         """"""
182         pass
183 
184 
","163             pass
164 
165         @abstractmethod
166         def collectValidationResultBatch(self, output, groundTruth):
167             """"""
168             Collects, for validation, the given output and ground truth data (tensors holding data on one batch,
169             where the first dimensions is the batch)
170 
171             :param output: the model's output
172             :param groundTruth: the corresponding ground truth
173             :return:
174             """"""
175             pass
176 
177         @abstractmethod
","Before: 175, 176, 177, 178
After: 171, 172, 173, 174, 175",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1219,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 8, 'identifier': 30, 'call': 1, 'argument_list': 1, '(': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, ')': 1, ',': 1, 'for': 1, ':': 2, 'return_statement': 1, 'return': 1, 'comparison_operator': 1, 'is': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",2.426681203221825e-78,1.4781425952290532e-78,"(tensor([0.7338]), tensor([0.7553]), tensor([0.7444]), tensor([0.7531]))"
"172         """"""
173         pass
174 
175     def getValidationMetricName(self) -> str:
176         """"""
177         Gets the name of the metric (key of dictionary as returned by endValidationCollection), which
178         is defining for the quality of the model
179 
180         :return: the name of the metrics that is indicated of model quality
181         """"""
182         pass
183 
184 
","172             :param groundTruth: the corresponding ground truth
173             :return:
174             """"""
175             pass
176 
177         @abstractmethod
178         def endValidationCollection(self) -> OrderedDict:
179             """"""
180             Computes validation metrics based on the data previously collected.
181 
","Before: 180, 181, 182
After: 177, 178, 179, 180, 181, 182, 183, 184",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1273,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 8, 'identifier': 30, 'call': 1, 'argument_list': 1, '(': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, ')': 1, ',': 1, 'for': 1, ':': 2, 'return_statement': 1, 'return': 1, 'comparison_operator': 1, 'is': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.10533146936323341,0.08400151839739997,"(tensor([0.7912]), tensor([0.7909]), tensor([0.7910]), tensor([0.7909]))"
"191         MSELOSS = ""MSELoss""
192         SMOOTHL1LOSS = ""SmoothL1Loss""
193 
194     def __init__(self, lossFn: LossFunction):
195         if lossFn is None:
196             lossFn = self.LossFunction.L2LOSS
197         try:
198             self.lossFn = self.LossFunction(lossFn)
199         except ValueError:
200             raise Exception(f""Loss function {lossFn} not supported. Available are: {[e.value for e in self.LossFunction]}"")
201 
202         # transient members: state for validation
203         self.total_loss_l1 = None
204         self.total_loss_l2 = None
205         self.outputDims = None
206         self.allTrueOutputs = None
207 
208     def __str__(self):
","204     def __str__(self):
205         return f""{self.__class__.__name__}[{self.lossFn}]""
206 
207     def startTraining(self, cuda):
208         return self.State(cuda)
209 
210     def getTrainingCriterion(self):
","Before: 202, 203, 204, 205, 206, 207, 212, 213, 214, 215, 216
After: 208",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1375,"{'module': 1, 'expression_statement': 8, 'assignment': 8, 'identifier': 32, '=': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 1, 'typed_parameter': 1, ':': 5, 'type': 1, ')': 3, 'block': 4, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 5, 'attribute': 10, '.': 10, 'try_statement': 1, 'try': 1, 'call': 2, 'argument_list': 2, 'except_clause': 1, 'except': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'comment': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.043557302208200255,0.039695425268897334,"(tensor([0.7518]), tensor([0.6713]), tensor([0.7093]), tensor([0.6785]))"
"226             raise AssertionError(f""Loss function {self.lossFn} defined but instantiation not implemented."")
227         return criterion
228 
229     def startValidationCollection(self, groundTruthShape):
230         if len(groundTruthShape) != 1:
231             raise ValueError(""Outputs that are not vectors are currently unsupported"")
232         self.outputDims = groundTruthShape[-1]
233         self.total_loss_l1 = np.zeros(self.outputDims)
234         self.total_loss_l2 = np.zeros(self.outputDims)
235         self.allTrueOutputs = None
236 
237     def collectValidationResultBatch(self, output, groundTruth):
","216             criterion = nn.SmoothL1Loss(reduction='sum')
217         else:
218             raise AssertionError(f""Loss function {self.lossFn} defined but instantiation not implemented."")
219         return criterion
220 
221     class State(NNLossEvaluator.State):
222         def __init__(self, cuda: bool):
223             self.total_loss_l1 = None
224             self.total_loss_l2 = None
225             self.outputDims = None
","Before: 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272
After: 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1705,"{'module': 1, 'raise_statement': 2, 'raise': 2, 'call': 5, 'identifier': 27, 'argument_list': 5, '(': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'interpolation': 1, '{': 1, 'attribute': 9, '.': 9, '}': 1, 'string_end': 2, ')': 6, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'integer': 2, 'expression_statement': 4, 'assignment': 4, '=': 4, 'subscript': 1, '[': 1, 'unary_operator': 1, '-': 1, ']': 1, 'none': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.22725179880576868,0.2114782076422686,"(tensor([0.8491]), tensor([0.8101]), tensor([0.8291]), tensor([0.8139]))"
"286     class LossFunction(Enum):
287         CROSSENTROPY = ""CrossEntropy""
288 
289     def __init__(self, lossFn: LossFunction):
290         if lossFn is None:
291             lossFn = self.LossFunction.CROSSENTROPY
292         try:
293             self.lossFn = self.LossFunction(lossFn)
294         except ValueError:
295             raise Exception(f""Loss function {lossFn} not supported. Available are: {[e.value for e in self.LossFunction]}"")
296 
297         # transient members: state for validation
298         self.totalLossCE = None
299         self.numValidationSamples = None
300 
301     def __str__(self):
","301     def __str__(self):
302         return f""{self.__class__.__name__}[{self.lossFn}]""
303 
304     def startTraining(self, cuda):
305         return self.State(cuda)
306 
307     def getTrainingCriterion(self):
","Before: 297, 298, 299, 300, 305, 306, 307
After: 305",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,2586,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 29, 'argument_list': 3, '(': 4, ')': 4, ':': 6, 'block': 5, 'expression_statement': 5, 'assignment': 5, '=': 5, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, 'type': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 3, 'attribute': 8, '.': 8, 'try_statement': 1, 'try': 1, 'call': 2, 'except_clause': 1, 'except': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'comment': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.05840806651080677,0.04752432799623216,"(tensor([0.7600]), tensor([0.6871]), tensor([0.7217]), tensor([0.6937]))"
"313             raise AssertionError(f""Loss function {self.lossFn} defined but instantiation not implemented."")
314         return criterion
315 
316     def startValidationCollection(self, groundTruthShape):
317         self.totalLossCE = 0
318         self.numValidationSamples = 0
319 
320     def collectValidationResultBatch(self, output, groundTruth):
","309             criterion = nn.CrossEntropyLoss(reduction='sum')
310         else:
311             raise AssertionError(f""Loss function {self.lossFn} defined but instantiation not implemented."")
312         return criterion
313 
314     class State(NNLossEvaluator.State):
315         def __init__(self, cuda: bool):
316             self.totalLossCE = None
317             self.numValidationSamples = None
318             self.evaluateCE = nn.CrossEntropyLoss(reduction=""sum"")
","Before: 316, 317, 318
After: 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,2773,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 1, 'identifier': 11, 'argument_list': 1, '(': 2, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 1, '{': 1, 'attribute': 3, '.': 3, '}': 1, 'string_end': 1, ')': 2, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 1, 'block': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'integer': 2}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.32271659286344573,0.314876994112188,"(tensor([0.8067]), tensor([0.8562]), tensor([0.8307]), tensor([0.8510]))"
"317         self.totalLossCE = 0
318         self.numValidationSamples = 0
319 
320     def collectValidationResultBatch(self, output, groundTruth):
321         self.totalLossCE += self.evaluateCE(output, groundTruth).item()
322         self.numValidationSamples += output.shape[0]
323 
324     def endValidationCollection(self):
","323             self.totalLossCE = 0
324             self.numValidationSamples = 0
325 
326         def collectValidationResultBatch(self, output, groundTruth):
327             self.totalLossCE += self.evaluateCE(output, groundTruth).item()
328             self.numValidationSamples += output.shape[0]
329 
330         def endValidationCollection(self):
","Before: 320, 321, 322
After: 326, 327, 328",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,2802,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'attribute': 7, 'identifier': 19, '.': 7, '=': 2, 'integer': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 3, ')': 3, ':': 1, 'block': 1, 'augmented_assignment': 2, '+=': 2, 'call': 2, 'argument_list': 2, 'subscript': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6506388533301984,0.6189228052361917,"(tensor([0.9710]), tensor([0.9700]), tensor([0.9705]), tensor([0.9701]))"
"321         self.totalLossCE += self.evaluateCE(output, groundTruth).item()
322         self.numValidationSamples += output.shape[0]
323 
324     def endValidationCollection(self):
325         ce = self.totalLossCE / self.numValidationSamples
326         metrics = OrderedDict([(""CE"", ce), (""GeoMeanProbTrueClass"", math.exp(-ce))])
327         return metrics
328 
329     def getValidationMetricName(self):
","327             self.totalLossCE += self.evaluateCE(output, groundTruth).item()
328             self.numValidationSamples += output.shape[0]
329 
330         def endValidationCollection(self):
331             ce = self.totalLossCE / self.numValidationSamples
332             metrics = OrderedDict([(""CE"", ce), (""GeoMeanProbTrueClass"", math.exp(-ce))])
333             return metrics
334 
335     def getValidationMetricName(self):
","Before: 324, 325, 326, 327
After: 330, 331, 332, 333",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,2851,"{'module': 1, 'expression_statement': 4, 'augmented_assignment': 2, 'attribute': 8, 'identifier': 25, '.': 8, '+=': 2, 'call': 4, 'argument_list': 4, '(': 7, ',': 4, ')': 7, 'subscript': 1, '[': 2, 'integer': 1, ']': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'assignment': 2, '=': 2, 'binary_operator': 1, '/': 1, 'list': 1, 'tuple': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'unary_operator': 1, '-': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7417049041520257,0.7247118381595946,"(tensor([0.9725]), tensor([0.9733]), tensor([0.9729]), tensor([0.9732]))"
"387             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
388             f""optimiserArgs={self.optimiserArgs}]""
389 
390     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
391         self.log.info(f""Learning parameters of {model} via {self}"")
392 
393         def toDataSetProvider(d) -> TorchDataSetProvider:
394             if isinstance(d, TorchDataSetProvider):
395                 return d
396             elif isinstance(d, DataUtil):
397                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
398 
399         if type(data) != list:
400             dataSetProviders = [toDataSetProvider(data)]
401         else:
402             dataSetProviders = [toDataSetProvider(item) for item in data]
403 
404         # initialise data to be generated
405         self.trainingLog = []
406         self.bestEpoch = None
407 
408         def trainingLog(s):
409             self.log.info(s)
410             self.trainingLog.append(s)
411 
412         self._init_cuda()
413 
414         # Set the random seed manually for reproducibility.
415         seed = 42
416         torch.manual_seed(seed)
417         if self.cuda:
418             torchcuda.manual_seed_all(seed)
419         torch.backends.cudnn.benchmark = False
420         torch.backends.cudnn.deterministic = True
421 
422         # obtain data, splitting it into training and validation set(s)
423         validationSets = []
424         trainingSets = []
425         outputScalers = []
426         self.log.info(""Obtaining input/output training instances"")
427         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
428             outputScalers.append(dataSetProvider.getOutputTensorScaler())
429             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
430             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
431             validationSets.append(valS)
432             trainingSets.append(trainS)
433         trainingLog(""Number of validation sets: %d"" % len(validationSets))
434 
435         torchModel = model.createTorchModule()
436         if self.cuda:
437             torchModel.cuda()
438         model.setTorchModule(torchModel)
439 
440         nParams = sum([p.nelement() for p in torchModel.parameters()])
441         trainingLog('Number of parameters: %d' % nParams)
442 
443         criterion = self.lossEvaluator.getTrainingCriterion()
444 
445         if self.cuda:
446             criterion = criterion.cuda()
447 
448         best_val = 1e9
449         best_epoch = 0
450         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
451             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
452             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
453 
454         bestModelBytes = model.getModuleBytes()
455         self.lossEvaluator.startTraining(self.cuda)
456         validationMetricName = self.lossEvaluator.getValidationMetricName()
457         try:
458             self.log.info('Begin training')
459             self.log.info('Press Ctrl+C to end training early')
460             for epoch in range(1, self.epochs + 1):
461                 epoch_start_time = time.time()
462 
463                 # perform training step, processing all the training data once
464                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
465 
466                 # perform validation, computing the mean metrics across all validation sets (if more than one)
467                 metricsSum = None
468                 metricsKeys = None
469                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
470                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
471                     metricsArray = np.array(list(metrics.values()))
472                     if i == 0:
473                         metricsSum = metricsArray
474                         metricsKeys = metrics.keys()
475                     else:
476                         metricsSum += metricsArray
477                 metricsSum /= len(validationSets)  # mean results
478                 metrics = dict(zip(metricsKeys, metricsSum))
479 
480                 # check for new best result according to validation results
481                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
482                 isNewBest = current_val < best_val
483                 if isNewBest:
484                     best_val = current_val
485                     best_epoch = epoch
486                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
487                 else:
488                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
489                 trainingLog(
490                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
491                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
492                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
493                         bestStr))
494                 if isNewBest:
495                     bestModelBytes = model.getModuleBytes()
496             trainingLog(""Training complete"")
497         except KeyboardInterrupt:
498             trainingLog('Exiting from training early')
499         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
500         self.bestEpoch = best_epoch
501 
502         # reload best model
503         model.setModuleBytes(bestModelBytes)
504 
505     def getTrainingLog(self):
","394             f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
395             f""optimiserArgs={self.optimiserArgs}]""
396 
397     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider]):
398         self.log.info(f""Learning parameters of {model} via {self}"")
399 
400         def toDataSetProvider(d) -> TorchDataSetProvider:
401             if isinstance(d, TorchDataSetProvider):
402                 return d
403             elif isinstance(d, DataUtil):
404                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
405 
406         if type(data) != list:
407             dataSetProviders = [toDataSetProvider(data)]
408         else:
409             dataSetProviders = [toDataSetProvider(item) for item in data]
410 
411         # initialise data to be generated
412         self.trainingLog = []
413         self.bestEpoch = None
414 
415         def trainingLog(s):
416             self.log.info(s)
417             self.trainingLog.append(s)
418 
419         self._init_cuda()
420 
421         # Set the random seed manually for reproducibility.
422         seed = 42
423         torch.manual_seed(seed)
424         if self.cuda:
425             torchcuda.manual_seed_all(seed)
426         torch.backends.cudnn.benchmark = False
427         torch.backends.cudnn.deterministic = True
428 
429         # obtain data, splitting it into training and validation set(s)
430         validationSets = []
431         trainingSets = []
432         outputScalers = []
433         self.log.info(""Obtaining input/output training instances"")
434         for idxDataSetProvider, dataSetProvider in enumerate(dataSetProviders):
435             outputScalers.append(dataSetProvider.getOutputTensorScaler())
436             trainS, valS = dataSetProvider.provideSplit(self.trainFraction)
437             trainingLog(f""Data set {idxDataSetProvider+1}/{len(dataSetProviders)}: #train={trainS.size()}, #validation={valS.size()}"")
438             validationSets.append(valS)
439             trainingSets.append(trainS)
440         trainingLog(""Number of validation sets: %d"" % len(validationSets))
441 
442         torchModel = model.createTorchModule()
443         if self.cuda:
444             torchModel.cuda()
445         model.setTorchModule(torchModel)
446 
447         nParams = sum([p.nelement() for p in torchModel.parameters()])
448         trainingLog('Number of parameters: %d' % nParams)
449 
450         criterion = self.lossEvaluator.getTrainingCriterion()
451 
452         if self.cuda:
453             criterion = criterion.cuda()
454 
455         best_val = 1e9
456         best_epoch = 0
457         optim = _Optimiser(torchModel.parameters(), method=self.optimiser, lr=self.optimiserLR,
458             max_grad_norm=self.optimiserClip, lr_decay=self.optimiserLRDecay, start_decay_at=self.startLRDecayAtEpoch,
459             use_shrinkage=self.useShrinkage, **self.optimiserArgs)
460 
461         bestModelBytes = model.getModuleBytes()
462         self.lossEvaluatorState = self.lossEvaluator.startTraining(self.cuda)
463         validationMetricName = self.lossEvaluator.getValidationMetricName()
464         try:
465             self.log.info('Begin training')
466             self.log.info('Press Ctrl+C to end training early')
467             for epoch in range(1, self.epochs + 1):
468                 epoch_start_time = time.time()
469 
470                 # perform training step, processing all the training data once
471                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.batchSize, self.cuda, outputScalers)
472 
473                 # perform validation, computing the mean metrics across all validation sets (if more than one)
474                 metricsSum = None
475                 metricsKeys = None
476                 for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
477                     metrics = self._evaluate(validationSet, torchModel, outputScaler)
478                     metricsArray = np.array(list(metrics.values()))
479                     if i == 0:
480                         metricsSum = metricsArray
481                         metricsKeys = metrics.keys()
482                     else:
483                         metricsSum += metricsArray
484                 metricsSum /= len(validationSets)  # mean results
485                 metrics = dict(zip(metricsKeys, metricsSum))
486 
487                 # check for new best result according to validation results
488                 current_val = metrics[self.lossEvaluator.getValidationMetricName()]
489                 isNewBest = current_val < best_val
490                 if isNewBest:
491                     best_val = current_val
492                     best_epoch = epoch
493                     bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
494                 else:
495                     bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
496                 trainingLog(
497                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f} | validation {:s} | {:s}'.format(
498                         epoch, self.epochs, (time.time() - epoch_start_time), train_loss,
499                         "", "".join([""%s %5.4f"" % e for e in metrics.items()]),
500                         bestStr))
501                 if isNewBest:
502                     bestModelBytes = model.getModuleBytes()
503             trainingLog(""Training complete"")
504         except KeyboardInterrupt:
505             trainingLog('Exiting from training early')
506         trainingLog('Best model is from epoch %d with %s %f on validation set' % (best_epoch, validationMetricName, best_val))
507         self.bestEpoch = best_epoch
508 
509         # reload best model
510         model.setModuleBytes(bestModelBytes)
511 
512     def getTrainingLog(self):
","Before: 455
After: 462",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,4098,"{'module': 1, 'expression_statement': 63, 'concatenated_string': 1, 'string': 18, 'string_start': 18, 'string_content': 28, 'interpolation': 12, '{': 12, 'attribute': 84, 'identifier': 303, '.': 84, '}': 12, 'string_end': 18, 'function_definition': 3, 'def': 3, 'parameters': 3, '(': 73, ',': 39, 'typed_parameter': 2, ':': 22, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 11, ']': 11, ')': 73, 'block': 20, 'call': 67, 'argument_list': 67, '->': 1, 'if_statement': 8, 'if': 8, 'return_statement': 2, 'return': 2, 'elif_clause': 1, 'elif': 1, 'comparison_operator': 3, '!=': 1, 'assignment': 37, '=': 43, 'list': 5, 'else_clause': 3, 'else': 3, 'list_comprehension': 3, 'for_in_clause': 3, 'for': 6, 'in': 6, 'comment': 8, 'none': 3, 'integer': 6, 'false': 1, 'true': 1, 'for_statement': 3, 'pattern_list': 3, 'binary_operator': 7, '+': 2, '%': 4, 'float': 1, 'keyword_argument': 6, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, '==': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'except_clause': 1, 'except': 1, 'tuple': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7320659952321988,0.730243823261541,"(tensor([0.9822]), tensor([0.9827]), tensor([0.9824]), tensor([0.9826]))"
"546                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
547         return total_loss / n_samples
548 
549     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
550         """"""Evaluates the model on the given data set (a validation set)""""""
551         model.eval()
552 
553         groundTruthShape = None
554         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):
555             if groundTruthShape is None:
556                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
557                 self.lossEvaluator.startValidationCollection(groundTruthShape)
558             with torch.no_grad():
559                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
560             self.lossEvaluator.collectValidationResultBatch(output, groundTruth)
561 
562         return self.lossEvaluator.endValidationCollection()
563 
564     def _init_cuda(self):
","553                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
554         return total_loss / n_samples
555 
556     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
557         """"""Evaluates the model on the given data set (a validation set)""""""
558         model.eval()
559 
560         groundTruthShape = None
561         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):
562             if groundTruthShape is None:
563                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
564                 self.lossEvaluatorState.startValidationCollection(groundTruthShape)
565             with torch.no_grad():
566                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
567             self.lossEvaluatorState.collectValidationResultBatch(output, groundTruth)
568 
569         return self.lossEvaluatorState.endValidationCollection()
570 
571     def _init_cuda(self):
","Before: 557
After: 564",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,5162,"{'module': 1, 'expression_statement': 8, 'augmented_assignment': 1, 'identifier': 50, '+=': 1, 'binary_operator': 2, '*': 1, 'return_statement': 2, 'return': 2, '/': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 10, 'typed_parameter': 3, ':': 8, 'type': 3, 'attribute': 13, '.': 13, ')': 8, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 7, 'argument_list': 7, 'assignment': 3, '=': 4, 'none': 2, 'for_statement': 1, 'for': 1, 'pattern_list': 2, 'in': 1, 'keyword_argument': 1, 'false': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, '[': 1, 'slice': 1, 'integer': 1, ']': 1, 'comment': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7031715240048954,0.6923418665028033,"(tensor([0.9731]), tensor([0.9794]), tensor([0.9762]), tensor([0.9788]))"
"546                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
547         return total_loss / n_samples
548 
549     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
550         """"""Evaluates the model on the given data set (a validation set)""""""
551         model.eval()
552 
553         groundTruthShape = None
554         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):
555             if groundTruthShape is None:
556                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
557                 self.lossEvaluator.startValidationCollection(groundTruthShape)
558             with torch.no_grad():
559                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
560             self.lossEvaluator.collectValidationResultBatch(output, groundTruth)
561 
562         return self.lossEvaluator.endValidationCollection()
563 
564     def _init_cuda(self):
","553                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
554         return total_loss / n_samples
555 
556     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
557         """"""Evaluates the model on the given data set (a validation set)""""""
558         model.eval()
559 
560         groundTruthShape = None
561         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):
562             if groundTruthShape is None:
563                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
564                 self.lossEvaluatorState.startValidationCollection(groundTruthShape)
565             with torch.no_grad():
566                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
567             self.lossEvaluatorState.collectValidationResultBatch(output, groundTruth)
568 
569         return self.lossEvaluatorState.endValidationCollection()
570 
571     def _init_cuda(self):
","Before: 560
After: 567",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,5213,"{'module': 1, 'expression_statement': 8, 'augmented_assignment': 1, 'identifier': 50, '+=': 1, 'binary_operator': 2, '*': 1, 'return_statement': 2, 'return': 2, '/': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 10, 'typed_parameter': 3, ':': 8, 'type': 3, 'attribute': 13, '.': 13, ')': 8, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 7, 'argument_list': 7, 'assignment': 3, '=': 4, 'none': 2, 'for_statement': 1, 'for': 1, 'pattern_list': 2, 'in': 1, 'keyword_argument': 1, 'false': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, '[': 1, 'slice': 1, 'integer': 1, ']': 1, 'comment': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7031715240048954,0.6923418665028033,"(tensor([0.9731]), tensor([0.9794]), tensor([0.9762]), tensor([0.9788]))"
"546                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
547         return total_loss / n_samples
548 
549     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
550         """"""Evaluates the model on the given data set (a validation set)""""""
551         model.eval()
552 
553         groundTruthShape = None
554         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):
555             if groundTruthShape is None:
556                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
557                 self.lossEvaluator.startValidationCollection(groundTruthShape)
558             with torch.no_grad():
559                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
560             self.lossEvaluator.collectValidationResultBatch(output, groundTruth)
561 
562         return self.lossEvaluator.endValidationCollection()
563 
564     def _init_cuda(self):
","553                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
554         return total_loss / n_samples
555 
556     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
557         """"""Evaluates the model on the given data set (a validation set)""""""
558         model.eval()
559 
560         groundTruthShape = None
561         for X, Y in dataSet.iterBatches(self.batchSize, shuffle=False):
562             if groundTruthShape is None:
563                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
564                 self.lossEvaluatorState.startValidationCollection(groundTruthShape)
565             with torch.no_grad():
566                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
567             self.lossEvaluatorState.collectValidationResultBatch(output, groundTruth)
568 
569         return self.lossEvaluatorState.endValidationCollection()
570 
571     def _init_cuda(self):
","Before: 562
After: 569",convert `state` and `nnlossevaluatorstate` to use it,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,5226,"{'module': 1, 'expression_statement': 8, 'augmented_assignment': 1, 'identifier': 50, '+=': 1, 'binary_operator': 2, '*': 1, 'return_statement': 2, 'return': 2, '/': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 10, 'typed_parameter': 3, ':': 8, 'type': 3, 'attribute': 13, '.': 13, ')': 8, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 7, 'argument_list': 7, 'assignment': 3, '=': 4, 'none': 2, 'for_statement': 1, 'for': 1, 'pattern_list': 2, 'in': 1, 'keyword_argument': 1, 'false': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, '[': 1, 'slice': 1, 'integer': 1, ']': 1, 'comment': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 27, 'end_line': 52, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7031715240048954,0.6923418665028033,"(tensor([0.9731]), tensor([0.9794]), tensor([0.9762]), tensor([0.9788]))"
"62         pass
63 
64 
65 def loadPickle(path):
66     with open(path, ""rb"") as f:
67         return pickle.load(f)
68 
69 
","63         pass
64 
65 
66 def loadPickle(path, backend=""pickle""):
67     with open(path, ""rb"") as f:
68         if backend == ""pickle"":
69             return pickle.load(f)
70         elif backend == ""joblib"":
71             return joblib.load(f)
72         else:
73             raise ValueError(f""Unknown backend '{backend}'"")
74 
75 
","Before: 65
After: 66",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,225,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 8, 'parameters': 1, '(': 3, ')': 3, ':': 2, 'block': 2, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 1, 'call': 2, 'argument_list': 2, ',': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'as': 1, 'as_pattern_target': 1, 'return_statement': 1, 'return': 1, 'attribute': 1, '.': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.245083485596478,0.240186352711462,"(tensor([0.7976]), tensor([0.9204]), tensor([0.8546]), tensor([0.9064]))"
"62         pass
63 
64 
65 def loadPickle(path):
66     with open(path, ""rb"") as f:
67         return pickle.load(f)
68 
69 
","63         pass
64 
65 
66 def loadPickle(path, backend=""pickle""):
67     with open(path, ""rb"") as f:
68         if backend == ""pickle"":
69             return pickle.load(f)
70         elif backend == ""joblib"":
71             return joblib.load(f)
72         else:
73             raise ValueError(f""Unknown backend '{backend}'"")
74 
75 
","Before: 67
After: 68, 69, 70, 71, 72, 73",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,257,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'identifier': 8, 'parameters': 1, '(': 3, ')': 3, ':': 2, 'block': 2, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 1, 'call': 2, 'argument_list': 2, ',': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'as': 1, 'as_pattern_target': 1, 'return_statement': 1, 'return': 1, 'attribute': 1, '.': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.245083485596478,0.240186352711462,"(tensor([0.7976]), tensor([0.9204]), tensor([0.8546]), tensor([0.9064]))"
"67         return pickle.load(f)
68 
69 
70 def dumpPickle(obj, picklePath):
71     dirName = os.path.dirname(picklePath)
72     if dirName != """":
73         os.makedirs(dirName, exist_ok=True)
74     with open(picklePath, ""wb"") as f:
75         try:
76             pickle.dump(obj, f)
77         except AttributeError as e:
78             failingPaths = PickleFailureDebugger.debugFailure(obj)
79             raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
80 
81 
","73             raise ValueError(f""Unknown backend '{backend}'"")
74 
75 
76 def dumpPickle(obj, picklePath, backend=""pickle""):
77     dirName = os.path.dirname(picklePath)
78     if dirName != """":
79         os.makedirs(dirName, exist_ok=True)
80     with open(picklePath, ""wb"") as f:
81         if backend == ""pickle"":
82             try:
83                 pickle.dump(obj, f)
84             except AttributeError as e:
85                 failingPaths = PickleFailureDebugger.debugFailure(obj)
86                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
87         elif backend == ""joblib"":
88             joblib.dump(obj, f)
89         else:
90             raise ValueError(f""Unknown backend '{backend}'"")
91 
92 
","Before: 70
After: 76",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,268,"{'module': 1, 'return_statement': 1, 'return': 1, 'call': 8, 'attribute': 6, 'identifier': 34, '.': 6, 'argument_list': 8, '(': 9, ')': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, ':': 5, 'block': 5, 'expression_statement': 4, 'assignment': 2, '=': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'string': 3, 'string_start': 3, 'string_end': 3, 'keyword_argument': 1, 'true': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 2, 'string_content': 4, 'as': 2, 'as_pattern_target': 2, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 3, '{': 3, '}': 3}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.44204281416199637,0.4320713101769879,"(tensor([0.8770]), tensor([0.9358]), tensor([0.9054]), tensor([0.9296]))"
"67         return pickle.load(f)
68 
69 
70 def dumpPickle(obj, picklePath):
71     dirName = os.path.dirname(picklePath)
72     if dirName != """":
73         os.makedirs(dirName, exist_ok=True)
74     with open(picklePath, ""wb"") as f:
75         try:
76             pickle.dump(obj, f)
77         except AttributeError as e:
78             failingPaths = PickleFailureDebugger.debugFailure(obj)
79             raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
80 
81 
","73             raise ValueError(f""Unknown backend '{backend}'"")
74 
75 
76 def dumpPickle(obj, picklePath, backend=""pickle""):
77     dirName = os.path.dirname(picklePath)
78     if dirName != """":
79         os.makedirs(dirName, exist_ok=True)
80     with open(picklePath, ""wb"") as f:
81         if backend == ""pickle"":
82             try:
83                 pickle.dump(obj, f)
84             except AttributeError as e:
85                 failingPaths = PickleFailureDebugger.debugFailure(obj)
86                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
87         elif backend == ""joblib"":
88             joblib.dump(obj, f)
89         else:
90             raise ValueError(f""Unknown backend '{backend}'"")
91 
92 
","Before: 75, 76, 77, 78, 79
After: 81, 82, 83, 84, 85, 86, 87, 88, 89, 90",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,333,"{'module': 1, 'return_statement': 1, 'return': 1, 'call': 8, 'attribute': 6, 'identifier': 34, '.': 6, 'argument_list': 8, '(': 9, ')': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, ':': 5, 'block': 5, 'expression_statement': 4, 'assignment': 2, '=': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'string': 3, 'string_start': 3, 'string_end': 3, 'keyword_argument': 1, 'true': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 2, 'string_content': 4, 'as': 2, 'as_pattern_target': 2, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 3, '{': 3, '}': 3}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.44204281416199637,0.4320713101769879,"(tensor([0.8770]), tensor([0.9358]), tensor([0.9054]), tensor([0.9296]))"
"513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None) -> T:
517     """"""
518     :param fn: the function whose result is to be cached
519     :param picklePath: the path in which to store the cached result
520     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
521         informative)
522     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
523         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
524         the function fn is called to compute the result and the cached result is updated.
525     :return: the res (either obtained from the cache or the function)
526     """"""
527     if functionName is None:
528         functionName = fn.__name__
529 
530     def callFnAndCacheResult():
531         res = fn()
532         _log.info(f""Saving cached res in {picklePath}"")
533         dumpPickle(res, picklePath)
534         return res
535 
536     if os.path.exists(picklePath):
537         _log.info(f""Loading cached res of function '{functionName}' from {picklePath}"")
538         result = loadPickle(picklePath)
539         if validityCheckFn is not None:
540             if not validityCheckFn(result):
541                 _log.info(f""Cached result is no longer valid, recomputing ..."")
542                 result = callFnAndCacheResult()
543         return result
544     else:
545         _log.info(f""No cached res found in {picklePath}, calling function '{functionName}' ..."")
546         return callFnAndCacheResult()
547 
548 
","524         pass
525 
526 
527 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
528         backend=""pickle"") -> T:
529     """"""
530     :param fn: the function whose result is to be cached
531     :param picklePath: the path in which to store the cached result
532     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
533         informative)
534     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
535         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
536         the function fn is called to compute the result and the cached result is updated.
537     :return: the res (either obtained from the cache or the function)
538     """"""
539     if functionName is None:
540         functionName = fn.__name__
541 
542     def callFnAndCacheResult():
543         res = fn()
544         _log.info(f""Saving cached result in {picklePath}"")
545         dumpPickle(res, picklePath, backend=backend)
546         return res
547 
548     if os.path.exists(picklePath):
549         _log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
550         result = loadPickle(picklePath, backend=backend)
551         if validityCheckFn is not None:
552             if not validityCheckFn(result):
553                 _log.info(f""Cached result is no longer valid, recomputing ..."")
554                 result = callFnAndCacheResult()
555         return result
556     else:
557         _log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
558         return callFnAndCacheResult()
559 
560 
","Before: 516
After: 527, 528",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,3808,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 2, 'def': 2, 'identifier': 50, 'parameters': 2, '(': 13, 'typed_parameter': 1, ':': 9, 'type': 8, 'generic_type': 3, 'type_parameter': 3, '[': 5, 'list': 2, ']': 5, ',': 6, 'default_parameter': 1, '=': 6, 'none': 4, 'typed_default_parameter': 1, ')': 13, '->': 1, 'block': 7, 'expression_statement': 10, 'string': 5, 'string_start': 5, 'string_content': 8, 'string_end': 5, 'if_statement': 4, 'if': 4, 'comparison_operator': 2, 'is': 1, 'assignment': 4, 'attribute': 7, '.': 7, 'call': 11, 'argument_list': 11, 'interpolation': 5, '{': 5, '}': 5, 'return_statement': 3, 'return': 3, 'is not': 2, 'not_operator': 1, 'not': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7277059994729583,0.7267835238655481,"(tensor([0.9583]), tensor([0.9679]), tensor([0.9631]), tensor([0.9669]))"
"513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None) -> T:
517     """"""
518     :param fn: the function whose result is to be cached
519     :param picklePath: the path in which to store the cached result
520     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
521         informative)
522     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
523         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
524         the function fn is called to compute the result and the cached result is updated.
525     :return: the res (either obtained from the cache or the function)
526     """"""
527     if functionName is None:
528         functionName = fn.__name__
529 
530     def callFnAndCacheResult():
531         res = fn()
532         _log.info(f""Saving cached res in {picklePath}"")
533         dumpPickle(res, picklePath)
534         return res
535 
536     if os.path.exists(picklePath):
537         _log.info(f""Loading cached res of function '{functionName}' from {picklePath}"")
538         result = loadPickle(picklePath)
539         if validityCheckFn is not None:
540             if not validityCheckFn(result):
541                 _log.info(f""Cached result is no longer valid, recomputing ..."")
542                 result = callFnAndCacheResult()
543         return result
544     else:
545         _log.info(f""No cached res found in {picklePath}, calling function '{functionName}' ..."")
546         return callFnAndCacheResult()
547 
548 
","524         pass
525 
526 
527 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
528         backend=""pickle"") -> T:
529     """"""
530     :param fn: the function whose result is to be cached
531     :param picklePath: the path in which to store the cached result
532     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
533         informative)
534     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
535         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
536         the function fn is called to compute the result and the cached result is updated.
537     :return: the res (either obtained from the cache or the function)
538     """"""
539     if functionName is None:
540         functionName = fn.__name__
541 
542     def callFnAndCacheResult():
543         res = fn()
544         _log.info(f""Saving cached result in {picklePath}"")
545         dumpPickle(res, picklePath, backend=backend)
546         return res
547 
548     if os.path.exists(picklePath):
549         _log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
550         result = loadPickle(picklePath, backend=backend)
551         if validityCheckFn is not None:
552             if not validityCheckFn(result):
553                 _log.info(f""Cached result is no longer valid, recomputing ..."")
554                 result = callFnAndCacheResult()
555         return result
556     else:
557         _log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
558         return callFnAndCacheResult()
559 
560 
","Before: 532, 533
After: 544, 545",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,3863,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 2, 'def': 2, 'identifier': 50, 'parameters': 2, '(': 13, 'typed_parameter': 1, ':': 9, 'type': 8, 'generic_type': 3, 'type_parameter': 3, '[': 5, 'list': 2, ']': 5, ',': 6, 'default_parameter': 1, '=': 6, 'none': 4, 'typed_default_parameter': 1, ')': 13, '->': 1, 'block': 7, 'expression_statement': 10, 'string': 5, 'string_start': 5, 'string_content': 8, 'string_end': 5, 'if_statement': 4, 'if': 4, 'comparison_operator': 2, 'is': 1, 'assignment': 4, 'attribute': 7, '.': 7, 'call': 11, 'argument_list': 11, 'interpolation': 5, '{': 5, '}': 5, 'return_statement': 3, 'return': 3, 'is not': 2, 'not_operator': 1, 'not': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7277059994729583,0.7267835238655481,"(tensor([0.9583]), tensor([0.9679]), tensor([0.9631]), tensor([0.9669]))"
"513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None) -> T:
517     """"""
518     :param fn: the function whose result is to be cached
519     :param picklePath: the path in which to store the cached result
520     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
521         informative)
522     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
523         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
524         the function fn is called to compute the result and the cached result is updated.
525     :return: the res (either obtained from the cache or the function)
526     """"""
527     if functionName is None:
528         functionName = fn.__name__
529 
530     def callFnAndCacheResult():
531         res = fn()
532         _log.info(f""Saving cached res in {picklePath}"")
533         dumpPickle(res, picklePath)
534         return res
535 
536     if os.path.exists(picklePath):
537         _log.info(f""Loading cached res of function '{functionName}' from {picklePath}"")
538         result = loadPickle(picklePath)
539         if validityCheckFn is not None:
540             if not validityCheckFn(result):
541                 _log.info(f""Cached result is no longer valid, recomputing ..."")
542                 result = callFnAndCacheResult()
543         return result
544     else:
545         _log.info(f""No cached res found in {picklePath}, calling function '{functionName}' ..."")
546         return callFnAndCacheResult()
547 
548 
","524         pass
525 
526 
527 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
528         backend=""pickle"") -> T:
529     """"""
530     :param fn: the function whose result is to be cached
531     :param picklePath: the path in which to store the cached result
532     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
533         informative)
534     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
535         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
536         the function fn is called to compute the result and the cached result is updated.
537     :return: the res (either obtained from the cache or the function)
538     """"""
539     if functionName is None:
540         functionName = fn.__name__
541 
542     def callFnAndCacheResult():
543         res = fn()
544         _log.info(f""Saving cached result in {picklePath}"")
545         dumpPickle(res, picklePath, backend=backend)
546         return res
547 
548     if os.path.exists(picklePath):
549         _log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
550         result = loadPickle(picklePath, backend=backend)
551         if validityCheckFn is not None:
552             if not validityCheckFn(result):
553                 _log.info(f""Cached result is no longer valid, recomputing ..."")
554                 result = callFnAndCacheResult()
555         return result
556     else:
557         _log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
558         return callFnAndCacheResult()
559 
560 
","Before: 537, 538
After: 549, 550",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,3913,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 2, 'def': 2, 'identifier': 50, 'parameters': 2, '(': 13, 'typed_parameter': 1, ':': 9, 'type': 8, 'generic_type': 3, 'type_parameter': 3, '[': 5, 'list': 2, ']': 5, ',': 6, 'default_parameter': 1, '=': 6, 'none': 4, 'typed_default_parameter': 1, ')': 13, '->': 1, 'block': 7, 'expression_statement': 10, 'string': 5, 'string_start': 5, 'string_content': 8, 'string_end': 5, 'if_statement': 4, 'if': 4, 'comparison_operator': 2, 'is': 1, 'assignment': 4, 'attribute': 7, '.': 7, 'call': 11, 'argument_list': 11, 'interpolation': 5, '{': 5, '}': 5, 'return_statement': 3, 'return': 3, 'is not': 2, 'not_operator': 1, 'not': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7277059994729583,0.7267835238655481,"(tensor([0.9583]), tensor([0.9679]), tensor([0.9631]), tensor([0.9669]))"
"513         pass
514 
515 
516 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None) -> T:
517     """"""
518     :param fn: the function whose result is to be cached
519     :param picklePath: the path in which to store the cached result
520     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
521         informative)
522     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
523         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
524         the function fn is called to compute the result and the cached result is updated.
525     :return: the res (either obtained from the cache or the function)
526     """"""
527     if functionName is None:
528         functionName = fn.__name__
529 
530     def callFnAndCacheResult():
531         res = fn()
532         _log.info(f""Saving cached res in {picklePath}"")
533         dumpPickle(res, picklePath)
534         return res
535 
536     if os.path.exists(picklePath):
537         _log.info(f""Loading cached res of function '{functionName}' from {picklePath}"")
538         result = loadPickle(picklePath)
539         if validityCheckFn is not None:
540             if not validityCheckFn(result):
541                 _log.info(f""Cached result is no longer valid, recomputing ..."")
542                 result = callFnAndCacheResult()
543         return result
544     else:
545         _log.info(f""No cached res found in {picklePath}, calling function '{functionName}' ..."")
546         return callFnAndCacheResult()
547 
548 
","524         pass
525 
526 
527 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
528         backend=""pickle"") -> T:
529     """"""
530     :param fn: the function whose result is to be cached
531     :param picklePath: the path in which to store the cached result
532     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
533         informative)
534     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
535         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
536         the function fn is called to compute the result and the cached result is updated.
537     :return: the res (either obtained from the cache or the function)
538     """"""
539     if functionName is None:
540         functionName = fn.__name__
541 
542     def callFnAndCacheResult():
543         res = fn()
544         _log.info(f""Saving cached result in {picklePath}"")
545         dumpPickle(res, picklePath, backend=backend)
546         return res
547 
548     if os.path.exists(picklePath):
549         _log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
550         result = loadPickle(picklePath, backend=backend)
551         if validityCheckFn is not None:
552             if not validityCheckFn(result):
553                 _log.info(f""Cached result is no longer valid, recomputing ..."")
554                 result = callFnAndCacheResult()
555         return result
556     else:
557         _log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
558         return callFnAndCacheResult()
559 
560 
","Before: 545
After: 557",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,3996,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 2, 'def': 2, 'identifier': 50, 'parameters': 2, '(': 13, 'typed_parameter': 1, ':': 9, 'type': 8, 'generic_type': 3, 'type_parameter': 3, '[': 5, 'list': 2, ']': 5, ',': 6, 'default_parameter': 1, '=': 6, 'none': 4, 'typed_default_parameter': 1, ')': 13, '->': 1, 'block': 7, 'expression_statement': 10, 'string': 5, 'string_start': 5, 'string_content': 8, 'string_end': 5, 'if_statement': 4, 'if': 4, 'comparison_operator': 2, 'is': 1, 'assignment': 4, 'attribute': 7, '.': 7, 'call': 11, 'argument_list': 11, 'interpolation': 5, '{': 5, '}': 5, 'return_statement': 3, 'return': 3, 'is not': 2, 'not_operator': 1, 'not': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7277059994729583,0.7267835238655481,"(tensor([0.9583]), tensor([0.9679]), tensor([0.9631]), tensor([0.9669]))"
"547 
548 
549 class PickleCached(object):
550     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None):
551         """"""
552 
553         :param cacheBasePath:
554         :param filenamePrefix:
555         :param filename:
556         """"""
557         self.filename = filename
558         self.cacheBasePath = cacheBasePath
559         self.filenamePrefix = filenamePrefix
560 
561         if self.filenamePrefix is None:
562             self.filenamePrefix = """"
563         else:
564             self.filenamePrefix += ""-""
565 
566     def __call__(self, fn, *args, **kwargs):
","559 
560 
561 class PickleCached(object):
562     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None, backend=""pickle""):
563         """"""
564 
565         :param cacheBasePath:
566         :param filenamePrefix:
567         :param filename:
568         """"""
569         self.filename = filename
570         self.cacheBasePath = cacheBasePath
571         self.filenamePrefix = filenamePrefix
572         self.backend = backend
573 
574         if self.filenamePrefix is None:
575             self.filenamePrefix = """"
576         else:
577             self.filenamePrefix += ""-""
578 
579     def __call__(self, fn, *args, **kwargs):
","Before: 550
After: 562",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,4043,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 25, 'argument_list': 1, '(': 2, ')': 2, ':': 7, 'block': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, 'typed_parameter': 1, 'type': 3, 'typed_default_parameter': 2, '=': 6, 'none': 3, 'expression_statement': 6, 'string': 3, 'string_start': 3, 'string_content': 2, 'string_end': 3, 'assignment': 4, 'attribute': 6, '.': 6, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'augmented_assignment': 1, '+=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5448208705690631,0.5263147632062063,"(tensor([0.9353]), tensor([0.9505]), tensor([0.9429]), tensor([0.9490]))"
"563         else:
564             self.filenamePrefix += ""-""
565 
566     def __call__(self, fn, *args, **kwargs):
567         if self.filename is None:
568             self.filename = self.filenamePrefix + fn.__qualname__ + "".cache.pickle""
569         picklePath = os.path.join(self.cacheBasePath,  self.filename)
570         return lambda *args, **kwargs: cached(lambda: fn(*args, **kwargs), picklePath, functionName=fn.__name__)
","576         else:
577             self.filenamePrefix += ""-""
578 
579     def __call__(self, fn, *args, **kwargs):
580         if self.filename is None:
581             self.filename = self.filenamePrefix + fn.__qualname__ + "".cache.pickle""
582         picklePath = os.path.join(self.cacheBasePath,  self.filename)
583         return lambda *args, **kwargs: cached(lambda: fn(*args, **kwargs), picklePath, functionName=fn.__name__, backend=self.backend)
","Before: 570
After: 583",add backend support to pickle and joblib,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,4228,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'identifier': 24, ':': 3, 'type': 1, 'attribute': 9, '.': 9, 'ERROR': 1, '+': 3, '=': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 4, 'list_splat_pattern': 1, '*': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'binary_operator': 2, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7483677389342142,0.7222156688846741,"(tensor([0.9601]), tensor([0.9459]), tensor([0.9530]), tensor([0.9473]))"
"10     def fromUnsortedKeyValuePairs(cls, unsortedKeyValuePairs: Sequence[Tuple[TKey, TValue]]):
11         return cls(sorted(unsortedKeyValuePairs, key=lambda x: x[0]))
12 
13     def __init__(self, sortedKeyValuePairs: Sequence[Tuple[TKey, TValue]]):
14         self.entries = sortedKeyValuePairs
15         self.keys = [t[0] for t in sortedKeyValuePairs]
16 
17     def _value(self, idx: Optional[int]) -> Optional[TValue]:
","77     def fromUnsortedKeyValuePairs(cls, unsortedKeyValuePairs: Sequence[Tuple[TKey, TValue]]):
78         return cls(sorted(unsortedKeyValuePairs, key=lambda x: x[0]))
79 
80     def __init__(self, sortedKeyValuePairs: Sequence[Tuple[TKey, TValue]]):
81         self.entries = sortedKeyValuePairs
82         self._sortedKeys = SortedValues([t[0] for t in sortedKeyValuePairs])
83 
84     def _value(self, idx: Optional[int]) -> Optional[TValue]:
","Before: 15
After: 82",add binary search for sorted values,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,191,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 28, 'parameters': 2, '(': 4, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 8, 'generic_type': 4, 'type_parameter': 4, '[': 7, ']': 7, ')': 4, 'block': 2, 'return_statement': 1, 'return': 1, 'call': 2, 'argument_list': 2, 'keyword_argument': 1, '=': 3, 'lambda': 2, 'lambda_parameters': 1, 'subscript': 2, 'integer': 2, 'expression_statement': 2, 'assignment': 2, 'attribute': 2, '.': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 35, 'name': 'fromUnsortedKeyValuePairs', 'long_name': 'fromUnsortedKeyValuePairs( cls , unsortedKeyValuePairs : Sequence [ Tuple [ TKey , TValue ] ] )', 'start_line': 10, 'end_line': 11, 'full_parameters': ['cls', ' unsortedKeyValuePairs : Sequence [ Tuple [ TKey', ' TValue ] ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 17, 'name': '__init__', 'long_name': '__init__( self , sortedValues : Sequence [ TValue ] )', 'start_line': 12, 'end_line': 13, 'full_parameters': ['self', ' sortedValues : Sequence [ TValue ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7845558834191673,0.761671503811116,"(tensor([0.9679]), tensor([0.9765]), tensor([0.9722]), tensor([0.9757]))"
"19             return None
20         return self.entries[idx][1]
21 
22     def floorIndex(self, key) -> Optional[int]:
23         """"""Finds the rightmost index less than or equal to key""""""
24         idx = bisect_right(self.keys, key)
25         if idx:
26             return idx - 1
27         return None
28 
29     def floorValue(self, key) -> Optional[TValue]:
","86             return None
87         return self.entries[idx][1]
88 
89     def floorIndex(self, key) -> Optional[int]:
90         """"""Finds the rightmost index where the key is less than or equal to the given key""""""
91         return self._sortedKeys.floorIndex(key)
92 
93     def floorValue(self, key) -> Optional[TValue]:
","Before: 23, 24, 25, 26, 27
After: 90, 91",add binary search for sorted values,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,271,"{'module': 1, 'return_statement': 4, 'return': 4, 'none': 2, 'subscript': 2, 'attribute': 2, 'identifier': 15, '.': 2, '[': 3, ']': 3, 'integer': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 2, ')': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, ':': 2, 'block': 2, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 1, 'call': 1, 'argument_list': 1, 'if_statement': 1, 'if': 1, 'binary_operator': 1, '-': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 35, 'name': 'fromUnsortedKeyValuePairs', 'long_name': 'fromUnsortedKeyValuePairs( cls , unsortedKeyValuePairs : Sequence [ Tuple [ TKey , TValue ] ] )', 'start_line': 10, 'end_line': 11, 'full_parameters': ['cls', ' unsortedKeyValuePairs : Sequence [ Tuple [ TKey', ' TValue ] ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 17, 'name': '__init__', 'long_name': '__init__( self , sortedValues : Sequence [ TValue ] )', 'start_line': 12, 'end_line': 13, 'full_parameters': ['self', ' sortedValues : Sequence [ TValue ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5454512523207194,0.5202548150027874,"(tensor([0.9090]), tensor([0.8912]), tensor([0.9000]), tensor([0.8929]))"
"29     def floorValue(self, key) -> Optional[TValue]:
30         return self._value(self.floorIndex(key))
31 
32     def ceilIndex(self, key) -> Optional[int]:
33         """"""Find leftmost index where the key is greater than or equal to the given key""""""
34         idx = bisect_left(self.keys, key)
35         if idx != len(self.keys):
36             return idx
37         return None
38 
39     def ceilValue(self, key) -> Optional[TValue]:
","93     def floorValue(self, key) -> Optional[TValue]:
94         return self._value(self.floorIndex(key))
95 
96     def ceilIndex(self, key) -> Optional[int]:
97         """"""Find leftmost index where the key is greater than or equal to the given key""""""
98         return self._sortedKeys.ceilIndex(key)
99 
100     def ceilValue(self, key) -> Optional[TValue]:
","Before: 34, 35, 36, 37
After: 98",add binary search for sorted values,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,379,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 25, 'parameters': 2, '(': 6, ',': 3, ')': 6, '->': 2, 'type': 4, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, ':': 3, 'block': 3, 'return_statement': 3, 'return': 3, 'call': 4, 'attribute': 4, '.': 4, 'argument_list': 4, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 35, 'name': 'fromUnsortedKeyValuePairs', 'long_name': 'fromUnsortedKeyValuePairs( cls , unsortedKeyValuePairs : Sequence [ Tuple [ TKey , TValue ] ] )', 'start_line': 10, 'end_line': 11, 'full_parameters': ['cls', ' unsortedKeyValuePairs : Sequence [ Tuple [ TKey', ' TValue ] ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 17, 'name': '__init__', 'long_name': '__init__( self , sortedValues : Sequence [ TValue ] )', 'start_line': 12, 'end_line': 13, 'full_parameters': ['self', ' sortedValues : Sequence [ TValue ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6223234182313838,0.6039627690017093,"(tensor([0.9374]), tensor([0.9019]), tensor([0.9193]), tensor([0.9054]))"
"1 from typing import Sequence
2 
3 import matplotlib.figure
4 from matplotlib import pyplot as plt
5 import numpy as np
","1 import logging
2 from matplotlib.colors import LinearSegmentedColormap
3 from typing import Sequence, Callable
4 
5 import matplotlib.figure
","Before: 1
After: 1, 2, 3, 10, 11, 12",add a basic plot for the nparrays,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,9,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 5, 'identifier': 7, 'import': 3, 'import_statement': 1, '.': 1, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.17728339004105292,0.12226062919807183,"(tensor([0.8536]), tensor([0.8850]), tensor([0.8690]), tensor([0.8817]))"
"5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9),
9         titleAdd: str = None) -> matplotlib.figure.Figure:
10     """"""
11     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
12     :param title: the plot's title
13     :param xticklabels: the labels for the x-axis ticks
14     :param yticklabels: the labels for the y-axis ticks
15     :param xlabel: the label for the x-axis
16     :param ylabel: the label for the y-axis
17     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
18     :param titleAdd: an optional second line to add to the title
19     :return: the figure object
20     """"""
21     matrix = np.transpose(matrix)
22 
23     if titleAdd is not None:
24         title += ""\n"" + titleAdd
25 
26     if normalize:
27         matrix = matrix.astype('float') / matrix.sum()
28     fig, ax = plt.subplots(figsize=figsize)
29     fig.canvas.set_window_title(title)
30     # We want to show all ticks...
31     ax.set(xticks=np.arange(matrix.shape[1]),
32         yticks=np.arange(matrix.shape[0]),
33         # ... and label them with the respective list entries
34         xticklabels=xticklabels, yticklabels=yticklabels,
35         title=title,
36         xlabel=xlabel,
37         ylabel=ylabel)
38     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
39     ax.figure.colorbar(im, ax=ax)
40 
41     # Rotate the tick labels and set their alignment.
42     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
43         rotation_mode=""anchor"")
44 
45     # Loop over data dimensions and create text annotations.
46     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
47     thresh = matrix.max() / 2.
48     for i in range(matrix.shape[0]):
49         for j in range(matrix.shape[1]):
50             ax.text(j, i, format(matrix[i, j], fmt),
51                 ha=""center"", va=""center"",
52                 color=""white"" if matrix[i, j] > thresh else ""black"")
53     fig.tight_layout()
54     return fig","10 log = logging.getLogger(__name__)
11 
12 
13 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9),
14         titleAdd: str = None) -> matplotlib.figure.Figure:
15     """"""
16     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
17     :param title: the plot's title
18     :param xticklabels: the labels for the x-axis ticks
19     :param yticklabels: the labels for the y-axis ticks
20     :param xlabel: the label for the x-axis
21     :param ylabel: the label for the y-axis
22     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
23     :param titleAdd: an optional second line to add to the title
24     :return: the figure object
25     """"""
26     matrix = np.transpose(matrix)
27 
28     if titleAdd is not None:
29         title += f""\n {titleAdd} ""
30 
31     if normalize:
32         matrix = matrix.astype('float') / matrix.sum()
33     fig, ax = plt.subplots(figsize=figsize)
34     fig.canvas.set_window_title(title.replace(""\n"", "" ""))
35     # We want to show all ticks...
36     ax.set(xticks=np.arange(matrix.shape[1]),
37         yticks=np.arange(matrix.shape[0]),
38         # ... and label them with the respective list entries
39         xticklabels=xticklabels, yticklabels=yticklabels,
40         title=title,
41         xlabel=xlabel,
42         ylabel=ylabel)
43     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
44     ax.figure.colorbar(im, ax=ax)
45 
46     # Rotate the tick labels and set their alignment.
47     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
48         rotation_mode=""anchor"")
49 
50     # Loop over data dimensions and create text annotations.
51     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
52     thresh = matrix.max() / 2.
53     for i in range(matrix.shape[0]):
54         for j in range(matrix.shape[1]):
55             ax.text(j, i, format(matrix[i, j], fmt),
56                 ha=""center"", va=""center"",
57                 color=""white"" if matrix[i, j] > thresh else ""black"")
58     fig.tight_layout()
59     return fig
60 
61 
","Before: 24
After: 29",add a basic plot for the nparrays,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,150,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 122, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 20, ',': 30, 'typed_parameter': 4, ':': 10, 'type': 8, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 2, '=': 26, 'true': 1, 'tuple': 1, 'integer': 7, ')': 20, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'attribute': 26, '.': 26, 'block': 5, 'expression_statement': 13, 'string': 13, 'string_start': 13, 'string_content': 13, 'string_end': 13, 'assignment': 6, 'call': 17, 'argument_list': 17, 'if_statement': 2, 'if': 5, 'comparison_operator': 3, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 3, 'escape_sequence': 1, '+': 1, '/': 2, 'pattern_list': 1, 'keyword_argument': 17, 'comment': 4, 'subscript': 6, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.73365241150548,0.7314877309414142,"(tensor([0.9615]), tensor([0.9661]), tensor([0.9638]), tensor([0.9656]))"
"5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9),
9         titleAdd: str = None) -> matplotlib.figure.Figure:
10     """"""
11     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
12     :param title: the plot's title
13     :param xticklabels: the labels for the x-axis ticks
14     :param yticklabels: the labels for the y-axis ticks
15     :param xlabel: the label for the x-axis
16     :param ylabel: the label for the y-axis
17     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
18     :param titleAdd: an optional second line to add to the title
19     :return: the figure object
20     """"""
21     matrix = np.transpose(matrix)
22 
23     if titleAdd is not None:
24         title += ""\n"" + titleAdd
25 
26     if normalize:
27         matrix = matrix.astype('float') / matrix.sum()
28     fig, ax = plt.subplots(figsize=figsize)
29     fig.canvas.set_window_title(title)
30     # We want to show all ticks...
31     ax.set(xticks=np.arange(matrix.shape[1]),
32         yticks=np.arange(matrix.shape[0]),
33         # ... and label them with the respective list entries
34         xticklabels=xticklabels, yticklabels=yticklabels,
35         title=title,
36         xlabel=xlabel,
37         ylabel=ylabel)
38     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
39     ax.figure.colorbar(im, ax=ax)
40 
41     # Rotate the tick labels and set their alignment.
42     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
43         rotation_mode=""anchor"")
44 
45     # Loop over data dimensions and create text annotations.
46     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
47     thresh = matrix.max() / 2.
48     for i in range(matrix.shape[0]):
49         for j in range(matrix.shape[1]):
50             ax.text(j, i, format(matrix[i, j], fmt),
51                 ha=""center"", va=""center"",
52                 color=""white"" if matrix[i, j] > thresh else ""black"")
53     fig.tight_layout()
54     return fig","10 log = logging.getLogger(__name__)
11 
12 
13 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9),
14         titleAdd: str = None) -> matplotlib.figure.Figure:
15     """"""
16     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
17     :param title: the plot's title
18     :param xticklabels: the labels for the x-axis ticks
19     :param yticklabels: the labels for the y-axis ticks
20     :param xlabel: the label for the x-axis
21     :param ylabel: the label for the y-axis
22     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
23     :param titleAdd: an optional second line to add to the title
24     :return: the figure object
25     """"""
26     matrix = np.transpose(matrix)
27 
28     if titleAdd is not None:
29         title += f""\n {titleAdd} ""
30 
31     if normalize:
32         matrix = matrix.astype('float') / matrix.sum()
33     fig, ax = plt.subplots(figsize=figsize)
34     fig.canvas.set_window_title(title.replace(""\n"", "" ""))
35     # We want to show all ticks...
36     ax.set(xticks=np.arange(matrix.shape[1]),
37         yticks=np.arange(matrix.shape[0]),
38         # ... and label them with the respective list entries
39         xticklabels=xticklabels, yticklabels=yticklabels,
40         title=title,
41         xlabel=xlabel,
42         ylabel=ylabel)
43     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
44     ax.figure.colorbar(im, ax=ax)
45 
46     # Rotate the tick labels and set their alignment.
47     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
48         rotation_mode=""anchor"")
49 
50     # Loop over data dimensions and create text annotations.
51     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
52     thresh = matrix.max() / 2.
53     for i in range(matrix.shape[0]):
54         for j in range(matrix.shape[1]):
55             ax.text(j, i, format(matrix[i, j], fmt),
56                 ha=""center"", va=""center"",
57                 color=""white"" if matrix[i, j] > thresh else ""black"")
58     fig.tight_layout()
59     return fig
60 
61 
","Before: 29
After: 34",add a basic plot for the nparrays,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,213,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 122, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 20, ',': 30, 'typed_parameter': 4, ':': 10, 'type': 8, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 2, '=': 26, 'true': 1, 'tuple': 1, 'integer': 7, ')': 20, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'attribute': 26, '.': 26, 'block': 5, 'expression_statement': 13, 'string': 13, 'string_start': 13, 'string_content': 13, 'string_end': 13, 'assignment': 6, 'call': 17, 'argument_list': 17, 'if_statement': 2, 'if': 5, 'comparison_operator': 3, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 3, 'escape_sequence': 1, '+': 1, '/': 2, 'pattern_list': 1, 'keyword_argument': 17, 'comment': 4, 'subscript': 6, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.73365241150548,0.7314877309414142,"(tensor([0.9615]), tensor([0.9661]), tensor([0.9638]), tensor([0.9656]))"
"5 import numpy as np
6 
7 
8 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9),
9         titleAdd: str = None) -> matplotlib.figure.Figure:
10     """"""
11     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
12     :param title: the plot's title
13     :param xticklabels: the labels for the x-axis ticks
14     :param yticklabels: the labels for the y-axis ticks
15     :param xlabel: the label for the x-axis
16     :param ylabel: the label for the y-axis
17     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
18     :param titleAdd: an optional second line to add to the title
19     :return: the figure object
20     """"""
21     matrix = np.transpose(matrix)
22 
23     if titleAdd is not None:
24         title += ""\n"" + titleAdd
25 
26     if normalize:
27         matrix = matrix.astype('float') / matrix.sum()
28     fig, ax = plt.subplots(figsize=figsize)
29     fig.canvas.set_window_title(title)
30     # We want to show all ticks...
31     ax.set(xticks=np.arange(matrix.shape[1]),
32         yticks=np.arange(matrix.shape[0]),
33         # ... and label them with the respective list entries
34         xticklabels=xticklabels, yticklabels=yticklabels,
35         title=title,
36         xlabel=xlabel,
37         ylabel=ylabel)
38     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
39     ax.figure.colorbar(im, ax=ax)
40 
41     # Rotate the tick labels and set their alignment.
42     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
43         rotation_mode=""anchor"")
44 
45     # Loop over data dimensions and create text annotations.
46     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
47     thresh = matrix.max() / 2.
48     for i in range(matrix.shape[0]):
49         for j in range(matrix.shape[1]):
50             ax.text(j, i, format(matrix[i, j], fmt),
51                 ha=""center"", va=""center"",
52                 color=""white"" if matrix[i, j] > thresh else ""black"")
53     fig.tight_layout()
54     return fig","10 log = logging.getLogger(__name__)
11 
12 
13 def plotMatrix(matrix, title, xticklabels: Sequence[str], yticklabels: Sequence[str], xlabel: str, ylabel: str, normalize=True, figsize=(9,9),
14         titleAdd: str = None) -> matplotlib.figure.Figure:
15     """"""
16     :param matrix: matrix whose data to plot, where matrix[i, j] will be rendered at x=i, y=j
17     :param title: the plot's title
18     :param xticklabels: the labels for the x-axis ticks
19     :param yticklabels: the labels for the y-axis ticks
20     :param xlabel: the label for the x-axis
21     :param ylabel: the label for the y-axis
22     :param normalize: whether to normalise the matrix before plotting it (dividing each entry by the sum of all entries)
23     :param titleAdd: an optional second line to add to the title
24     :return: the figure object
25     """"""
26     matrix = np.transpose(matrix)
27 
28     if titleAdd is not None:
29         title += f""\n {titleAdd} ""
30 
31     if normalize:
32         matrix = matrix.astype('float') / matrix.sum()
33     fig, ax = plt.subplots(figsize=figsize)
34     fig.canvas.set_window_title(title.replace(""\n"", "" ""))
35     # We want to show all ticks...
36     ax.set(xticks=np.arange(matrix.shape[1]),
37         yticks=np.arange(matrix.shape[0]),
38         # ... and label them with the respective list entries
39         xticklabels=xticklabels, yticklabels=yticklabels,
40         title=title,
41         xlabel=xlabel,
42         ylabel=ylabel)
43     im = ax.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)
44     ax.figure.colorbar(im, ax=ax)
45 
46     # Rotate the tick labels and set their alignment.
47     plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",
48         rotation_mode=""anchor"")
49 
50     # Loop over data dimensions and create text annotations.
51     fmt = '.2f' if normalize else ('.2f' if matrix.dtype == np.float else 'd')
52     thresh = matrix.max() / 2.
53     for i in range(matrix.shape[0]):
54         for j in range(matrix.shape[1]):
55             ax.text(j, i, format(matrix[i, j], fmt),
56                 ha=""center"", va=""center"",
57                 color=""white"" if matrix[i, j] > thresh else ""black"")
58     fig.tight_layout()
59     return fig
60 
61 
","Before: 54
After: 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98",add a basic plot for the nparrays,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,547,"{'module': 1, 'import_statement': 1, 'import': 1, 'aliased_import': 1, 'dotted_name': 1, 'identifier': 122, 'as': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 20, ',': 30, 'typed_parameter': 4, ':': 10, 'type': 8, 'generic_type': 2, 'type_parameter': 2, '[': 8, ']': 8, 'default_parameter': 2, '=': 26, 'true': 1, 'tuple': 1, 'integer': 7, ')': 20, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'attribute': 26, '.': 26, 'block': 5, 'expression_statement': 13, 'string': 13, 'string_start': 13, 'string_content': 13, 'string_end': 13, 'assignment': 6, 'call': 17, 'argument_list': 17, 'if_statement': 2, 'if': 5, 'comparison_operator': 3, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 3, 'escape_sequence': 1, '+': 1, '/': 2, 'pattern_list': 1, 'keyword_argument': 17, 'comment': 4, 'subscript': 6, 'conditional_expression': 3, 'else': 3, 'parenthesized_expression': 1, '==': 1, 'float': 1, 'for_statement': 2, 'for': 2, 'in': 2, '>': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.73365241150548,0.7314877309414142,"(tensor([0.9615]), tensor([0.9661]), tensor([0.9638]), tensor([0.9656]))"
"255     def setName(self, name):
256         self._name = name
257 
258     def getName(self):
259         if self._name is None:
260             return ""unnamed-%x"" % id(self)
261         return self._name
262 
263     def setFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]):
","255     def setName(self, name):
256         self._name = name
257 
258     def getName(self):
259         if self._name is None:
260             return ""unnamed-%s-%x"" % (self.__class__.__name__, id(self))
261         return self._name
262 
263     def setFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]):
","Before: 260
After: 260",fix vector_model.py for python3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,64ca3077cd2e31a6030cb562741a8a62b8677e6e,c1d84d1f8d929b6a681936eb93e84192a25b07cd,0,1879,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 14, 'parameters': 2, '(': 3, ',': 1, ')': 3, ':': 3, 'block': 3, 'expression_statement': 1, 'assignment': 1, 'attribute': 3, '.': 3, '=': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'return_statement': 2, 'return': 2, 'binary_operator': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '%': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8514772791298109,0.855577181382208,"(tensor([0.9711]), tensor([0.9918]), tensor([0.9814]), tensor([0.9897]))"
"218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
219         self._trackedExperiment = trackedExperiment
220 
221     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
222         """"""
223         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
224         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
225             Note that the column names that are generated depend on the evaluator/validator being applied.
226         :return: the data frame with all evaluation results
227         """"""
228         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
229         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
230 
231         def collectResult(values):
232             if values is None:
233                 return
234             if loggingCallback is not None:
235                 loggingCallback(values)
236             paramsMetricsCollection.addValues(values)
237             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
238 
239         if self.numProcesses == 1:
240             for parameterOptions in self.parameterOptionsList:
241                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
242                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
243         else:
244             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
245             futures = []
246             for parameterOptions in self.parameterOptionsList:
247                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
248                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
249                         **paramsDict))
250             for i, future in enumerate(futures):
251                 collectResult(future.result())
252 
253         return paramsMetricsCollection.getDataFrame()
254 
255 
","212     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
213         self._trackedExperiment = trackedExperiment
214 
215     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
216         """"""
217         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
218         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
219             Note that the column names that are generated depend on the evaluator/validator being applied.
220         :return: the data frame with all evaluation results
221         """"""
222         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
223         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
224 
225         def collectResult(values):
226             if values is None:
227                 return
228             if loggingCallback is not None:
229                 loggingCallback(values)
230             paramsMetricsCollection.addValues(values)
231             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
232 
233         if self.numProcesses == 1:
234             for parameterOptions in self.parameterOptionsList:
235                 for paramsDict in iterParamCombinations(parameterOptions):
236                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
237         else:
238             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
239             futures = []
240             for parameterOptions in self.parameterOptionsList:
241                 for paramsDict in iterParamCombinations(parameterOptions):
242                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
243                         **paramsDict))
244             for future in futures:
245                 collectResult(future.result())
246 
247         return paramsMetricsCollection.getDataFrame()
248 
249 
","Before: 241
After: 235",remove unnecessary uses of iterparamcombinations,hyperopt: minor simplifications in interfaces and code - breaking change!,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,d7ef2e072429b077f46b1b45e966e6aa7acd5dce,145bd43d79740d3a9ebf20aa9724a8f592eb2d8e,0,1910,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 97, 'parameters': 3, '(': 22, ',': 15, 'typed_parameter': 2, ':': 14, 'type': 5, ')': 22, 'block': 12, 'expression_statement': 12, 'assignment': 5, 'attribute': 24, '.': 24, '=': 9, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'default_parameter': 1, 'none': 5, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'conditional_expression': 1, 'if': 4, 'comparison_operator': 4, 'is not': 4, 'else': 2, 'call': 19, 'argument_list': 19, 'keyword_argument': 3, 'if_statement': 3, 'is': 1, 'return_statement': 2, 'return': 2, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'pattern_list': 3, 'dictionary_splat': 2, '**': 2, 'else_clause': 1, 'list': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6768893485407061,0.6749393966917697,"(tensor([0.9806]), tensor([0.9702]), tensor([0.9754]), tensor([0.9712]))"
"218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
219         self._trackedExperiment = trackedExperiment
220 
221     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
222         """"""
223         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
224         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
225             Note that the column names that are generated depend on the evaluator/validator being applied.
226         :return: the data frame with all evaluation results
227         """"""
228         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
229         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
230 
231         def collectResult(values):
232             if values is None:
233                 return
234             if loggingCallback is not None:
235                 loggingCallback(values)
236             paramsMetricsCollection.addValues(values)
237             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
238 
239         if self.numProcesses == 1:
240             for parameterOptions in self.parameterOptionsList:
241                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
242                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
243         else:
244             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
245             futures = []
246             for parameterOptions in self.parameterOptionsList:
247                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
248                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
249                         **paramsDict))
250             for i, future in enumerate(futures):
251                 collectResult(future.result())
252 
253         return paramsMetricsCollection.getDataFrame()
254 
255 
","212     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
213         self._trackedExperiment = trackedExperiment
214 
215     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
216         """"""
217         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
218         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
219             Note that the column names that are generated depend on the evaluator/validator being applied.
220         :return: the data frame with all evaluation results
221         """"""
222         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
223         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
224 
225         def collectResult(values):
226             if values is None:
227                 return
228             if loggingCallback is not None:
229                 loggingCallback(values)
230             paramsMetricsCollection.addValues(values)
231             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
232 
233         if self.numProcesses == 1:
234             for parameterOptions in self.parameterOptionsList:
235                 for paramsDict in iterParamCombinations(parameterOptions):
236                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
237         else:
238             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
239             futures = []
240             for parameterOptions in self.parameterOptionsList:
241                 for paramsDict in iterParamCombinations(parameterOptions):
242                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
243                         **paramsDict))
244             for future in futures:
245                 collectResult(future.result())
246 
247         return paramsMetricsCollection.getDataFrame()
248 
249 
","Before: 247
After: 241",remove unnecessary uses of iterparamcombinations,hyperopt: minor simplifications in interfaces and code - breaking change!,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,d7ef2e072429b077f46b1b45e966e6aa7acd5dce,145bd43d79740d3a9ebf20aa9724a8f592eb2d8e,0,1996,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 97, 'parameters': 3, '(': 22, ',': 15, 'typed_parameter': 2, ':': 14, 'type': 5, ')': 22, 'block': 12, 'expression_statement': 12, 'assignment': 5, 'attribute': 24, '.': 24, '=': 9, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'default_parameter': 1, 'none': 5, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'conditional_expression': 1, 'if': 4, 'comparison_operator': 4, 'is not': 4, 'else': 2, 'call': 19, 'argument_list': 19, 'keyword_argument': 3, 'if_statement': 3, 'is': 1, 'return_statement': 2, 'return': 2, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'pattern_list': 3, 'dictionary_splat': 2, '**': 2, 'else_clause': 1, 'list': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6768893485407061,0.6749393966917697,"(tensor([0.9806]), tensor([0.9702]), tensor([0.9754]), tensor([0.9712]))"
"218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
219         self._trackedExperiment = trackedExperiment
220 
221     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
222         """"""
223         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
224         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
225             Note that the column names that are generated depend on the evaluator/validator being applied.
226         :return: the data frame with all evaluation results
227         """"""
228         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
229         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
230 
231         def collectResult(values):
232             if values is None:
233                 return
234             if loggingCallback is not None:
235                 loggingCallback(values)
236             paramsMetricsCollection.addValues(values)
237             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
238 
239         if self.numProcesses == 1:
240             for parameterOptions in self.parameterOptionsList:
241                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
242                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
243         else:
244             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
245             futures = []
246             for parameterOptions in self.parameterOptionsList:
247                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
248                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
249                         **paramsDict))
250             for i, future in enumerate(futures):
251                 collectResult(future.result())
252 
253         return paramsMetricsCollection.getDataFrame()
254 
255 
","212     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
213         self._trackedExperiment = trackedExperiment
214 
215     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
216         """"""
217         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
218         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
219             Note that the column names that are generated depend on the evaluator/validator being applied.
220         :return: the data frame with all evaluation results
221         """"""
222         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
223         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
224 
225         def collectResult(values):
226             if values is None:
227                 return
228             if loggingCallback is not None:
229                 loggingCallback(values)
230             paramsMetricsCollection.addValues(values)
231             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
232 
233         if self.numProcesses == 1:
234             for parameterOptions in self.parameterOptionsList:
235                 for paramsDict in iterParamCombinations(parameterOptions):
236                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
237         else:
238             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
239             futures = []
240             for parameterOptions in self.parameterOptionsList:
241                 for paramsDict in iterParamCombinations(parameterOptions):
242                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
243                         **paramsDict))
244             for future in futures:
245                 collectResult(future.result())
246 
247         return paramsMetricsCollection.getDataFrame()
248 
249 
","Before: 250
After: 244",remove unnecessary uses of iterparamcombinations,hyperopt: minor simplifications in interfaces and code - breaking change!,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,d7ef2e072429b077f46b1b45e966e6aa7acd5dce,145bd43d79740d3a9ebf20aa9724a8f592eb2d8e,0,2048,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 97, 'parameters': 3, '(': 22, ',': 15, 'typed_parameter': 2, ':': 14, 'type': 5, ')': 22, 'block': 12, 'expression_statement': 12, 'assignment': 5, 'attribute': 24, '.': 24, '=': 9, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'default_parameter': 1, 'none': 5, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'conditional_expression': 1, 'if': 4, 'comparison_operator': 4, 'is not': 4, 'else': 2, 'call': 19, 'argument_list': 19, 'keyword_argument': 3, 'if_statement': 3, 'is': 1, 'return_statement': 2, 'return': 2, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'pattern_list': 3, 'dictionary_splat': 2, '**': 2, 'else_clause': 1, 'list': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6768893485407061,0.6749393966917697,"(tensor([0.9806]), tensor([0.9702]), tensor([0.9754]), tensor([0.9712]))"
"341         self._trackedExperiment = trackedExperiment
342 
343     @classmethod
344     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
345             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
346         metrics = None
347         if parameterCombinationEquivalenceClassValueCache is not None:
348             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
349         if metrics is not None:
350             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
351             return metrics
352         else:
353             cls._log.info(f""Evaluating parameter combination {params}"")
354             model = modelFactory(**params)
355             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
356             cls._log.info(f""Got metrics {metrics} for {params}"")
357             if trackedExperiment is not None:
358                 values = dict(metrics)
359                 values[""str(model)""] = str(model)
360                 values.update(**params)
361                 trackedExperiment.trackValues(values)
362             if parametersMetricsCollection is not None:
363                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
364                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
365             if parameterCombinationEquivalenceClassValueCache is not None:
366                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
367         return metrics
368 
369     def _computeMetric(self, params):
","335         self._trackedExperiment = trackedExperiment
336 
337     @classmethod
338     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
339             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
340         metrics = None
341         if parameterCombinationEquivalenceClassValueCache is not None:
342             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
343         if metrics is not None:
344             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
345             return metrics
346         else:
347             cls._log.info(f""Evaluating parameter combination {params}"")
348             model = modelFactory(**params)
349             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
350             cls._log.info(f""Got metrics {metrics} for {params}"")
351 
352             values = dict(metrics)
353             values[""str(model)""] = str(model)
354             values.update(**params)
355             if trackedExperiment is not None:
356                 trackedExperiment.trackValues(values)
357             if parametersMetricsCollection is not None:
358                 parametersMetricsCollection.addValues(values)
359                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
360             if parameterCombinationEquivalenceClassValueCache is not None:
361                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
362         return metrics
363 
364     def _computeMetric(self, params):
","Before: 358, 359, 360, 363
After: 358",remove unnecessary uses of iterparamcombinations,hyperopt: minor simplifications in interfaces and code - breaking change!,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,d7ef2e072429b077f46b1b45e966e6aa7acd5dce,145bd43d79740d3a9ebf20aa9724a8f592eb2d8e,0,2991,"{'module': 1, 'expression_statement': 15, 'assignment': 7, 'attribute': 16, 'identifier': 73, '.': 16, '=': 7, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 16, ',': 10, 'typed_parameter': 1, ':': 8, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 16, 'block': 7, 'none': 6, 'if_statement': 5, 'if': 5, 'comparison_operator': 5, 'is not': 10, 'call': 15, 'argument_list': 15, 'string': 5, 'string_start': 5, 'string_content': 8, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 5, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7295390487791403,0.7248195345283029,"(tensor([0.9783]), tensor([0.9771]), tensor([0.9777]), tensor([0.9772]))"
"1 from typing import Sequence
2 import logging
3 import lightgbm
4 import pandas as pd
5 
","1 from typing import Sequence, Union, Optional
2 import logging
3 import lightgbm
4 import pandas as pd
5 import re
","Before: 1
After: 1, 5, 7",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,9,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'dotted_name': 5, 'identifier': 6, 'import': 4, 'import_statement': 3, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6436045454907839,0.5667521832510313,"(tensor([0.9435]), tensor([0.9672]), tensor([0.9552]), tensor([0.9648]))"
"11 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
12     _log = _log.getChild(__qualname__)
13 
14     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
15         """"""
16         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
17             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
18             need not be specified (should be inferred automatically).
19             In general, passing categorical features is preferable to using one-hot encoding, for example.
20         :param random_state: the random seed to use
21         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
22         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
23         """"""
24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     _log = _log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31, **modelArgs):
17         """"""
18         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
19             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
20             need not be specified (should be inferred automatically).
21             In general, passing categorical features is preferable to using one-hot encoding, for example.
22         :param random_state: the random seed to use
23         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
24         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
25         """"""
26         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
27 
28         if type(categoricalFeatureNames) == str:
29             categoricalFeatureNameRegex = categoricalFeatureNames
30         else:
31             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
32                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
33             else:
34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 14
After: 16",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,111,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 27, 'argument_list': 4, '(': 5, ')': 5, ':': 3, 'block': 2, 'expression_statement': 4, 'assignment': 2, '=': 7, 'call': 3, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6135185733918942,0.6118695203608663,"(tensor([0.9217]), tensor([0.9703]), tensor([0.9454]), tensor([0.9652]))"
"11 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
12     _log = _log.getChild(__qualname__)
13 
14     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
15         """"""
16         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
17             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
18             need not be specified (should be inferred automatically).
19             In general, passing categorical features is preferable to using one-hot encoding, for example.
20         :param random_state: the random seed to use
21         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
22         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
23         """"""
24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     _log = _log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31, **modelArgs):
17         """"""
18         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
19             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
20             need not be specified (should be inferred automatically).
21             In general, passing categorical features is preferable to using one-hot encoding, for example.
22         :param random_state: the random seed to use
23         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
24         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
25         """"""
26         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
27 
28         if type(categoricalFeatureNames) == str:
29             categoricalFeatureNameRegex = categoricalFeatureNames
30         else:
31             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
32                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
33             else:
34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 24
After: 28, 29, 30, 31, 32, 33, 34, 35, 36",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,124,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 27, 'argument_list': 4, '(': 5, ')': 5, ':': 3, 'block': 2, 'expression_statement': 4, 'assignment': 2, '=': 7, 'call': 3, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6135185733918942,0.6118695203608663,"(tensor([0.9217]), tensor([0.9703]), tensor([0.9454]), tensor([0.9652]))"
"24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
28         if self.categoricalFeatureNames is not None:
29             cols = list(inputs.columns)
30             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
31             args = {""cat_column"": colIndices}
32             self._log.info(f""Updating model parameters with {args}"")
33             self.modelArgs.update(args)
34 
35 
","34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
38         if self._categoricalFeatureNameRegex is not None:
39             cols = list(inputs.columns)
40             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
41             colIndices = [cols.index(f) for f in categoricalFeatureNames]
42             args = {""cat_column"": colIndices}
43             self._log.info(f""Updating model parameters with {args}"")
44             self.modelArgs.update(args)
45 
46 
","Before: 28
After: 38",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,197,"{'module': 1, 'expression_statement': 7, 'assignment': 4, 'attribute': 14, 'identifier': 44, '.': 14, '=': 6, 'call': 6, 'argument_list': 6, '(': 7, ')': 7, ',': 5, 'keyword_argument': 2, 'dictionary_splat': 1, '**': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 2, 'pair': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '}': 2, 'interpolation': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5086759311424307,0.47617248354487524,"(tensor([0.9131]), tensor([0.8904]), tensor([0.9016]), tensor([0.8926]))"
"24         self.categoricalFeatureNames = categoricalFeatureNames
25         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
26 
27     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
28         if self.categoricalFeatureNames is not None:
29             cols = list(inputs.columns)
30             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
31             args = {""cat_column"": colIndices}
32             self._log.info(f""Updating model parameters with {args}"")
33             self.modelArgs.update(args)
34 
35 
","34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
38         if self._categoricalFeatureNameRegex is not None:
39             cols = list(inputs.columns)
40             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
41             colIndices = [cols.index(f) for f in categoricalFeatureNames]
42             args = {""cat_column"": colIndices}
43             self._log.info(f""Updating model parameters with {args}"")
44             self.modelArgs.update(args)
45 
46 
","Before: 30
After: 40, 41",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,234,"{'module': 1, 'expression_statement': 7, 'assignment': 4, 'attribute': 14, 'identifier': 44, '.': 14, '=': 6, 'call': 6, 'argument_list': 6, '(': 7, ')': 7, ',': 5, 'keyword_argument': 2, 'dictionary_splat': 1, '**': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 2, 'pair': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '}': 2, 'interpolation': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5086759311424307,0.47617248354487524,"(tensor([0.9131]), tensor([0.8904]), tensor([0.9016]), tensor([0.8926]))"
"36 class LightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
37     _log = _log.getChild(__qualname__)
38 
39     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
40         """"""
41         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical
42             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
43             need not be specified (should be inferred automatically, but we have never actually tested this behaviour
44             successfully for a classification model).
45             In general, passing categorical features may be preferable to using one-hot encoding, for example.
46         :param random_state: the random seed to use
47         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
48         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
49         """"""
50         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
51         self.categoricalFeatureNames = categoricalFeatureNames
52 
53     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","47 class LightGBMVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
48     _log = _log.getChild(__qualname__)
49 
50     def __init__(self, categoricalFeatureNames: Sequence[str] = None, random_state=42, num_leaves=31, **modelArgs):
51         """"""
52         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical
53             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
54             need not be specified (should be inferred automatically, but we have never actually tested this behaviour
55             successfully for a classification model).
56             In general, passing categorical features may be preferable to using one-hot encoding, for example.
57         :param random_state: the random seed to use
58         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
59         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
60         """"""
61         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
62 
63         if type(categoricalFeatureNames) == str:
64             categoricalFeatureNameRegex = categoricalFeatureNames
65         else:
66             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
67                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
68             else:
69                 categoricalFeatureNameRegex = None
70         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
71 
72     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 51
After: 62, 63, 64, 65, 66, 67, 68, 69, 70",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,387,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 27, 'argument_list': 4, '(': 5, ')': 5, ':': 3, 'block': 2, 'expression_statement': 4, 'assignment': 2, '=': 7, 'call': 3, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_default_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'none': 1, 'default_parameter': 2, 'integer': 2, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6470006300051246,0.644341048029572,"(tensor([0.9237]), tensor([0.9727]), tensor([0.9476]), tensor([0.9676]))"
"50         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
51         self.categoricalFeatureNames = categoricalFeatureNames
52 
53     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
54         if self.categoricalFeatureNames is not None:
55             cols = list(inputs.columns)
56             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
57             args = {""cat_column"": colIndices}
58             self._log.info(f""Updating model parameters with {args}"")
59             self.modelArgs.update(args)","69                 categoricalFeatureNameRegex = None
70         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
71 
72     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
73         if self._categoricalFeatureNameRegex is not None:
74             cols = list(inputs.columns)
75             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
76             colIndices = [cols.index(f) for f in categoricalFeatureNames]
77             args = {""cat_column"": colIndices}
78             self._log.info(f""Updating model parameters with {args}"")
79             self.modelArgs.update(args)","Before: 54
After: 73",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,426,"{'module': 1, 'expression_statement': 5, 'call': 4, 'attribute': 10, 'identifier': 36, 'argument_list': 4, '(': 5, ')': 5, '.': 10, ',': 5, 'keyword_argument': 2, '=': 6, 'dictionary_splat': 1, '**': 1, 'assignment': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 1, 'pair': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5153083957710737,0.47976065316646294,"(tensor([0.9127]), tensor([0.8895]), tensor([0.9009]), tensor([0.8918]))"
"50         super().__init__(lightgbm.sklearn.LGBMClassifier, random_state=random_state, num_leaves=num_leaves, **modelArgs)
51         self.categoricalFeatureNames = categoricalFeatureNames
52 
53     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
54         if self.categoricalFeatureNames is not None:
55             cols = list(inputs.columns)
56             colIndices = [cols.index(f) for f in self.categoricalFeatureNames]
57             args = {""cat_column"": colIndices}
58             self._log.info(f""Updating model parameters with {args}"")
59             self.modelArgs.update(args)","69                 categoricalFeatureNameRegex = None
70         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
71 
72     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
73         if self._categoricalFeatureNameRegex is not None:
74             cols = list(inputs.columns)
75             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
76             colIndices = [cols.index(f) for f in categoricalFeatureNames]
77             args = {""cat_column"": colIndices}
78             self._log.info(f""Updating model parameters with {args}"")
79             self.modelArgs.update(args)","Before: 56
After: 75, 76",add orregexgroup to lightgbmvectorregressionmodel,Fixed imports,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,6546f2106d85e8f3344872aa2f3435c19ac12399,8e290843062234acf978eb940b5d1e45f19bcff0,1,463,"{'module': 1, 'expression_statement': 5, 'call': 4, 'attribute': 10, 'identifier': 36, 'argument_list': 4, '(': 5, ')': 5, '.': 10, ',': 5, 'keyword_argument': 2, '=': 6, 'dictionary_splat': 1, '**': 1, 'assignment': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1, 'dictionary': 1, '{': 1, 'pair': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 54, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Sequence [ str ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 14, 'end_line': 25, 'full_parameters': ['self', ' categoricalFeatureNames : Sequence [ str ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5153083957710737,0.47976065316646294,"(tensor([0.9127]), tensor([0.8895]), tensor([0.9009]), tensor([0.8918]))"
"152                 os.makedirs(dirname, exist_ok=True)
153             self.df.to_csv(self.csvPath, index=False)
154 
155     def addModelParamsMetrics(self, model, params, metrics):
156         values = dict(metrics)
157         values.update(params)
158         values[""str(model)""] = str(model)
159         self.addValues(values)
160 
161     def getDataFrame(self) -> pd.DataFrame:
","165     def getDataFrame(self) -> pd.DataFrame:
166         return self.df
167 
168 
169 class GridSearch(TrackedExperimentDataProvider):
170     """"""
171     Instances of this class can be used for evaluating models with different user-provided parametrizations
172     over the same data and persisting the results
173     """"""
174     _log = _log.getChild(__qualname__)
","Before: 155, 156, 157, 158, 159, 160
After: 170, 171, 172, 173",add utility class for holding and persisting results,Fixed typo in comment,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,2644b18545861cebb09740aa2e2baec05b4c9f29,6546f2106d85e8f3344872aa2f3435c19ac12399,0,1080,"{'module': 1, 'expression_statement': 6, 'call': 6, 'attribute': 6, 'identifier': 27, '.': 6, 'argument_list': 6, '(': 7, ',': 5, 'keyword_argument': 2, '=': 4, 'true': 1, ')': 7, 'false': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'assignment': 2, 'subscript': 1, '[': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.12774652376231568,0.10095372832561395,"(tensor([0.7134]), tensor([0.7301]), tensor([0.7217]), tensor([0.7284]))"
"218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
219         self._trackedExperiment = trackedExperiment
220 
221     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
222         """"""
223         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
224         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
225             Note that the column names that are generated depend on the evaluator/validator being applied.
226         :return: the data frame with all evaluation results
227         """"""
228         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
229         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
230 
231         def collectResult(values):
232             if values is None:
233                 return
234             if loggingCallback is not None:
235                 loggingCallback(values)
236             paramsMetricsCollection.addValues(values)
237             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
238 
239         if self.numProcesses == 1:
240             for parameterOptions in self.parameterOptionsList:
241                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
242                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
243         else:
244             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
245             futures = []
246             for parameterOptions in self.parameterOptionsList:
247                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
248                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
249                         **paramsDict))
250             for i, future in enumerate(futures):
251                 collectResult(future.result())
252 
253         return paramsMetricsCollection.getDataFrame()
254 
255 
","226     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
227         self._trackedExperiment = trackedExperiment
228 
229     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
230         """"""
231         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
232         to that file directly after being computed
233 
234         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
235         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
236             Note that the column names that are generated depend on the evaluator/validator being applied.
237         :return: the data frame with all evaluation results
238         """"""
239         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
240         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
241 
242         def collectResult(values):
243             if values is None:
244                 return
245             if loggingCallback is not None:
246                 loggingCallback(values)
247             paramsMetricsCollection.addValues(values)
248             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
249 
250         if self.numProcesses == 1:
251             for parameterOptions in self.parameterOptionsList:
252                 for paramsDict in iterParamCombinations(parameterOptions):
253                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
254         else:
255             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
256             futures = []
257             for parameterOptions in self.parameterOptionsList:
258                 for paramsDict in iterParamCombinations(parameterOptions):
259                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
260                         **paramsDict))
261             for future in futures:
262                 collectResult(future.result())
263 
264         return paramsMetricsCollection.getDataFrame()
265 
266 
","Before: 241
After: 252",add utility class for holding and persisting results,Fixed typo in comment,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,2644b18545861cebb09740aa2e2baec05b4c9f29,6546f2106d85e8f3344872aa2f3435c19ac12399,0,1910,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 97, 'parameters': 3, '(': 22, ',': 15, 'typed_parameter': 2, ':': 14, 'type': 5, ')': 22, 'block': 12, 'expression_statement': 12, 'assignment': 5, 'attribute': 24, '.': 24, '=': 9, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'default_parameter': 1, 'none': 5, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'conditional_expression': 1, 'if': 4, 'comparison_operator': 4, 'is not': 4, 'else': 2, 'call': 19, 'argument_list': 19, 'keyword_argument': 3, 'if_statement': 3, 'is': 1, 'return_statement': 2, 'return': 2, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'pattern_list': 3, 'dictionary_splat': 2, '**': 2, 'else_clause': 1, 'list': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6381342192933287,0.6303886663890887,"(tensor([0.9398]), tensor([0.9460]), tensor([0.9429]), tensor([0.9453]))"
"218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
219         self._trackedExperiment = trackedExperiment
220 
221     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
222         """"""
223         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
224         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
225             Note that the column names that are generated depend on the evaluator/validator being applied.
226         :return: the data frame with all evaluation results
227         """"""
228         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
229         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
230 
231         def collectResult(values):
232             if values is None:
233                 return
234             if loggingCallback is not None:
235                 loggingCallback(values)
236             paramsMetricsCollection.addValues(values)
237             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
238 
239         if self.numProcesses == 1:
240             for parameterOptions in self.parameterOptionsList:
241                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
242                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
243         else:
244             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
245             futures = []
246             for parameterOptions in self.parameterOptionsList:
247                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
248                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
249                         **paramsDict))
250             for i, future in enumerate(futures):
251                 collectResult(future.result())
252 
253         return paramsMetricsCollection.getDataFrame()
254 
255 
","226     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
227         self._trackedExperiment = trackedExperiment
228 
229     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
230         """"""
231         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
232         to that file directly after being computed
233 
234         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
235         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
236             Note that the column names that are generated depend on the evaluator/validator being applied.
237         :return: the data frame with all evaluation results
238         """"""
239         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
240         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
241 
242         def collectResult(values):
243             if values is None:
244                 return
245             if loggingCallback is not None:
246                 loggingCallback(values)
247             paramsMetricsCollection.addValues(values)
248             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
249 
250         if self.numProcesses == 1:
251             for parameterOptions in self.parameterOptionsList:
252                 for paramsDict in iterParamCombinations(parameterOptions):
253                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
254         else:
255             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
256             futures = []
257             for parameterOptions in self.parameterOptionsList:
258                 for paramsDict in iterParamCombinations(parameterOptions):
259                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
260                         **paramsDict))
261             for future in futures:
262                 collectResult(future.result())
263 
264         return paramsMetricsCollection.getDataFrame()
265 
266 
","Before: 247
After: 258",add utility class for holding and persisting results,Fixed typo in comment,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,2644b18545861cebb09740aa2e2baec05b4c9f29,6546f2106d85e8f3344872aa2f3435c19ac12399,0,1996,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 97, 'parameters': 3, '(': 22, ',': 15, 'typed_parameter': 2, ':': 14, 'type': 5, ')': 22, 'block': 12, 'expression_statement': 12, 'assignment': 5, 'attribute': 24, '.': 24, '=': 9, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'default_parameter': 1, 'none': 5, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'conditional_expression': 1, 'if': 4, 'comparison_operator': 4, 'is not': 4, 'else': 2, 'call': 19, 'argument_list': 19, 'keyword_argument': 3, 'if_statement': 3, 'is': 1, 'return_statement': 2, 'return': 2, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'pattern_list': 3, 'dictionary_splat': 2, '**': 2, 'else_clause': 1, 'list': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6381342192933287,0.6303886663890887,"(tensor([0.9398]), tensor([0.9460]), tensor([0.9429]), tensor([0.9453]))"
"218     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
219         self._trackedExperiment = trackedExperiment
220 
221     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
222         """"""
223         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
224         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
225             Note that the column names that are generated depend on the evaluator/validator being applied.
226         :return: the data frame with all evaluation results
227         """"""
228         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
229         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
230 
231         def collectResult(values):
232             if values is None:
233                 return
234             if loggingCallback is not None:
235                 loggingCallback(values)
236             paramsMetricsCollection.addValues(values)
237             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
238 
239         if self.numProcesses == 1:
240             for parameterOptions in self.parameterOptionsList:
241                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
242                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
243         else:
244             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
245             futures = []
246             for parameterOptions in self.parameterOptionsList:
247                 for i, paramsDict in enumerate(iterParamCombinations(parameterOptions)):
248                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
249                         **paramsDict))
250             for i, future in enumerate(futures):
251                 collectResult(future.result())
252 
253         return paramsMetricsCollection.getDataFrame()
254 
255 
","226     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
227         self._trackedExperiment = trackedExperiment
228 
229     def run(self, evaluatorOrValidator: Union[VectorModelEvaluator, VectorModelCrossValidator], sortColumnName=None) -> pd.DataFrame:
230         """"""
231         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
232         to that file directly after being computed
233 
234         :param evaluatorOrValidator: the evaluator or cross-validator with which to evaluate models
235         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
236             Note that the column names that are generated depend on the evaluator/validator being applied.
237         :return: the data frame with all evaluation results
238         """"""
239         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
240         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
241 
242         def collectResult(values):
243             if values is None:
244                 return
245             if loggingCallback is not None:
246                 loggingCallback(values)
247             paramsMetricsCollection.addValues(values)
248             _log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
249 
250         if self.numProcesses == 1:
251             for parameterOptions in self.parameterOptionsList:
252                 for paramsDict in iterParamCombinations(parameterOptions):
253                     collectResult(self._evalParams(self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider, **paramsDict))
254         else:
255             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
256             futures = []
257             for parameterOptions in self.parameterOptionsList:
258                 for paramsDict in iterParamCombinations(parameterOptions):
259                     futures.append(executor.submit(self._evalParams, self.modelFactory, evaluatorOrValidator, self.parameterCombinationSkipDecider,
260                         **paramsDict))
261             for future in futures:
262                 collectResult(future.result())
263 
264         return paramsMetricsCollection.getDataFrame()
265 
266 
","Before: 250
After: 261",add utility class for holding and persisting results,Fixed typo in comment,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,2644b18545861cebb09740aa2e2baec05b4c9f29,6546f2106d85e8f3344872aa2f3435c19ac12399,0,2048,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 97, 'parameters': 3, '(': 22, ',': 15, 'typed_parameter': 2, ':': 14, 'type': 5, ')': 22, 'block': 12, 'expression_statement': 12, 'assignment': 5, 'attribute': 24, '.': 24, '=': 9, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'default_parameter': 1, 'none': 5, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'conditional_expression': 1, 'if': 4, 'comparison_operator': 4, 'is not': 4, 'else': 2, 'call': 19, 'argument_list': 19, 'keyword_argument': 3, 'if_statement': 3, 'is': 1, 'return_statement': 2, 'return': 2, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'pattern_list': 3, 'dictionary_splat': 2, '**': 2, 'else_clause': 1, 'list': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6381342192933287,0.6303886663890887,"(tensor([0.9398]), tensor([0.9460]), tensor([0.9429]), tensor([0.9453]))"
"341         self._trackedExperiment = trackedExperiment
342 
343     @classmethod
344     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
345             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
346         metrics = None
347         if parameterCombinationEquivalenceClassValueCache is not None:
348             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
349         if metrics is not None:
350             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
351             return metrics
352         else:
353             cls._log.info(f""Evaluating parameter combination {params}"")
354             model = modelFactory(**params)
355             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
356             cls._log.info(f""Got metrics {metrics} for {params}"")
357             if trackedExperiment is not None:
358                 values = dict(metrics)
359                 values[""str(model)""] = str(model)
360                 values.update(**params)
361                 trackedExperiment.trackValues(values)
362             if parametersMetricsCollection is not None:
363                 parametersMetricsCollection.addModelParamsMetrics(model, params, metrics)
364                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
365             if parameterCombinationEquivalenceClassValueCache is not None:
366                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
367         return metrics
368 
369     def _computeMetric(self, params):
","352         self._trackedExperiment = trackedExperiment
353 
354     @classmethod
355     def _evalParams(cls, modelFactory, evaluatorOrValidator, parametersMetricsCollection: Optional[ParametersMetricsCollection],
356             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
357         metrics = None
358         if parameterCombinationEquivalenceClassValueCache is not None:
359             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
360         if metrics is not None:
361             cls._log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
362             return metrics
363         else:
364             cls._log.info(f""Evaluating parameter combination {params}"")
365             model = modelFactory(**params)
366             metrics = computeEvaluationMetricsDict(model, evaluatorOrValidator)
367             cls._log.info(f""Got metrics {metrics} for {params}"")
368 
369             values = dict(metrics)
370             values[""str(model)""] = str(model)
371             values.update(**params)
372             if trackedExperiment is not None:
373                 trackedExperiment.trackValues(values)
374             if parametersMetricsCollection is not None:
375                 parametersMetricsCollection.addValues(values)
376                 cls._log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
377             if parameterCombinationEquivalenceClassValueCache is not None:
378                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
379         return metrics
380 
381     def _computeMetric(self, params):
","Before: 358, 359, 360, 363
After: 375",add utility class for holding and persisting results,Fixed typo in comment,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,2644b18545861cebb09740aa2e2baec05b4c9f29,6546f2106d85e8f3344872aa2f3435c19ac12399,0,2991,"{'module': 1, 'expression_statement': 15, 'assignment': 7, 'attribute': 16, 'identifier': 73, '.': 16, '=': 7, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 16, ',': 10, 'typed_parameter': 1, ':': 8, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 16, 'block': 7, 'none': 6, 'if_statement': 5, 'if': 5, 'comparison_operator': 5, 'is not': 10, 'call': 15, 'argument_list': 15, 'string': 5, 'string_start': 5, 'string_content': 8, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 5, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7128711476519058,0.7045218773736682,"(tensor([0.9760]), tensor([0.9755]), tensor([0.9758]), tensor([0.9755]))"
"55     def _isCudaEnabled(self):
56         return self.cuda
57 
58     def _loadModel(self, modelFile):
59         try:
60             self.module = torch.load(modelFile)
61         except:
62             if self._isCudaEnabled():
63                 self.log.warning(""Loading of CUDA model failed, trying without CUDA..."")
64                 if type(modelFile) != str:
65                     modelFile.seek(0)
66                 self.module = torch.load(modelFile, map_location='cpu')
67                 self._setCudaEnabled(False)
68                 self.log.info(""Model successfully loaded to CPU"")
69             else:
70                 raise
71 
72     @abstractmethod
","56     def _isCudaEnabled(self):
57         return self.cuda
58 
59     def _loadModel(self, modelFile):
60         try:
61             self.module = torch.load(modelFile)
62             self._gpu = self._getGPUFromModelParameterDevice()
63         except:
64             if self._isCudaEnabled():
65                 if torch.cuda.device_count() > 0:
66                     newDevice = ""cuda:0""
67                 else:
68                     newDevice = ""cpu""
69                 self.log.warning(f""Loading of CUDA model failed, trying to map model to device {newDevice}..."")
70                 if type(modelFile) != str:
71                     modelFile.seek(0)
72                 try:
73                     self.module = torch.load(modelFile, map_location=newDevice)
74                 except:
75                     self.log.warning(f""Failure to map model to device {newDevice}, trying CPU..."")
76                     if newDevice != ""cpu"":
77                         newDevice = ""cpu""
78                         self.module = torch.load(modelFile, map_location=newDevice)
79                 if newDevice == ""cpu"":
80                     self._setCudaEnabled(False)
81                     self._gpu = None
82                 else:
83                     self._gpu = 0
84                 self.log.info(f""Model successfully loaded to {newDevice}"")
85             else:
86                 raise
87 
88     @abstractmethod
","Before: 63
After: 65, 66, 67, 68, 69",add support for gpu and cuda in TorchModel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,dbf16be5263eb80524614156eaee0b123094d325,1d7ac5cfcbac42e290c2bd8c0ca965d203e9c500,0,489,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 33, 'parameters': 2, '(': 10, ')': 10, ':': 7, 'block': 7, 'return_statement': 1, 'return': 1, 'attribute': 12, '.': 12, ',': 2, 'try_statement': 1, 'try': 1, 'expression_statement': 6, 'assignment': 2, '=': 3, 'call': 8, 'argument_list': 8, 'except_clause': 1, 'except': 1, 'if_statement': 2, 'if': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'comparison_operator': 1, '!=': 1, 'integer': 1, 'keyword_argument': 1, 'false': 1, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 55, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 36, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.24601775178536234,0.24644715729960176,"(tensor([0.8552]), tensor([0.9358]), tensor([0.8937]), tensor([0.9271]))"
"55     def _isCudaEnabled(self):
56         return self.cuda
57 
58     def _loadModel(self, modelFile):
59         try:
60             self.module = torch.load(modelFile)
61         except:
62             if self._isCudaEnabled():
63                 self.log.warning(""Loading of CUDA model failed, trying without CUDA..."")
64                 if type(modelFile) != str:
65                     modelFile.seek(0)
66                 self.module = torch.load(modelFile, map_location='cpu')
67                 self._setCudaEnabled(False)
68                 self.log.info(""Model successfully loaded to CPU"")
69             else:
70                 raise
71 
72     @abstractmethod
","56     def _isCudaEnabled(self):
57         return self.cuda
58 
59     def _loadModel(self, modelFile):
60         try:
61             self.module = torch.load(modelFile)
62             self._gpu = self._getGPUFromModelParameterDevice()
63         except:
64             if self._isCudaEnabled():
65                 if torch.cuda.device_count() > 0:
66                     newDevice = ""cuda:0""
67                 else:
68                     newDevice = ""cpu""
69                 self.log.warning(f""Loading of CUDA model failed, trying to map model to device {newDevice}..."")
70                 if type(modelFile) != str:
71                     modelFile.seek(0)
72                 try:
73                     self.module = torch.load(modelFile, map_location=newDevice)
74                 except:
75                     self.log.warning(f""Failure to map model to device {newDevice}, trying CPU..."")
76                     if newDevice != ""cpu"":
77                         newDevice = ""cpu""
78                         self.module = torch.load(modelFile, map_location=newDevice)
79                 if newDevice == ""cpu"":
80                     self._setCudaEnabled(False)
81                     self._gpu = None
82                 else:
83                     self._gpu = 0
84                 self.log.info(f""Model successfully loaded to {newDevice}"")
85             else:
86                 raise
87 
88     @abstractmethod
","Before: 66, 67, 68
After: 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84",add support for gpu and cuda in TorchModel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,dbf16be5263eb80524614156eaee0b123094d325,1d7ac5cfcbac42e290c2bd8c0ca965d203e9c500,0,536,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 33, 'parameters': 2, '(': 10, ')': 10, ':': 7, 'block': 7, 'return_statement': 1, 'return': 1, 'attribute': 12, '.': 12, ',': 2, 'try_statement': 1, 'try': 1, 'expression_statement': 6, 'assignment': 2, '=': 3, 'call': 8, 'argument_list': 8, 'except_clause': 1, 'except': 1, 'if_statement': 2, 'if': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'comparison_operator': 1, '!=': 1, 'integer': 1, 'keyword_argument': 1, 'false': 1, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 35, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 55, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 36, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.24601775178536234,0.24644715729960176,"(tensor([0.8552]), tensor([0.9358]), tensor([0.8937]), tensor([0.9271]))"
"60 
61 
62 class Plot:
63     def __init__(self, draw: Callable[[], plt.Axes]):
64         self.fig: matplotlib.figure.Figure = plt.figure()
65         self.ax = draw()
66 
67     def xlabel(self, label):
","60 
61 
62 class Plot:
63     def __init__(self, draw: Callable[[], plt.Axes] = None, name=None):
64         """"""
65         :param draw: function which returns a matplotlib.Axes object to show
66         :param name: name/number of the figure, which determines the window caption; it should be unique, as any plot
67             with the same name will have its contents rendered in the same window. By default, figures are number
68             sequentially.
69         """"""
70         self.fig: matplotlib.figure.Figure = plt.figure(name)
71         self.ax = draw()
72 
73     def xlabel(self, label):
","Before: 63, 64
After: 63, 64, 65, 66, 67, 68, 69, 70",add optional name to unique figures,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,dbf16be5263eb80524614156eaee0b123094d325,1d7ac5cfcbac42e290c2bd8c0ca965d203e9c500,0,629,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 17, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 2, 'typed_parameter': 1, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, 'attribute': 6, '.': 6, ')': 3, 'expression_statement': 2, 'assignment': 2, '=': 2, 'call': 2, 'argument_list': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.29994350365233863,0.2628029375859748,"(tensor([0.7575]), tensor([0.9376]), tensor([0.8380]), tensor([0.9159]))"
"12     recomputations of merge costs through the management of a priority queue of 
13     potential merges
14     """"""
15     log = log.getChild(__qualname__)
16 
17     class Cluster(object):
18         """"""
19         Base class for clusters that can be merged via GreedyAgglomerativeClustering
20         """"""
21 
","13     recomputations of merge costs through the management of a priority queue of 
14     potential merges
15     """"""
16     log = log.getChild(__qualname__)
17 
18     class Cluster(ABC):
19         """"""
20         Base class for clusters that can be merged via GreedyAgglomerativeClustering
21         """"""
22 
","Before: 17
After: 18, 23",add abstractmethod to greedy_clustering,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/clustering/greedy_clustering.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,64,"{'module': 1, 'ERROR': 4, 'identifier': 23, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'for': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 13, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 22, 'end_line': 24, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 24, 'end_line': 26, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5784406428267082,0.5564937824537285,"(tensor([0.9816]), tensor([0.9814]), tensor([0.9815]), tensor([0.9814]))"
"19         Base class for clusters that can be merged via GreedyAgglomerativeClustering
20         """"""
21 
22         def mergeCost(self, other):
23             """"""Computes the cost of merging the clusters other and self; if a merge is admissible, returns math.inf""""""
24             raise Exception(""Not implemented"")
25         
26         def merge(self, other):
","21         """"""
22 
23         @abstractmethod
24         def mergeCost(self, other):
25             """"""Computes the cost of merging the clusters other and self; if a merge is admissible, returns math.inf""""""
26             pass
27 
28         @abstractmethod
","Before: 24, 25
After: 26, 27, 28",add abstractmethod to greedy_clustering,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/clustering/greedy_clustering.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,96,"{'module': 1, 'ERROR': 3, 'identifier': 25, 'for': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ';': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, ',': 1, 'attribute': 1, '.': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 13, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 22, 'end_line': 24, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 24, 'end_line': 26, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4465422990126799,0.42703295288107296,"(tensor([0.9003]), tensor([0.8383]), tensor([0.8682]), tensor([0.8441]))"
"23             """"""Computes the cost of merging the clusters other and self; if a merge is admissible, returns math.inf""""""
24             raise Exception(""Not implemented"")
25         
26         def merge(self, other):
27             """"""Merges the given cluster into this cluster""""""
28             raise Exception(""Not implemented"")
29         
30     def __init__(self, clusters):
","26             pass
27 
28         @abstractmethod
29         def merge(self, other):
30             """"""Merges the given cluster into this cluster""""""
31             pass
32 
33     def __init__(self, clusters):
","Before: 28, 29
After: 31, 32",add abstractmethod to greedy_clustering,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/clustering/greedy_clustering.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,123,"{'module': 1, 'expression_statement': 2, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'raise_statement': 2, 'raise': 2, 'call': 2, 'identifier': 5, 'argument_list': 2, '(': 3, ')': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 13, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 22, 'end_line': 24, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 24, 'end_line': 26, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2593911906846504,0.2442596840530533,"(tensor([0.8650]), tensor([0.7796]), tensor([0.8201]), tensor([0.7874]))"
"37         for idx, c in enumerate(clusters):
38             self.wrappedClusters.append(GreedyAgglomerativeClustering.WrappedCluster(c, idx, self))
39         
40     def applyClustering(self):
41         """"""
42         Applies greedy agglomerative clustering and returns the list of clusters
43         """"""
44         
45         # compute all possible merges, adding them to the queue
46         self.log.info(""Computing initial merges"")
47         for idx, wc in enumerate(self.wrappedClusters):
48             self.log.info(""Computing potential merges for cluster index %d"" % idx)
49             wc.computeMerges(False)
50         
51         # perform greedy agglomerative clustering
52         steps = 0
53         while not self.prioritisedMerges.empty():
54             self.log.info(""Clustering step %d"" % (steps+1))
55             haveMerge = False
56             while not haveMerge and not self.prioritisedMerges.empty():
57                 merge = self.prioritisedMerges.get()
58                 if not merge.evaporated:
59                     haveMerge = True
60             if haveMerge:
61                 merge.apply()
62             steps += 1
63         
64         result = filter(lambda wc: not wc.isMerged, self.wrappedClusters)
65         result = list(map(lambda wc: wc.cluster, result))
66         return result
67         
68     class WrappedCluster(object):
","40         for idx, c in enumerate(clusters):
41             self.wrappedClusters.append(GreedyAgglomerativeClustering.WrappedCluster(c, idx, self))
42         
43     def applyClustering(self):
44         """"""
45         Applies greedy agglomerative clustering and returns the list of clusters
46         """"""
47         
48         # compute all possible merges, adding them to the queue
49         self.log.info(""Computing initial merges"")
50         for idx, wc in enumerate(self.wrappedClusters):
51             self.log.debug(""Computing potential merges for cluster index %d"" % idx)
52             wc.computeMerges(False)
53         
54         # perform greedy agglomerative clustering
55         steps = 0
56         while not self.prioritisedMerges.empty():
57             self.log.debug(""Clustering step %d"" % (steps+1))
58             haveMerge = False
59             while not haveMerge and not self.prioritisedMerges.empty():
60                 merge = self.prioritisedMerges.get()
61                 if not merge.evaporated:
62                     haveMerge = True
63             if haveMerge:
64                 merge.apply()
65             steps += 1
66         
67         result = filter(lambda wc: not wc.isMerged, self.wrappedClusters)
68         result = list(map(lambda wc: wc.cluster, result))
69         return result
70         
71     class WrappedCluster(object):
","Before: 48
After: 51",add abstractmethod to greedy_clustering,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/clustering/greedy_clustering.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,272,"{'module': 1, 'for_statement': 2, 'for': 2, 'pattern_list': 2, 'identifier': 67, ',': 6, 'in': 2, 'call': 15, 'argument_list': 15, '(': 17, ')': 17, ':': 9, 'block': 7, 'expression_statement': 14, 'attribute': 22, '.': 22, 'function_definition': 1, 'def': 1, 'parameters': 1, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'comment': 2, 'binary_operator': 3, '%': 2, 'false': 2, 'assignment': 6, '=': 6, 'integer': 3, 'while_statement': 2, 'while': 2, 'not_operator': 5, 'not': 5, 'parenthesized_expression': 1, '+': 1, 'boolean_operator': 1, 'and': 1, 'if_statement': 2, 'if': 2, 'true': 1, 'augmented_assignment': 1, '+=': 1, 'lambda': 4, 'lambda_parameters': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 13, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 22, 'end_line': 24, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 24, 'end_line': 26, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6837905694998543,0.6797957204598681,"(tensor([0.9870]), tensor([0.9869]), tensor([0.9869]), tensor([0.9869]))"
"37         for idx, c in enumerate(clusters):
38             self.wrappedClusters.append(GreedyAgglomerativeClustering.WrappedCluster(c, idx, self))
39         
40     def applyClustering(self):
41         """"""
42         Applies greedy agglomerative clustering and returns the list of clusters
43         """"""
44         
45         # compute all possible merges, adding them to the queue
46         self.log.info(""Computing initial merges"")
47         for idx, wc in enumerate(self.wrappedClusters):
48             self.log.info(""Computing potential merges for cluster index %d"" % idx)
49             wc.computeMerges(False)
50         
51         # perform greedy agglomerative clustering
52         steps = 0
53         while not self.prioritisedMerges.empty():
54             self.log.info(""Clustering step %d"" % (steps+1))
55             haveMerge = False
56             while not haveMerge and not self.prioritisedMerges.empty():
57                 merge = self.prioritisedMerges.get()
58                 if not merge.evaporated:
59                     haveMerge = True
60             if haveMerge:
61                 merge.apply()
62             steps += 1
63         
64         result = filter(lambda wc: not wc.isMerged, self.wrappedClusters)
65         result = list(map(lambda wc: wc.cluster, result))
66         return result
67         
68     class WrappedCluster(object):
","40         for idx, c in enumerate(clusters):
41             self.wrappedClusters.append(GreedyAgglomerativeClustering.WrappedCluster(c, idx, self))
42         
43     def applyClustering(self):
44         """"""
45         Applies greedy agglomerative clustering and returns the list of clusters
46         """"""
47         
48         # compute all possible merges, adding them to the queue
49         self.log.info(""Computing initial merges"")
50         for idx, wc in enumerate(self.wrappedClusters):
51             self.log.debug(""Computing potential merges for cluster index %d"" % idx)
52             wc.computeMerges(False)
53         
54         # perform greedy agglomerative clustering
55         steps = 0
56         while not self.prioritisedMerges.empty():
57             self.log.debug(""Clustering step %d"" % (steps+1))
58             haveMerge = False
59             while not haveMerge and not self.prioritisedMerges.empty():
60                 merge = self.prioritisedMerges.get()
61                 if not merge.evaporated:
62                     haveMerge = True
63             if haveMerge:
64                 merge.apply()
65             steps += 1
66         
67         result = filter(lambda wc: not wc.isMerged, self.wrappedClusters)
68         result = list(map(lambda wc: wc.cluster, result))
69         return result
70         
71     class WrappedCluster(object):
","Before: 54
After: 57",add abstractmethod to greedy_clustering,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/clustering/greedy_clustering.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,330,"{'module': 1, 'for_statement': 2, 'for': 2, 'pattern_list': 2, 'identifier': 67, ',': 6, 'in': 2, 'call': 15, 'argument_list': 15, '(': 17, ')': 17, ':': 9, 'block': 7, 'expression_statement': 14, 'attribute': 22, '.': 22, 'function_definition': 1, 'def': 1, 'parameters': 1, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'comment': 2, 'binary_operator': 3, '%': 2, 'false': 2, 'assignment': 6, '=': 6, 'integer': 3, 'while_statement': 2, 'while': 2, 'not_operator': 5, 'not': 5, 'parenthesized_expression': 1, '+': 1, 'boolean_operator': 1, 'and': 1, 'if_statement': 2, 'if': 2, 'true': 1, 'augmented_assignment': 1, '+=': 1, 'lambda': 4, 'lambda_parameters': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 13, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 22, 'end_line': 24, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 24, 'end_line': 26, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6837905694998543,0.6797957204598681,"(tensor([0.9870]), tensor([0.9869]), tensor([0.9869]), tensor([0.9869]))"
"110             self.mergeCost = mergeCost
111             self.evaporated = False
112 
113         def apply(self):
114             c1, c2 = self.c1, self.c2
115             self.log.info(""Merging %s into %s..."" % (str(c1), str(c2)))
116             c1.cluster.merge(c2.cluster)
117             c2.isMerged = True
118             c1.removeMerges()
119             c2.removeMerges()
120             self.log.info(""Computing new merge costs for %s..."" % str(c1))
121             c1.computeMerges(True)
122         
123         def __lt__(self, other):
","113             self.mergeCost = mergeCost
114             self.evaporated = False
115 
116         def apply(self):
117             c1, c2 = self.c1, self.c2
118             self.log.debug(""Merging %s into %s..."" % (str(c1), str(c2)))
119             c1.cluster.merge(c2.cluster)
120             c2.isMerged = True
121             c1.removeMerges()
122             c2.removeMerges()
123             self.log.debug(""Computing new merge costs for %s..."" % str(c1))
124             c1.computeMerges(True)
125         
126         def __lt__(self, other):
","Before: 115
After: 118",add abstractmethod to greedy_clustering,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/clustering/greedy_clustering.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,907,"{'module': 1, 'expression_statement': 10, 'assignment': 4, 'attribute': 15, 'identifier': 38, '.': 15, '=': 4, 'false': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 11, ')': 11, ':': 1, 'block': 1, 'pattern_list': 1, ',': 3, 'expression_list': 1, 'call': 9, 'argument_list': 9, 'binary_operator': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '%': 2, 'tuple': 1, 'true': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 13, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 22, 'end_line': 24, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 24, 'end_line': 26, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6833388114748605,0.6720332871318357,"(tensor([0.9825]), tensor([0.9823]), tensor([0.9824]), tensor([0.9823]))"
"110             self.mergeCost = mergeCost
111             self.evaporated = False
112 
113         def apply(self):
114             c1, c2 = self.c1, self.c2
115             self.log.info(""Merging %s into %s..."" % (str(c1), str(c2)))
116             c1.cluster.merge(c2.cluster)
117             c2.isMerged = True
118             c1.removeMerges()
119             c2.removeMerges()
120             self.log.info(""Computing new merge costs for %s..."" % str(c1))
121             c1.computeMerges(True)
122         
123         def __lt__(self, other):
","113             self.mergeCost = mergeCost
114             self.evaporated = False
115 
116         def apply(self):
117             c1, c2 = self.c1, self.c2
118             self.log.debug(""Merging %s into %s..."" % (str(c1), str(c2)))
119             c1.cluster.merge(c2.cluster)
120             c2.isMerged = True
121             c1.removeMerges()
122             c2.removeMerges()
123             self.log.debug(""Computing new merge costs for %s..."" % str(c1))
124             c1.computeMerges(True)
125         
126         def __lt__(self, other):
","Before: 120
After: 123",add abstractmethod to greedy_clustering,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/clustering/greedy_clustering.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,973,"{'module': 1, 'expression_statement': 10, 'assignment': 4, 'attribute': 15, 'identifier': 38, '.': 15, '=': 4, 'false': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 11, ')': 11, ':': 1, 'block': 1, 'pattern_list': 1, ',': 3, 'expression_list': 1, 'call': 9, 'argument_list': 9, 'binary_operator': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '%': 2, 'tuple': 1, 'true': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 13, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 22, 'end_line': 24, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'mergeCost', 'long_name': 'mergeCost( self , other )', 'start_line': 24, 'end_line': 26, 'full_parameters': ['self', ' other'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/clustering/greedy_clustering.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6833388114748605,0.6720332871318357,"(tensor([0.9825]), tensor([0.9823]), tensor([0.9824]), tensor([0.9823]))"
"18         os.makedirs(resultDir, exist_ok=True)
19         self.filenamePrefix = filenamePrefix
20 
21     def childWithAddedPrefix(self, prefix) -> ""ResultWriter"":
22         """"""
23         Creates a derived result writer with an added prefix, i.e. the given prefix is appended to this
24         result writer's prefix
25 
26         :param prefix: the prefix to append
27         :return: a new writer instance
28         """"""
29         return ResultWriter(self.resultDir, filenamePrefix=self.filenamePrefix + prefix)
30 
31     def path(self, filenameSuffix: str, extensionToAdd=None, validOtherExtensions: Optional[Sequence[str]] = None):
","18         os.makedirs(resultDir, exist_ok=True)
19         self.filenamePrefix = filenamePrefix
20 
21     def childWithAddedPrefix(self, prefix: str) -> ""ResultWriter"":
22         """"""
23         Creates a derived result writer with an added prefix, i.e. the given prefix is appended to this
24         result writer's prefix
25 
26         :param prefix: the prefix to append
27         :return: a new writer instance
28         """"""
29         return ResultWriter(self.resultDir, filenamePrefix=self.filenamePrefix + prefix)
30 
31     def childForSubdirectory(self, dirName: str):
","Before: 21
After: 21",add childforsubdirectory to util.io,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/io.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,143,"{'module': 1, 'expression_statement': 3, 'call': 2, 'attribute': 4, 'identifier': 17, '.': 4, 'argument_list': 2, '(': 3, ',': 3, 'keyword_argument': 2, '=': 3, 'true': 1, ')': 3, 'assignment': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'binary_operator': 1, '+': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 31, 'name': '__init__', 'long_name': '__init__( self , resultDir , filenamePrefix = """" )', 'start_line': 16, 'end_line': 19, 'full_parameters': ['self', ' resultDir', ' filenamePrefix = """"'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/io.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 31, 'name': '__init__', 'long_name': '__init__( self , resultDir , filenamePrefix = """" )', 'start_line': 16, 'end_line': 19, 'full_parameters': ['self', ' resultDir', ' filenamePrefix = """"'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/io.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8061398398702174,0.8031042753826585,"(tensor([0.9767]), tensor([0.9455]), tensor([0.9608]), tensor([0.9485]))"
"316         result = [self._labels[i] for i in maxIndices]
317         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
318 
319     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
320         """"""
321         :param x: the input data
322         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
323         """"""
324         x = self._computeInputs(x)
325         result = self._predictClassProbabilities(x)
326 
327         # check for correct columns
328         if list(result.columns) != self._labels:
329             raise Exception(f""_predictClassProbabilities returned DataFrame with incorrect columns: expected {self._labels}, got {result.columns}"")
330 
331         # check for normalisation
332         maxRowsToCheck = 5
333         dfToCheck = result.iloc[:maxRowsToCheck]
334         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
335             s = valueSeries.sum()
336             if abs(s-1.0) > 0.01:
337                 _log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
338 
339         return result
340 
341     @abstractmethod
","316         result = [self._labels[i] for i in maxIndices]
317         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
318 
319     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
320         """"""
321         :param x: the input data
322         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
323         """"""
324         x = self._computeInputs(x)
325         result = self._predictClassProbabilities(x)
326 
327         # check for correct columns
328         if list(result.columns) != self._labels:
329             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: expected {self._labels}, got {result.columns}"")
330 
331         # check for normalisation
332         maxRowsToCheck = 5
333         dfToCheck = result.iloc[:maxRowsToCheck]
334         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
335             s = valueSeries.sum()
336             if abs(s-1.0) > 0.01:
337                 _log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
338 
339         return result
340 
341     @abstractmethod
","Before: 329
After: 329",fix typo in vector_model.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,34b83244d40ffdc4a3861c50e14ee3f8d97a161c,094b51512c67983e377588ab7b3b5e5708fd56db,0,2466,"{'module': 1, 'expression_statement': 8, 'assignment': 6, 'identifier': 61, '=': 8, 'list_comprehension': 1, '[': 3, 'subscript': 2, 'attribute': 15, '.': 15, ']': 3, 'for_in_clause': 1, 'for': 2, 'in': 2, 'return_statement': 2, 'return': 2, 'call': 12, 'argument_list': 12, '(': 14, ',': 5, 'keyword_argument': 2, ')': 14, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 6, 'type': 2, '->': 1, 'block': 4, 'string': 3, 'string_start': 3, 'string_content': 6, 'string_end': 3, 'comment': 2, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 5, '{': 5, '}': 5, 'integer': 2, 'slice': 1, 'for_statement': 1, 'pattern_list': 1, 'tuple_pattern': 1, 'binary_operator': 1, '-': 1, 'float': 2, '>': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9800062233445563,0.9815714449791088,"(tensor([0.9920]), tensor([0.9928]), tensor([0.9924]), tensor([0.9927]))"
"11 class PickleFailureDebugger:
12     """"""
13     A collection of methods for testing whether objects can be pickled and logging useful infos in case they cannot
14     """"""
15 
16     enabled = False  # global flag controlling the behaviour of debugFailureIfEnabled
17 
18     @classmethod
19     def _debugFailure(cls, obj, path, failures, handledObjectIds):
20         if id(obj) in handledObjectIds:
","11 class PickleFailureDebugger:
12     """"""
13     A collection of methods for testing whether objects can be pickled and logging useful infos in case they cannot
14     """"""
15 
16     enabled = False  # global flag controlling the behaviour of logFailureIfEnabled
17 
18     @classmethod
19     def _debugFailure(cls, obj, path, failures, handledObjectIds):
20         if id(obj) in handledObjectIds:
","Before: 16
After: 16",fix typo in pickle.py,fixed minor typos,https://github.com/opcode81/sensAI,src/sensai/util/pickle.py,6049210cedae0866e59500f251bdea1cedbba60c,e0475a0be8bcaef1e1f3769fe86d89d1aaec1acf,0,63,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 9, ':': 2, 'block': 2, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 1, 'false': 1, 'comment': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 4, ')': 1}","{'cyclomatic_complexity': 11, 'nloc': 29, 'token_count': 177, 'name': '_debugFailure', 'long_name': '_debugFailure( cls , obj , path , failures , handledObjectIds )', 'start_line': 19, 'end_line': 53, 'full_parameters': ['cls', ' obj', ' path', ' failures', ' handledObjectIds'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pickle.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 29, 'token_count': 177, 'name': '_debugFailure', 'long_name': '_debugFailure( cls , obj , path , failures , handledObjectIds )', 'start_line': 19, 'end_line': 53, 'full_parameters': ['cls', ' obj', ' path', ' failures', ' handledObjectIds'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pickle.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9651391201669045,0.9634483195187238,"(tensor([0.9978]), tensor([0.9978]), tensor([0.9978]), tensor([0.9978]))"
"100             pickle.dump(self, f)
101 
102     @classmethod
103     def load(cls, path):
104         """"""
105         Loads a class instance from pickle
106         :param path:
107         :return: instance of the present class
108         """"""
109         log.info(f""Loading instance of {cls} from {path}"")
110         with open(path, 'rb') as f:
111             result = pickle.load(f)
112         if not isinstance(result, cls):
113             raise Exception(f""Excepted instance of {cls}, instead got: {result.__class__.__name__}"")
114         return result","100             pickle.dump(self, f)
101 
102     @classmethod
103     def load(cls, path):
104         """"""
105         Loads a class instance from pickle
106         :param path:
107         :return: instance of the present class
108         """"""
109         log.info(f""Loading instance of {cls} from {path}"")
110         with open(path, 'rb') as f:
111             result = pickle.load(f)
112         if not isinstance(result, cls):
113             raise Exception(f""Excepted instance of {cls}, instead got: {result.__class__.__name__}"")
114         return result
","Before: 114
After: 114",fix typo in pickle.py,fixed minor typos,https://github.com/opcode81/sensAI,src/sensai/util/pickle.py,6049210cedae0866e59500f251bdea1cedbba60c,e0475a0be8bcaef1e1f3769fe86d89d1aaec1acf,0,844,"{'module': 1, 'expression_statement': 4, 'call': 5, 'attribute': 3, 'identifier': 22, '.': 3, 'argument_list': 5, '(': 6, ',': 4, ')': 6, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'interpolation': 2, '{': 2, '}': 2, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, 'assignment': 1, '=': 1, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1}","{'cyclomatic_complexity': 11, 'nloc': 29, 'token_count': 177, 'name': '_debugFailure', 'long_name': '_debugFailure( cls , obj , path , failures , handledObjectIds )', 'start_line': 19, 'end_line': 53, 'full_parameters': ['cls', ' obj', ' path', ' failures', ' handledObjectIds'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pickle.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 29, 'token_count': 177, 'name': '_debugFailure', 'long_name': '_debugFailure( cls , obj , path , failures , handledObjectIds )', 'start_line': 19, 'end_line': 53, 'full_parameters': ['cls', ' obj', ' path', ' failures', ' handledObjectIds'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pickle.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",1.0,1.0,"(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]))"
"1 import collections
2 import logging
3 import random
4 import time
5 from abc import ABC, abstractmethod
6 import math
7 from typing import Optional, Tuple, Callable, Type, Sequence
8 
9 import numpy as np
10 import pandas as pd
","8 
9 import numpy as np
10 import pandas as pd
11 from matplotlib import pyplot as plt
12 
13 log = logging.getLogger(__name__)
14 
15 
16 class SATemperatureSchedule(ABC):
17     """"""
","Before: 6, 13, 14
After: 13",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,32,"{'module': 1, 'import_statement': 6, 'import': 8, 'dotted_name': 15, 'identifier': 16, 'import_from_statement': 2, 'from': 2, ',': 5, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2233966901361748,0.1519935081546653,"(tensor([0.7786]), tensor([0.7908]), tensor([0.7846]), tensor([0.7895]))"
"334         """"""
335         self.state = state
336 
337     def applyCostChange(self, costDelta: SACostValue, params):
338         """"""
339         Applies the cost change to the state given at construction
340 
341         :param costDelta: the cost change to apply
342         :param params: the operator parameters
343         """"""
344         self.state.cost = self.state.cost.add(costDelta)
345 
346     @abstractmethod
","333         """"""
334         self.state = state
335 
336     def applyCostChange(self, costDelta: SACostValue):
337         """"""
338         Applies the cost change to the state given at construction
339 
340         :param costDelta: the cost change to apply
341         """"""
342         self.state.cost = self.state.cost.add(costDelta)
343 
344     @abstractmethod
","Before: 337
After: 336",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,2169,"{'module': 1, 'expression_statement': 5, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 30, ':': 4, 'assignment': 3, 'type': 2, 'attribute': 5, '.': 5, '=': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4929719380128234,0.47734559115820724,"(tensor([0.9751]), tensor([0.9552]), tensor([0.9650]), tensor([0.9572]))"
"354         """"""
355         pass
356 
357     def apply(self, params: Tuple, costDelta: SACostValue):
358         """"""
359         Applies the operator to the state given at construction, changing the state and updating the costs appropriately
360 
361         :param params: the parameters with which the operator is to be applied
362         :param costDelta: the cost change that results from the application
363         :return:
364         """"""
365         self.applyCostChange(costDelta, params)
366         self.applyStateChange(*params)
367 
368     @abstractmethod
","352         """"""
353         pass
354 
355     def apply(self, params: Tuple, costDelta: SACostValue):
356         """"""
357         Applies the operator to the state given at construction, changing the state and updating the costs appropriately
358 
359         :param params: the parameters with which the operator is to be applied
360         :param costDelta: the cost change that results from the application
361         :return:
362         """"""
363         self.applyCostChange(costDelta)
364         self.applyStateChange(*params)
365 
366     @abstractmethod
","Before: 365
After: 363",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,2261,"{'module': 1, 'expression_statement': 7, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 9, 'identifier': 45, ',': 2, 'boolean_operator': 1, 'and': 1, ':': 6, 'assignment': 3, 'type': 5, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'comparison_operator': 1, 'is': 1, 'block': 1, 'constrained_type': 1, 'call': 2, 'attribute': 2, '.': 2, 'argument_list': 2, '(': 2, ')': 2, 'list_splat': 1, '*': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6728907947253202,0.668265921028543,"(tensor([0.9831]), tensor([0.9830]), tensor([0.9831]), tensor([0.9830]))"
"404 
405 
406 class SAChain:
407     """"""Manages the progression of one state during simulated annealing""""""
408 
409     _log = _log.getChild(__qualname__)
410 
411     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
412             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator]]], randomSeed, collectStats=False):
413         self.schedule = schedule
","402 
403 
404 class SAChain:
405     """"""Manages the progression of one state during simulated annealing""""""
406 
407     log = log.getChild(__qualname__)
408 
409     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
410             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]], randomSeed, collectStats=False):
411         self.schedule = schedule
","Before: 409
After: 407",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,2509,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 22, ':': 5, 'block': 2, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 2, 'call': 1, 'attribute': 2, '.': 2, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_parameter': 3, 'type': 9, 'generic_type': 4, 'type_parameter': 4, '[': 6, 'list': 2, ']': 6, 'default_parameter': 1, 'false': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7166100883751921,0.7060269086491533,"(tensor([0.9777]), tensor([0.9818]), tensor([0.9797]), tensor([0.9814]))"
"408 
409     _log = _log.getChild(__qualname__)
410 
411     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
412             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator]]], randomSeed, collectStats=False):
413         self.schedule = schedule
414         self.r = random.Random(randomSeed)
415         self.state = stateFactory(self.r)
416         self.collectStats = collectStats
417         operators, weights = zip(*opsAndWeights)
418         cumWeights, s = [], 0
419         for weight in weights:
420             s += weight
421             cumWeights.append(s)
422         self.ops = [cons(self.state) for cons in operators]
423         self.opCumWeights = cumWeights
424         self.stepsTaken = 0
425         self.countNoneParams = 0
426         self.countBestUpdates = -1
427         self.bestCost = None
428         self.bestStateRepr = None
429         self.loggedSeries = collections.defaultdict(lambda: [])
430         self._updateBestState()
431 
432         if self.collectStats:
433             self.operatorInapplicabilityCounters = {}
434             for op in self.ops:
435                 self.operatorInapplicabilityCounters[op] = RelativeFrequencyCounter()
436 
437     def _updateBestState(self):
","406 
407     log = log.getChild(__qualname__)
408 
409     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
410             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]], randomSeed, collectStats=False):
411         self.schedule = schedule
412         self.r = random.Random(randomSeed)
413         self.state = stateFactory(self.r)
414         self.collectStats = collectStats
415         operators, weights = zip(*opsAndWeights)
416         cumWeights, s = [], 0
417         for weight in weights:
418             s += weight
419             cumWeights.append(s)
420         self.ops = [cons(self.state) for cons in operators]
421         self.opCumWeights = cumWeights
422         self.stepsTaken = 0
423         self.countNoneParams = 0
424         self.countBestUpdates = -1
425         self.bestCost = None
426         self.bestStateRepr = None
427         self.loggedSeries = collections.defaultdict(lambda: [])
428         self._updateBestState()
429 
430         if self.collectStats:
431             self.operatorInapplicabilityCounters = {}
432             for op in self.ops:
433                 self.operatorInapplicabilityCounters[op] = RelativeFrequencyCounter()
434 
435     def _updateBestState(self):
","Before: 412
After: 410",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,2582,"{'module': 1, 'expression_statement': 20, 'assignment': 17, 'identifier': 87, '=': 18, 'call': 9, 'attribute': 24, '.': 24, 'argument_list': 9, '(': 10, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 10, 'typed_parameter': 3, ':': 8, 'type': 9, 'generic_type': 4, 'type_parameter': 4, '[': 10, 'list': 4, ']': 10, 'default_parameter': 1, 'false': 1, 'block': 4, 'pattern_list': 2, 'list_splat': 1, '*': 1, 'expression_list': 1, 'integer': 4, 'for_statement': 2, 'for': 3, 'in': 3, 'augmented_assignment': 1, '+=': 1, 'list_comprehension': 1, 'for_in_clause': 1, 'unary_operator': 1, '-': 1, 'none': 2, 'lambda': 2, 'if_statement': 1, 'if': 1, 'dictionary': 1, '{': 1, '}': 1, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.628600309229102,0.6253040719282972,"(tensor([0.9876]), tensor([0.9895]), tensor([0.9885]), tensor([0.9893]))"
"441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self._log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","439             self.bestStateRepr = self.state.getStateRepresentation()
440             self.countBestUpdates += 1
441 
442     def step(self, degreeOfCompletion):
443         r = self.r
444 
445         # make move
446         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
447         paramChoice = op.chooseParams()
448         if paramChoice is None:
449             self.countNoneParams += 1
450         else:
451             params, costChange = paramChoice
452             if costChange is None:
453                 costChange = op.costDelta(*params)
454             if costChange.value() < 0:
455                 makeMove = True
456             else:
457                 costChangeValue = costChange.value()
458                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
459                 makeMove = r.random() <= p
460                 self.log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
461                 if self.collectStats:
462                     self.loggedSeries[""temperatures""].append(T)
463                     self.loggedSeries[""probabilities""].append(p)
464             if makeMove:
465                 op.apply(params, costChange)
466                 self._updateBestState()
467             if self.collectStats:
468                 self.loggedSeries[""costDeltas""].append(costChange.value())
469         if self.collectStats:
470             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
471             self.loggedSeries[""costValues""].append(self.state.cost.value())
472             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
473 
474         self.stepsTaken += 1
475 
476         if self.log.isEnabledFor(logging.DEBUG):
477             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
478 
479     def logStats(self):
","Before: 462
After: 460",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,3139,"{'module': 1, 'expression_statement': 23, 'assignment': 10, 'attribute': 51, 'identifier': 121, '.': 51, '=': 12, 'call': 22, 'argument_list': 22, '(': 23, ')': 23, 'augmented_assignment': 3, '+=': 3, 'integer': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, ':': 11, 'block': 11, 'comment': 1, 'subscript': 7, 'keyword_argument': 2, '[': 7, ']': 7, 'if_statement': 8, 'if': 8, 'comparison_operator': 5, 'is': 3, 'none': 3, 'else_clause': 2, 'else': 2, 'pattern_list': 2, 'list_splat': 1, '*': 1, '<': 1, 'true': 1, '<=': 1, 'string': 7, 'string_start': 7, 'string_content': 12, 'interpolation': 7, '{': 7, '}': 7, 'string_end': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7061583650782739,0.7026222778419525,"(tensor([0.9907]), tensor([0.9911]), tensor([0.9909]), tensor([0.9911]))"
"441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self._log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","439             self.bestStateRepr = self.state.getStateRepresentation()
440             self.countBestUpdates += 1
441 
442     def step(self, degreeOfCompletion):
443         r = self.r
444 
445         # make move
446         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
447         paramChoice = op.chooseParams()
448         if paramChoice is None:
449             self.countNoneParams += 1
450         else:
451             params, costChange = paramChoice
452             if costChange is None:
453                 costChange = op.costDelta(*params)
454             if costChange.value() < 0:
455                 makeMove = True
456             else:
457                 costChangeValue = costChange.value()
458                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
459                 makeMove = r.random() <= p
460                 self.log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
461                 if self.collectStats:
462                     self.loggedSeries[""temperatures""].append(T)
463                     self.loggedSeries[""probabilities""].append(p)
464             if makeMove:
465                 op.apply(params, costChange)
466                 self._updateBestState()
467             if self.collectStats:
468                 self.loggedSeries[""costDeltas""].append(costChange.value())
469         if self.collectStats:
470             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
471             self.loggedSeries[""costValues""].append(self.state.cost.value())
472             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
473 
474         self.stepsTaken += 1
475 
476         if self.log.isEnabledFor(logging.DEBUG):
477             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
478 
479     def logStats(self):
","Before: 478, 479
After: 476, 477",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,3366,"{'module': 1, 'expression_statement': 23, 'assignment': 10, 'attribute': 51, 'identifier': 121, '.': 51, '=': 12, 'call': 22, 'argument_list': 22, '(': 23, ')': 23, 'augmented_assignment': 3, '+=': 3, 'integer': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, ':': 11, 'block': 11, 'comment': 1, 'subscript': 7, 'keyword_argument': 2, '[': 7, ']': 7, 'if_statement': 8, 'if': 8, 'comparison_operator': 5, 'is': 3, 'none': 3, 'else_clause': 2, 'else': 2, 'pattern_list': 2, 'list_splat': 1, '*': 1, '<': 1, 'true': 1, '<=': 1, 'string': 7, 'string_start': 7, 'string_content': 12, 'interpolation': 7, '{': 7, '}': 7, 'string_end': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7061583650782739,0.7026222778419525,"(tensor([0.9907]), tensor([0.9911]), tensor([0.9909]), tensor([0.9911]))"
"478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","476         if self.log.isEnabledFor(logging.DEBUG):
477             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
478 
479     def logStats(self):
480         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
481         if self.collectStats:
482             for op, counter in self.operatorInapplicabilityCounters.items():
483                 stats[f""useless moves of {op}""] = str(counter)
484             loggedCostDeltas = self.loggedSeries[""costDeltas""]
485             if loggedCostDeltas:
486                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
487                 absCostDeltas = np.abs(loggedCostDeltas)
488                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
489                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
490                 if positiveCostDeltas:
491                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
492                                                    f"" max={np.max(positiveCostDeltas):.3f}""
493         statsJoin = ""\n    "" if self.collectStats else ""; ""
494         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
495         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
496 
497     def applyBestState(self):
","Before: 488
After: 486",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,3556,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 21, ')': 21, ':': 7, 'block': 6, 'expression_statement': 12, 'string': 16, 'string_start': 16, 'string_content': 20, 'interpolation': 9, '{': 10, '}': 10, 'string_end': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 6, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'binary_operator': 5, '%': 3, 'tuple': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6635591763098024,0.6805624131230896,"(tensor([0.9753]), tensor([0.9769]), tensor([0.9761]), tensor([0.9768]))"
"478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","476         if self.log.isEnabledFor(logging.DEBUG):
477             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
478 
479     def logStats(self):
480         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
481         if self.collectStats:
482             for op, counter in self.operatorInapplicabilityCounters.items():
483                 stats[f""useless moves of {op}""] = str(counter)
484             loggedCostDeltas = self.loggedSeries[""costDeltas""]
485             if loggedCostDeltas:
486                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
487                 absCostDeltas = np.abs(loggedCostDeltas)
488                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
489                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
490                 if positiveCostDeltas:
491                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
492                                                    f"" max={np.max(positiveCostDeltas):.3f}""
493         statsJoin = ""\n    "" if self.collectStats else ""; ""
494         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
495         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
496 
497     def applyBestState(self):
","Before: 490
After: 488",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,3608,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 21, ')': 21, ':': 7, 'block': 6, 'expression_statement': 12, 'string': 16, 'string_start': 16, 'string_content': 20, 'interpolation': 9, '{': 10, '}': 10, 'string_end': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 6, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'binary_operator': 5, '%': 3, 'tuple': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6635591763098024,0.6805624131230896,"(tensor([0.9753]), tensor([0.9769]), tensor([0.9761]), tensor([0.9768]))"
"478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","476         if self.log.isEnabledFor(logging.DEBUG):
477             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
478 
479     def logStats(self):
480         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
481         if self.collectStats:
482             for op, counter in self.operatorInapplicabilityCounters.items():
483                 stats[f""useless moves of {op}""] = str(counter)
484             loggedCostDeltas = self.loggedSeries[""costDeltas""]
485             if loggedCostDeltas:
486                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
487                 absCostDeltas = np.abs(loggedCostDeltas)
488                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
489                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
490                 if positiveCostDeltas:
491                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
492                                                    f"" max={np.max(positiveCostDeltas):.3f}""
493         statsJoin = ""\n    "" if self.collectStats else ""; ""
494         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
495         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
496 
497     def applyBestState(self):
","Before: 493
After: 491, 492",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,3681,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 21, ')': 21, ':': 7, 'block': 6, 'expression_statement': 12, 'string': 16, 'string_start': 16, 'string_content': 20, 'interpolation': 9, '{': 10, '}': 10, 'string_end': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 6, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'binary_operator': 5, '%': 3, 'tuple': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6635591763098024,0.6805624131230896,"(tensor([0.9753]), tensor([0.9769]), tensor([0.9761]), tensor([0.9768]))"
"478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","476         if self.log.isEnabledFor(logging.DEBUG):
477             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
478 
479     def logStats(self):
480         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
481         if self.collectStats:
482             for op, counter in self.operatorInapplicabilityCounters.items():
483                 stats[f""useless moves of {op}""] = str(counter)
484             loggedCostDeltas = self.loggedSeries[""costDeltas""]
485             if loggedCostDeltas:
486                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
487                 absCostDeltas = np.abs(loggedCostDeltas)
488                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
489                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
490                 if positiveCostDeltas:
491                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
492                                                    f"" max={np.max(positiveCostDeltas):.3f}""
493         statsJoin = ""\n    "" if self.collectStats else ""; ""
494         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
495         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
496 
497     def applyBestState(self):
","Before: 495, 496
After: 494, 495",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,3758,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 21, ')': 21, ':': 7, 'block': 6, 'expression_statement': 12, 'string': 16, 'string_start': 16, 'string_content': 20, 'interpolation': 9, '{': 10, '}': 10, 'string_end': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 6, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'binary_operator': 5, '%': 3, 'tuple': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6635591763098024,0.6805624131230896,"(tensor([0.9753]), tensor([0.9769]), tensor([0.9761]), tensor([0.9768]))"
"521             raise Exception(""Unknown series"")
522         return pd.Series(self.loggedSeries[seriesName])
523 
524 
525 class SimulatedAnnealing:
526     _log = _log.getChild(__qualname__)
527 
528     """"""
529     The simulated annealing algorithm for discrete optimisation (cost minimisation)
530     """"""
","520             raise Exception(""Unknown series"")
521         return pd.Series(self.loggedSeries[seriesName])
522 
523 
524 class SimulatedAnnealing:
525     log = log.getChild(__qualname__)
526 
527     """"""
528     The simulated annealing algorithm for discrete optimisation (cost minimisation)
529     """"""
","Before: 526
After: 525",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,3967,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 4, 'identifier': 19, 'argument_list': 4, '(': 4, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ')': 4, 'return_statement': 1, 'return': 1, 'attribute': 3, '.': 3, 'subscript': 1, '[': 1, ']': 1, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'ERROR': 3, 'for': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5828233954152653,0.5653253155711295,"(tensor([0.9751]), tensor([0.9747]), tensor([0.9749]), tensor([0.9748]))"
"528     """"""
529     The simulated annealing algorithm for discrete optimisation (cost minimisation)
530     """"""
531     def __init__(self, scheduleFactory: Callable[[], SATemperatureSchedule], opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]], maxSteps=None, duration=None,
532                  randomSeed=42, collectStats=False):
533         """"""
534         :param scheduleFactory: a factory for the creation of the temperature schedule for the annealing process
535         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
536         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
537         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
538         :param randomSeed: the random seed to use for all random choices
539         :param collectStats: flag indicating whether to collect additional statics which will be logged
540         """"""
541         if maxSteps is not None and maxSteps <= 0:
542             raise ValueError(""The number of iterations should be greater than 0."")
543         if maxSteps is None and duration is None or (maxSteps is not None and duration is not None):
544             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
545         self.scheduleFactory = scheduleFactory
546         self.maxSteps = maxSteps
547         self.duration = duration
548         self.randomSeed = randomSeed
549         self.opsAndWeights = opsAndWeights
550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
","527     """"""
528     The simulated annealing algorithm for discrete optimisation (cost minimisation)
529     """"""
530     def __init__(self, scheduleFactory: Callable[[], SATemperatureSchedule], opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]],
531             maxSteps: int = None, duration: float = None, randomSeed=42, collectStats=False):
532         """"""
533         :param scheduleFactory: a factory for the creation of the temperature schedule for the annealing process
534         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
535         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
536         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
537         :param randomSeed: the random seed to use for all random choices
538         :param collectStats: flag indicating whether to collect additional statics which will be logged
539         """"""
540         if maxSteps is not None and maxSteps <= 0:
541             raise ValueError(""The number of iterations should be greater than 0."")
542         if maxSteps is None and duration is None or (maxSteps is not None and duration is not None):
543             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
544         if duration is not None and duration <= 0:
545             raise ValueError(""Duration must be greater than 0 if provided"")
546         self.scheduleFactory = scheduleFactory
547         self.maxSteps = maxSteps
548         self.duration = duration
549         self.randomSeed = randomSeed
550         self.opsAndWeights = opsAndWeights
551         self.collectStats = collectStats
552         self._chain = None
553 
554     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
","Before: 531, 532
After: 530, 531",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,4038,"{'module': 1, 'expression_statement': 9, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'function_definition': 1, 'def': 1, 'identifier': 44, 'parameters': 1, '(': 4, ',': 9, 'typed_parameter': 2, ':': 5, 'type': 9, 'generic_type': 4, 'type_parameter': 4, '[': 6, 'list': 2, ']': 6, 'default_parameter': 4, '=': 11, 'none': 8, 'integer': 2, 'false': 1, ')': 4, 'block': 3, 'if_statement': 2, 'if': 2, 'boolean_operator': 4, 'comparison_operator': 6, 'is not': 6, 'and': 3, '<=': 1, 'raise_statement': 2, 'raise': 2, 'call': 2, 'argument_list': 2, 'is': 2, 'or': 1, 'parenthesized_expression': 1, 'assignment': 7, 'attribute': 7, '.': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7436073384855288,0.7467374965334272,"(tensor([0.9791]), tensor([0.9863]), tensor([0.9827]), tensor([0.9856]))"
"550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self._log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self._log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","551         self.collectStats = collectStats
552         self._chain = None
553 
554     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
555         """"""
556         Applies the annealing process starting with a state created via the given factory.
557         The result of the optimisation (i.e. the final best state representation) is written via the state's
558         applyStateRepresentation method, which should write to an object the state receives at construction.
559 
560         :param stateFactory: the factory with which to create the (initial) state
561         """"""
562         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
563         self.log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
564         startTime = time.time()
565         while True:
566             timeElapsed = time.time() - startTime
567             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
568                 break
569             if self.maxSteps is not None:
570                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
571             else:
572                 degreeOfCompletion = timeElapsed / self.duration
573             chain.step(degreeOfCompletion)
574         self.log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
575         chain.logStats()
576         chain.applyBestState()
577         if self.collectStats:
578             self._chain = chain
579 
580     def getChain(self) -> Optional[SAChain]:
","Before: 562
After: 563",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,4322,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'attribute': 33, 'identifier': 87, '.': 33, '=': 11, 'none': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 14, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ')': 14, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 11, 'argument_list': 11, 'keyword_argument': 3, 'interpolation': 4, '{': 4, '}': 4, 'conditional_expression': 1, 'binary_operator': 6, '%': 2, 'if': 4, 'comparison_operator': 6, 'is not': 8, 'else': 2, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 3, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>=': 2, 'or': 1, 'break_statement': 1, 'break': 1, '/': 2, 'else_clause': 1, 'format_specifier': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.776787286882979,0.7749618805851002,"(tensor([0.9938]), tensor([0.9935]), tensor([0.9936]), tensor([0.9935]))"
"550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self._log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self._log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","551         self.collectStats = collectStats
552         self._chain = None
553 
554     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
555         """"""
556         Applies the annealing process starting with a state created via the given factory.
557         The result of the optimisation (i.e. the final best state representation) is written via the state's
558         applyStateRepresentation method, which should write to an object the state receives at construction.
559 
560         :param stateFactory: the factory with which to create the (initial) state
561         """"""
562         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
563         self.log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
564         startTime = time.time()
565         while True:
566             timeElapsed = time.time() - startTime
567             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
568                 break
569             if self.maxSteps is not None:
570                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
571             else:
572                 degreeOfCompletion = timeElapsed / self.duration
573             chain.step(degreeOfCompletion)
574         self.log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
575         chain.logStats()
576         chain.applyBestState()
577         if self.collectStats:
578             self._chain = chain
579 
580     def getChain(self) -> Optional[SAChain]:
","Before: 573
After: 574",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,4497,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'attribute': 33, 'identifier': 87, '.': 33, '=': 11, 'none': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 14, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ')': 14, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 11, 'argument_list': 11, 'keyword_argument': 3, 'interpolation': 4, '{': 4, '}': 4, 'conditional_expression': 1, 'binary_operator': 6, '%': 2, 'if': 4, 'comparison_operator': 6, 'is not': 8, 'else': 2, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 3, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>=': 2, 'or': 1, 'break_statement': 1, 'break': 1, '/': 2, 'else_clause': 1, 'format_specifier': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.776787286882979,0.7749618805851002,"(tensor([0.9938]), tensor([0.9935]), tensor([0.9936]), tensor([0.9935]))"
"584         """"""
585         return self._chain
586 
587 
588 class ParallelTempering:
589     _log = _log.getChild(__qualname__)
590 
591     """"""
592     The parallel tempering algorithm for discrete optimisation (cost minimisation)
593     """"""
","585         """"""
586         return self._chain
587 
588 
589 class ParallelTempering:
590     log = log.getChild(__qualname__)
591 
592     """"""
593     The parallel tempering algorithm for discrete optimisation (cost minimisation)
594     """"""
","Before: 589
After: 590",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,4578,"{'module': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'ERROR': 3, 'identifier': 8, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4430299249356568,0.40399880279760664,"(tensor([0.9730]), tensor([0.9714]), tensor([0.9722]), tensor([0.9715]))"
"591     """"""
592     The parallel tempering algorithm for discrete optimisation (cost minimisation)
593     """"""
594     def __init__(self, numChains, opsAndWeights: Sequence[Tuple[Type[SAOperator], float]],
595                  schedule: SATemperatureSchedule = None, probabilityFunction: SAProbabilityFunction = None,
596                  maxSteps=None, duration=None, randomSeed=42, logCostProgression=False):
597         """"""
598         Creates a parallel tempering optimiser with the given number of chains and operators for each chain.
599         To determine the schedule to use for each chain, either schedule or probabilityFunction must be provided.
600         It is usually more robust to use adaptive schedules and therefore to provide probabilityFunction.
601 
602         :param numChains: the number of parallel chains
603         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
604         :param schedule: the temperature schedule from which numChains temperatures of chains are drawn (using equidistant degrees of completion); if None, must provide probabilityFunction
605         :param probabilityFunction: the probability function from which numChains probabilities for adaptive probability schedules, each using a constant probability, are to be drawn; if None, must provide schedule
606         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
607         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
608         :param randomSeed: the random seed to use for all random choices
609         :param logCostProgression: whether to log cost progression of all chains (such that it can be plotted after the fact via plotCostProgression)
610         """"""
611         if maxSteps is not None and maxSteps <= 0:
612             raise ValueError(""The number of iterations should be greater than 0."")
613         if (maxSteps is None and duration is None) or (maxSteps is not None and duration is not None):
614             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
615         if numChains < 2:
616             raise ValueError(""Number of chains must be at least 2."")
617         if (schedule is None and probabilityFunction is None) or (schedule is not None and probabilityFunction is not None):
618             raise ValueError(""Exactly one of {schedule, probabilityFunction} must be given"")
619         self.maxSteps = maxSteps
620         self.duration = duration
621         self.randomSeed = randomSeed
622         self.numChains = numChains
623         self.baseSchedule = schedule
624         self.baseProbabilityFunction = probabilityFunction
625         self.opsAndWeights = opsAndWeights
626         self.logCostProgression = logCostProgression
627 
628         # transient members
629         self._costProgressions = None
630         self._scheduleParamStrings = None
631 
632     def _createSchedules(self):
","592     """"""
593     The parallel tempering algorithm for discrete optimisation (cost minimisation)
594     """"""
595     def __init__(self, numChains, opsAndWeights: Sequence[Tuple[Type[SAOperator], float]],
596                  schedule: SATemperatureSchedule = None, probabilityFunction: SAProbabilityFunction = None,
597                  maxSteps: int = None, duration: float = None, randomSeed=42, logCostProgression=False):
598         """"""
599         Creates a parallel tempering optimiser with the given number of chains and operators for each chain.
600         To determine the schedule to use for each chain, either schedule or probabilityFunction must be provided.
601         It is usually more robust to use adaptive schedules and therefore to provide probabilityFunction.
602 
603         :param numChains: the number of parallel chains
604         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
605         :param schedule: the temperature schedule from which numChains temperatures of chains are drawn (using equidistant degrees of completion); if None, must provide probabilityFunction
606         :param probabilityFunction: the probability function from which numChains probabilities for adaptive probability schedules, each using a constant probability, are to be drawn; if None, must provide schedule
607         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
608         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
609         :param randomSeed: the random seed to use for all random choices
610         :param logCostProgression: whether to log cost progression of all chains (such that it can be plotted after the fact via plotCostProgression)
611         """"""
612         if maxSteps is not None and maxSteps <= 0:
613             raise ValueError(""The number of iterations should be greater than 0."")
614         if (maxSteps is None and duration is None) or (maxSteps is not None and duration is not None):
615             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
616         if duration is not None and duration <= 0:
617             raise ValueError(""duration should be greater than 0 if provided."")
618         if numChains < 2:
619             raise ValueError(""Number of chains must be at least 2."")
620         if (schedule is None and probabilityFunction is None) or (schedule is not None and probabilityFunction is not None):
621             raise ValueError(""Exactly one of {schedule, probabilityFunction} must be given"")
622         self.maxSteps = maxSteps
623         self.duration = duration
624         self.randomSeed = randomSeed
625         self.numChains = numChains
626         self.baseSchedule = schedule
627         self.baseProbabilityFunction = probabilityFunction
628         self.opsAndWeights = opsAndWeights
629         self.logCostProgression = logCostProgression
630 
631         # transient members
632         self._costProgressions = None
633         self._scheduleParamStrings = None
634 
635     def _createSchedules(self):
","Before: 596
After: 597",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,4657,"{'module': 1, 'expression_statement': 12, 'string': 6, 'string_start': 6, 'string_content': 6, 'string_end': 6, 'function_definition': 1, 'def': 1, 'identifier': 60, 'parameters': 1, '(': 9, ',': 9, 'typed_parameter': 1, ':': 8, 'type': 7, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'typed_default_parameter': 2, '=': 16, 'none': 15, 'default_parameter': 4, 'integer': 3, 'false': 1, ')': 9, 'block': 5, 'if_statement': 4, 'if': 4, 'boolean_operator': 7, 'comparison_operator': 11, 'is not': 10, 'and': 5, '<=': 1, 'raise_statement': 4, 'raise': 4, 'call': 4, 'argument_list': 4, 'parenthesized_expression': 4, 'is': 4, 'or': 2, '<': 1, 'assignment': 10, 'attribute': 10, '.': 10, 'comment': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7723940438084771,0.7730148712914441,"(tensor([0.9757]), tensor([0.9807]), tensor([0.9782]), tensor([0.9802]))"
"643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","646             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
647             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
648 
649     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
650         """"""
651         Applies the optimisation process starting, in each chain, with a state created via the given factory.
652         The result of the optimisation (i.e. the final best state representation) is written by calling the
653         applyStateRepresentation method on one of the states, which should write to a suitable object each
654         state receives at construction.
655 
656         :param stateFactory: the factory with which to create the states for all chains
657         """"""
658         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
659 
660         r = random.Random(self.randomSeed)
661         chains = []
662         costProgressions = []
663         for i, schedule in enumerate(self._createSchedules(), start=1):
664             self.log.info(f""Chain {i} uses {schedule}"")
665             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
666             costProgressions.append([])
667 
668         startTime = time.time()
669         step = 0
670         numChainSwaps = 0
671         while True:
672             timeElapsed = time.time() - startTime
673             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
674                 break
675 
676             # take one step in each chain
677             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
678             for chain in chains:
679                 chain.step(degreeOfCompletion)
680 
681             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
682             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
683             # in the chains array, which shall always be in descending order of temperature)
684             for idxHighTempChain in range(0, self.numChains-1):
685                 idxLowTempChain = idxHighTempChain+1
686                 highTempChain = chains[idxHighTempChain]
687                 lowTempChain = chains[idxLowTempChain]
688                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
689                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
690                     chains[idxLowTempChain] = highTempChain
691                     chains[idxHighTempChain] = lowTempChain
692                     numChainSwaps += 1
693 
694             if self.logCostProgression:
695                 for idxChain, chain in enumerate(chains):
696                     costProgressions[idxChain].append(chain.state.cost.value())
697 
698             step += 1
699 
700         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
701         if self.logCostProgression: self._costProgressions = costProgressions
702 
703         # apply best solution
704         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
705         chains[bestChainIdx].applyBestState()
706 
707     def plotCostProgression(self):
","Before: 655
After: 658",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,5181,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 164, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 25, 'argument_list': 25, '(': 28, 'none': 5, ',': 13, ')': 28, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.759784272540137,0.7593416553892807,"(tensor([0.9872]), tensor([0.9878]), tensor([0.9875]), tensor([0.9877]))"
"643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","646             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
647             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
648 
649     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
650         """"""
651         Applies the optimisation process starting, in each chain, with a state created via the given factory.
652         The result of the optimisation (i.e. the final best state representation) is written by calling the
653         applyStateRepresentation method on one of the states, which should write to a suitable object each
654         state receives at construction.
655 
656         :param stateFactory: the factory with which to create the states for all chains
657         """"""
658         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
659 
660         r = random.Random(self.randomSeed)
661         chains = []
662         costProgressions = []
663         for i, schedule in enumerate(self._createSchedules(), start=1):
664             self.log.info(f""Chain {i} uses {schedule}"")
665             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
666             costProgressions.append([])
667 
668         startTime = time.time()
669         step = 0
670         numChainSwaps = 0
671         while True:
672             timeElapsed = time.time() - startTime
673             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
674                 break
675 
676             # take one step in each chain
677             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
678             for chain in chains:
679                 chain.step(degreeOfCompletion)
680 
681             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
682             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
683             # in the chains array, which shall always be in descending order of temperature)
684             for idxHighTempChain in range(0, self.numChains-1):
685                 idxLowTempChain = idxHighTempChain+1
686                 highTempChain = chains[idxHighTempChain]
687                 lowTempChain = chains[idxLowTempChain]
688                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
689                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
690                     chains[idxLowTempChain] = highTempChain
691                     chains[idxHighTempChain] = lowTempChain
692                     numChainSwaps += 1
693 
694             if self.logCostProgression:
695                 for idxChain, chain in enumerate(chains):
696                     costProgressions[idxChain].append(chain.state.cost.value())
697 
698             step += 1
699 
700         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
701         if self.logCostProgression: self._costProgressions = costProgressions
702 
703         # apply best solution
704         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
705         chains[bestChainIdx].applyBestState()
706 
707     def plotCostProgression(self):
","Before: 661
After: 664",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,5263,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 164, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 25, 'argument_list': 25, '(': 28, 'none': 5, ',': 13, ')': 28, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.759784272540137,0.7593416553892807,"(tensor([0.9872]), tensor([0.9878]), tensor([0.9875]), tensor([0.9877]))"
"643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","646             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
647             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
648 
649     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
650         """"""
651         Applies the optimisation process starting, in each chain, with a state created via the given factory.
652         The result of the optimisation (i.e. the final best state representation) is written by calling the
653         applyStateRepresentation method on one of the states, which should write to a suitable object each
654         state receives at construction.
655 
656         :param stateFactory: the factory with which to create the states for all chains
657         """"""
658         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
659 
660         r = random.Random(self.randomSeed)
661         chains = []
662         costProgressions = []
663         for i, schedule in enumerate(self._createSchedules(), start=1):
664             self.log.info(f""Chain {i} uses {schedule}"")
665             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
666             costProgressions.append([])
667 
668         startTime = time.time()
669         step = 0
670         numChainSwaps = 0
671         while True:
672             timeElapsed = time.time() - startTime
673             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
674                 break
675 
676             # take one step in each chain
677             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
678             for chain in chains:
679                 chain.step(degreeOfCompletion)
680 
681             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
682             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
683             # in the chains array, which shall always be in descending order of temperature)
684             for idxHighTempChain in range(0, self.numChains-1):
685                 idxLowTempChain = idxHighTempChain+1
686                 highTempChain = chains[idxHighTempChain]
687                 lowTempChain = chains[idxLowTempChain]
688                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
689                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
690                     chains[idxLowTempChain] = highTempChain
691                     chains[idxHighTempChain] = lowTempChain
692                     numChainSwaps += 1
693 
694             if self.logCostProgression:
695                 for idxChain, chain in enumerate(chains):
696                     costProgressions[idxChain].append(chain.state.cost.value())
697 
698             step += 1
699 
700         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
701         if self.logCostProgression: self._costProgressions = costProgressions
702 
703         # apply best solution
704         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
705         chains[bestChainIdx].applyBestState()
706 
707     def plotCostProgression(self):
","Before: 697
After: 700",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,5656,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 164, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 25, 'argument_list': 25, '(': 28, 'none': 5, ',': 13, ')': 28, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.759784272540137,0.7593416553892807,"(tensor([0.9872]), tensor([0.9878]), tensor([0.9875]), tensor([0.9877]))"
"643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","646             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
647             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
648 
649     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
650         """"""
651         Applies the optimisation process starting, in each chain, with a state created via the given factory.
652         The result of the optimisation (i.e. the final best state representation) is written by calling the
653         applyStateRepresentation method on one of the states, which should write to a suitable object each
654         state receives at construction.
655 
656         :param stateFactory: the factory with which to create the states for all chains
657         """"""
658         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
659 
660         r = random.Random(self.randomSeed)
661         chains = []
662         costProgressions = []
663         for i, schedule in enumerate(self._createSchedules(), start=1):
664             self.log.info(f""Chain {i} uses {schedule}"")
665             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
666             costProgressions.append([])
667 
668         startTime = time.time()
669         step = 0
670         numChainSwaps = 0
671         while True:
672             timeElapsed = time.time() - startTime
673             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
674                 break
675 
676             # take one step in each chain
677             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
678             for chain in chains:
679                 chain.step(degreeOfCompletion)
680 
681             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
682             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
683             # in the chains array, which shall always be in descending order of temperature)
684             for idxHighTempChain in range(0, self.numChains-1):
685                 idxLowTempChain = idxHighTempChain+1
686                 highTempChain = chains[idxHighTempChain]
687                 lowTempChain = chains[idxLowTempChain]
688                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
689                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
690                     chains[idxLowTempChain] = highTempChain
691                     chains[idxHighTempChain] = lowTempChain
692                     numChainSwaps += 1
693 
694             if self.logCostProgression:
695                 for idxChain, chain in enumerate(chains):
696                     costProgressions[idxChain].append(chain.state.cost.value())
697 
698             step += 1
699 
700         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
701         if self.logCostProgression: self._costProgressions = costProgressions
702 
703         # apply best solution
704         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
705         chains[bestChainIdx].applyBestState()
706 
707     def plotCostProgression(self):
","Before: 701
After: 704",remove unused imports,"local_search: minor, mostly aesthetic improvements",https://github.com/opcode81/sensAI,src/sensai/local_search.py,175c927559c0971a4764fd1fd446e261b7b82102,cdfe0553e7e9423ec015c23e59314634bfb7d789,0,5704,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 164, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 25, 'argument_list': 25, '(': 28, 'none': 5, ',': 13, ')': 28, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.759784272540137,0.7593416553892807,"(tensor([0.9872]), tensor([0.9878]), tensor([0.9875]), tensor([0.9877]))"
"8 from . import sklearn
9 from . import util
10 from .data_ingest import InputOutputData
11 from .data_transformation import DataFrameTransformer, RuleBasedDataFrameTransformer
12 from .ensemble import AveragingVectorRegressionModel
13 from .evaluation.eval_stats import eval_stats
14 from .normalisation import NormalisationMode
15 from .vector_model import VectorModel, VectorRegressionModel, VectorClassificationModel
16 
17 # The following submodules are not imported by default to avoid necessarily requiring their dependencies:
","8 from . import sklearn
9 from . import util
10 from .data_ingest import InputOutputData
11 from .data_transformation import DataFrameTransformer, RuleBasedDataFrameTransformer
12 from .ensemble import AveragingVectorRegressionModel
13 from .evaluation.eval_stats import eval_stats_classification, eval_stats_regression
14 from .normalisation import NormalisationMode
15 from .vector_model import VectorModel, VectorRegressionModel, VectorClassificationModel
16 
17 # The following submodules are not imported by default to avoid necessarily requiring their dependencies:
","Before: 13
After: 13",remove unused imports,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/__init__.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,119,"{'module': 1, 'import_from_statement': 8, 'from': 8, 'relative_import': 8, 'import_prefix': 8, '.': 9, 'import': 8, 'dotted_name': 17, 'identifier': 18, ',': 3}",{},{},0.9297257490286697,0.9272798249586792,"(tensor([0.9876]), tensor([0.9938]), tensor([0.9907]), tensor([0.9932]))"
"1 import copy
2 import logging
3 import time
4 from abc import ABC, abstractmethod
5 from typing import Tuple, Dict, Any, Union, Generator, Generic, TypeVar, List, Optional, Sequence, Callable
6 
7 import matplotlib.figure
","1 import copy
2 import logging
3 from abc import ABC, abstractmethod
4 from typing import Tuple, Any, Union, Generator, Generic, TypeVar, List
5 
6 import numpy as np
7 
8 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
","Before: 3, 5
After: 4",improve crossval.py script,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,14,"{'module': 1, 'import_statement': 3, 'import': 5, 'dotted_name': 18, 'identifier': 18, 'import_from_statement': 2, 'from': 2, ',': 11}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , trainedModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 26, 'end_line': 30, 'full_parameters': ['self', ' trainedModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5731013926209534,0.5410623903808304,"(tensor([0.8774]), tensor([0.9147]), tensor([0.8956]), tensor([0.9108]))"
"2 import logging
3 import time
4 from abc import ABC, abstractmethod
5 from typing import Tuple, Dict, Any, Union, Generator, Generic, TypeVar, List, Optional, Sequence, Callable
6 
7 import matplotlib.figure
8 import matplotlib.pyplot as plt
9 import numpy as np
10 import pandas as pd
11 import seaborn as sns
","3 from abc import ABC, abstractmethod
4 from typing import Tuple, Any, Union, Generator, Generic, TypeVar, List
5 
6 import numpy as np
7 
8 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
9 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
10 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
11 from .evaluation import VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData, \
12     VectorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator
","Before: 7, 8, 10, 11, 13, 14, 15, 16, 17, 18, 19
After: 8, 9, 10, 11, 12, 13, 14, 15",improve crossval.py script,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,67,"{'module': 1, 'import_statement': 6, 'import': 8, 'dotted_name': 21, 'identifier': 26, 'import_from_statement': 2, 'from': 2, ',': 11, '.': 2, 'aliased_import': 3, 'as': 3}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , trainedModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 26, 'end_line': 30, 'full_parameters': ['self', ' trainedModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.35559293367653333,0.3315173174763654,"(tensor([0.7100]), tensor([0.8233]), tensor([0.7625]), tensor([0.8104]))"
"16     ClassificationEvalStatsCollection, EvalStats, EvalStatsCollection, ClassificationMetric
17 from sensai.util.io import ResultWriter
18 from sensai.util.typing import PandasNamedTuple
19 from sensai.vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel, PredictorModel
20 
21 _log = logging.getLogger(__name__)
22 
23 TModel = TypeVar(""TModel"", bound=VectorModel)
24 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
25 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
","12     VectorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator
13 from ..data_ingest import InputOutputData
14 from ..util.typing import PandasNamedTuple
15 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel
16 
17 log = logging.getLogger(__name__)
18 
19 TModel = TypeVar(""TModel"", bound=VectorModel)
20 TEvalStats = TypeVar(""TEvalStats"", bound=PredictionEvalStats)
21 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
","Before: 21
After: 17",improve crossval.py script,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,198,"{'module': 1, 'expression_statement': 4, 'identifier': 30, ',': 8, 'import_from_statement': 3, 'from': 3, 'dotted_name': 9, '.': 6, 'import': 3, 'assignment': 3, '=': 5, 'call': 3, 'attribute': 1, 'argument_list': 3, '(': 3, ')': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , trainedModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 26, 'end_line': 30, 'full_parameters': ['self', ' trainedModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.48745086177080493,0.444002646325067,"(tensor([0.9080]), tensor([0.9092]), tensor([0.9086]), tensor([0.9091]))"
"19 from sensai.vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel, PredictorModel
20 
21 _log = logging.getLogger(__name__)
22 
23 TModel = TypeVar(""TModel"", bound=VectorModel)
24 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
25 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
26 TEvaluator = TypeVar(""TEvaluator"", bound=""VectorModelEvaluator"")
27 TEvalData = TypeVar(""TEvalData"", bound=""VectorModelEvaluationData"")
28 TCrossValData = TypeVar(""TCrossValData"", bound=""VectorModelCrossValidationData"")
","15 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel
16 
17 log = logging.getLogger(__name__)
18 
19 TModel = TypeVar(""TModel"", bound=VectorModel)
20 TEvalStats = TypeVar(""TEvalStats"", bound=PredictionEvalStats)
21 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
22 TEvalData = TypeVar(""TEvalData"", bound=VectorModelEvaluationData)
23 
24 
","Before: 24
After: 20",improve crossval.py script,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,234,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'dotted_name': 5, 'identifier': 28, '.': 2, 'import': 1, ',': 8, 'expression_statement': 6, 'assignment': 6, '=': 11, 'call': 6, 'attribute': 1, 'argument_list': 6, '(': 6, ')': 6, 'string': 7, 'string_start': 7, 'string_content': 7, 'string_end': 7, 'keyword_argument': 5}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , trainedModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 26, 'end_line': 30, 'full_parameters': ['self', ' trainedModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.39023466035400045,0.3611301813574216,"(tensor([0.9508]), tensor([0.9149]), tensor([0.9325]), tensor([0.9184]))"
"21 _log = logging.getLogger(__name__)
22 
23 TModel = TypeVar(""TModel"", bound=VectorModel)
24 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
25 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
26 TEvaluator = TypeVar(""TEvaluator"", bound=""VectorModelEvaluator"")
27 TEvalData = TypeVar(""TEvalData"", bound=""VectorModelEvaluationData"")
28 TCrossValData = TypeVar(""TCrossValData"", bound=""VectorModelCrossValidationData"")
29 
30 
","17 log = logging.getLogger(__name__)
18 
19 TModel = TypeVar(""TModel"", bound=VectorModel)
20 TEvalStats = TypeVar(""TEvalStats"", bound=PredictionEvalStats)
21 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
22 TEvalData = TypeVar(""TEvalData"", bound=VectorModelEvaluationData)
23 
24 
25 class VectorModelCrossValidationData(ABC, Generic[TModel, TEvalData, TEvalStats, TEvalStatsCollection]):
26     def __init__(self, trainedModels: List[TModel], evalDataList: List[TEvalData], predictedVarNames: List[str], testIndicesList=None):
","Before: 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222
After: 22",improve crossval.py script,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,273,"{'module': 1, 'expression_statement': 7, 'assignment': 7, 'identifier': 25, '=': 13, 'call': 7, 'attribute': 1, '.': 1, 'argument_list': 7, '(': 7, ')': 7, 'string': 9, 'string_start': 9, 'string_content': 9, 'string_end': 9, ',': 6, 'keyword_argument': 6}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , trainedModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 26, 'end_line': 30, 'full_parameters': ['self', ' trainedModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3577190042568604,0.29294619378001835,"(tensor([0.8757]), tensor([0.9197]), tensor([0.8971]), tensor([0.9151]))"
"334     def _createModelEvaluator(self, trainingData: InputOutputData, testData: InputOutputData):
335         return VectorClassificationModelEvaluator(trainingData, testData=testData, **self.evaluatorParams)
336 
337     def _createResultData(self, trainedModels, evalDataList, testIndicesList, predictedVarNames) -> VectorClassificationModelCrossValidationData:
338         return VectorClassificationModelCrossValidationData(trainedModels, evalDataList, predictedVarNames)
339 
340 
","138     def _createModelEvaluator(self, trainingData: InputOutputData, testData: InputOutputData):
139         return VectorClassificationModelEvaluator(trainingData, testData=testData, **self.evaluatorParams)
140 
141     def _createResultData(self, trainedModels, evalDataList, testIndicesList, predictedVarNames) -> VectorClassificationModelCrossValidationData:
142         return VectorClassificationModelCrossValidationData(trainedModels, evalDataList, predictedVarNames, testIndicesList)
","Before: 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621
After: 142",improve crossval.py script,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,3513,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 23, 'parameters': 2, '(': 4, ',': 10, 'typed_parameter': 2, ':': 4, 'type': 3, ')': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 2, 'argument_list': 2, 'keyword_argument': 1, '=': 1, 'dictionary_splat': 1, '**': 1, 'attribute': 1, '.': 1, '->': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , trainedModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 26, 'end_line': 30, 'full_parameters': ['self', ' trainedModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.778711937264911,0.745626883328766,"(tensor([0.9801]), tensor([0.9775]), tensor([0.9788]), tensor([0.9778]))"
"1 from abc import ABC, abstractmethod
2 from typing import Generic, TypeVar, List, Union, Dict
3 
4 import numpy as np
5 import pandas as pd
6 import seaborn as sns
","1 from abc import ABC, abstractmethod
2 from typing import Generic, TypeVar, List, Union, Dict, Sequence
3 
4 import numpy as np
5 import pandas as pd
6 import seaborn as sns
","Before: 2
After: 2","add tmetric, tevalstats, and vectormodel to eval_stats_base",Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,31,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 11, 'identifier': 13, 'import': 4, ',': 5, 'import_statement': 2, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 19, 'end_line': 20, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9024521756077707,0.9000928033303266,"(tensor([0.9929]), tensor([0.9968]), tensor([0.9948]), tensor([0.9964]))"
"3 
4 import numpy as np
5 import pandas as pd
6 import seaborn as sns
7 from matplotlib import pyplot as plt
8 from sensai import VectorModel
9 
10 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
11 TMetric = TypeVar(""TMetric"", bound=Metric)
12 TVectorModelEvalStats = TypeVar(""TVectorModelEvalStats"", bound=VectorModelEvalStats)
","4 import numpy as np
5 import pandas as pd
6 import seaborn as sns
7 from matplotlib import pyplot as plt
8 
9 from ...vector_model import VectorModel
10 
11 # Note: in the 2020.2 version of PyCharm passing strings to bound is highlighted as error
12 # It does not cause runtime errors and the static type checker ignores the bound anyway, so it does not matter for now.
13 # However, this might cause problems with type checking in the future. Therefore, I moved the definition of TEvalStats
","Before: 8, 10, 11, 12
After: 9, 10, 11, 12, 13, 14, 15, 16, 17","add tmetric, tevalstats, and vectormodel to eval_stats_base",Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,69,"{'module': 1, 'import_statement': 3, 'import': 5, 'aliased_import': 4, 'dotted_name': 7, 'identifier': 19, 'as': 4, 'import_from_statement': 2, 'from': 2, 'expression_statement': 2, 'assignment': 2, '=': 4, 'call': 2, 'argument_list': 2, '(': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ',': 2, 'keyword_argument': 2, ')': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 19, 'end_line': 20, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.24882794910576594,0.2296303710471529,"(tensor([0.7681]), tensor([0.8475]), tensor([0.8058]), tensor([0.8388]))"
"101     def __str__(self):
102         return f""{self.__class__.__name__}["" + \
103                "", "".join([f""{key}={self.aggStats()[key]:.4f}"" for key in self.metrics]) + ""]""
104 
105 
106 class VectorModelEvalStats(EvalStats[TMetric], ABC):
107     """"""
108     Collects data for the evaluation of a model and computes corresponding metrics
109     """"""
110     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray, metrics: List[TMetric]):
","106     def __str__(self):
107         return f""{self.__class__.__name__}["" + \
108                "", "".join([f""{key}={self.aggStats()[key]:.4f}"" for key in self.metrics]) + ""]""
109 
110 
111 class PredictionEvalStats(EvalStats[TMetric], ABC):
112     """"""
113     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
114     and computes corresponding metrics
115 
","Before: 106
After: 111","add tmetric, tevalstats, and vectormodel to eval_stats_base",Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,1136,"{'module': 1, 'function_definition': 1, 'def': 1, 'identifier': 17, 'parameters': 1, '(': 4, ')': 4, ':': 3, 'block': 2, 'return_statement': 1, 'return': 1, 'binary_operator': 2, 'string': 5, 'string_start': 5, 'interpolation': 3, '{': 3, 'attribute': 5, '.': 5, '}': 3, 'string_content': 5, 'string_end': 5, '+': 2, 'call': 2, 'argument_list': 3, 'list_comprehension': 1, '[': 3, 'subscript': 2, ']': 3, 'format_specifier': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'class_definition': 1, 'class': 1, ',': 1, 'expression_statement': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 19, 'end_line': 20, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5683836945053308,0.5518475462479304,"(tensor([0.9350]), tensor([0.9128]), tensor([0.9237]), tensor([0.9149]))"
"103                "", "".join([f""{key}={self.aggStats()[key]:.4f}"" for key in self.metrics]) + ""]""
104 
105 
106 class VectorModelEvalStats(EvalStats[TMetric], ABC):
107     """"""
108     Collects data for the evaluation of a model and computes corresponding metrics
109     """"""
110     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray, metrics: List[TMetric]):
111         self.y_true = []
112         self.y_predicted = []
","108                "", "".join([f""{key}={self.aggStats()[key]:.4f}"" for key in self.metrics]) + ""]""
109 
110 
111 class PredictionEvalStats(EvalStats[TMetric], ABC):
112     """"""
113     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
114     and computes corresponding metrics
115 
116     :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
117         one column per dimension should be passed
","Before: 108
After: 113, 114, 115, 116, 117, 118, 119","add tmetric, tevalstats, and vectormodel to eval_stats_base",Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,1151,"{'module': 1, 'expression_statement': 3, 'binary_operator': 1, 'call': 2, 'attribute': 4, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, '.': 4, 'identifier': 23, 'argument_list': 3, '(': 4, 'list_comprehension': 1, '[': 5, 'interpolation': 2, '{': 2, '}': 2, 'subscript': 2, ')': 4, ']': 5, 'format_specifier': 1, ':': 6, 'for_in_clause': 1, 'for': 1, 'in': 1, '+': 1, 'class_definition': 1, 'class': 1, ',': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 3, 'type': 4, 'generic_type': 1, 'type_parameter': 1, 'assignment': 1, '=': 1, 'list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 19, 'end_line': 20, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5024428871717798,0.46526670540115045,"(tensor([0.8515]), tensor([0.8968]), tensor([0.8736]), tensor([0.8921]))"
"4 import numpy as np
5 import pandas as pd
6 import sklearn
7 from sklearn.metrics import confusion_matrix, accuracy_score
8 
9 from .eval_stats_base import PredictionArray, VectorModelEvalStats, VectorModelEvalStatsCollection, Metric
10 from ...util.plot import plotMatrix
11 
12 
13 class ClassificationMetric(Metric[""ClassificationEvalStats""], ABC):
","4 import numpy as np
5 import pandas as pd
6 import sklearn
7 from sklearn.metrics import confusion_matrix, accuracy_score
8 
9 from .eval_stats_base import PredictionArray, PredictionEvalStats, EvalStatsCollection, Metric
10 from ...util.plot import plotMatrix
11 
12 
13 class ClassificationMetric(Metric[""ClassificationEvalStats""], ABC):
","Before: 9
After: 9",update eval_stats_classification.py for python 3.8,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,71,"{'module': 1, 'import_statement': 3, 'import': 6, 'aliased_import': 2, 'dotted_name': 13, 'identifier': 17, 'as': 2, 'import_from_statement': 3, 'from': 3, '.': 6, ',': 4, 'relative_import': 2, 'import_prefix': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9179450020177299,0.9113251203659606,"(tensor([0.9904]), tensor([0.9802]), tensor([0.9853]), tensor([0.9812]))"
"66             if y_true[i] in (x[0] for x in pairs[:self.n]):
67                 cnt += 1
68         return cnt / len(y_true)
69 
70 
71 class ClassificationEvalStats(VectorModelEvalStats[""ClassificationMetric""]):
72     def __init__(self, y_predicted: PredictionArray = None,
73                  y_true: PredictionArray = None,
74                  y_predictedClassProbabilities: pd.DataFrame = None,
75                  labels: PredictionArray = None,
","66             if y_true[i] in (x[0] for x in pairs[:self.n]):
67                 cnt += 1
68         return cnt / len(y_true)
69 
70 
71 # TODO: make some kwargs to args, infer y_predicted when classProbabilities are passed. We can also infer labels
72 #  (though not reliably) if they are not passed
73 class ClassificationEvalStats(PredictionEvalStats[""ClassificationMetric""]):
74     def __init__(self, y_predicted: PredictionArray = None,
75                  y_true: PredictionArray = None,
","Before: 71
After: 71, 72, 73",update eval_stats_classification.py for python 3.8,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,739,"{'module': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'subscript': 4, 'identifier': 22, '[': 4, ']': 4, 'in': 2, 'generator_expression': 1, '(': 4, 'integer': 2, 'for_in_clause': 1, 'for': 1, 'slice': 1, ':': 6, 'attribute': 2, '.': 2, ')': 3, 'block': 2, 'expression_statement': 1, 'augmented_assignment': 1, '+=': 1, 'return_statement': 1, 'return': 1, 'binary_operator': 1, '/': 1, 'call': 1, 'argument_list': 2, 'class_definition': 1, 'class': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'ERROR': 1, 'def': 1, ',': 4, 'typed_default_parameter': 3, 'type': 3, '=': 3, 'none': 3}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5880170408725901,0.5590051209914508,"(tensor([0.8557]), tensor([0.9424]), tensor([0.8970]), tensor([0.9329]))"
"122         # based on https://scikit-learn.org/0.20/auto_examples/model_selection/plot_confusion_matrix.html
123         confusionMatrix = self.getConfusionMatrix()
124         return confusionMatrix.plot(normalize=normalize, titleAdd=titleAdd)
125 
126 
127 class ClassificationEvalStatsCollection(VectorModelEvalStatsCollection[ClassificationEvalStats]):
128     def __init__(self, evalStatsList: List[ClassificationEvalStats]):
129         super().__init__(evalStatsList)
130         self.globalStats = None
131 
","124         # based on https://scikit-learn.org/0.20/auto_examples/model_selection/plot_confusion_matrix.html
125         confusionMatrix = self.getConfusionMatrix()
126         return confusionMatrix.plot(normalize=normalize, titleAdd=titleAdd)
127 
128 
129 class ClassificationEvalStatsCollection(EvalStatsCollection[ClassificationEvalStats]):
130     def __init__(self, evalStatsList: List[ClassificationEvalStats]):
131         super().__init__(evalStatsList)
132         self.globalStats = None
133 
","Before: 127
After: 129",update eval_stats_classification.py for python 3.8,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,1246,"{'module': 1, 'comment': 1, 'expression_statement': 3, 'assignment': 2, 'identifier': 22, '=': 4, 'call': 4, 'attribute': 4, '.': 4, 'argument_list': 5, '(': 6, ')': 6, 'return_statement': 1, 'return': 1, 'keyword_argument': 2, ',': 2, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 2, ']': 2, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.637091245637754,0.6271872449544862,"(tensor([0.9850]), tensor([0.9798]), tensor([0.9824]), tensor([0.9803]))"
"127 class ClassificationEvalStatsCollection(VectorModelEvalStatsCollection[ClassificationEvalStats]):
128     def __init__(self, evalStatsList: List[ClassificationEvalStats]):
129         super().__init__(evalStatsList)
130         self.globalStats = None
131 
132     # TODO: move to base class and use the new get_args method (importing from future) to infer the generic type at runtime
133     #  https://docs.python.org/3/library/typing.html#typing.get_args
134     def getGlobalStats(self) -> ClassificationEvalStats:
135         """"""
136         Gets an evaluation statistics object that combines the data from all contained eval stats objects
","129 class ClassificationEvalStatsCollection(EvalStatsCollection[ClassificationEvalStats]):
130     def __init__(self, evalStatsList: List[ClassificationEvalStats]):
131         super().__init__(evalStatsList)
132         self.globalStats = None
133 
134     # TODO once we moved to python 3.8: move to base class and use the new get_args method to infer the generic type at runtime
135     #  https://docs.python.org/3/library/typing.html#typing.get_args
136     def getGlobalStats(self) -> ClassificationEvalStats:
137         """"""
138         Gets an evaluation statistics object that combines the data from all contained eval stats objects
","Before: 132
After: 134",update eval_stats_classification.py for python 3.8,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,1290,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 16, 'argument_list': 3, '(': 5, 'subscript': 1, '[': 2, ']': 2, ')': 5, ':': 4, 'block': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 1, 'typed_parameter': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'expression_statement': 2, 'call': 2, 'attribute': 2, '.': 2, 'assignment': 1, '=': 1, 'none': 1, 'comment': 2, '->': 1, 'ERROR': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6676844356331225,0.6620624264402212,"(tensor([0.9691]), tensor([0.9678]), tensor([0.9685]), tensor([0.9680]))"
"1 import logging
2 from abc import abstractmethod, ABC
3 from typing import Union, List, TypeVar, Generic, Sequence
4 
5 import numpy as np
6 import pandas as pd
7 import seaborn as sns
","1 import logging
2 from abc import abstractmethod, ABC
3 from typing import Union, List, Sequence
4 
5 import numpy as np
6 import pandas as pd
7 import seaborn as sns
","Before: 3
After: 3",add evalstatscollection to eval_stats_regression,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,35,"{'module': 1, 'import_statement': 3, 'import': 5, 'dotted_name': 12, 'identifier': 14, 'import_from_statement': 2, 'from': 2, ',': 5, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 23, 'end_line': 24, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8710000373596634,0.8710000373596634,"(tensor([0.9933]), tensor([0.9787]), tensor([0.9859]), tensor([0.9801]))"
"3 from typing import Union, List, TypeVar, Generic, Sequence
4 
5 import numpy as np
6 import pandas as pd
7 import seaborn as sns
8 import sklearn.utils.multiclass
9 from matplotlib import pyplot as plt
10 from matplotlib.colors import LinearSegmentedColormap
11 from sklearn.metrics import accuracy_score, confusion_matrix
12 
","6 import pandas as pd
7 import seaborn as sns
8 from matplotlib import pyplot as plt
9 from matplotlib.colors import LinearSegmentedColormap
10 
11 from .eval_stats_base import PredictionEvalStats, Metric, EvalStatsCollection
12 
13 log = logging.getLogger(__name__)
14 
15 
","Before: 8, 11, 13
After: 11",add evalstatscollection to eval_stats_regression,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,64,"{'module': 1, 'import_from_statement': 4, 'from': 4, 'dotted_name': 17, 'identifier': 25, 'import': 8, ',': 5, 'import_statement': 4, 'aliased_import': 4, 'as': 4, '.': 4}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 23, 'end_line': 24, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.349930856876726,0.311718877982039,"(tensor([0.8584]), tensor([0.8440]), tensor([0.8512]), tensor([0.8455]))"
"10 from matplotlib.colors import LinearSegmentedColormap
11 from sklearn.metrics import accuracy_score, confusion_matrix
12 
13 from sensai.util.plot import plotMatrix
14 
15 _log = logging.getLogger(__name__)
16 
17 
18 TEvalStats = TypeVar(""TEvalStats"", bound=""EvalStats"")
19 TMetric = TypeVar(""TMetric"", bound=""Metric"")
","8 from matplotlib import pyplot as plt
9 from matplotlib.colors import LinearSegmentedColormap
10 
11 from .eval_stats_base import PredictionEvalStats, Metric, EvalStatsCollection
12 
13 log = logging.getLogger(__name__)
14 
15 
16 class RegressionMetric(Metric[""RegressionEvalStats""], ABC):
17     def __init__(self, name):
","Before: 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229
After: 13",add evalstatscollection to eval_stats_regression,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,119,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 7, 'identifier': 18, '.': 5, 'import': 3, ',': 2, 'expression_statement': 2, 'assignment': 2, '=': 3, 'call': 2, 'attribute': 1, 'argument_list': 2, '(': 2, ')': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 23, 'end_line': 24, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.15160309210376496,0.1329468324765787,"(tensor([0.8654]), tensor([0.8563]), tensor([0.8608]), tensor([0.8572]))"
"325     @classmethod
326     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
327         return np.median(cls.computeAbsErrors(y_true, y_predicted))
328 
329 
330 class RegressionEvalStats(EvalStats[RegressionMetric]):
331     """"""Collects data for the evaluation of a model and computes corresponding metrics""""""
332 
333     def __init__(self, y_predicted: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
334             y_true: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
","109     @classmethod
110     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
111         return np.median(cls.computeAbsErrors(y_true, y_predicted))
112 
113 
114 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
115     """"""Collects data for the evaluation of a model and computes corresponding metrics""""""
116 
117     def __init__(self, y_predicted: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
118             y_true: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
","Before: 330
After: 114",add evalstatscollection to eval_stats_regression,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,3362,"{'module': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 29, 'function_definition': 1, 'def': 2, 'parameters': 1, '(': 5, ',': 8, 'typed_parameter': 2, ':': 5, 'type': 7, 'attribute': 7, '.': 7, ')': 4, 'block': 2, 'return_statement': 1, 'return': 1, 'call': 2, 'argument_list': 3, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 2, ']': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'ERROR': 1, 'typed_default_parameter': 1, 'generic_type': 1, 'type_parameter': 1, '=': 1, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 23, 'end_line': 24, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7428556628505346,0.7192804112152428,"(tensor([0.9697]), tensor([0.9728]), tensor([0.9713]), tensor([0.9725]))"
"330 class RegressionEvalStats(EvalStats[RegressionMetric]):
331     """"""Collects data for the evaluation of a model and computes corresponding metrics""""""
332 
333     def __init__(self, y_predicted: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
334             y_true: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
335             metrics: Sequence[RegressionMetric] = None,
336             additionalMetrics: Sequence[RegressionMetric] = None):
337 
338         if metrics is None:
339             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
340                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
341                        RegressionMetricStdDevAE()]
342         metrics = list(metrics)
343         if additionalMetrics is not None:
344             metrics.extend(additionalMetrics)
345 
346         super().__init__(y_predicted=y_predicted, y_true=y_true, metrics=metrics)
347 
348     def getMSE(self):
","114 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
115     """"""Collects data for the evaluation of a model and computes corresponding metrics""""""
116 
117     def __init__(self, y_predicted: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
118             y_true: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
119             metrics: Sequence[""RegressionMetric""] = None,
120             additionalMetrics: Sequence[""RegressionMetric""] = None):
121 
122         if metrics is None:
123             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
124                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
125                        RegressionMetricStdDevAE()]
126         metrics = list(metrics)
127         if additionalMetrics is not None:
128             metrics.extend(additionalMetrics)
129 
130         super().__init__(y_predicted=y_predicted, y_true=y_true, metrics=metrics)
131 
132     def getMSE(self):
","Before: 335, 336
After: 119, 120",add evalstatscollection to eval_stats_regression,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,3451,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 53, 'argument_list': 12, '(': 13, 'subscript': 1, '[': 6, ']': 6, ')': 13, ':': 8, 'block': 4, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 18, 'typed_default_parameter': 4, 'type': 14, 'generic_type': 4, 'type_parameter': 4, 'attribute': 8, '.': 8, '=': 9, 'none': 6, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'assignment': 2, 'list': 1, 'call': 11, 'is not': 2, 'keyword_argument': 3}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 23, 'end_line': 24, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6746429838657446,0.648172683921483,"(tensor([0.9697]), tensor([0.9714]), tensor([0.9706]), tensor([0.9713]))"
"1 import copy
2 import logging
3 import time
4 from abc import ABC, abstractmethod
5 from typing import Tuple, Dict, Any, Union, Generator, Generic, TypeVar, List, Optional, Sequence, Callable
","1 import logging
2 from abc import ABC, abstractmethod
3 from typing import Tuple, Dict, Any, Union, Generic, TypeVar, Optional, Sequence, Callable
4 
5 import matplotlib.figure
6 import matplotlib.pyplot as plt
7 import pandas as pd
","Before: 1, 3, 5
After: 3",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,6,"{'module': 1, 'import_statement': 3, 'import': 4, 'dotted_name': 6, 'identifier': 6, 'import_from_statement': 1, 'from': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5341978165773844,0.49457107584195314,"(tensor([0.8514]), tensor([0.9416]), tensor([0.8943]), tensor([0.9318]))"
"4 from abc import ABC, abstractmethod
5 from typing import Tuple, Dict, Any, Union, Generator, Generic, TypeVar, List, Optional, Sequence, Callable
6 
7 import matplotlib.figure
8 import matplotlib.pyplot as plt
9 import numpy as np
10 import pandas as pd
11 import seaborn as sns
12 
13 from sensai.data_ingest import DataSplitter, DataSplitterFractional, InputOutputData
","5 import matplotlib.figure
6 import matplotlib.pyplot as plt
7 import pandas as pd
8 import seaborn as sns
9 
10 from .crossval import VectorModelCrossValidationData, VectorModelCrossValidator, \
11     VectorRegressionModelCrossValidationData, VectorClassificationModelCrossValidationData
12 from .eval_stats.eval_stats_base import EvalStats, EvalStatsCollection
13 from .eval_stats.eval_stats_classification import ClassificationEvalStats
14 from .eval_stats.eval_stats_regression import RegressionEvalStats
","Before: 9, 13, 14, 15, 16, 17, 18, 19
After: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,83,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 20, 'identifier': 26, 'import': 7, ',': 11, 'import_statement': 5, '.': 2, 'aliased_import': 4, 'as': 4}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.1405810705085766,0.12375231343473007,"(tensor([0.8022]), tensor([0.7984]), tensor([0.8003]), tensor([0.7988]))"
"16     ClassificationEvalStatsCollection, EvalStats, EvalStatsCollection, ClassificationMetric
17 from sensai.util.io import ResultWriter
18 from sensai.util.typing import PandasNamedTuple
19 from sensai.vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel, PredictorModel
20 
21 _log = logging.getLogger(__name__)
22 
23 TModel = TypeVar(""TModel"", bound=VectorModel)
24 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
25 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
","16     VectorRegressionModelEvaluationData, VectorClassificationModelEvaluator, VectorClassificationModelEvaluationData
17 from ..data_ingest import InputOutputData
18 from ..util.io import ResultWriter
19 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel
20 
21 log = logging.getLogger(__name__)
22 
23 TModel = TypeVar(""TModel"", bound=VectorModel)
24 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
25 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
","Before: 21
After: 21",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,198,"{'module': 1, 'expression_statement': 4, 'identifier': 30, ',': 8, 'import_from_statement': 3, 'from': 3, 'dotted_name': 9, '.': 6, 'import': 3, 'assignment': 3, '=': 5, 'call': 3, 'attribute': 1, 'argument_list': 3, '(': 3, ')': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6639700803881996,0.6294170479527282,"(tensor([0.9252]), tensor([0.9076]), tensor([0.9163]), tensor([0.9093]))"
"21 _log = logging.getLogger(__name__)
22 
23 TModel = TypeVar(""TModel"", bound=VectorModel)
24 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
25 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
26 TEvaluator = TypeVar(""TEvaluator"", bound=""VectorModelEvaluator"")
27 TEvalData = TypeVar(""TEvalData"", bound=""VectorModelEvaluationData"")
28 TCrossValData = TypeVar(""TCrossValData"", bound=""VectorModelCrossValidationData"")
29 
30 
","21 log = logging.getLogger(__name__)
22 
23 TModel = TypeVar(""TModel"", bound=VectorModel)
24 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
25 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
26 TEvaluator = TypeVar(""TEvaluator"", bound=VectorModelEvaluator)
27 TEvalData = TypeVar(""TEvalData"", bound=VectorModelEvaluationData)
28 TCrossValData = TypeVar(""TCrossValData"", bound=VectorModelCrossValidationData)
29 
30 
","Before: 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338
After: 26, 27, 28",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,273,"{'module': 1, 'expression_statement': 7, 'assignment': 7, 'identifier': 25, '=': 13, 'call': 7, 'attribute': 1, '.': 1, 'argument_list': 7, '(': 7, ')': 7, 'string': 9, 'string_start': 9, 'string_content': 9, 'string_end': 9, ',': 6, 'keyword_argument': 6}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7760022785718405,0.7525939266374905,"(tensor([0.9950]), tensor([0.9950]), tensor([0.9950]), tensor([0.9950]))"
"352         raise ValueError(f""Unexpected evaluator/validator of type {type(evaluatorOrValidator)}"")
353 
354 
355 def evalModelViaEvaluator(model: TModel, inputOutputData: InputOutputData, testFraction=0.2,
356         plotTargetDistribution=False, computeProbabilities=True, normalizePlots=True, randomSeed=60) -> TEvalData:
357     """"""
358     Evaluates the given model via a simple evaluation mechanism that uses a single split
359 
360     :param model: the model to evaluate
361     :param inputOutputData: data on which to evaluate
362     :param testFraction: the fraction of the data to test on
363     :param plotTargetDistribution: whether to plot the target values distribution in the entire dataset
364     :param computeProbabilities: only relevant if the model is a classifier
365     :param randomSeed:
366 
367     :return: the evaluation data
368     """"""
369 
370     if plotTargetDistribution:
371         title = ""Distribution of target values in entire dataset""
372         fig = plt.figure(title)
373 
374         outputDistributionSeries = inputOutputData.outputs.iloc[:, 0]
375         _log.info(f""Description of target column in training set: \n{outputDistributionSeries.describe()}"")
376         if not model.isRegressionModel():
377             outputDistributionSeries = outputDistributionSeries.value_counts(normalize=normalizePlots)
378             ax = sns.barplot(outputDistributionSeries.index, outputDistributionSeries.values)
379             ax.set_ylabel(""%"")
380         else:
381             ax = sns.distplot(outputDistributionSeries)
382             ax.set_ylabel(""Probability density"")
383         ax.set_title(title)
384         ax.set_xlabel(""target value"")
385         fig.show()
386 
387     if model.isRegressionModel():
388         evaluatorParams = dict(testFraction=testFraction, randomSeed=randomSeed)
389     else:
390         evaluatorParams = dict(testFraction=testFraction, computeProbabilities=computeProbabilities, randomSeed=randomSeed)
391     ev = EvaluationUtil.forModelType(model.isRegressionModel(), inputOutputData, evaluatorParams=evaluatorParams)
392     return ev.performSimpleEvaluation(model, showPlots=True, logResults=True)
393 
394 
","42         raise ValueError(f""Unexpected evaluator/validator of type {type(evaluatorOrValidator)}"")
43 
44 
45 def evalModelViaEvaluator(model: TModel, inputOutputData: InputOutputData, testFraction=0.2,
46         plotTargetDistribution=False, computeProbabilities=True, normalizePlots=True, randomSeed=60) -> TEvalData:
47     """"""
48     Evaluates the given model via a simple evaluation mechanism that uses a single split
49 
50     :param model: the model to evaluate
51     :param inputOutputData: data on which to evaluate
52     :param testFraction: the fraction of the data to test on
53     :param plotTargetDistribution: whether to plot the target values distribution in the entire dataset
54     :param computeProbabilities: only relevant if the model is a classifier
55     :param normalizePlots: whether to normalize plotted distributions such that the sum/integrate to 1
56     :param randomSeed:
57 
58     :return: the evaluation data
59     """"""
60 
61     if plotTargetDistribution:
62         title = ""Distribution of target values in entire dataset""
63         fig = plt.figure(title)
64 
65         outputDistributionSeries = inputOutputData.outputs.iloc[:, 0]
66         log.info(f""Description of target column in training set: \n{outputDistributionSeries.describe()}"")
67         if not model.isRegressionModel():
68             outputDistributionSeries = outputDistributionSeries.value_counts(normalize=normalizePlots)
69             ax = sns.barplot(outputDistributionSeries.index, outputDistributionSeries.values)
70             ax.set_ylabel(""%"")
71         else:
72             ax = sns.distplot(outputDistributionSeries)
73             ax.set_ylabel(""Probability density"")
74         ax.set_title(title)
75         ax.set_xlabel(""target value"")
76         fig.show()
77 
78     if model.isRegressionModel():
79         evaluatorParams = dict(testFraction=testFraction, randomSeed=randomSeed)
80     else:
81         evaluatorParams = dict(testFraction=testFraction, computeProbabilities=computeProbabilities, randomSeed=randomSeed)
82     ev = EvaluationUtil.forModelType(model.isRegressionModel(), inputOutputData, evaluatorParams=evaluatorParams)
83     return ev.performSimpleEvaluation(model, showPlots=True, logResults=True)
84 
85 
","Before: 375
After: 66",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,3804,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 20, 'identifier': 86, 'argument_list': 20, '(': 21, 'string': 7, 'string_start': 7, 'string_content': 7, 'interpolation': 2, '{': 2, ')': 21, '}': 2, 'string_end': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 9, 'type': 3, ',': 15, 'default_parameter': 5, '=': 23, 'float': 1, 'false': 1, 'true': 4, 'integer': 2, '->': 1, 'block': 6, 'expression_statement': 16, 'if_statement': 3, 'if': 3, 'assignment': 9, 'attribute': 20, '.': 20, 'subscript': 1, '[': 1, 'slice': 1, ']': 1, 'escape_sequence': 1, 'not_operator': 1, 'not': 1, 'keyword_argument': 9, 'else_clause': 2, 'else': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6647853136717757,0.6455519866130734,"(tensor([0.9510]), tensor([0.9609]), tensor([0.9559]), tensor([0.9599]))"
"440         """"""
441         return VectorModelEvaluator.forModel(model, self.inputOutputData, **self.evaluatorParams)
442 
443     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
444             additionalEvaluationOnTrainingData=False) -> TEvalData:
445         evaluator = self.createEvaluator(model)
446         evaluator.fitModel(model)
447 
448         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
449             strEvalResults = """"
450             for predictedVarName in model.getPredictedVariableNames():
451                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
452                 if logResults:
453                     _log.info(f""Evaluation results for {predictedVarName}: {strEvalResult}"")
454                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
455             if resultWriter is not None:
456                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
457             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
458 
459         evalResultData = evaluator.evalModel(model)
460         gatherResults(evalResultData, resultWriter)
461         if additionalEvaluationOnTrainingData:
462             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
463             gatherResults(evalResultDataTrain, resultWriter.childWithAddedPrefix(""-onTrain-""), subtitlePrefix=""[onTrain] "")
464 
465         return evalResultData
466 
467     @staticmethod
","131         """"""
132         return VectorModelEvaluator.forModel(model, self.inputOutputData, **self.evaluatorParams)
133 
134     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
135             additionalEvaluationOnTrainingData=False) -> TEvalData:
136         evaluator = self.createEvaluator(model)
137         evaluator.fitModel(model)
138 
139         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
140             strEvalResults = """"
141             for predictedVarName in model.getPredictedVariableNames():
142                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
143                 if logResults:
144                     log.info(f""Evaluation results for {predictedVarName}: {strEvalResult}"")
145                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
146             if resultWriter is not None:
147                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
148             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
149 
150         evalResultData = evaluator.evalModel(model)
151         gatherResults(evalResultData, resultWriter)
152         if additionalEvaluationOnTrainingData:
153             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
154             gatherResults(evalResultDataTrain, resultWriter.childWithAddedPrefix(""-onTrain-""), subtitlePrefix=""[onTrain] "")
155 
156         return evalResultData
157 
158     @staticmethod
","Before: 453
After: 144",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,4667,"{'module': 1, 'ERROR': 1, 'string_start': 1, 'escape_sequence': 1, ')': 1, 'return': 1, 'identifier': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.694694331377062,0.6691395553632836,"(tensor([0.9755]), tensor([0.9751]), tensor([0.9753]), tensor([0.9751]))"
"470             return None
471         return resultWriter.childWithAddedPrefix(model.getName() + ""-"")
472 
473     def performCrossValidation(self, model: TModel, showPlots=False, logResults=True, resultWriter: Optional[ResultWriter] = None) -> TCrossValData:
474         """"""
475         Evaluates the given model via cross-validation
476 
477         :param model: the model to evaluate
478         :param showPlots: whether to show plots that visualise evaluation results (combining all folds)
479         :param logResults: whether to log evaluation results
480         :param resultWriter: a writer with which to store text files and plots. The evaluated model's name is added to each filename
481             automatically
482         :return: cross-validation result data
483         """"""
484         resultWriter = self._resultWriterForModel(resultWriter, model)
485         crossValidator = VectorModelCrossValidator.forModel(model, self.inputOutputData, **self.crossValidatorParams)
486         crossValidationData = crossValidator.evalModel(model)
487         strEvalResults = str(crossValidationData.getEvalStatsCollection().aggStats())
488         if logResults:
489             _log.info(f""Cross-validation results: {strEvalResults}"")
490         if resultWriter is not None:
491             resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
492         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
493         return crossValidationData
494 
495     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","161             return None
162         return resultWriter.childWithAddedPrefix(model.getName() + ""-"")
163 
164     def performCrossValidation(self, model: TModel, showPlots=False, logResults=True, resultWriter: Optional[ResultWriter] = None) -> TCrossValData:
165         """"""
166         Evaluates the given model via cross-validation
167 
168         :param model: the model to evaluate
169         :param showPlots: whether to show plots that visualise evaluation results (combining all folds)
170         :param logResults: whether to log evaluation results
171         :param resultWriter: a writer with which to store text files and plots. The evaluated model's name is added to each filename
172             automatically
173         :return: cross-validation result data
174         """"""
175         resultWriter = self._resultWriterForModel(resultWriter, model)
176         crossValidator = VectorModelCrossValidator.forModel(model, self.inputOutputData, **self.crossValidatorParams)
177         crossValidationData = crossValidator.evalModel(model)
178         strEvalResults = str(crossValidationData.getEvalStatsCollection().aggStats())
179         if logResults:
180             log.info(f""Cross-validation results: {strEvalResults}"")
181         if resultWriter is not None:
182             resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
183         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
184         return crossValidationData
185 
186     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 489
After: 180",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,5032,"{'module': 1, 'return_statement': 3, 'return': 3, 'none': 3, 'call': 11, 'attribute': 12, 'identifier': 52, '.': 12, 'argument_list': 11, '(': 12, 'binary_operator': 1, ')': 12, '+': 1, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 10, 'typed_parameter': 1, ':': 5, 'type': 4, 'default_parameter': 2, '=': 9, 'false': 1, 'true': 1, 'typed_default_parameter': 1, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 3, 'expression_statement': 8, 'assignment': 4, 'dictionary_splat': 1, '**': 1, 'if_statement': 2, 'if': 2, 'interpolation': 1, '{': 1, '}': 1, 'comparison_operator': 1, 'is not': 2, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7511060281918878,0.7354695291108261,"(tensor([0.9775]), tensor([0.9779]), tensor([0.9777]), tensor([0.9779]))"
"492         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
493         return crossValidationData
494 
495     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
496         """"""
497         Compares several models via cross-validation
498 
499         :param models: the models to compare
500         :param resultWriter: a writer with which to store results of the comparison
501         :return: a data frame containing evaluation metrics on all models
502         """"""
503         statsList = []
504         for model in models:
505             crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
506             stats = crossValidationResult.getEvalStatsCollection().aggStats()
507             stats[""modelName""] = model.getName()
508             statsList.append(stats)
509         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
510         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
511         _log.info(strResults)
512         if resultWriter is not None:
513             resultWriter.writeTextFile(""model-comparison-results"", strResults)
514         return resultsDF
515 
516     def createPlots(self, data: Union[TEvalData, TCrossValData], showPlots=True, resultWriter: Optional[ResultWriter] = None, subtitlePrefix: str = """"):
","183         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
184         return crossValidationData
185 
186     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
187         """"""
188         Compares several models via cross-validation
189 
190         :param models: the models to compare
191         :param resultWriter: a writer with which to store results of the comparison
192         :return: a data frame containing evaluation metrics on all models
193         """"""
194         statsList = []
195         for model in models:
196             crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
197             stats = crossValidationResult.getEvalStatsCollection().aggStats()
198             stats[""modelName""] = model.getName()
199             statsList.append(stats)
200         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
201         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
202         log.info(strResults)
203         if resultWriter is not None:
204             resultWriter.writeTextFile(""model-comparison-results"", strResults)
205         return resultsDF
206 
207     # TODO: a parameter predictedVarName seemed to have been present at some point but has now disappeared
","Before: 511
After: 202, 207, 208",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,5259,"{'module': 1, 'expression_statement': 11, 'call': 11, 'attribute': 12, 'identifier': 53, '.': 12, 'argument_list': 11, '(': 12, ',': 6, 'keyword_argument': 3, '=': 10, ')': 12, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 5, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'block': 3, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'assignment': 6, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5931588309589259,0.5688311804613193,"(tensor([0.9296]), tensor([0.9430]), tensor([0.9363]), tensor([0.9417]))"
"577         self.inputOutputDataDict = inputOutputDataDict
578         self.keyName = keyName
579 
580     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
581             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
582             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
583         """"""
584         :param isRegression: flag indicating whether the models to evaluate are regression models
585         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
586         :param resultWriter: a writer with which to store results
587         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
588             dataset in a subdirectory named according to the name of the dataset
589         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
590         :param columnNameForModelRanking: column name to use for ranking models
591         :param rankMax: if true, use max for ranking, else min
592         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
593             for every dataset and meanDF contains one row for each model with results averaged across datasets
594         """"""
595         allResults = pd.DataFrame()
596         for key, inputOutputData in self.inputOutputDataDict.items():
597             _log.info(f""Evaluating models for {key}"")
598             ev = EvaluationUtil.forModelType(isRegression, inputOutputData, crossValidatorParams=crossValidatorParams)
599             models = [f() for f in modelFactories]
600             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
601             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
602             df[self.keyName] = key
603             df[""modelName""] = df.index
604             if columnNameForModelRanking is not None:
605                 if columnNameForModelRanking not in df.columns:
606                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
607                 df[""bestModel""] = 0
608                 if rankMax:
609                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
610                 else:
611                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
612             df = df.reset_index(drop=True)
613             allResults = pd.concat((allResults, df))
614         strAllResults = f""All results:\n{allResults.to_string()}""
615         _log.info(strAllResults)
616         meanResults = allResults.groupby(""modelName"").mean()
617         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
618         _log.info(strMeanResults)
619         if resultWriter is not None:
620             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
621         return allResults, meanResults
","269         self.inputOutputDataDict = inputOutputDataDict
270         self.keyName = keyName
271 
272     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
273             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
274             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
275         """"""
276         :param isRegression: flag indicating whether the models to evaluate are regression models
277         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
278         :param resultWriter: a writer with which to store results
279         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
280             dataset in a subdirectory named according to the name of the dataset
281         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
282         :param columnNameForModelRanking: column name to use for ranking models
283         :param rankMax: if true, use max for ranking, else min
284         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
285             for every dataset and meanDF contains one row for each model with results averaged across datasets
286         """"""
287         allResults = pd.DataFrame()
288         for key, inputOutputData in self.inputOutputDataDict.items():
289             log.info(f""Evaluating models for {key}"")
290             ev = EvaluationUtil.forModelType(isRegression, inputOutputData, crossValidatorParams=crossValidatorParams)
291             models = [f() for f in modelFactories]
292             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
293             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
294             df[self.keyName] = key
295             df[""modelName""] = df.index
296             if columnNameForModelRanking is not None:
297                 if columnNameForModelRanking not in df.columns:
298                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
299                 df[""bestModel""] = 0
300                 if rankMax:
301                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
302                 else:
303                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
304             df = df.reset_index(drop=True)
305             allResults = pd.concat((allResults, df))
306         strAllResults = f""All results:\n{allResults.to_string()}""
307         log.info(strAllResults)
308         meanResults = allResults.groupby(""modelName"").mean()
309         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
310         log.info(strMeanResults)
311         if resultWriter is not None:
312             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
313         return allResults, meanResults
","Before: 597
After: 289",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,6097,"{'module': 1, 'expression_statement': 22, 'assignment': 17, 'attribute': 28, 'identifier': 120, '.': 28, '=': 25, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 21, ',': 16, 'typed_parameter': 1, ':': 11, 'type': 14, 'generic_type': 6, 'type_parameter': 6, '[': 17, 'list': 1, ']': 17, 'typed_default_parameter': 3, 'none': 6, 'default_parameter': 2, 'true': 3, ')': 21, '->': 1, 'block': 7, 'string': 12, 'string_start': 12, 'string_content': 13, 'string_end': 12, 'call': 19, 'argument_list': 19, 'for_statement': 1, 'for': 2, 'pattern_list': 1, 'in': 2, 'interpolation': 5, '{': 5, '}': 5, 'keyword_argument': 3, 'list_comprehension': 1, 'for_in_clause': 1, 'conditional_expression': 1, 'if': 5, 'else': 2, 'subscript': 9, 'if_statement': 4, 'comparison_operator': 3, 'is not': 4, 'not in': 2, 'raise_statement': 1, 'raise': 1, 'integer': 3, 'else_clause': 1, 'tuple': 1, 'escape_sequence': 4, 'binary_operator': 2, '+': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7746599720796883,0.7628274647320972,"(tensor([0.9770]), tensor([0.9742]), tensor([0.9756]), tensor([0.9745]))"
"577         self.inputOutputDataDict = inputOutputDataDict
578         self.keyName = keyName
579 
580     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
581             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
582             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
583         """"""
584         :param isRegression: flag indicating whether the models to evaluate are regression models
585         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
586         :param resultWriter: a writer with which to store results
587         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
588             dataset in a subdirectory named according to the name of the dataset
589         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
590         :param columnNameForModelRanking: column name to use for ranking models
591         :param rankMax: if true, use max for ranking, else min
592         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
593             for every dataset and meanDF contains one row for each model with results averaged across datasets
594         """"""
595         allResults = pd.DataFrame()
596         for key, inputOutputData in self.inputOutputDataDict.items():
597             _log.info(f""Evaluating models for {key}"")
598             ev = EvaluationUtil.forModelType(isRegression, inputOutputData, crossValidatorParams=crossValidatorParams)
599             models = [f() for f in modelFactories]
600             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
601             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
602             df[self.keyName] = key
603             df[""modelName""] = df.index
604             if columnNameForModelRanking is not None:
605                 if columnNameForModelRanking not in df.columns:
606                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
607                 df[""bestModel""] = 0
608                 if rankMax:
609                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
610                 else:
611                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
612             df = df.reset_index(drop=True)
613             allResults = pd.concat((allResults, df))
614         strAllResults = f""All results:\n{allResults.to_string()}""
615         _log.info(strAllResults)
616         meanResults = allResults.groupby(""modelName"").mean()
617         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
618         _log.info(strMeanResults)
619         if resultWriter is not None:
620             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
621         return allResults, meanResults
","269         self.inputOutputDataDict = inputOutputDataDict
270         self.keyName = keyName
271 
272     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
273             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
274             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
275         """"""
276         :param isRegression: flag indicating whether the models to evaluate are regression models
277         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
278         :param resultWriter: a writer with which to store results
279         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
280             dataset in a subdirectory named according to the name of the dataset
281         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
282         :param columnNameForModelRanking: column name to use for ranking models
283         :param rankMax: if true, use max for ranking, else min
284         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
285             for every dataset and meanDF contains one row for each model with results averaged across datasets
286         """"""
287         allResults = pd.DataFrame()
288         for key, inputOutputData in self.inputOutputDataDict.items():
289             log.info(f""Evaluating models for {key}"")
290             ev = EvaluationUtil.forModelType(isRegression, inputOutputData, crossValidatorParams=crossValidatorParams)
291             models = [f() for f in modelFactories]
292             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
293             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
294             df[self.keyName] = key
295             df[""modelName""] = df.index
296             if columnNameForModelRanking is not None:
297                 if columnNameForModelRanking not in df.columns:
298                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
299                 df[""bestModel""] = 0
300                 if rankMax:
301                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
302                 else:
303                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
304             df = df.reset_index(drop=True)
305             allResults = pd.concat((allResults, df))
306         strAllResults = f""All results:\n{allResults.to_string()}""
307         log.info(strAllResults)
308         meanResults = allResults.groupby(""modelName"").mean()
309         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
310         log.info(strMeanResults)
311         if resultWriter is not None:
312             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
313         return allResults, meanResults
","Before: 615
After: 307",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,6386,"{'module': 1, 'expression_statement': 22, 'assignment': 17, 'attribute': 28, 'identifier': 120, '.': 28, '=': 25, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 21, ',': 16, 'typed_parameter': 1, ':': 11, 'type': 14, 'generic_type': 6, 'type_parameter': 6, '[': 17, 'list': 1, ']': 17, 'typed_default_parameter': 3, 'none': 6, 'default_parameter': 2, 'true': 3, ')': 21, '->': 1, 'block': 7, 'string': 12, 'string_start': 12, 'string_content': 13, 'string_end': 12, 'call': 19, 'argument_list': 19, 'for_statement': 1, 'for': 2, 'pattern_list': 1, 'in': 2, 'interpolation': 5, '{': 5, '}': 5, 'keyword_argument': 3, 'list_comprehension': 1, 'for_in_clause': 1, 'conditional_expression': 1, 'if': 5, 'else': 2, 'subscript': 9, 'if_statement': 4, 'comparison_operator': 3, 'is not': 4, 'not in': 2, 'raise_statement': 1, 'raise': 1, 'integer': 3, 'else_clause': 1, 'tuple': 1, 'escape_sequence': 4, 'binary_operator': 2, '+': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7746599720796883,0.7628274647320972,"(tensor([0.9770]), tensor([0.9742]), tensor([0.9756]), tensor([0.9745]))"
"577         self.inputOutputDataDict = inputOutputDataDict
578         self.keyName = keyName
579 
580     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
581             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
582             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
583         """"""
584         :param isRegression: flag indicating whether the models to evaluate are regression models
585         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
586         :param resultWriter: a writer with which to store results
587         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
588             dataset in a subdirectory named according to the name of the dataset
589         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
590         :param columnNameForModelRanking: column name to use for ranking models
591         :param rankMax: if true, use max for ranking, else min
592         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
593             for every dataset and meanDF contains one row for each model with results averaged across datasets
594         """"""
595         allResults = pd.DataFrame()
596         for key, inputOutputData in self.inputOutputDataDict.items():
597             _log.info(f""Evaluating models for {key}"")
598             ev = EvaluationUtil.forModelType(isRegression, inputOutputData, crossValidatorParams=crossValidatorParams)
599             models = [f() for f in modelFactories]
600             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
601             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
602             df[self.keyName] = key
603             df[""modelName""] = df.index
604             if columnNameForModelRanking is not None:
605                 if columnNameForModelRanking not in df.columns:
606                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
607                 df[""bestModel""] = 0
608                 if rankMax:
609                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
610                 else:
611                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
612             df = df.reset_index(drop=True)
613             allResults = pd.concat((allResults, df))
614         strAllResults = f""All results:\n{allResults.to_string()}""
615         _log.info(strAllResults)
616         meanResults = allResults.groupby(""modelName"").mean()
617         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
618         _log.info(strMeanResults)
619         if resultWriter is not None:
620             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
621         return allResults, meanResults
","269         self.inputOutputDataDict = inputOutputDataDict
270         self.keyName = keyName
271 
272     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
273             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
274             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
275         """"""
276         :param isRegression: flag indicating whether the models to evaluate are regression models
277         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
278         :param resultWriter: a writer with which to store results
279         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
280             dataset in a subdirectory named according to the name of the dataset
281         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
282         :param columnNameForModelRanking: column name to use for ranking models
283         :param rankMax: if true, use max for ranking, else min
284         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
285             for every dataset and meanDF contains one row for each model with results averaged across datasets
286         """"""
287         allResults = pd.DataFrame()
288         for key, inputOutputData in self.inputOutputDataDict.items():
289             log.info(f""Evaluating models for {key}"")
290             ev = EvaluationUtil.forModelType(isRegression, inputOutputData, crossValidatorParams=crossValidatorParams)
291             models = [f() for f in modelFactories]
292             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
293             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
294             df[self.keyName] = key
295             df[""modelName""] = df.index
296             if columnNameForModelRanking is not None:
297                 if columnNameForModelRanking not in df.columns:
298                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
299                 df[""bestModel""] = 0
300                 if rankMax:
301                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
302                 else:
303                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
304             df = df.reset_index(drop=True)
305             allResults = pd.concat((allResults, df))
306         strAllResults = f""All results:\n{allResults.to_string()}""
307         log.info(strAllResults)
308         meanResults = allResults.groupby(""modelName"").mean()
309         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
310         log.info(strMeanResults)
311         if resultWriter is not None:
312             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
313         return allResults, meanResults
","Before: 618
After: 310",fix imports in src/sensai/evaluation/eval_util.py,Refactoring: moved evaluation related modules to separate package,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ec9e1e4a397ad7e0493c22c80aa84e460719578c,3999a1393974b60e792061e14808315282e1a20a,0,6439,"{'module': 1, 'expression_statement': 22, 'assignment': 17, 'attribute': 28, 'identifier': 120, '.': 28, '=': 25, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 21, ',': 16, 'typed_parameter': 1, ':': 11, 'type': 14, 'generic_type': 6, 'type_parameter': 6, '[': 17, 'list': 1, ']': 17, 'typed_default_parameter': 3, 'none': 6, 'default_parameter': 2, 'true': 3, ')': 21, '->': 1, 'block': 7, 'string': 12, 'string_start': 12, 'string_content': 13, 'string_end': 12, 'call': 19, 'argument_list': 19, 'for_statement': 1, 'for': 2, 'pattern_list': 1, 'in': 2, 'interpolation': 5, '{': 5, '}': 5, 'keyword_argument': 3, 'list_comprehension': 1, 'for_in_clause': 1, 'conditional_expression': 1, 'if': 5, 'else': 2, 'subscript': 9, 'if_statement': 4, 'comparison_operator': 3, 'is not': 4, 'not in': 2, 'raise_statement': 1, 'raise': 1, 'integer': 3, 'else_clause': 1, 'tuple': 1, 'escape_sequence': 4, 'binary_operator': 2, '+': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , statsDict : Dict [ str , TEvalStats ] , inputData : pd . DataFrame , model : PredictorModel )', 'start_line': 32, 'end_line': 41, 'full_parameters': ['self', ' statsDict : Dict [ str', ' TEvalStats ]', ' inputData : pd . DataFrame', ' model : PredictorModel'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 12, 'token_count': 97, 'name': 'computeEvaluationMetricsDict', 'long_name': 'computeEvaluationMetricsDict( model , evaluatorOrValidator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 31, 'end_line': 42, 'full_parameters': ['model', ' evaluatorOrValidator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7746599720796883,0.7628274647320972,"(tensor([0.9770]), tensor([0.9742]), tensor([0.9756]), tensor([0.9745]))"
"71 # TODO: make some kwargs to args, infer y_predicted when classProbabilities are passed. We can also infer labels
72 #  (though not reliably) if they are not passed
73 class ClassificationEvalStats(PredictionEvalStats[""ClassificationMetric""]):
74     def __init__(self, y_predicted: PredictionArray = None,
75                  y_true: PredictionArray = None,
76                  y_predictedClassProbabilities: pd.DataFrame = None,
77                  labels: PredictionArray = None,
78                  metrics: Sequence[""ClassificationMetric""] = None,
79                  additionalMetrics: Sequence[""ClassificationMetric""] = None):
80         """"""
81         :param y_predicted: the predicted class labels
82         :param y_true: the true class labels
83         :param y_predictedClassProbabilities: a data frame whose columns are the class labels and whose values are probabilities
84         :param labels: the list of class labels
85         :param metrics: the metrics to compute for evaluation; if None, use default metrics
86         :param additionalMetrics: the metrics to additionally compute
87         """"""
88         self.labels = labels
89         self.y_predictedClassProbabilities = y_predictedClassProbabilities
90         self._probabilitiesAvailable = y_predictedClassProbabilities is not None
91         if self._probabilitiesAvailable:
92             colSet = set(y_predictedClassProbabilities.columns)
93             if colSet != set(labels):
94                 raise ValueError(f""Set of columns in class probabilities data frame ({colSet}) does not correspond to labels ({labels}"")
95             if len(y_predictedClassProbabilities) != len(y_true):
96                 raise ValueError(""Row count in class probabilities data frame does not match ground truth"")
97 
98         if metrics is None:
99             metrics = [ClassificationMetricAccuracy(), ClassificationMetricGeometricMeanOfTrueClassProbability()]
100         metrics = list(metrics)
101         if additionalMetrics is not None:
102             for m in additionalMetrics:
103                 if not self._probabilitiesAvailable and m.requiresProbabilities:
104                     raise ValueError(f""Additional metric {m} not supported, as class probabilities were not provided"")
105             metrics.extend(additionalMetrics)
106 
107         super().__init__(y_predicted=y_predicted, y_true=y_true, metrics=metrics)
108 
109     def getConfusionMatrix(self) -> ""ConfusionMatrix"":
","71 # TODO: make some kwargs to args, infer y_predicted when classProbabilities are passed. We can also infer labels
72 #  (though not reliably) if they are not passed
73 class ClassificationEvalStats(PredictionEvalStats[""ClassificationMetric""]):
74     def __init__(self, y_predicted: PredictionArray = None,
75                  y_true: PredictionArray = None,
76                  y_predictedClassProbabilities: pd.DataFrame = None,
77                  labels: PredictionArray = None,
78                  metrics: Sequence[""ClassificationMetric""] = None,
79                  additionalMetrics: Sequence[""ClassificationMetric""] = None):
80         """"""
81         :param y_predicted: the predicted class labels
82         :param y_true: the true class labels
83         :param y_predictedClassProbabilities: a data frame whose columns are the class labels and whose values are probabilities
84         :param labels: the list of class labels
85         :param metrics: the metrics to compute for evaluation; if None, use default metrics
86         :param additionalMetrics: the metrics to additionally compute
87         """"""
88         self.labels = labels
89         self.y_predictedClassProbabilities = y_predictedClassProbabilities
90         self._probabilitiesAvailable = y_predictedClassProbabilities is not None
91         if self._probabilitiesAvailable:
92             colSet = set(y_predictedClassProbabilities.columns)
93             if colSet != set(labels):
94                 raise ValueError(f""Set of columns in class probabilities data frame ({colSet}) does not correspond to labels ({labels}"")
95             if len(y_predictedClassProbabilities) != len(y_true):
96                 raise ValueError(""Row count in class probabilities data frame does not match ground truth"")
97 
98         if metrics is None:
99             metrics = [ClassificationMetricAccuracy(), ClassificationMetricGeometricMeanOfTrueClassProbability()]
100         metrics = list(metrics)
101         if additionalMetrics is not None:
102             for m in additionalMetrics:
103                 if not self._probabilitiesAvailable and m.requiresProbabilities:
104                     raise ValueError(f""Additional metric {m} not supported, as class probabilities were not provided"")
105             metrics.extend(additionalMetrics)
106 
107         super().__init__(y_predicted, y_true, metrics)
108 
109     def getConfusionMatrix(self) -> ""ConfusionMatrix"":
","Before: 107
After: 107",fix typo in eval_stats_classification.py,"Changes in eval_stats, mostly aesthetic",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,51be62aea9ba478607a0233188162c843437ba9c,ec9e1e4a397ad7e0493c22c80aa84e460719578c,0,1056,"{'module': 1, 'comment': 2, 'class_definition': 1, 'class': 1, 'identifier': 70, 'argument_list': 14, '(': 15, 'subscript': 1, '[': 4, 'string': 7, 'string_start': 7, 'string_content': 9, 'string_end': 7, ']': 4, ')': 15, ':': 15, 'block': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_default_parameter': 6, 'type': 8, '=': 15, 'none': 9, 'attribute': 10, '.': 10, 'generic_type': 2, 'type_parameter': 2, 'expression_statement': 9, 'assignment': 6, 'comparison_operator': 5, 'is not': 4, 'if_statement': 6, 'if': 6, 'call': 13, '!=': 2, 'raise_statement': 3, 'raise': 3, 'interpolation': 3, '{': 3, '}': 3, 'is': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1, 'keyword_argument': 3}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9842876185866716,0.9841497705564026,"(tensor([0.9980]), tensor([0.9969]), tensor([0.9974]), tensor([0.9970]))"
"1 import logging
2 from abc import abstractmethod, ABC
3 from typing import Union, List, Sequence
4 
5 import numpy as np
6 import pandas as pd
7 import seaborn as sns
","1 import logging
2 from abc import abstractmethod, ABC
3 from typing import List, Sequence
4 
5 import numpy as np
6 import seaborn as sns
7 from matplotlib import pyplot as plt
","Before: 3
After: 3",fix eval_stats_regression.py for python 3,"Changes in eval_stats, mostly aesthetic",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,51be62aea9ba478607a0233188162c843437ba9c,ec9e1e4a397ad7e0493c22c80aa84e460719578c,0,29,"{'module': 1, 'import_statement': 3, 'import': 5, 'dotted_name': 10, 'identifier': 12, 'import_from_statement': 2, 'from': 2, ',': 3, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7370051090471165,0.7184881055393693,"(tensor([0.9415]), tensor([0.9630]), tensor([0.9521]), tensor([0.9608]))"
"1 import logging
2 from abc import abstractmethod, ABC
3 from typing import Union, List, Sequence
4 
5 import numpy as np
6 import pandas as pd
7 import seaborn as sns
8 from matplotlib import pyplot as plt
9 from matplotlib.colors import LinearSegmentedColormap
10 
","5 import numpy as np
6 import seaborn as sns
7 from matplotlib import pyplot as plt
8 from matplotlib.colors import LinearSegmentedColormap
9 
10 from .eval_stats_base import PredictionEvalStats, Metric, EvalStatsCollection, PredictionArray
11 
12 log = logging.getLogger(__name__)
13 
14 
","Before: 6, 11
After: 10",fix eval_stats_regression.py for python 3,"Changes in eval_stats, mostly aesthetic",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,51be62aea9ba478607a0233188162c843437ba9c,ec9e1e4a397ad7e0493c22c80aa84e460719578c,0,43,"{'module': 1, 'import_statement': 4, 'import': 8, 'dotted_name': 15, 'identifier': 20, 'import_from_statement': 4, 'from': 4, ',': 3, 'aliased_import': 4, 'as': 4, '.': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.337865777995539,0.29343755235532976,"(tensor([0.8418]), tensor([0.8623]), tensor([0.8519]), tensor([0.8602]))"
"110     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
111         return np.median(cls.computeAbsErrors(y_true, y_predicted))
112 
113 
114 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
115     """"""Collects data for the evaluation of a model and computes corresponding metrics""""""
116 
117     def __init__(self, y_predicted: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
118             y_true: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
119             metrics: Sequence[""RegressionMetric""] = None,
","109     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
110         return np.median(cls.computeAbsErrors(y_true, y_predicted))
111 
112 
113 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
114     """"""
115     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
116 
117     :param y_predicted: the predicted values
118     :param y_true: the true values
","Before: 115
After: 114, 115",fix eval_stats_regression.py for python 3,"Changes in eval_stats, mostly aesthetic",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,51be62aea9ba478607a0233188162c843437ba9c,ec9e1e4a397ad7e0493c22c80aa84e460719578c,0,1130,"{'module': 1, 'function_definition': 1, 'def': 2, 'identifier': 36, 'parameters': 1, '(': 5, ',': 12, 'typed_parameter': 2, ':': 6, 'type': 12, 'attribute': 10, '.': 10, ')': 4, 'block': 2, 'return_statement': 1, 'return': 1, 'call': 2, 'argument_list': 3, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 3, 'expression_statement': 1, 'ERROR': 1, 'typed_default_parameter': 2, 'generic_type': 2, 'type_parameter': 2, '=': 2, 'none': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3622647431993734,0.349270136297981,"(tensor([0.9399]), tensor([0.8846]), tensor([0.9114]), tensor([0.8898]))"
"114 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
115     """"""Collects data for the evaluation of a model and computes corresponding metrics""""""
116 
117     def __init__(self, y_predicted: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
118             y_true: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
119             metrics: Sequence[""RegressionMetric""] = None,
120             additionalMetrics: Sequence[""RegressionMetric""] = None):
121 
122         if metrics is None:
123             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
124                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
125                        RegressionMetricStdDevAE()]
126         metrics = list(metrics)
127         if additionalMetrics is not None:
128             metrics.extend(additionalMetrics)
129 
130         super().__init__(y_predicted=y_predicted, y_true=y_true, metrics=metrics)
131 
132     def getMSE(self):
","112 
113 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
114     """"""
115     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
116 
117     :param y_predicted: the predicted values
118     :param y_true: the true values
119     :param metrics: the metrics to compute for evaluation; if None, use default metrics
120     :param additionalMetrics: the metrics to additionally compute
121     """"""
","Before: 117, 118, 119, 120
After: 117, 118, 119, 120, 121, 122, 123",fix eval_stats_regression.py for python 3,"Changes in eval_stats, mostly aesthetic",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,51be62aea9ba478607a0233188162c843437ba9c,ec9e1e4a397ad7e0493c22c80aa84e460719578c,0,1168,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 50, 'argument_list': 12, '(': 13, 'subscript': 1, '[': 6, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ']': 6, ')': 13, ':': 8, 'block': 4, 'expression_statement': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 18, 'typed_default_parameter': 4, 'type': 14, 'generic_type': 4, 'type_parameter': 4, 'attribute': 8, '.': 8, '=': 9, 'none': 6, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'assignment': 2, 'list': 1, 'call': 11, 'is not': 2, 'keyword_argument': 3}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.08895798084930093,0.0825206833634826,"(tensor([0.8476]), tensor([0.7349]), tensor([0.7872]), tensor([0.7448]))"
"114 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
115     """"""Collects data for the evaluation of a model and computes corresponding metrics""""""
116 
117     def __init__(self, y_predicted: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
118             y_true: Union[list, pd.Series, pd.DataFrame, np.ndarray] = None,
119             metrics: Sequence[""RegressionMetric""] = None,
120             additionalMetrics: Sequence[""RegressionMetric""] = None):
121 
122         if metrics is None:
123             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
124                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
125                        RegressionMetricStdDevAE()]
126         metrics = list(metrics)
127         if additionalMetrics is not None:
128             metrics.extend(additionalMetrics)
129 
130         super().__init__(y_predicted=y_predicted, y_true=y_true, metrics=metrics)
131 
132     def getMSE(self):
","119     :param metrics: the metrics to compute for evaluation; if None, use default metrics
120     :param additionalMetrics: the metrics to additionally compute
121     """"""
122     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
123             metrics: Sequence[""RegressionMetric""] = None, additionalMetrics: Sequence[""RegressionMetric""] = None):
124 
125         if metrics is None:
126             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
127                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
128                        RegressionMetricStdDevAE()]
129         metrics = list(metrics)
130         if additionalMetrics is not None:
131             metrics.extend(additionalMetrics)
132 
133         super().__init__(y_predicted, y_true, metrics)
134 
135     def getMSE(self):
","Before: 130
After: 133",fix eval_stats_regression.py for python 3,"Changes in eval_stats, mostly aesthetic",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,51be62aea9ba478607a0233188162c843437ba9c,ec9e1e4a397ad7e0493c22c80aa84e460719578c,0,1349,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 50, 'argument_list': 12, '(': 13, 'subscript': 1, '[': 6, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ']': 6, ')': 13, ':': 8, 'block': 4, 'expression_statement': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 18, 'typed_default_parameter': 4, 'type': 14, 'generic_type': 4, 'type_parameter': 4, 'attribute': 8, '.': 8, '=': 9, 'none': 6, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'assignment': 2, 'list': 1, 'call': 11, 'is not': 2, 'keyword_argument': 3}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.46383396830397144,0.43997486764221455,"(tensor([0.9138]), tensor([0.8736]), tensor([0.8933]), tensor([0.8774]))"
"9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictsProvider
15 from ..data_ingest import InputOutputData
16 from ..util.typing import PandasNamedTuple
17 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel, PredictorModel
18 
","9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictProvider
15 from ..data_ingest import InputOutputData
16 from ..util.typing import PandasNamedTuple
17 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel, PredictorModel
18 
","Before: 14
After: 14",fix typos in src/src/sensai/evaluation/crossval.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,125,"{'module': 1, 'import_from_statement': 7, 'from': 7, 'relative_import': 7, 'import_prefix': 7, '.': 14, 'dotted_name': 25, 'identifier': 29, 'import': 7, ',': 11, 'line_continuation': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9601328622880726,0.9555873481949169,"(tensor([0.9988]), tensor([0.9988]), tensor([0.9988]), tensor([0.9988]))"
"62 
63 
64 TCrossValData = TypeVar(""TCrossValData"", bound=PredictorModelCrossValidationData)
65 
66 
67 class VectorModelCrossValidator(MetricsDictsProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None):
69         """"""
70         :param data: the data set
71         :param folds: the number of folds
","62 
63 
64 TCrossValData = TypeVar(""TCrossValData"", bound=PredictorModelCrossValidationData)
65 
66 
67 class VectorModelCrossValidator(MetricsDictProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None):
69         """"""
70         :param data: the data set
71         :param folds: the number of folds
","Before: 67
After: 67",fix typos in src/src/sensai/evaluation/crossval.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,697,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 24, '=': 6, 'call': 1, 'argument_list': 2, '(': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ',': 8, 'keyword_argument': 1, ')': 3, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, ']': 1, ':': 7, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 4, 'typed_default_parameter': 2, 'integer': 2, 'default_parameter': 2, 'false': 1, 'none': 1, 'ERROR': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9665395661812441,0.963083881384112,"(tensor([0.9975]), tensor([0.9975]), tensor([0.9975]), tensor([0.9975]))"
"110 
111 class PredictionEvalStats(EvalStats[TMetric], ABC):
112     """"""
113     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
114     and computes corresponding metrics
115 
116     :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
117         one column per dimension should be passed
118     :param y_true: sequence of ground truth labels of same shape as y_predicted
119     :param metrics: list of metrics to be computed on the provided data
","113     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
114     and computes corresponding metrics
115     """"""
116     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray, metrics: List[TMetric]):
117         """"""
118         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
119             one column per dimension should be passed
120         :param y_true: sequence of ground truth labels of same shape as y_predicted
121         :param metrics: list of metrics to be computed on the provided data
122         """"""
123         self.y_true = []
124         self.y_predicted = []
125         self.y_true_multidim = None
126         self.y_predicted_multidim = None
127         if y_predicted is not None:
128             self._addAll(y_predicted, y_true)
129         super().__init__(metrics)
130 
131     def _add(self, y_predicted, y_true):
","Before: 115, 116, 117, 118, 119
After: 117, 118, 119, 120, 121, 122",fix typo in eval_stats_base.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,1148,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 51, 'argument_list': 2, '(': 2, 'subscript': 1, '[': 1, ']': 1, ',': 2, ')': 2, ':': 5, 'ERROR': 7, 'string_start': 1, 'for': 1, 'call': 1, 'binary_operator': 2, '-': 2, 'and': 1, 'attribute': 1, '.': 1, 'block': 2, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'expression_statement': 1, 'assignment': 1, 'type': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.44254309420747945,0.44332222238940855,"(tensor([0.8419]), tensor([0.9202]), tensor([0.8793]), tensor([0.9117]))"
"111 
112 
113 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
114     """"""
115     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
116 
117     :param y_predicted: the predicted values
118     :param y_true: the true values
119     :param metrics: the metrics to compute for evaluation; if None, use default metrics
120     :param additionalMetrics: the metrics to additionally compute
","114     """"""
115     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
116     """"""
117     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
118             metrics: Sequence[""RegressionMetric""] = None, additionalMetrics: Sequence[""RegressionMetric""] = None):
119         """"""
120         :param y_predicted: the predicted values
121         :param y_true: the true values
122         :param metrics: the metrics to compute for evaluation; if None, use default metrics
123         :param additionalMetrics: the metrics to additionally compute
124         """"""
125 
126         if metrics is None:
127             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
128                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
129                        RegressionMetricStdDevAE()]
130         metrics = list(metrics)
131         if additionalMetrics is not None:
132             metrics.extend(additionalMetrics)
133 
134         super().__init__(y_predicted, y_true, metrics)
135 
136     def getMSE(self):
","Before: 116, 117, 118, 119, 120
After: 119, 120, 121, 122, 123, 124",fix typo in eval_stats/eval_stats_regression.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,1134,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 34, 'argument_list': 1, '(': 1, 'subscript': 1, '[': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ']': 1, ')': 1, ':': 7, 'ERROR': 3, 'for': 2, ';': 1, 'if': 1, 'none': 1, ',': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.29644863066206606,0.297635125022734,"(tensor([0.7764]), tensor([0.8895]), tensor([0.8292]), tensor([0.8768]))"
"17 TModel = TypeVar(""TModel"", bound=VectorModel)
18 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
19 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
20 
21 
22 class MetricsDictsProvider(ABC):
23     @abstractmethod
24     def computeMetrics(self, model, **kwargs) -> Dict[str, float]:
25         pass
26 
","17 TModel = TypeVar(""TModel"", bound=VectorModel)
18 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
19 TEvalStatsCollection = TypeVar(""TEvalStatsCollection"", bound=EvalStatsCollection)
20 
21 
22 class MetricsDictProvider(ABC):
23     @abstractmethod
24     def computeMetrics(self, model, **kwargs) -> Dict[str, float]:
25         pass
26 
","Before: 22
After: 22",fix typos in src/sensai/evaluation/evaluator.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,223,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'identifier': 22, '=': 6, 'call': 3, 'argument_list': 4, '(': 5, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, ',': 6, 'keyword_argument': 3, ')': 5, 'class_definition': 1, 'class': 1, ':': 2, 'block': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': 'computeMetrics', 'long_name': 'computeMetrics( self , model , ** kwargs )', 'start_line': 24, 'end_line': 25, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': 'computeMetrics', 'long_name': 'computeMetrics( self , model , ** kwargs )', 'start_line': 24, 'end_line': 25, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9625248317849852,0.9589875653798859,"(tensor([0.9977]), tensor([0.9977]), tensor([0.9977]), tensor([0.9977]))"
"72 class VectorRegressionModelEvaluationData(PredictorModelEvaluationData[RegressionEvalStats]):
73     def getEvalStatsCollection(self):
74         return RegressionEvalStatsCollection(list(self.evalStatsByVarName.values()))
75 
76 
77 class VectorModelEvaluator(MetricsDictsProvider, ABC):
78     def __init__(self, data: InputOutputData, testData: InputOutputData = None, dataSplitter: DataSplitter = None,
79             testFraction=None, randomSeed=42, shuffle=True):
80         """"""
81         Constructs an evaluator with test and training data.
","72 class VectorRegressionModelEvaluationData(PredictorModelEvaluationData[RegressionEvalStats]):
73     def getEvalStatsCollection(self):
74         return RegressionEvalStatsCollection(list(self.evalStatsByVarName.values()))
75 
76 
77 class VectorModelEvaluator(MetricsDictProvider, ABC):
78     def __init__(self, data: InputOutputData, testData: InputOutputData = None, dataSplitter: DataSplitter = None,
79             testFraction=None, randomSeed=42, shuffle=True):
80         """"""
81         Constructs an evaluator with test and training data.
","Before: 77
After: 77",fix typos in src/sensai/evaluation/evaluator.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,799,"{'module': 1, 'class_definition': 2, 'class': 2, 'identifier': 24, 'argument_list': 5, '(': 7, 'subscript': 1, '[': 1, ']': 1, ')': 7, ':': 7, 'block': 4, 'function_definition': 2, 'def': 2, 'parameters': 2, 'return_statement': 1, 'return': 1, 'call': 3, 'attribute': 2, '.': 2, ',': 7, 'typed_parameter': 1, 'type': 3, 'typed_default_parameter': 2, '=': 5, 'none': 3, 'default_parameter': 3, 'integer': 1, 'true': 1, 'ERROR': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': 'computeMetrics', 'long_name': 'computeMetrics( self , model , ** kwargs )', 'start_line': 24, 'end_line': 25, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': 'computeMetrics', 'long_name': 'computeMetrics( self , model , ** kwargs )', 'start_line': 24, 'end_line': 25, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9690278768043126,0.9662134647723792,"(tensor([0.9986]), tensor([0.9986]), tensor([0.9986]), tensor([0.9986]))"
"6 from random import Random
7 from typing import Dict, Sequence, Any, Callable, Generator, Union, Tuple, List, Optional, Hashable
8 
9 import pandas as pd
10 
11 from .evaluation.evaluator import MetricsDictsProvider
12 from .local_search import SACostValue, SACostValueNumeric, SAOperator, SAState, SimulatedAnnealing, \
13     SAProbabilitySchedule, SAProbabilityFunctionLinear
14 from .tracking.tracking_base import TrackedExperimentDataProvider, TrackedExperiment
15 from .vector_model import VectorModel
","6 from random import Random
7 from typing import Dict, Sequence, Any, Callable, Generator, Union, Tuple, List, Optional, Hashable
8 
9 import pandas as pd
10 
11 from .evaluation.evaluator import MetricsDictProvider
12 from .local_search import SACostValue, SACostValueNumeric, SAOperator, SAState, SimulatedAnnealing, \
13     SAProbabilitySchedule, SAProbabilityFunctionLinear
14 from .tracking.tracking_base import TrackedExperimentDataProvider, TrackedExperiment
15 from .vector_model import VectorModel
","Before: 11
After: 11",fix typos in src/sensai/hyperopt.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,93,"{'module': 1, 'import_from_statement': 5, 'from': 5, 'dotted_name': 27, 'identifier': 30, 'import': 6, ',': 16, 'import_statement': 1, 'aliased_import': 1, 'as': 1, 'relative_import': 3, 'import_prefix': 3, '.': 5, 'line_continuation': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9636162552773174,0.960056766465623,"(tensor([0.9980]), tensor([0.9980]), tensor([0.9980]), tensor([0.9980]))"
"209         self._trackedExperiment = None
210 
211     @classmethod
212     def _evalParams(cls, modelFactory, metricsEvaluator: MetricsDictsProvider, skipDecider: ParameterCombinationSkipDecider, **params) -> Optional[Dict[str, Any]]:
213         if skipDecider is not None:
214             if skipDecider.isSkipped(params):
215                 cls.log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
216                 return None
217         cls.log.info(f""Evaluating {params}"")
218         model = modelFactory(**params)
219         # TODO or not TODO: for some evaluators additional kwargs can be passed, e.g. onTrainingData
220         values = metricsEvaluator.computeMetrics(model)
221         values[""str(model)""] = str(model)
222         values.update(**params)
223         if skipDecider is not None:
224             skipDecider.tell(params, values)
225         return values
226 
227     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","209         self._trackedExperiment = None
210 
211     @classmethod
212     def _evalParams(cls, modelFactory, metricsEvaluator: MetricsDictProvider, skipDecider: ParameterCombinationSkipDecider, **params) -> Optional[Dict[str, Any]]:
213         if skipDecider is not None:
214             if skipDecider.isSkipped(params):
215                 cls.log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
216                 return None
217         cls.log.info(f""Evaluating {params}"")
218         model = modelFactory(**params)
219         # TODO or not TODO: for some evaluators additional kwargs can be passed, e.g. onTrainingData
220         values = metricsEvaluator.computeMetrics(model)
221         values[""str(model)""] = str(model)
222         values.update(**params)
223         if skipDecider is not None:
224             skipDecider.tell(params, values)
225         return values
226 
227     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","Before: 212
After: 212",fix typos in src/sensai/hyperopt.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,1491,"{'module': 1, 'expression_statement': 8, 'assignment': 4, 'attribute': 9, 'identifier': 47, '.': 9, '=': 4, 'none': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 6, 'typed_parameter': 2, ':': 6, 'type': 6, 'dictionary_splat_pattern': 1, '**': 3, ')': 9, '->': 1, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 4, 'if_statement': 3, 'if': 3, 'comparison_operator': 2, 'is not': 4, 'call': 8, 'argument_list': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'interpolation': 3, '{': 3, '}': 3, 'string_end': 3, 'return_statement': 2, 'return': 2, 'dictionary_splat': 2, 'comment': 1, 'subscript': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9854426852008968,0.9844827883163029,"(tensor([0.9988]), tensor([0.9988]), tensor([0.9988]), tensor([0.9988]))"
"227     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
228         self._trackedExperiment = trackedExperiment
229 
230     def run(self, metricsEvaluator: MetricsDictsProvider, sortColumnName=None) -> pd.DataFrame:
231         """"""
232         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
233         to that file directly after being computed
234 
235         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
236         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
237             Note that the column names that are generated depend on the evaluator/validator being applied.
238         :return: the data frame with all evaluation results
239         """"""
240         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
241         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
242 
243         def collectResult(values):
244             if values is None:
245                 return
246             if loggingCallback is not None:
247                 loggingCallback(values)
248             paramsMetricsCollection.addValues(values)
249             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
250 
251         if self.numProcesses == 1:
252             for parameterOptions in self.parameterOptionsList:
253                 for paramsDict in iterParamCombinations(parameterOptions):
254                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, **paramsDict))
255         else:
256             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
257             futures = []
258             for parameterOptions in self.parameterOptionsList:
259                 for paramsDict in iterParamCombinations(parameterOptions):
260                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
261                                                    **paramsDict))
262             for future in futures:
263                 collectResult(future.result())
264 
265         return paramsMetricsCollection.getDataFrame()
266 
267 
","227     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
228         self._trackedExperiment = trackedExperiment
229 
230     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None) -> pd.DataFrame:
231         """"""
232         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
233         to that file directly after being computed
234 
235         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
236         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
237             Note that the column names that are generated depend on the evaluator/validator being applied.
238         :return: the data frame with all evaluation results
239         """"""
240         loggingCallback = self._trackedExperiment.trackValues if self._trackedExperiment is not None else None
241         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName)
242 
243         def collectResult(values):
244             if values is None:
245                 return
246             if loggingCallback is not None:
247                 loggingCallback(values)
248             paramsMetricsCollection.addValues(values)
249             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
250 
251         if self.numProcesses == 1:
252             for parameterOptions in self.parameterOptionsList:
253                 for paramsDict in iterParamCombinations(parameterOptions):
254                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, **paramsDict))
255         else:
256             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
257             futures = []
258             for parameterOptions in self.parameterOptionsList:
259                 for paramsDict in iterParamCombinations(parameterOptions):
260                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
261                                                    **paramsDict))
262             for future in futures:
263                 collectResult(future.result())
264 
265         return paramsMetricsCollection.getDataFrame()
266 
267 
","Before: 230
After: 230",fix typos in src/sensai/hyperopt.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,1689,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 89, 'parameters': 3, '(': 19, ',': 11, 'typed_parameter': 2, ':': 14, 'type': 3, ')': 19, 'block': 12, 'expression_statement': 12, 'assignment': 5, 'attribute': 24, '.': 24, '=': 9, 'default_parameter': 1, 'none': 5, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'conditional_expression': 1, 'if': 4, 'comparison_operator': 4, 'is not': 4, 'else': 2, 'call': 16, 'argument_list': 16, 'keyword_argument': 3, 'if_statement': 3, 'is': 1, 'return_statement': 2, 'return': 2, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'dictionary_splat': 2, '**': 2, 'else_clause': 1, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9921071377785972,0.9917076761501523,"(tensor([0.9995]), tensor([0.9995]), tensor([0.9995]), tensor([0.9995]))"
"306         def _chooseChangedModelParameters(self) -> Dict[str, Any]:
307             pass
308 
309     def __init__(self, modelFactory: Callable[..., VectorModel],
310                  opsAndWeights: List[Tuple[Callable[['SAHyperOpt.State'], 'SAHyperOpt.ParameterChangeOperator'], float]],
311                  initialParameters: Dict[str, Any], metricsEvaluator: MetricsDictsProvider,
312                  metricToOptimise, minimiseMetric=False,
313                  collectDataFrame=True, csvResultsPath: Optional[str] = None,
314                  parameterCombinationEquivalenceClassValueCache: ParameterCombinationEquivalenceClassValueCache = None,
315                  p0=0.5, p1=0.0):
316         """"""
317         :param modelFactory: a factory for the generation of models which is called with the current parameter combination
318             (all keyword arguments), initially initialParameters
319         :param opsAndWeights: a sequence of tuples (operator factory, operator weight) for simulated annealing
320         :param initialParameters: the initial parameter combination
321         :param metricsEvaluator: the evaluator/validator to use in order to evaluate models
322         :param metricToOptimise: the name of the metric (as generated by the evaluator/validator) to optimise
323         :param minimiseMetric: whether the metric is to be minimised; if False, maximise the metric
324         :param collectDataFrame: whether to collect (and regularly log) the data frame of all parameter combinations and
325             evaluation results
326         :param csvResultsPath: the (optional) path of a CSV file in which to store a table of all computed results;
327             if this is not None, then collectDataFrame is automatically set to True
328         :param parameterCombinationEquivalenceClassValueCache: a cache in which to store computed results and whose notion
329             of equivalence can be used to avoid duplicate computations
330         :param p0: the initial probability (at the start of the optimisation) of accepting a state with an inferior evaluation
331             to the current state's (for the mean observed evaluation delta)
332         :param p1: the final probability (at the end of the optimisation) of accepting a state with an inferior evaluation
333             to the current state's (for the mean observed evaluation delta)
334         """"""
335         self.minimiseMetric = minimiseMetric
336         self.evaluatorOrValidator = metricsEvaluator
337         self.metricToOptimise = metricToOptimise
338         self.initialParameters = initialParameters
339         self.opsAndWeights = opsAndWeights
340         self.modelFactory = modelFactory
341         self.csvResultsPath = csvResultsPath
342         if csvResultsPath is not None:
343             collectDataFrame = True
344         self.parametersMetricsCollection = ParametersMetricsCollection(csvPath=csvResultsPath) if collectDataFrame else None
345         self.parameterCombinationEquivalenceClassValueCache = parameterCombinationEquivalenceClassValueCache
346         self.p0 = p0
347         self.p1 = p1
348         self._sa = None
349         self._trackedExperiment = None
350 
351     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","306         def _chooseChangedModelParameters(self) -> Dict[str, Any]:
307             pass
308 
309     def __init__(self, modelFactory: Callable[..., VectorModel],
310                  opsAndWeights: List[Tuple[Callable[['SAHyperOpt.State'], 'SAHyperOpt.ParameterChangeOperator'], float]],
311                  initialParameters: Dict[str, Any], metricsEvaluator: MetricsDictProvider,
312                  metricToOptimise, minimiseMetric=False,
313                  collectDataFrame=True, csvResultsPath: Optional[str] = None,
314                  parameterCombinationEquivalenceClassValueCache: ParameterCombinationEquivalenceClassValueCache = None,
315                  p0=0.5, p1=0.0):
316         """"""
317         :param modelFactory: a factory for the generation of models which is called with the current parameter combination
318             (all keyword arguments), initially initialParameters
319         :param opsAndWeights: a sequence of tuples (operator factory, operator weight) for simulated annealing
320         :param initialParameters: the initial parameter combination
321         :param metricsEvaluator: the evaluator/validator to use in order to evaluate models
322         :param metricToOptimise: the name of the metric (as generated by the evaluator/validator) to optimise
323         :param minimiseMetric: whether the metric is to be minimised; if False, maximise the metric
324         :param collectDataFrame: whether to collect (and regularly log) the data frame of all parameter combinations and
325             evaluation results
326         :param csvResultsPath: the (optional) path of a CSV file in which to store a table of all computed results;
327             if this is not None, then collectDataFrame is automatically set to True
328         :param parameterCombinationEquivalenceClassValueCache: a cache in which to store computed results and whose notion
329             of equivalence can be used to avoid duplicate computations
330         :param p0: the initial probability (at the start of the optimisation) of accepting a state with an inferior evaluation
331             to the current state's (for the mean observed evaluation delta)
332         :param p1: the final probability (at the end of the optimisation) of accepting a state with an inferior evaluation
333             to the current state's (for the mean observed evaluation delta)
334         """"""
335         self.minimiseMetric = minimiseMetric
336         self.evaluatorOrValidator = metricsEvaluator
337         self.metricToOptimise = metricToOptimise
338         self.initialParameters = initialParameters
339         self.opsAndWeights = opsAndWeights
340         self.modelFactory = modelFactory
341         self.csvResultsPath = csvResultsPath
342         if csvResultsPath is not None:
343             collectDataFrame = True
344         self.parametersMetricsCollection = ParametersMetricsCollection(csvPath=csvResultsPath) if collectDataFrame else None
345         self.parameterCombinationEquivalenceClassValueCache = parameterCombinationEquivalenceClassValueCache
346         self.p0 = p0
347         self.p1 = p1
348         self._sa = None
349         self._trackedExperiment = None
350 
351     def setTrackedExperiment(self, trackedExperiment: TrackedExperiment):
","Before: 311
After: 311",fix typos in src/sensai/hyperopt.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,2509,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 73, 'parameters': 2, '(': 3, ')': 3, '->': 1, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 8, ',': 16, ']': 8, ':': 9, 'block': 3, 'pass_statement': 1, 'pass': 1, 'typed_parameter': 4, 'ellipsis': 1, 'list': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'default_parameter': 4, '=': 21, 'false': 1, 'true': 2, 'typed_default_parameter': 2, 'none': 6, 'float': 2, 'expression_statement': 15, 'assignment': 14, 'attribute': 13, '.': 13, 'if_statement': 1, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'conditional_expression': 1, 'call': 1, 'argument_list': 1, 'keyword_argument': 1, 'else': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9945739983508803,0.9942896182303658,"(tensor([0.9996]), tensor([0.9996]), tensor([0.9996]), tensor([0.9996]))"
"352         self._trackedExperiment = trackedExperiment
353 
354     @classmethod
355     def _evalParams(cls, modelFactory, metricsEvaluator: MetricsDictsProvider, parametersMetricsCollection: Optional[ParametersMetricsCollection],
356                     parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
357         metrics = None
358         if parameterCombinationEquivalenceClassValueCache is not None:
359             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
360         if metrics is not None:
361             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
362             return metrics
363         else:
364             cls.log.info(f""Evaluating parameter combination {params}"")
365             model = modelFactory(**params)
366             metrics = metricsEvaluator.computeMetrics(model)
367             cls.log.info(f""Got metrics {metrics} for {params}"")
368 
369             values = dict(metrics)
370             values[""str(model)""] = str(model)
371             values.update(**params)
372             if trackedExperiment is not None:
373                 trackedExperiment.trackValues(values)
374             if parametersMetricsCollection is not None:
375                 parametersMetricsCollection.addValues(values)
376                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
377             if parameterCombinationEquivalenceClassValueCache is not None:
378                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
379         return metrics
380 
381     def _computeMetric(self, params):
","352         self._trackedExperiment = trackedExperiment
353 
354     @classmethod
355     def _evalParams(cls, modelFactory, metricsEvaluator: MetricsDictProvider, parametersMetricsCollection: Optional[ParametersMetricsCollection],
356                     parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
357         metrics = None
358         if parameterCombinationEquivalenceClassValueCache is not None:
359             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
360         if metrics is not None:
361             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
362             return metrics
363         else:
364             cls.log.info(f""Evaluating parameter combination {params}"")
365             model = modelFactory(**params)
366             metrics = metricsEvaluator.computeMetrics(model)
367             cls.log.info(f""Got metrics {metrics} for {params}"")
368 
369             values = dict(metrics)
370             values[""str(model)""] = str(model)
371             values.update(**params)
372             if trackedExperiment is not None:
373                 trackedExperiment.trackValues(values)
374             if parametersMetricsCollection is not None:
375                 parametersMetricsCollection.addValues(values)
376                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
377             if parameterCombinationEquivalenceClassValueCache is not None:
378                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
379         return metrics
380 
381     def _computeMetric(self, params):
","Before: 355
After: 355",fix typos in src/sensai/hyperopt.py,Fixed typos and improved docstrings,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,ca83cb8ffecb0a787b1875277ad53e66b997d14e,0,2744,"{'module': 1, 'expression_statement': 15, 'assignment': 7, 'attribute': 17, 'identifier': 72, '.': 17, '=': 7, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 16, ',': 7, 'typed_parameter': 2, ':': 9, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 16, 'block': 7, 'none': 6, 'if_statement': 5, 'if': 5, 'comparison_operator': 5, 'is not': 10, 'call': 15, 'argument_list': 15, 'string': 5, 'string_start': 5, 'string_content': 8, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 5, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9895283225469969,0.9887580893384103,"(tensor([0.9993]), tensor([0.9993]), tensor([0.9993]), tensor([0.9993]))"
"1 from abc import ABC, abstractmethod
2 from typing import Generic, TypeVar, List, Union, Dict, Sequence
3 
4 import numpy as np
5 import pandas as pd
","1 import numpy as np
2 import pandas as pd
3 import seaborn as sns
4 from abc import ABC, abstractmethod
5 from matplotlib import pyplot as plt
6 from typing import Generic, TypeVar, List, Union, Dict, Sequence
7 
8 from ...util.tracking import timed
","Before: 1, 2, 3
After: 4, 6, 8",add support for additional metrics in evalstats,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,12,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 11, 'identifier': 12, 'import': 3, ',': 6, 'import_statement': 1, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.45339258743216015,0.43368210497169574,"(tensor([0.8611]), tensor([0.9348]), tensor([0.8964]), tensor([0.9269]))"
"21 
22 
23 class EvalStats(Generic[TMetric]):
24     def __init__(self, metrics: List[TMetric]):
25         if len(metrics) == 0:
26             raise ValueError(""No metrics provided"")
27         self.metrics = metrics
28         self.name = None
29 
30     def setName(self, name):
","21 
22 
23 class EvalStats(Generic[TMetric]):
24     def __init__(self, metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
25         if len(metrics) == 0:
26             raise ValueError(""No metrics provided"")
27         self.metrics = metrics
28         # Implementations of EvalStats will typically provide default metrics, therefore we include
29         # the possibility for passing additional metrics here
30         if additionalMetrics is not None:
31             self.metrics.extend(additionalMetrics)
32         self.name = None
33 
34     def setName(self, name: str):
","Before: 24
After: 24, 28, 29, 30, 31",add support for additional metrics in evalstats,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,180,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 16, 'argument_list': 3, '(': 4, 'subscript': 1, '[': 2, ']': 2, ')': 4, ':': 4, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'call': 2, '==': 1, 'integer': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 2, '.': 2, '=': 2, 'none': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5016654345124908,0.4823555378852067,"(tensor([0.8436]), tensor([0.9677]), tensor([0.9014]), tensor([0.9536]))"
"27         self.metrics = metrics
28         self.name = None
29 
30     def setName(self, name):
31         self.name = name
32 
33     def addMetric(self, metric: TMetric):
","31             self.metrics.extend(additionalMetrics)
32         self.name = None
33 
34     def setName(self, name: str):
35         self.name = name
36 
37     def addMetric(self, metric: TMetric):
","Before: 30
After: 34",add support for additional metrics in evalstats,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,231,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'attribute': 3, 'identifier': 11, '.': 3, '=': 3, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, ')': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.46136915223605046,0.4208598069524091,"(tensor([0.9079]), tensor([0.9470]), tensor([0.9271]), tensor([0.9430]))"
"113     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
114     and computes corresponding metrics
115     """"""
116     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray, metrics: List[TMetric]):
117         """"""
118         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
119             one column per dimension should be passed
120         :param y_true: sequence of ground truth labels of same shape as y_predicted
121         :param metrics: list of metrics to be computed on the provided data
122         """"""
123         self.y_true = []
124         self.y_predicted = []
125         self.y_true_multidim = None
126         self.y_predicted_multidim = None
127         if y_predicted is not None:
128             self._addAll(y_predicted, y_true)
129         super().__init__(metrics)
130 
131     def _add(self, y_predicted, y_true):
","118     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
119     and computes corresponding metrics
120     """"""
121     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
122                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
123         """"""
124         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
125             one column per dimension should be passed
126         :param y_true: sequence of ground truth labels of same shape as y_predicted
127         :param metrics: list of metrics to be computed on the provided data
128         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
129         """"""
130         self.y_true = []
131         self.y_predicted = []
132         self.y_true_multidim = None
133         self.y_predicted_multidim = None
134         if y_predicted is not None:
135             self._addAll(y_predicted, y_true)
136         super().__init__(metrics, additionalMetrics=additionalMetrics)
137 
138     def _add(self, y_predicted, y_true):
","Before: 116
After: 121, 122, 128",add support for additional metrics in evalstats,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,1159,"{'module': 1, 'ERROR': 10, 'identifier': 59, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, 'binary_operator': 2, '-': 2, ')': 1, 'and': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 6, 'attribute': 1, '.': 1, ',': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'block': 1, 'expression_statement': 2, 'assignment': 2, 'type': 2, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5884993094015128,0.5806716774882992,"(tensor([0.9287]), tensor([0.9591]), tensor([0.9436]), tensor([0.9560]))"
"113     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
114     and computes corresponding metrics
115     """"""
116     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray, metrics: List[TMetric]):
117         """"""
118         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
119             one column per dimension should be passed
120         :param y_true: sequence of ground truth labels of same shape as y_predicted
121         :param metrics: list of metrics to be computed on the provided data
122         """"""
123         self.y_true = []
124         self.y_predicted = []
125         self.y_true_multidim = None
126         self.y_predicted_multidim = None
127         if y_predicted is not None:
128             self._addAll(y_predicted, y_true)
129         super().__init__(metrics)
130 
131     def _add(self, y_predicted, y_true):
","118     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
119     and computes corresponding metrics
120     """"""
121     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
122                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
123         """"""
124         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
125             one column per dimension should be passed
126         :param y_true: sequence of ground truth labels of same shape as y_predicted
127         :param metrics: list of metrics to be computed on the provided data
128         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
129         """"""
130         self.y_true = []
131         self.y_predicted = []
132         self.y_true_multidim = None
133         self.y_predicted_multidim = None
134         if y_predicted is not None:
135             self._addAll(y_predicted, y_true)
136         super().__init__(metrics, additionalMetrics=additionalMetrics)
137 
138     def _add(self, y_predicted, y_true):
","Before: 129
After: 136",add support for additional metrics in evalstats,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,1235,"{'module': 1, 'ERROR': 10, 'identifier': 59, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, 'binary_operator': 2, '-': 2, ')': 1, 'and': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 6, 'attribute': 1, '.': 1, ',': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'block': 1, 'expression_statement': 2, 'assignment': 2, 'type': 2, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] )', 'start_line': 24, 'end_line': 28, 'full_parameters': ['self', ' metrics : List [ TMetric ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5884993094015128,0.5806716774882992,"(tensor([0.9287]), tensor([0.9591]), tensor([0.9436]), tensor([0.9560]))"
"1 from abc import ABC, abstractmethod
2 from typing import List, Sequence
3 
4 import numpy as np
5 import pandas as pd
","1 import numpy as np
2 import pandas as pd
3 import sklearn
4 from abc import ABC, abstractmethod
5 from sklearn.metrics import confusion_matrix, accuracy_score
6 from typing import List, Sequence
7 
8 from .eval_stats_base import PredictionArray, PredictionEvalStats, EvalStatsCollection, Metric
","Before: 1, 2, 3
After: 4, 6",update eval_stats_classification.py for new api,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,12,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 7, 'identifier': 8, 'import': 3, ',': 2, 'import_statement': 1, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3146733304901473,0.29653106416175107,"(tensor([0.8152]), tensor([0.9283]), tensor([0.8681]), tensor([0.9156]))"
"71 # TODO: make some kwargs to args, infer y_predicted when classProbabilities are passed. We can also infer labels
72 #  (though not reliably) if they are not passed
73 class ClassificationEvalStats(PredictionEvalStats[""ClassificationMetric""]):
74     def __init__(self, y_predicted: PredictionArray = None,
75                  y_true: PredictionArray = None,
76                  y_predictedClassProbabilities: pd.DataFrame = None,
77                  labels: PredictionArray = None,
78                  metrics: Sequence[""ClassificationMetric""] = None,
79                  additionalMetrics: Sequence[""ClassificationMetric""] = None):
80         """"""
81         :param y_predicted: the predicted class labels
82         :param y_true: the true class labels
83         :param y_predictedClassProbabilities: a data frame whose columns are the class labels and whose values are probabilities
84         :param labels: the list of class labels
85         :param metrics: the metrics to compute for evaluation; if None, use default metrics
86         :param additionalMetrics: the metrics to additionally compute
87         """"""
88         self.labels = labels
89         self.y_predictedClassProbabilities = y_predictedClassProbabilities
90         self._probabilitiesAvailable = y_predictedClassProbabilities is not None
91         if self._probabilitiesAvailable:
92             colSet = set(y_predictedClassProbabilities.columns)
93             if colSet != set(labels):
94                 raise ValueError(f""Set of columns in class probabilities data frame ({colSet}) does not correspond to labels ({labels}"")
95             if len(y_predictedClassProbabilities) != len(y_true):
96                 raise ValueError(""Row count in class probabilities data frame does not match ground truth"")
97 
98         if metrics is None:
99             metrics = [ClassificationMetricAccuracy(), ClassificationMetricGeometricMeanOfTrueClassProbability()]
100         metrics = list(metrics)
101         if additionalMetrics is not None:
102             for m in additionalMetrics:
103                 if not self._probabilitiesAvailable and m.requiresProbabilities:
104                     raise ValueError(f""Additional metric {m} not supported, as class probabilities were not provided"")
105             metrics.extend(additionalMetrics)
106 
107         super().__init__(y_predicted, y_true, metrics)
108 
109     def getConfusionMatrix(self) -> ""ConfusionMatrix"":
","70 # TODO: make some kwargs to args, infer y_predicted when classProbabilities are passed. We can also infer labels
71 #  (though not reliably) if they are not passed
72 class ClassificationEvalStats(PredictionEvalStats[""ClassificationMetric""]):
73     def __init__(self, y_predicted: PredictionArray = None,
74                  y_true: PredictionArray = None,
75                  y_predictedClassProbabilities: pd.DataFrame = None,
76                  labels: PredictionArray = None,
77                  metrics: Sequence[""ClassificationMetric""] = None,
78                  additionalMetrics: Sequence[""ClassificationMetric""] = None):
79         """"""
80         :param y_predicted: the predicted class labels
81         :param y_true: the true class labels
82         :param y_predictedClassProbabilities: a data frame whose columns are the class labels and whose values are probabilities
83         :param labels: the list of class labels
84         :param metrics: the metrics to compute for evaluation; if None, use default metrics
85         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
86         """"""
87         self.labels = labels
88         self.y_predictedClassProbabilities = y_predictedClassProbabilities
89         self._probabilitiesAvailable = y_predictedClassProbabilities is not None
90         if self._probabilitiesAvailable:
91             colSet = set(y_predictedClassProbabilities.columns)
92             if colSet != set(labels):
93                 raise ValueError(f""Set of columns in class probabilities data frame ({colSet}) does not correspond to labels ({labels}"")
94             if len(y_predictedClassProbabilities) != len(y_true):
95                 raise ValueError(""Row count in class probabilities data frame does not match ground truth"")
96 
97         if metrics is None:
98             metrics = [ClassificationMetricAccuracy(), ClassificationMetricGeometricMeanOfTrueClassProbability()]
99         metrics = list(metrics)
100         if additionalMetrics is not None:
101             for m in additionalMetrics:
102                 if not self._probabilitiesAvailable and m.requiresProbabilities:
103                     raise ValueError(f""Additional metric {m} not supported, as class probabilities were not provided"")
104 
105         super().__init__(y_predicted, y_true, metrics, additionalMetrics=additionalMetrics)
106 
107     def getConfusionMatrix(self) -> ""ConfusionMatrix"":
","Before: 86
After: 85",update eval_stats_classification.py for new api,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,925,"{'module': 1, 'comment': 2, 'class_definition': 1, 'class': 1, 'identifier': 67, 'argument_list': 14, '(': 15, 'subscript': 1, '[': 4, 'string': 7, 'string_start': 7, 'string_content': 9, 'string_end': 7, ']': 4, ')': 15, ':': 15, 'block': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_default_parameter': 6, 'type': 8, '=': 12, 'none': 9, 'attribute': 10, '.': 10, 'generic_type': 2, 'type_parameter': 2, 'expression_statement': 9, 'assignment': 6, 'comparison_operator': 5, 'is not': 4, 'if_statement': 6, 'if': 6, 'call': 13, '!=': 2, 'raise_statement': 3, 'raise': 3, 'interpolation': 3, '{': 3, '}': 3, 'is': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7743626631289516,0.7728426635693921,"(tensor([0.9827]), tensor([0.9861]), tensor([0.9844]), tensor([0.9858]))"
"71 # TODO: make some kwargs to args, infer y_predicted when classProbabilities are passed. We can also infer labels
72 #  (though not reliably) if they are not passed
73 class ClassificationEvalStats(PredictionEvalStats[""ClassificationMetric""]):
74     def __init__(self, y_predicted: PredictionArray = None,
75                  y_true: PredictionArray = None,
76                  y_predictedClassProbabilities: pd.DataFrame = None,
77                  labels: PredictionArray = None,
78                  metrics: Sequence[""ClassificationMetric""] = None,
79                  additionalMetrics: Sequence[""ClassificationMetric""] = None):
80         """"""
81         :param y_predicted: the predicted class labels
82         :param y_true: the true class labels
83         :param y_predictedClassProbabilities: a data frame whose columns are the class labels and whose values are probabilities
84         :param labels: the list of class labels
85         :param metrics: the metrics to compute for evaluation; if None, use default metrics
86         :param additionalMetrics: the metrics to additionally compute
87         """"""
88         self.labels = labels
89         self.y_predictedClassProbabilities = y_predictedClassProbabilities
90         self._probabilitiesAvailable = y_predictedClassProbabilities is not None
91         if self._probabilitiesAvailable:
92             colSet = set(y_predictedClassProbabilities.columns)
93             if colSet != set(labels):
94                 raise ValueError(f""Set of columns in class probabilities data frame ({colSet}) does not correspond to labels ({labels}"")
95             if len(y_predictedClassProbabilities) != len(y_true):
96                 raise ValueError(""Row count in class probabilities data frame does not match ground truth"")
97 
98         if metrics is None:
99             metrics = [ClassificationMetricAccuracy(), ClassificationMetricGeometricMeanOfTrueClassProbability()]
100         metrics = list(metrics)
101         if additionalMetrics is not None:
102             for m in additionalMetrics:
103                 if not self._probabilitiesAvailable and m.requiresProbabilities:
104                     raise ValueError(f""Additional metric {m} not supported, as class probabilities were not provided"")
105             metrics.extend(additionalMetrics)
106 
107         super().__init__(y_predicted, y_true, metrics)
108 
109     def getConfusionMatrix(self) -> ""ConfusionMatrix"":
","70 # TODO: make some kwargs to args, infer y_predicted when classProbabilities are passed. We can also infer labels
71 #  (though not reliably) if they are not passed
72 class ClassificationEvalStats(PredictionEvalStats[""ClassificationMetric""]):
73     def __init__(self, y_predicted: PredictionArray = None,
74                  y_true: PredictionArray = None,
75                  y_predictedClassProbabilities: pd.DataFrame = None,
76                  labels: PredictionArray = None,
77                  metrics: Sequence[""ClassificationMetric""] = None,
78                  additionalMetrics: Sequence[""ClassificationMetric""] = None):
79         """"""
80         :param y_predicted: the predicted class labels
81         :param y_true: the true class labels
82         :param y_predictedClassProbabilities: a data frame whose columns are the class labels and whose values are probabilities
83         :param labels: the list of class labels
84         :param metrics: the metrics to compute for evaluation; if None, use default metrics
85         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
86         """"""
87         self.labels = labels
88         self.y_predictedClassProbabilities = y_predictedClassProbabilities
89         self._probabilitiesAvailable = y_predictedClassProbabilities is not None
90         if self._probabilitiesAvailable:
91             colSet = set(y_predictedClassProbabilities.columns)
92             if colSet != set(labels):
93                 raise ValueError(f""Set of columns in class probabilities data frame ({colSet}) does not correspond to labels ({labels}"")
94             if len(y_predictedClassProbabilities) != len(y_true):
95                 raise ValueError(""Row count in class probabilities data frame does not match ground truth"")
96 
97         if metrics is None:
98             metrics = [ClassificationMetricAccuracy(), ClassificationMetricGeometricMeanOfTrueClassProbability()]
99         metrics = list(metrics)
100         if additionalMetrics is not None:
101             for m in additionalMetrics:
102                 if not self._probabilitiesAvailable and m.requiresProbabilities:
103                     raise ValueError(f""Additional metric {m} not supported, as class probabilities were not provided"")
104 
105         super().__init__(y_predicted, y_true, metrics, additionalMetrics=additionalMetrics)
106 
107     def getConfusionMatrix(self) -> ""ConfusionMatrix"":
","Before: 105, 107
After: 105",update eval_stats_classification.py for new api,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,1029,"{'module': 1, 'comment': 2, 'class_definition': 1, 'class': 1, 'identifier': 67, 'argument_list': 14, '(': 15, 'subscript': 1, '[': 4, 'string': 7, 'string_start': 7, 'string_content': 9, 'string_end': 7, ']': 4, ')': 15, ':': 15, 'block': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_default_parameter': 6, 'type': 8, '=': 12, 'none': 9, 'attribute': 10, '.': 10, 'generic_type': 2, 'type_parameter': 2, 'expression_statement': 9, 'assignment': 6, 'comparison_operator': 5, 'is not': 4, 'if_statement': 6, 'if': 6, 'call': 13, '!=': 2, 'raise_statement': 3, 'raise': 3, 'interpolation': 3, '{': 3, '}': 3, 'is': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 14, 'end_line': 16, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7743626631289516,0.7728426635693921,"(tensor([0.9827]), tensor([0.9861]), tensor([0.9844]), tensor([0.9858]))"
"1 import logging
2 from abc import abstractmethod, ABC
3 from typing import List, Sequence
4 
5 import numpy as np
6 import seaborn as sns
","1 import logging
2 import numpy as np
3 import seaborn as sns
4 from abc import abstractmethod, ABC
5 from matplotlib import pyplot as plt
6 from matplotlib.colors import LinearSegmentedColormap
7 from typing import List, Sequence
8 
","Before: 2, 3, 4
After: 4, 7",fix bug in eval_stats_regression.py,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,16,"{'module': 1, 'import_statement': 2, 'import': 4, 'dotted_name': 8, 'identifier': 9, 'import_from_statement': 2, 'from': 2, ',': 2, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.418112416307462,0.404155598903426,"(tensor([0.8514]), tensor([0.9399]), tensor([0.8935]), tensor([0.9302]))"
"114     """"""
115     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
116     """"""
117     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
118             metrics: Sequence[""RegressionMetric""] = None, additionalMetrics: Sequence[""RegressionMetric""] = None):
119         """"""
120         :param y_predicted: the predicted values
121         :param y_true: the true values
122         :param metrics: the metrics to compute for evaluation; if None, use default metrics
123         :param additionalMetrics: the metrics to additionally compute
124         """"""
125 
126         if metrics is None:
127             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
128                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
129                        RegressionMetricStdDevAE()]
130         metrics = list(metrics)
131         if additionalMetrics is not None:
132             metrics.extend(additionalMetrics)
133 
134         super().__init__(y_predicted, y_true, metrics)
135 
136     def getMSE(self):
","110     """"""
111     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
112     """"""
113     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
114             metrics: Sequence[""RegressionMetric""] = None, additionalMetrics: Sequence[""RegressionMetric""] = None):
115         """"""
116         :param y_predicted: the predicted values
117         :param y_true: the true values
118         :param metrics: the metrics to compute for evaluation; if None, use default metrics
119         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
120         """"""
121 
122         if metrics is None:
123             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
124                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
125                        RegressionMetricStdDevAE()]
126         metrics = list(metrics)
127 
128         super().__init__(y_predicted, y_true, metrics, additionalMetrics=additionalMetrics)
129 
130     def getMSE(self):
","Before: 123
After: 119",fix bug in eval_stats_regression.py,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,1242,"{'module': 1, 'expression_statement': 6, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 12, ',': 12, 'typed_parameter': 2, ':': 7, 'type': 6, 'typed_default_parameter': 2, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, '=': 4, 'none': 4, ')': 12, 'block': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'assignment': 2, 'list': 1, 'call': 11, 'argument_list': 11, 'is not': 2, 'attribute': 2, '.': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7026436578043374,0.688627283172022,"(tensor([0.9665]), tensor([0.9680]), tensor([0.9672]), tensor([0.9678]))"
"114     """"""
115     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
116     """"""
117     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
118             metrics: Sequence[""RegressionMetric""] = None, additionalMetrics: Sequence[""RegressionMetric""] = None):
119         """"""
120         :param y_predicted: the predicted values
121         :param y_true: the true values
122         :param metrics: the metrics to compute for evaluation; if None, use default metrics
123         :param additionalMetrics: the metrics to additionally compute
124         """"""
125 
126         if metrics is None:
127             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
128                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
129                        RegressionMetricStdDevAE()]
130         metrics = list(metrics)
131         if additionalMetrics is not None:
132             metrics.extend(additionalMetrics)
133 
134         super().__init__(y_predicted, y_true, metrics)
135 
136     def getMSE(self):
","110     """"""
111     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
112     """"""
113     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
114             metrics: Sequence[""RegressionMetric""] = None, additionalMetrics: Sequence[""RegressionMetric""] = None):
115         """"""
116         :param y_predicted: the predicted values
117         :param y_true: the true values
118         :param metrics: the metrics to compute for evaluation; if None, use default metrics
119         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
120         """"""
121 
122         if metrics is None:
123             metrics = [RegressionMetricRRSE(), RegressionMetricR2(), RegressionMetricPCC(),
124                        RegressionMetricMAE(), RegressionMetricMSE(), RegressionMetricRMSE(),
125                        RegressionMetricStdDevAE()]
126         metrics = list(metrics)
127 
128         super().__init__(y_predicted, y_true, metrics, additionalMetrics=additionalMetrics)
129 
130     def getMSE(self):
","Before: 131, 132, 134
After: 128",fix bug in eval_stats_regression.py,Minor improvements in evaluation,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,44935f119f6e26920f2a7774ef540081fc132143,1c10a4bbe5897c99cf42b89f50038ba0f393c42d,0,1258,"{'module': 1, 'expression_statement': 6, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'function_definition': 1, 'def': 1, 'identifier': 31, 'parameters': 1, '(': 12, ',': 12, 'typed_parameter': 2, ':': 7, 'type': 6, 'typed_default_parameter': 2, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, '=': 4, 'none': 4, ')': 12, 'block': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'assignment': 2, 'list': 1, 'call': 11, 'argument_list': 11, 'is not': 2, 'attribute': 2, '.': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 15, 'name': '__init__', 'long_name': '__init__( self , name )', 'start_line': 16, 'end_line': 17, 'full_parameters': ['self', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7026436578043374,0.688627283172022,"(tensor([0.9665]), tensor([0.9680]), tensor([0.9672]), tensor([0.9678]))"
"57 
58 
59 class Metric(Generic[TEvalStats], ABC):
60     def __init__(self, name: str):
61         self.name = name
62 
63     @abstractmethod
","55 
56 TEvalStats = TypeVar(""TEvalStats"", bound=EvalStats)
57 
58 
59 class Metric(Generic[TEvalStats], ABC):
60     name: str
61 
62     @abstractmethod
63     def computeValueForEvalStats(self, evalStats: TEvalStats) -> float:
64         pass
","Before: 60, 61
After: 60",fix typo in eval_stats_base.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,544,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 11, 'argument_list': 1, '(': 2, 'subscript': 1, '[': 1, ']': 1, ',': 2, ')': 2, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 1, 'expression_statement': 1, 'assignment': 1, 'attribute': 1, '.': 1, '=': 1}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.369114023422421,0.35844428924221405,"(tensor([0.8440]), tensor([0.9101]), tensor([0.8758]), tensor([0.9030]))"
"10 
11 
12 class ClassificationMetric(Metric[""ClassificationEvalStats""], ABC):
13     def __init__(self, name, requiresProbabilities=False):
14         super().__init__(name)
15         self.requiresProbabilities = requiresProbabilities
16 
17     def computeValueForEvalStats(self, evalStats: ""ClassificationEvalStats""):
","8 from .eval_stats_base import PredictionArray, PredictionEvalStats, EvalStatsCollection, Metric
9 from ...util.plot import plotMatrix
10 
11 
12 class ClassificationMetric(Metric[""ClassificationEvalStats""], ABC):
13     requiresProbabilities = False
14 
15     def computeValueForEvalStats(self, evalStats: ""ClassificationEvalStats""):
16         return self.computeValue(evalStats.y_true, evalStats.y_predicted, evalStats.y_predictedClassProbabilities)
17 
","Before: 13, 14, 15
After: 13",remove unnecessary dependencies in src/sensai/evaluation/eval_stats_classification.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,119,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 13, 'argument_list': 3, '(': 4, 'subscript': 1, '[': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ']': 1, ',': 3, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, '=': 2, 'false': 1, 'expression_statement': 2, 'call': 2, 'attribute': 2, '.': 2, 'assignment': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 26, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""ClassificationEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""ClassificationEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4611506842072338,0.4196360574037784,"(tensor([0.8550]), tensor([0.9029]), tensor([0.8783]), tensor([0.8978]))"
"28 
29 
30 class ClassificationMetricAccuracy(ClassificationMetric):
31     def __init__(self):
32         super().__init__(""ACC"")
33 
34     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
","24     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
25         pass
26 
27 
28 class ClassificationMetricAccuracy(ClassificationMetric):
29     name = ""ACC""
30 
31     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
32         return accuracy_score(y_true=y_true, y_pred=y_predicted)
33 
","Before: 31, 32
After: 29",remove unnecessary dependencies in src/sensai/evaluation/eval_stats_classification.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,284,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 26, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""ClassificationEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""ClassificationEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3823157124138218,0.3496833803187555,"(tensor([0.9083]), tensor([0.9174]), tensor([0.9128]), tensor([0.9165]))"
"36 
37 
38 class ClassificationMetricGeometricMeanOfTrueClassProbability(ClassificationMetric):
39     def __init__(self):
40         super().__init__(""GeoMeanTrueClassProb"", requiresProbabilities=True)
41 
42     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
","31     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
32         return accuracy_score(y_true=y_true, y_pred=y_predicted)
33 
34 
35 class ClassificationMetricGeometricMeanOfTrueClassProbability(ClassificationMetric):
36     name = ""GeoMeanTrueClassProb""
37     requiresProbabilities = True
38 
39     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
40         y_predicted_proba_true_class = np.zeros(len(y_true))
","Before: 39, 40
After: 36, 37",remove unnecessary dependencies in src/sensai/evaluation/eval_stats_classification.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,350,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 7, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ',': 1, 'keyword_argument': 1, '=': 1, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 26, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""ClassificationEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""ClassificationEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3160065544178008,0.2766989499995966,"(tensor([0.8862]), tensor([0.9097]), tensor([0.8978]), tensor([0.9073]))"
"53 
54 
55 class ClassificationMetricTopNAccuracy(ClassificationMetric):
56     def __init__(self, n: int):
57         super().__init__(f""Top{n}Accuracy"", requiresProbabilities=True)
58         self.n = n
59 
60     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
","52 class ClassificationMetricTopNAccuracy(ClassificationMetric):
53     requiresProbabilities = True
54 
55     def __init__(self, n: int):
56         # TODO or not TODO: this is the only metric where the name is not static. We could make a static name like
57         #   ""TopNAccuracy"" without losing much information
58         self.name = f""Top{n}Accuracy""
59         self.n = n
60 
61     def _computeValue(self, y_true, y_predicted, y_predictedClassProbabilities):
","Before: 57
After: 56, 57, 58",remove unnecessary dependencies in src/sensai/evaluation/eval_stats_classification.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,577,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 13, 'argument_list': 3, '(': 4, ')': 4, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'type': 1, 'expression_statement': 2, 'call': 2, 'attribute': 2, '.': 2, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 1, '{': 1, '}': 1, 'string_end': 1, 'keyword_argument': 1, '=': 2, 'true': 1, 'assignment': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 26, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""ClassificationEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""ClassificationEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.38860128724835,0.35357902111396033,"(tensor([0.8306]), tensor([0.9401]), tensor([0.8820]), tensor([0.9279]))"
"147         self.labels = sklearn.utils.multiclass.unique_labels(y_true, y_predicted)
148         self.confusionMatrix = confusion_matrix(y_true, y_predicted, labels=self.labels)
149 
150     def plot(self, normalize=True, titleAdd: str = None):
151         title = 'Normalized Confusion Matrix' if normalize else 'Confusion Matrix (Counts)'
152         return plotMatrix(self.confusionMatrix, title, self.labels, self.labels, 'true class', 'predicted class', normalize=normalize,
153             titleAdd=titleAdd)","148         self.labels = sklearn.utils.multiclass.unique_labels(y_true, y_predicted)
149         self.confusionMatrix = confusion_matrix(y_true, y_predicted, labels=self.labels)
150 
151     def plot(self, normalize=True, titleAdd: str = None):
152         title = 'Normalized Confusion Matrix' if normalize else 'Confusion Matrix (Counts)'
153         return plotMatrix(self.confusionMatrix, title, self.labels, self.labels, 'true class', 'predicted class', normalize=normalize,
154             titleAdd=titleAdd)
","Before: 153
After: 154",remove unnecessary dependencies in src/sensai/evaluation/eval_stats_classification.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_classification.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,1528,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'attribute': 6, 'identifier': 23, '.': 6, '=': 6, 'call': 2, 'argument_list': 2, '(': 3, ',': 5, ')': 3, 'keyword_argument': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, 'true': 1, 'typed_default_parameter': 1, ':': 2, 'type': 1, 'none': 1, 'block': 1, 'conditional_expression': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self , name , requiresProbabilities = False )', 'start_line': 13, 'end_line': 15, 'full_parameters': ['self', ' name', ' requiresProbabilities = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 26, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""ClassificationEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""ClassificationEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8269102549499526,0.8237948014102942,"(tensor([0.9944]), tensor([0.9943]), tensor([0.9944]), tensor([0.9944]))"
"54 
55 
56 class RemovedNoiseUnsupervisedMetric(ClusteringUnsupervisedMetric):
57     def __init__(self, name: str, worstValue=0):
58         self.worstValue = worstValue
59         super().__init__(name)
60 
61     def computeValueForEvalStats(self, evalStats: ""ClusteringUnsupervisedEvalStats"") -> float:
","52 class ClusteringUnsupervisedMetric(Metric[""ClusteringUnsupervisedEvalStats""], ABC):
53     pass
54 
55 
56 class RemovedNoiseUnsupervisedMetric(ClusteringUnsupervisedMetric):
57     worstValue = 0
58 
59     def computeValueForEvalStats(self, evalStats: ""ClusteringUnsupervisedEvalStats"") -> float:
60         if len(evalStats.clustersLabels) == 0:  # all is noise
61             return 0
","Before: 57, 58, 59
After: 57",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,630,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 13, 'argument_list': 3, '(': 4, ')': 4, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'type': 1, 'default_parameter': 1, '=': 2, 'integer': 1, 'expression_statement': 2, 'assignment': 1, 'attribute': 2, '.': 2, 'call': 2}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4060324436538932,0.3731960601499781,"(tensor([0.9016]), tensor([0.9164]), tensor([0.9089]), tensor([0.9149]))"
"70 
71 
72 class CalinskiHarabaszScore(RemovedNoiseUnsupervisedMetric):
73     def __init__(self):
74         super().__init__(""CalinskiHarabaszScore"")
75 
76     @staticmethod
","66     def computeValue(datapoints: np.ndarray, labels: Sequence[int]):
67         pass
68 
69 
70 class CalinskiHarabaszScore(RemovedNoiseUnsupervisedMetric):
71     name = ""CalinskiHarabaszScore""
72 
73     @staticmethod
74     def computeValue(datapoints: np.ndarray, labels: Sequence[int]):
75         return sklearn.metrics.calinski_harabasz_score(datapoints, labels)
","Before: 73, 74
After: 71",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,767,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.12812331641066646,0.11989351973771412,"(tensor([0.7941]), tensor([0.8870]), tensor([0.8380]), tensor([0.8767]))"
"79 
80 
81 class DaviesBouldinScore(RemovedNoiseUnsupervisedMetric):
82     def __init__(self):
83         # TODO: I think in some edge cases this score could be larger than one, one should look into that
84         super().__init__(""DaviesBouldinScore"", worstValue=1)
85 
86     @staticmethod
","74     def computeValue(datapoints: np.ndarray, labels: Sequence[int]):
75         return sklearn.metrics.calinski_harabasz_score(datapoints, labels)
76 
77 
78 class DaviesBouldinScore(RemovedNoiseUnsupervisedMetric):
79     name = ""DaviesBouldinScore""
80     # TODO: I think in some edge cases this score could be larger than one, one should look into that
81     worstValue = 1
82 
83     @staticmethod
","Before: 82, 83, 84
After: 79, 80, 81",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,850,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 7, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'comment': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ',': 1, 'keyword_argument': 1, '=': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.42858937751807497,0.40832641122561036,"(tensor([0.8384]), tensor([0.9107]), tensor([0.8731]), tensor([0.9029]))"
"90 
91 # Note: this takes a lot of time to compute for many datapoints
92 class SilhouetteScore(RemovedNoiseUnsupervisedMetric):
93     def __init__(self):
94         super().__init__(""SilhouetteScore"", worstValue=-1)
95 
96     @staticmethod
","85         return sklearn.metrics.davies_bouldin_score(datapoints, labels)
86 
87 
88 # Note: this takes a lot of time to compute for many datapoints
89 class SilhouetteScore(RemovedNoiseUnsupervisedMetric):
90     name = ""SilhouetteScore""
91     worstValue = -1
92 
93     @staticmethod
94     def computeValue(datapoints: np.ndarray, labels: Sequence[int]):
","Before: 93, 94
After: 90, 91",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,940,"{'module': 1, 'comment': 1, 'class_definition': 1, 'class': 1, 'identifier': 7, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ',': 1, 'keyword_argument': 1, '=': 1, 'unary_operator': 1, '-': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.34683198689374517,0.3286977863328388,"(tensor([0.8507]), tensor([0.9134]), tensor([0.8809]), tensor([0.9067]))"
"132 
133 
134 class RemovedCommonNoiseSupervisedMetric(ClusteringSupervisedMetric, ABC):
135     def __init__(self, name, worstValue=0):
136         self.worstValue = worstValue
137         super().__init__(name)
138 
139     def computeValueForEvalStats(self, evalStats: ""ClusteringSupervisedEvalStats"") -> float:
","127 class ClusteringSupervisedMetric(Metric[""ClusteringSupervisedEvalStats""], ABC):
128     pass
129 
130 
131 class RemovedCommonNoiseSupervisedMetric(ClusteringSupervisedMetric, ABC):
132     worstValue = 0
133 
134     def computeValueForEvalStats(self, evalStats: ""ClusteringSupervisedEvalStats"") -> float:
135         labels, trueLabels = evalStats.labelsWithRemovedCommonNoise()
136         if len(labels) == 0:
","Before: 135, 136, 137
After: 132",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,1315,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 13, 'argument_list': 3, '(': 4, ',': 3, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, '=': 2, 'integer': 1, 'expression_statement': 2, 'assignment': 1, 'attribute': 2, '.': 2, 'call': 2}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3731173052035916,0.3371209204137408,"(tensor([0.8884]), tensor([0.9018]), tensor([0.8950]), tensor([0.9004]))"
"149 
150 
151 class VMeasureScore(RemovedCommonNoiseSupervisedMetric):
152     def __init__(self):
153         super().__init__(""VMeasureScore"")
154 
155     @staticmethod
","142     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
143         pass
144 
145 
146 class VMeasureScore(RemovedCommonNoiseSupervisedMetric):
147     name = ""VMeasureScore""
148 
149     @staticmethod
150     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
151         return sklearn.metrics.v_measure_score(labels, trueLabels)
","Before: 152, 153
After: 147",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,1463,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.11272262535520242,0.10591104665758579,"(tensor([0.7680]), tensor([0.8589]), tensor([0.8109]), tensor([0.8488]))"
"158 
159 
160 class AdjustedRandScore(RemovedCommonNoiseSupervisedMetric):
161     def __init__(self):
162         super().__init__(""AdjustedRandScore"", worstValue=-1)
163 
164     @staticmethod
","150     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
151         return sklearn.metrics.v_measure_score(labels, trueLabels)
152 
153 
154 class AdjustedRandScore(RemovedCommonNoiseSupervisedMetric):
155     name = ""AdjustedRandScore""
156     worstValue = -1
157 
158     @staticmethod
159     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
","Before: 161, 162
After: 155, 156",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,1549,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 7, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ',': 1, 'keyword_argument': 1, '=': 1, 'unary_operator': 1, '-': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.10938176499343763,0.09990712082669076,"(tensor([0.7833]), tensor([0.8771]), tensor([0.8276]), tensor([0.8667]))"
"167 
168 
169 class FowlkesMallowsScore(RemovedCommonNoiseSupervisedMetric):
170     def __init__(self):
171         super().__init__(""FowlkesMallowsScore"")
172 
173     @staticmethod
","159     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
160         return sklearn.metrics.adjusted_rand_score(labels, trueLabels)
161 
162 
163 class FowlkesMallowsScore(RemovedCommonNoiseSupervisedMetric):
164     name = ""FowlkesMallowsScore""
165 
166     @staticmethod
167     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
168         return sklearn.metrics.fowlkes_mallows_score(labels, trueLabels)
","Before: 170, 171
After: 164",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,1642,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.10206699518441356,0.09488266033303483,"(tensor([0.7578]), tensor([0.8613]), tensor([0.8062]), tensor([0.8497]))"
"176 
177 
178 class AdjustedMutualInfoScore(RemovedCommonNoiseSupervisedMetric):
179     def __init__(self):
180         super().__init__(""AdjustedMutualInfoScore"")
181 
182     @staticmethod
","167     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
168         return sklearn.metrics.fowlkes_mallows_score(labels, trueLabels)
169 
170 
171 class AdjustedMutualInfoScore(RemovedCommonNoiseSupervisedMetric):
172     name = ""AdjustedMutualInfoScore""
173 
174     @staticmethod
175     def computeValue(labels: Sequence[int], trueLabels: Sequence[int]):
176         return sklearn.metrics.adjusted_mutual_info_score(labels, trueLabels)
","Before: 179, 180
After: 172",update eval_stats_clustering.py to new format,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,1728,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 137, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 31, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.10082959080862604,0.0921295017387785,"(tensor([0.7524]), tensor([0.8551]), tensor([0.8005]), tensor([0.8436]))"
"30 
31 
32 class RegressionMetricMAE(RegressionMetric):
33     def __init__(self):
34         super().__init__(""MAE"")
35 
36     @classmethod
","28     def computeAbsErrors(cls, y_true: np.ndarray, y_predicted: np.ndarray):
29         return np.abs(cls.computeErrors(y_true, y_predicted))
30 
31 
32 class RegressionMetricMAE(RegressionMetric):
33     name = ""MAE""
34 
35     @classmethod
36     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
37         return cls.computeAbsErrors(y_true, y_predicted)
","Before: 33, 34
After: 33",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,305,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.18167871962037796,0.19643168907107492,"(tensor([0.7310]), tensor([0.8687]), tensor([0.7939]), tensor([0.8526]))"
"39 
40 
41 class RegressionMetricMSE(RegressionMetric):
42     def __init__(self):
43         super().__init__(""MSE"")
44 
45     @classmethod
","36     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
37         return cls.computeAbsErrors(y_true, y_predicted)
38 
39 
40 class RegressionMetricMSE(RegressionMetric):
41     name = ""MSE""
42 
43     @classmethod
44     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
45         residuals = y_predicted - y_true
","Before: 42, 43
After: 41",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,384,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.12851766330991743,0.13251275494772205,"(tensor([0.7047]), tensor([0.8299]), tensor([0.7622]), tensor([0.8154]))"
"49 
50 
51 class RegressionMetricRMSE(RegressionMetric):
52     def __init__(self):
53         super().__init__(""MSE"")
54 
55     @classmethod
","45         residuals = y_predicted - y_true
46         return np.sum(residuals * residuals) / len(residuals)
47 
48 
49 class RegressionMetricRMSE(RegressionMetric):
50     name = ""RMSE""
51 
52     @classmethod
53     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
54         errors = cls.computeErrors(y_true, y_predicted)
","Before: 52, 53
After: 50",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,480,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.11077491751229575,0.10476576988229207,"(tensor([0.7294]), tensor([0.8468]), tensor([0.7837]), tensor([0.8334]))"
"59 
60 
61 class RegressionMetricRRSE(RegressionMetric):
62     def __init__(self):
63         super().__init__(""RRSE"")
64 
65     @classmethod
","54         errors = cls.computeErrors(y_true, y_predicted)
55         return np.sqrt(np.mean(errors * errors))
56 
57 
58 class RegressionMetricRRSE(RegressionMetric):
59     name = ""RRSE""
60 
61     @classmethod
62     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
63         mean_y = np.mean(y_true)
","Before: 62, 63
After: 59",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,583,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.1426349570124933,0.13688189678624352,"(tensor([0.7451]), tensor([0.8525]), tensor([0.7952]), tensor([0.8404]))"
"71 
72 
73 class RegressionMetricR2(RegressionMetric):
74     def __init__(self):
75         super().__init__(""R2"")
76 
77     def computeValue(self, y_true: np.ndarray, y_predicted: np.ndarray):
","65         mean_deviation = y_true - mean_y
66         return np.sqrt(np.sum(residuals * residuals) / np.sum(mean_deviation * mean_deviation))
67 
68 
69 class RegressionMetricR2(RegressionMetric):
70     name = ""R2""
71 
72     def computeValue(self, y_true: np.ndarray, y_predicted: np.ndarray):
73         rrse = RegressionMetricRRSE.computeValue(y_true, y_predicted)
74         return 1.0 - rrse*rrse
","Before: 74, 75
After: 70",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,714,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2900006356809727,0.24295350448588318,"(tensor([0.7840]), tensor([0.8889]), tensor([0.8331]), tensor([0.8772]))"
"80 
81 
82 class RegressionMetricPCC(RegressionMetric):
83     def __init__(self):
84         super().__init__(""PCC"")
85 
86     def computeValue(self, y_true: np.ndarray, y_predicted: np.ndarray):
","73         rrse = RegressionMetricRRSE.computeValue(y_true, y_predicted)
74         return 1.0 - rrse*rrse
75 
76 
77 class RegressionMetricPCC(RegressionMetric):
78     name = ""PCC""
79 
80     def computeValue(self, y_true: np.ndarray, y_predicted: np.ndarray):
81         cov = np.cov([y_true, y_predicted])
82         return cov[0][1] / np.sqrt(cov[0][0] * cov[1][1])
","Before: 83, 84
After: 78",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,800,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.23381401733912366,0.2096328561472593,"(tensor([0.7952]), tensor([0.8924]), tensor([0.8410]), tensor([0.8817]))"
"89 
90 
91 class RegressionMetricStdDevAE(RegressionMetric):
92     def __init__(self):
93         super().__init__(""StdDevAE"")
94 
95     @classmethod
","81         cov = np.cov([y_true, y_predicted])
82         return cov[0][1] / np.sqrt(cov[0][0] * cov[1][1])
83 
84 
85 class RegressionMetricStdDevAE(RegressionMetric):
86     name = ""StdDevAE""
87 
88     @classmethod
89     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
90         return np.std(cls.computeAbsErrors(y_true, y_predicted))
","Before: 92, 93
After: 86",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,921,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.08845536998662136,0.08559226629459028,"(tensor([0.7204]), tensor([0.8492]), tensor([0.7795]), tensor([0.8342]))"
"98 
99 
100 class RegressionMetricMedianAE(RegressionMetric):
101     def __init__(self):
102         super().__init__(""MedianAE"")
103 
104     @classmethod
","89     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
90         return np.std(cls.computeAbsErrors(y_true, y_predicted))
91 
92 
93 class RegressionMetricMedianAE(RegressionMetric):
94     name = ""MedianAE""
95 
96     @classmethod
97     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
98         return np.median(cls.computeAbsErrors(y_true, y_predicted))
","Before: 101, 102
After: 94",fix typos in src/src/sensai/evaluation/eval_stats_regression.py,Refactoring: made name and requiresProbabilities static attributes of Metric classes,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,679c6cb4b5a658bb662cb1c00699b58b95b6fa5f,f624e74ea60e045483a2fcdd7684d3fb9a101d50,0,1008,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 6, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.10373190245889077,0.10434156021353629,"(tensor([0.7269]), tensor([0.8529]), tensor([0.7848]), tensor([0.8383]))"
"21 
22 
23 class EvalStats(Generic[TMetric]):
24     def __init__(self, metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
25         if len(metrics) == 0:
26             raise ValueError(""No metrics provided"")
27         self.metrics = metrics
28         # Implementations of EvalStats will typically provide default metrics, therefore we include
29         # the possibility for passing additional metrics here
30         if additionalMetrics is not None:
31             self.metrics.extend(additionalMetrics)
32         self.name = None
33 
34     def setName(self, name: str):
","21 
22 
23 class EvalStats(Generic[TMetric]):
24     def __init__(self, metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
25         if len(metrics) == 0:
26             raise ValueError(""No metrics provided"")
27         self.metrics = metrics
28         # Implementations of EvalStats will typically provide default metrics, therefore we include
29         # the possibility for passing additional metrics here
30         if additionalMetrics is not None:
31             self.metrics = self.metrics + additionalMetrics
32         self.name = None
33 
34     def setName(self, name: str):
","Before: 31
After: 31",fix typo in eval_stats_base.py,Fixed undesired mutation of input metrics in EvalStats,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,fc027a75f9532ad2f6463c60a8ca7bcd104dd15f,5d7e86bbb7ba94e3f8d2d9be65c2449cceeb720b,0,264,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 24, 'argument_list': 4, '(': 5, 'subscript': 1, '[': 3, ']': 3, ')': 5, ':': 6, 'block': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'type': 4, 'generic_type': 2, 'type_parameter': 2, 'typed_default_parameter': 1, '=': 3, 'none': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'call': 3, '==': 1, 'integer': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 4, '.': 4, 'comment': 2, 'is not': 2}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9421460583740165,0.9399161112409832,"(tensor([0.9928]), tensor([0.9895]), tensor([0.9911]), tensor([0.9898]))"
"8 setup(
9     name='sensai',
10     package_dir={"""": ""src""},
11     packages=find_packages(where=""src""),
12     include_package_data=True,
13     version='0.0.3',
14     description='Library for sensible AI',
15     install_requires=open(""requirements.txt"").readlines(),
16     dependency_links=[""https://download.pytorch.org/whl/torch_stable.html""],
17     setup_requires=[""wheel""],
","8 setup(
9     name='sensai',
10     package_dir={"""": ""src""},
11     packages=find_packages(where=""src""),
12     include_package_data=True,
13     version='0.0.4',
14     description='Library for sensible AI',
15     install_requires=open(""requirements.txt"").readlines(),
16     dependency_links=[""https://download.pytorch.org/whl/torch_stable.html""],
17     setup_requires=[""wheel""],
","Before: 13
After: 13",update version number in setup.py,Bumped version again due to error in previous release,https://github.com/opcode81/sensAI,setup.py,927154555da3301c49d6cb9179bc1780f16ed7f6,8fc4f25974ed9e5ea4652262d1533f1a5aa599dd,0,138,"{'module': 1, 'ERROR': 1, 'identifier': 13, '(': 4, 'keyword_argument': 9, '=': 9, 'string': 8, 'string_start': 8, 'string_content': 7, 'string_end': 8, ',': 8, 'dictionary': 1, '{': 1, 'pair': 1, ':': 1, '}': 1, 'call': 3, 'argument_list': 3, ')': 3, 'true': 1, 'attribute': 1, '.': 1, 'list': 1, '[': 1, ']': 1}",{},{},0.9641384561775734,0.9600144467698959,"(tensor([0.9979]), tensor([0.9979]), tensor([0.9979]), tensor([0.9979]))"
"33     name = ""MAE""
34 
35     @classmethod
36     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
37         return cls.computeAbsErrors(y_true, y_predicted)
38 
39 
","33     name = ""MAE""
34 
35     @classmethod
36     def computeValue(cls, y_true: np.ndarray, y_predicted: np.ndarray):
37         return np.mean(cls.computeAbsErrors(y_true, y_predicted))
38 
39 
","Before: 37
After: 37",fix typo in eval_stats_regression.py,Bugfix: added forgotten np.mean in MAE,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,d56cdf74d5ffca09eefd3f7411dba1365dd70a48,927154555da3301c49d6cb9179bc1780f16ed7f6,0,348,"{'module': 1, 'expression_statement': 1, 'assignment': 1, 'identifier': 14, '=': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 3, 'typed_parameter': 2, ':': 3, 'type': 2, 'attribute': 3, '.': 3, ')': 2, 'block': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.852467409037365,0.8553214410472344,"(tensor([0.9856]), tensor([0.9933]), tensor([0.9894]), tensor([0.9925]))"
"305         self.inputOutputDataDict = inputOutputDataDict
306         self.keyName = keyName
307 
308     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
309             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
310             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
311         """"""
312         :param isRegression: flag indicating whether the models to evaluate are regression models
313         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
314         :param resultWriter: a writer with which to store results
315         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
316             dataset in a subdirectory named according to the name of the dataset
317         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
318         :param columnNameForModelRanking: column name to use for ranking models
319         :param rankMax: if true, use max for ranking, else min
320         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
321             for every dataset and meanDF contains one row for each model with results averaged across datasets
322         """"""
323         allResults = pd.DataFrame()
324         for key, inputOutputData in self.inputOutputDataDict.items():
325             log.info(f""Evaluating models for {key}"")
326             ev = createEvaluationUtil(inputOutputData, isRegression=isRegression, crossValidatorParams=crossValidatorParams)
327             models = [f() for f in modelFactories]
328             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
329             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
330             df[self.keyName] = key
331             df[""modelName""] = df.index
332             if columnNameForModelRanking is not None:
333                 if columnNameForModelRanking not in df.columns:
334                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
335                 df[""bestModel""] = 0
336                 if rankMax:
337                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
338                 else:
339                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
340             df = df.reset_index(drop=True)
341             allResults = pd.concat((allResults, df))
342         strAllResults = f""All results:\n{allResults.to_string()}""
343         log.info(strAllResults)
344         meanResults = allResults.groupby(""modelName"").mean()
345         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
346         log.info(strMeanResults)
347         if resultWriter is not None:
348             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
349         return allResults, meanResults
","305         self.inputOutputDataDict = inputOutputDataDict
306         self.keyName = keyName
307 
308     def compareModelsCrossValidation(self, modelFactories: Sequence[Callable[[], VectorModel]],
309             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
310             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
311         """"""
312         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
313         :param resultWriter: a writer with which to store results
314         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
315             dataset in a subdirectory named according to the name of the dataset
316         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
317         :param columnNameForModelRanking: column name to use for ranking models
318         :param rankMax: if true, use max for ranking, else min
319         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
320             for every dataset and meanDF contains one row for each model with results averaged across datasets
321         """"""
322         allResults = pd.DataFrame()
323         for key, inputOutputData in self.inputOutputDataDict.items():
324             log.info(f""Evaluating models for {key}"")
325             models = [f() for f in modelFactories]
326             modelsAreRegression = [model.isRegressionModel() for model in models]
327             if all(modelsAreRegression):
328                 isRegression = True
329             elif not any(modelsAreRegression):
330                 isRegression = False
331             else:
332                 raise ValueError(""The models have to be either all regression models or all classification, not a mixture"")
333             ev = createEvaluationUtil(inputOutputData, isRegression=isRegression, crossValidatorParams=crossValidatorParams)
334             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
335             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
336             df[self.keyName] = key
337             df[""modelName""] = df.index
338             if columnNameForModelRanking is not None:
339                 if columnNameForModelRanking not in df.columns:
340                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
341                 df[""bestModel""] = 0
342                 if rankMax:
343                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
344                 else:
345                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
346             df = df.reset_index(drop=True)
347             allResults = pd.concat((allResults, df))
348         strAllResults = f""All results:\n{allResults.to_string()}""
349         log.info(strAllResults)
350         meanResults = allResults.groupby(""modelName"").mean()
351         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
352         log.info(strMeanResults)
353         if resultWriter is not None:
354             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
355         return allResults, meanResults
","Before: 308
After: 308",improve multidataevaluationutil.py script,"MultiEvalUtil: breaking change, removed the need for passing isRegression",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,04c459b8d25f35ac90b146265613fc3bcdc4ff65,5cc709b11128216bbe1495711ad3c4f472503284,0,3001,"{'module': 1, 'expression_statement': 22, 'assignment': 17, 'attribute': 27, 'identifier': 120, '.': 27, '=': 26, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 21, ',': 16, 'typed_parameter': 1, ':': 11, 'type': 14, 'generic_type': 6, 'type_parameter': 6, '[': 17, 'list': 1, ']': 17, 'typed_default_parameter': 3, 'none': 6, 'default_parameter': 2, 'true': 3, ')': 21, '->': 1, 'block': 7, 'string': 12, 'string_start': 12, 'string_content': 13, 'string_end': 12, 'call': 19, 'argument_list': 19, 'for_statement': 1, 'for': 2, 'pattern_list': 1, 'in': 2, 'interpolation': 5, '{': 5, '}': 5, 'keyword_argument': 4, 'list_comprehension': 1, 'for_in_clause': 1, 'conditional_expression': 1, 'if': 5, 'else': 2, 'subscript': 9, 'if_statement': 4, 'comparison_operator': 3, 'is not': 4, 'not in': 2, 'raise_statement': 1, 'raise': 1, 'integer': 3, 'else_clause': 1, 'tuple': 1, 'escape_sequence': 4, 'binary_operator': 2, '+': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.747976305912914,0.7464094237134999,"(tensor([0.9473]), tensor([0.9588]), tensor([0.9530]), tensor([0.9577]))"
"305         self.inputOutputDataDict = inputOutputDataDict
306         self.keyName = keyName
307 
308     def compareModelsCrossValidation(self, isRegression, modelFactories: Sequence[Callable[[], VectorModel]],
309             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
310             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
311         """"""
312         :param isRegression: flag indicating whether the models to evaluate are regression models
313         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
314         :param resultWriter: a writer with which to store results
315         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
316             dataset in a subdirectory named according to the name of the dataset
317         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
318         :param columnNameForModelRanking: column name to use for ranking models
319         :param rankMax: if true, use max for ranking, else min
320         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
321             for every dataset and meanDF contains one row for each model with results averaged across datasets
322         """"""
323         allResults = pd.DataFrame()
324         for key, inputOutputData in self.inputOutputDataDict.items():
325             log.info(f""Evaluating models for {key}"")
326             ev = createEvaluationUtil(inputOutputData, isRegression=isRegression, crossValidatorParams=crossValidatorParams)
327             models = [f() for f in modelFactories]
328             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
329             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
330             df[self.keyName] = key
331             df[""modelName""] = df.index
332             if columnNameForModelRanking is not None:
333                 if columnNameForModelRanking not in df.columns:
334                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
335                 df[""bestModel""] = 0
336                 if rankMax:
337                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
338                 else:
339                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
340             df = df.reset_index(drop=True)
341             allResults = pd.concat((allResults, df))
342         strAllResults = f""All results:\n{allResults.to_string()}""
343         log.info(strAllResults)
344         meanResults = allResults.groupby(""modelName"").mean()
345         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
346         log.info(strMeanResults)
347         if resultWriter is not None:
348             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
349         return allResults, meanResults
","305         self.inputOutputDataDict = inputOutputDataDict
306         self.keyName = keyName
307 
308     def compareModelsCrossValidation(self, modelFactories: Sequence[Callable[[], VectorModel]],
309             resultWriter: Optional[ResultWriter] = None, writePerDatasetResults=True,
310             crossValidatorParams: Optional[Dict[str, Any]] = None, columnNameForModelRanking: str = None, rankMax=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
311         """"""
312         :param modelFactories: a sequence of factory functions for the creation of models to evaluate
313         :param resultWriter: a writer with which to store results
314         :param writePerDatasetResults: whether to use resultWriter (if not None) in order to generate detailed results for each
315             dataset in a subdirectory named according to the name of the dataset
316         :param crossValidatorParams: parameters to use for the instantiation of cross-validators
317         :param columnNameForModelRanking: column name to use for ranking models
318         :param rankMax: if true, use max for ranking, else min
319         :return: a pair of data frames (allDF, meanDF) where allDF contains all the individual cross-validation results
320             for every dataset and meanDF contains one row for each model with results averaged across datasets
321         """"""
322         allResults = pd.DataFrame()
323         for key, inputOutputData in self.inputOutputDataDict.items():
324             log.info(f""Evaluating models for {key}"")
325             models = [f() for f in modelFactories]
326             modelsAreRegression = [model.isRegressionModel() for model in models]
327             if all(modelsAreRegression):
328                 isRegression = True
329             elif not any(modelsAreRegression):
330                 isRegression = False
331             else:
332                 raise ValueError(""The models have to be either all regression models or all classification, not a mixture"")
333             ev = createEvaluationUtil(inputOutputData, isRegression=isRegression, crossValidatorParams=crossValidatorParams)
334             childResultWriter = resultWriter.childForSubdirectory(key) if writePerDatasetResults else None
335             df = ev.compareModelsCrossValidation(models, resultWriter=childResultWriter)
336             df[self.keyName] = key
337             df[""modelName""] = df.index
338             if columnNameForModelRanking is not None:
339                 if columnNameForModelRanking not in df.columns:
340                     raise ValueError(f""Rank metric {columnNameForModelRanking} not contained in columns {df.columns}"")
341                 df[""bestModel""] = 0
342                 if rankMax:
343                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmax()] = 1
344                 else:
345                     df[""bestModel""].loc[df[columnNameForModelRanking].idxmin()] = 1
346             df = df.reset_index(drop=True)
347             allResults = pd.concat((allResults, df))
348         strAllResults = f""All results:\n{allResults.to_string()}""
349         log.info(strAllResults)
350         meanResults = allResults.groupby(""modelName"").mean()
351         strMeanResults = f""Mean results:\n{meanResults.to_string()}""
352         log.info(strMeanResults)
353         if resultWriter is not None:
354             resultWriter.writeTextFile(""model-comparison-results"", strMeanResults + ""\n\n"" + strAllResults)
355         return allResults, meanResults
","Before: 326
After: 326, 327, 328, 329, 330, 331, 332, 333",improve multidataevaluationutil.py script,"MultiEvalUtil: breaking change, removed the need for passing isRegression",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,04c459b8d25f35ac90b146265613fc3bcdc4ff65,5cc709b11128216bbe1495711ad3c4f472503284,0,3151,"{'module': 1, 'expression_statement': 22, 'assignment': 17, 'attribute': 27, 'identifier': 120, '.': 27, '=': 26, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 21, ',': 16, 'typed_parameter': 1, ':': 11, 'type': 14, 'generic_type': 6, 'type_parameter': 6, '[': 17, 'list': 1, ']': 17, 'typed_default_parameter': 3, 'none': 6, 'default_parameter': 2, 'true': 3, ')': 21, '->': 1, 'block': 7, 'string': 12, 'string_start': 12, 'string_content': 13, 'string_end': 12, 'call': 19, 'argument_list': 19, 'for_statement': 1, 'for': 2, 'pattern_list': 1, 'in': 2, 'interpolation': 5, '{': 5, '}': 5, 'keyword_argument': 4, 'list_comprehension': 1, 'for_in_clause': 1, 'conditional_expression': 1, 'if': 5, 'else': 2, 'subscript': 9, 'if_statement': 4, 'comparison_operator': 3, 'is not': 4, 'not in': 2, 'raise_statement': 1, 'raise': 1, 'integer': 3, 'else_clause': 1, 'tuple': 1, 'escape_sequence': 4, 'binary_operator': 2, '+': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.747976305912914,0.7464094237134999,"(tensor([0.9473]), tensor([0.9588]), tensor([0.9530]), tensor([0.9577]))"
"289         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
290         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
291         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
292 
293 
294 class ClassificationEvaluationUtil(EvaluationUtil[VectorClassificationModel, VectorClassificationModelEvaluator, VectorClassificationModelCrossValidator, VectorClassificationModelEvaluationData, VectorClassificationModelCrossValidationData, ClassificationEvalStats]):
295     def _createEvalStatsPlots(self, evalStats: ClassificationEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
296         resultCollector.addFigure(""confusion-matrix"", evalStats.plotConfusionMatrix(titleAdd=subtitle))
297 
298 
","289         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
290         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
291         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
292 
293 
294 class ClassificationEvaluationUtil(EvaluationUtil[VectorClassificationModel, VectorClassificationModelEvaluator, VectorClassificationModelEvaluationData, VectorClassificationModelCrossValidator, VectorClassificationModelCrossValidationData, ClassificationEvalStats]):
295     def _createEvalStatsPlots(self, evalStats: ClassificationEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
296         resultCollector.addFigure(""confusion-matrix"", evalStats.plotConfusionMatrix(titleAdd=subtitle))
297 
298 
","Before: 294
After: 294",fix typo in eval_util.py,Fixed typo in generics,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,da780fad9f2a1a3dcdbf606d78771f973b39a8de,b80cc681da911af35462ac5d270f0819e05c6d39,0,2854,"{'module': 1, 'expression_statement': 4, 'call': 8, 'attribute': 9, 'identifier': 40, '.': 9, 'argument_list': 9, '(': 10, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ',': 12, 'keyword_argument': 4, '=': 5, ')': 10, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, ']': 1, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, 'type': 2, 'default_parameter': 1, 'none': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9735043406730486,0.9735043406730486,"(tensor([0.9980]), tensor([0.9980]), tensor([0.9980]), tensor([0.9980]))"
"34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: np.max(self.clusterSizeDistribution),
43             self.MIN_SIZE: np.min(self.clusterSizeDistribution),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = self.noiseClusterSize
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: int(np.max(self.clusterSizeDistribution)),
43             self.MIN_SIZE: int(np.min(self.clusterSizeDistribution)),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = self.noiseClusterSize
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","Before: 42, 43
After: 42, 43",fix eval_stats_clustering.py for python 3,Minor change in clustering_eval_stats - changed some fields to ints,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,5332a68c0314a4ee73b45c758a1a5360bc6ffc3f,e8e5ba8467c6263b89c7ce589a9e64645e472a64,0,487,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'attribute': 23, 'identifier': 56, '.': 23, '=': 4, 'call': 9, 'argument_list': 9, '(': 10, 'integer': 1, ')': 10, ',': 7, 'keyword_argument': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, ':': 8, 'block': 2, 'dictionary': 1, '{': 1, 'pair': 6, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'subscript': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.912703567275647,0.9161917845233178,"(tensor([0.9928]), tensor([0.9976]), tensor([0.9952]), tensor([0.9971]))"
"34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: int(np.max(self.clusterSizeDistribution)),
43             self.MIN_SIZE: int(np.min(self.clusterSizeDistribution)),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = self.noiseClusterSize
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: int(np.max(self.clusterSizeDistribution)),
43             self.MIN_SIZE: int(np.min(self.clusterSizeDistribution)),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = int(self.noiseClusterSize)
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","Before: 47
After: 47",fix typo in eval_stats_clustering.py,Minor change in clustering_eval_stats - changed some fields to ints,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,ec80e0f09405ede214cca28c5fc964415aea1d7e,a0aa21c668afc11ba0377685ea13df4ab9454a46,0,564,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'attribute': 23, 'identifier': 58, '.': 23, '=': 4, 'call': 11, 'argument_list': 11, '(': 12, 'integer': 1, ')': 12, ',': 7, 'keyword_argument': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, ':': 8, 'block': 2, 'dictionary': 1, '{': 1, 'pair': 6, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'subscript': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9576454505578073,0.9592882012276226,"(tensor([0.9981]), tensor([0.9987]), tensor([0.9984]), tensor([0.9987]))"
"34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: np.max(self.clusterSizeDistribution),
43             self.MIN_SIZE: np.min(self.clusterSizeDistribution),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = self.noiseClusterSize
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: int(np.max(self.clusterSizeDistribution)),
43             self.MIN_SIZE: int(np.min(self.clusterSizeDistribution)),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = int(self.noiseClusterSize)
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","Before: 42, 43
After: 42, 43",fix eval_stats_clustering.py for python 3,Minor change in clustering_eval_stats - changed some fields to ints,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,2dda3a4e2b84e7cc3365b432db838cb92116b2c2,355ad2e8ea9578ea521e20f26a66018095b546ec,0,487,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'attribute': 23, 'identifier': 56, '.': 23, '=': 4, 'call': 9, 'argument_list': 9, '(': 10, 'integer': 1, ')': 10, ',': 7, 'keyword_argument': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, ':': 8, 'block': 2, 'dictionary': 1, '{': 1, 'pair': 6, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'subscript': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8720182993854689,0.876829157834348,"(tensor([0.9899]), tensor([0.9969]), tensor([0.9934]), tensor([0.9962]))"
"34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: np.max(self.clusterSizeDistribution),
43             self.MIN_SIZE: np.min(self.clusterSizeDistribution),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = self.noiseClusterSize
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","34             self.clusterSizeDistribution = np.zeros(1)
35         super().__init__(defaultMetrics, additionalMetrics=additionalMetrics)
36 
37     def getDistributionSummary(self) -> Dict[str, float]:
38         result = {
39             self.NUM_CLUSTERS: len(self.clusterIdentifiers),
40             self.AV_SIZE: self.clusterSizeDistribution.mean(),
41             self.STDDEV_SIZE: self.clusterSizeDistribution.std(),
42             self.MAX_SIZE: int(np.max(self.clusterSizeDistribution)),
43             self.MIN_SIZE: int(np.min(self.clusterSizeDistribution)),
44             self.MEDIAN_SIZE: np.median(self.clusterSizeDistribution)
45         }
46         if self.noiseLabel is not None:
47             result[self.NOISE_SIZE] = int(self.noiseClusterSize)
48         return result
49 
50     def getAll(self) -> Dict[str, float]:
","Before: 47
After: 47",fix eval_stats_clustering.py for python 3,Minor change in clustering_eval_stats - changed some fields to ints,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_clustering.py,2dda3a4e2b84e7cc3365b432db838cb92116b2c2,355ad2e8ea9578ea521e20f26a66018095b546ec,0,554,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'attribute': 23, 'identifier': 56, '.': 23, '=': 4, 'call': 9, 'argument_list': 9, '(': 10, 'integer': 1, ')': 10, ',': 7, 'keyword_argument': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, ':': 8, 'block': 2, 'dictionary': 1, '{': 1, 'pair': 6, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'subscript': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 13, 'token_count': 157, 'name': '__init__', 'long_name': '__init__( self , labels : Sequence [ int ] , noiseLabel : int , defaultMetrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 19, 'end_line': 35, 'full_parameters': ['self', ' labels : Sequence [ int ]', ' noiseLabel : int', ' defaultMetrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_clustering.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8720182993854689,0.876829157834348,"(tensor([0.9899]), tensor([0.9969]), tensor([0.9934]), tensor([0.9962]))"
"193     """"""
194     Base class for models that map dataframes to predictions and can be fitted on dataframes
195     """"""
196     def __init__(self, checkInputColumns=True):
197         """"""
198 
199         :param checkInputColumns: Whether to check if the input column list (after feature generation)
200             during inference coincides with the input column list during fit.
201             This should be disabled if feature generation is not performed by the model itself,
202             e.g. in ensemble models.
203         """"""
204         super().__init__()
205         self._isFitted = False
206         self._predictedVariableNames = None
207         self._modelInputVariableNames = None
208         self._modelOutputVariableNames = [""UNKNOWN""]
209         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = _IdentityDFT()
210         self.checkInputColumns = checkInputColumns
211 
212     def withTargetTransformer(self, targetTransformer: InvertibleDataFrameTransformer) -> __qualname__:
","195     """"""
196     Base class for models that map dataframes to predictions and can be fitted on dataframes
197     """"""
198     def __init__(self, checkInputColumns=True):
199         """"""
200 
201         :param checkInputColumns: Whether to check if the input column list (after feature generation)
202             during inference coincides with the input column list during fit.
203             This should be disabled if feature generation is not performed by the model itself,
204             e.g. in ensemble models.
205         """"""
206         super().__init__()
207         self._isFitted = False
208         self._predictedVariableNames = None
209         self._modelInputVariableNames = None
210         self._modelOutputVariableNames = [""UNKNOWN""]
211         self._targetTransformer: InvertibleDataFrameTransformer = _IdentityDFT()
212         self.checkInputColumns = checkInputColumns
213 
214     def withTargetTransformer(self, targetTransformer: InvertibleDataFrameTransformer) -> __qualname__:
","Before: 209
After: 211",fix vector_model and predictor_model,Minor fix in setting featuregen,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,d52fbc080e368896bf99497137efd49ec98a3476,ec3a023538b711e0ae4cfb8b94a6a27bc83e7702,0,1362,"{'module': 1, 'expression_statement': 9, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'function_definition': 1, 'def': 1, 'identifier': 21, 'parameters': 1, '(': 4, ',': 1, 'default_parameter': 1, '=': 7, 'true': 1, ')': 4, ':': 2, 'block': 1, 'call': 3, 'attribute': 7, 'argument_list': 3, '.': 7, 'assignment': 6, 'false': 1, 'none': 2, 'list': 1, '[': 2, ']': 2, 'type': 2, 'generic_type': 1, 'type_parameter': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 21, 'name': '_generate', 'long_name': '_generate( self , df : pd . DataFrame , ctx = None )', 'start_line': 19, 'end_line': 20, 'full_parameters': ['self', ' df : pd . DataFrame', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 21, 'name': '_generate', 'long_name': '_generate( self , df : pd . DataFrame , ctx = None )', 'start_line': 19, 'end_line': 20, 'full_parameters': ['self', ' df : pd . DataFrame', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6950518125220214,0.6884344626771058,"(tensor([0.9877]), tensor([0.9842]), tensor([0.9859]), tensor([0.9845]))"
"1 import collections
2 import typing
3 
4 import datetime
5 import logging
6 from abc import ABC, abstractmethod
","1 import collections
2 import datetime
3 import logging
4 import typing
5 from abc import ABC, abstractmethod
6 from typing import Callable, List, Iterable, Optional
7 
8 import numpy as np
","Before: 2, 3
After: 4",fix typo in nearest_neighbors.py,Breaking Change: added .isFitted and .summary methods to feature generators,https://github.com/opcode81/sensAI,src/sensai/nearest_neighbors.py,0f7a0069693c86d8415c5e631d537ccc9034a02e,ab576020953e55aa095f1f4719a14525d4db8a99,0,10,"{'module': 1, 'import_statement': 4, 'import': 4, 'dotted_name': 4, 'identifier': 4}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 23, 'end_line': 26, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 22, 'end_line': 25, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.28854640540579785,0.2657962151991068,"(tensor([0.8723]), tensor([0.9467]), tensor([0.9080]), tensor([0.9387]))"
"2 import typing
3 
4 import datetime
5 import logging
6 from abc import ABC, abstractmethod
7 from typing import Callable, List, Iterable
8 
9 import numpy as np
10 import pandas as pd
11 
","1 import collections
2 import datetime
3 import logging
4 import typing
5 from abc import ABC, abstractmethod
6 from typing import Callable, List, Iterable, Optional
7 
8 import numpy as np
9 import pandas as pd
10 
","Before: 7
After: 6",fix typo in nearest_neighbors.py,Breaking Change: added .isFitted and .summary methods to feature generators,https://github.com/opcode81/sensAI,src/sensai/nearest_neighbors.py,0f7a0069693c86d8415c5e631d537ccc9034a02e,ab576020953e55aa095f1f4719a14525d4db8a99,0,41,"{'module': 1, 'import_statement': 5, 'import': 7, 'dotted_name': 12, 'identifier': 14, 'import_from_statement': 2, 'from': 2, ',': 3, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 23, 'end_line': 26, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 22, 'end_line': 25, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.468573804413532,0.4633204748192575,"(tensor([0.9594]), tensor([0.9690]), tensor([0.9642]), tensor([0.9680]))"
"8 
9 import numpy as np
10 import pandas as pd
11 
12 from . import distance_metric, util, data_transformation
13 from .vector_model import VectorClassificationModel, VectorRegressionModel
14 from .distance_metric import DistanceMetric
15 from .featuregen import FeatureGeneratorFromNamedTuples
16 from .util.string import objectRepr
17 from .util.typing import PandasNamedTuple
","11 from . import distance_metric, util, data_transformation
12 from .distance_metric import DistanceMetric
13 from .featuregen import FeatureGeneratorFromNamedTuples
14 from .util.string import objectRepr
15 from .util.typing import PandasNamedTuple
16 from .vector_model import VectorClassificationModel, VectorRegressionModel
17 
18 _log = logging.getLogger(__name__)
19 
20 
","Before: 13
After: 16",fix typo in nearest_neighbors.py,Breaking Change: added .isFitted and .summary methods to feature generators,https://github.com/opcode81/sensAI,src/sensai/nearest_neighbors.py,0f7a0069693c86d8415c5e631d537ccc9034a02e,ab576020953e55aa095f1f4719a14525d4db8a99,0,82,"{'module': 1, 'import_statement': 2, 'import': 7, 'aliased_import': 2, 'dotted_name': 14, 'identifier': 17, 'as': 2, 'import_from_statement': 5, 'from': 5, 'relative_import': 5, 'import_prefix': 5, '.': 6, ',': 3}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 23, 'end_line': 26, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 22, 'end_line': 25, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4722349545161163,0.44600748840622206,"(tensor([0.9264]), tensor([0.9166]), tensor([0.9215]), tensor([0.9175]))"
"342     at construction. The feature name ""distance"", which indicates the distance of the neighbor to the data point is
343     always present.
344     """"""
345     def __init__(self, numNeighbors: int,
346             neighborAttributes: typing.List[str],
347             distanceMetric: distance_metric.DistanceMetric,
348             neighborProviderFactory: typing.Callable[[pd.DataFrame], NeighborProvider] = AllNeighborsProvider,
349             cache: util.cache.PersistentKeyValueCache = None,
350             categoricalFeatureNames: typing.Sequence[str] = (),
351             normalisationRules: typing.Sequence[data_transformation.DFTNormalisation.Rule] = ()):
352         """"""
353         :param numNeighbors: the number of neighbors for to generate features
354         :param neighborAttributes: the attributes of the neighbor's named tuple to include as features (in addition to ""distance"")
355         :param distanceMetric: the distance metric defining which neighbors are near
356         :param neighborProviderFactory: a factory for the creation of neighbor provider
357         :param cache: an optional key-value cache in which feature values are stored by data point identifier (as given by the DataFrame's index)
358         """"""
359         super().__init__(cache=cache, categoricalFeatureNames=categoricalFeatureNames, normalisationRules=normalisationRules)
360         self.neighborAttributes = neighborAttributes
361         self.distanceMetric = distanceMetric
362         self.neighborProviderFactory = neighborProviderFactory
363         self.numNeighbors = numNeighbors
364         self._knnFinder: KNearestNeighboursFinder = None
365         self._trainX = None
366 
367     def _generate(self, df: pd.DataFrame, ctx=None):
","341     at construction. The feature name ""distance"", which indicates the distance of the neighbor to the data point is
342     always present.
343     """"""
344     def __init__(self, numNeighbors: int,
345             neighborAttributes: typing.List[str],
346             distanceMetric: distance_metric.DistanceMetric,
347             neighborProviderFactory: typing.Callable[[pd.DataFrame], NeighborProvider] = AllNeighborsProvider,
348             cache: util.cache.PersistentKeyValueCache = None,
349             categoricalFeatureNames: typing.Sequence[str] = (),
350             normalisationRules: typing.Sequence[data_transformation.DFTNormalisation.Rule] = ()):
351         """"""
352         :param numNeighbors: the number of neighbors for to generate features
353         :param neighborAttributes: the attributes of the neighbor's named tuple to include as features (in addition to ""distance"")
354         :param distanceMetric: the distance metric defining which neighbors are near
355         :param neighborProviderFactory: a factory for the creation of neighbor provider
356         :param cache: an optional key-value cache in which feature values are stored by data point identifier (as given by the DataFrame's index)
357         """"""
358         super().__init__(cache=cache, categoricalFeatureNames=categoricalFeatureNames, normalisationRules=normalisationRules)
359         self.neighborAttributes = neighborAttributes
360         self.distanceMetric = distanceMetric
361         self.neighborProviderFactory = neighborProviderFactory
362         self.numNeighbors = numNeighbors
363         self._knnFinder: Optional[KNearestNeighboursFinder] = None
364         self._trainX = None
365 
366     def _generate(self, df: pd.DataFrame, ctx=None):
","Before: 364
After: 363",fix typo in nearest_neighbors.py,Breaking Change: added .isFitted and .summary methods to feature generators,https://github.com/opcode81/sensAI,src/sensai/nearest_neighbors.py,0f7a0069693c86d8415c5e631d537ccc9034a02e,ab576020953e55aa095f1f4719a14525d4db8a99,0,3515,"{'module': 1, 'ERROR': 14, 'identifier': 86, 'attribute': 1, '.': 2, 'expression_statement': 6, 'string': 3, 'string_start': 6, 'string_content': 3, 'string_end': 3, ',': 1, 'comparison_operator': 1, 'is': 1, ':': 10, 'assignment': 4, 'type': 4, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, 'call': 2, 'argument_list': 2, '(': 2, ')': 2, 'for': 1, '-': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 23, 'end_line': 26, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 22, 'end_line': 25, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7598296311116123,0.7604526038536826,"(tensor([0.9923]), tensor([0.9935]), tensor([0.9929]), tensor([0.9934]))"
"380                 result[f""n{i}_{attr}""] = getattr(neighbor.value, attr)
381         return result
382 
383     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, ctx=None):
384         self._trainX = X
","379                 result[f""n{i}_{attr}""] = getattr(neighbor.value, attr)
380         return result
381 
382     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, ctx=None):
383         self._trainX = X
","Before: 383
After: 382",fix typo in nearest_neighbors.py,Breaking Change: added .isFitted and .summary methods to feature generators,https://github.com/opcode81/sensAI,src/sensai/nearest_neighbors.py,0f7a0069693c86d8415c5e631d537ccc9034a02e,ab576020953e55aa095f1f4719a14525d4db8a99,0,3784,"{'module': 1, 'expression_statement': 1, 'assignment': 1, 'subscript': 1, 'identifier': 17, '[': 1, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 2, '{': 2, '}': 2, 'string_end': 1, ']': 1, '=': 2, 'call': 1, 'argument_list': 1, '(': 2, 'attribute': 3, '.': 3, ',': 4, ')': 2, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 3, 'type': 2, 'default_parameter': 1, 'none': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 23, 'end_line': 26, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 30, 'name': '__init__', 'long_name': '__init__( self , value : PandasNamedTuple , distance : float )', 'start_line': 22, 'end_line': 25, 'full_parameters': ['self', ' value : PandasNamedTuple', ' distance : float'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/nearest_neighbors.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7010847790743447,0.693644741177963,"(tensor([0.9812]), tensor([0.9833]), tensor([0.9823]), tensor([0.9831]))"
"1 import numpy as np
2 import pandas as pd
3 import pytest
4 
5 from sensai.featuregen import FeatureGeneratorFlattenColumns, FeatureGeneratorTakeColumns, flattenedFeatureGenerator
6 
7 
8 def test_take_columns():
9     inputDf = pd.DataFrame({""a"": [1], ""b"": [2], ""c"": [3]})
","1 import numpy as np
2 import pandas as pd
3 import pytest
4 
5 from sensai.featuregen import FeatureGeneratorFlattenColumns, FeatureGeneratorTakeColumns, flattenedFeatureGenerator, \
6     FeatureGenerator, RuleBasedFeatureGenerator, MultiFeatureGenerator, ChainedFeatureGenerator
7 
8 
9 def test_take_columns():
","Before: 5
After: 5, 6",add more tests for featuregen,Breaking Change: added .isFitted and .summary methods to feature generators,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,0f7a0069693c86d8415c5e631d537ccc9034a02e,ab576020953e55aa095f1f4719a14525d4db8a99,0,35,"{'module': 1, 'import_statement': 3, 'import': 4, 'aliased_import': 2, 'dotted_name': 7, 'identifier': 11, 'as': 2, 'import_from_statement': 1, 'from': 1, '.': 1, ',': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ')': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 8, 'end_line': 19, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4004078865574581,0.39465265158467555,"(tensor([0.9462]), tensor([0.9247]), tensor([0.9353]), tensor([0.9269]))"
"4 models. Hence the name of the module and of the central base class VectorModel.
5 """"""
6 
7 import logging
8 from abc import ABC, abstractmethod
9 from typing import Sequence, List, Any, Optional, Union, TypeVar, Type
10 
11 import numpy as np
12 import pandas as pd
13 
","4 models. Hence the name of the module and of the central base class VectorModel.
5 """"""
6 
7 import logging
8 from abc import ABC, abstractmethod
9 from typing import List, Any, Optional, Union, TypeVar, Type
10 
11 import numpy as np
12 import pandas as pd
13 
","Before: 9
After: 9",fix vector_model.py -- a/src/sensai/vector_model.py,Made DFTChain signature compatible with the Multifeaturegen signature,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,158d7ae3ee2e93bf8426915c40b7a735b09e3f6c,7088cbbe3dd309f0c5e889e7056cdee64187f33a,0,46,"{'module': 1, 'expression_statement': 1, 'boolean_operator': 1, 'attribute': 1, 'identifier': 29, '.': 2, 'ERROR': 2, 'and': 1, 'class': 1, 'string_start': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 13, 'import': 4, ',': 7, 'import_statement': 2, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 38, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9408790268768904,0.9407316114353313,"(tensor([0.9974]), tensor([0.9942]), tensor([0.9958]), tensor([0.9945]))"
"25     """"""
26     Base class for models that map data frames to predictions
27     """"""
28     def __init__(self):
29         self._featureGenerator: Optional[FeatureGenerator] = None
30         self._inputTransformerChain = DataFrameTransformerChain(())
31         self._outputTransformerChain = DataFrameTransformerChain(())
32         self._name = None
33 
34     @abstractmethod
","25     """"""
26     Base class for models that map data frames to predictions
27     """"""
28     def __init__(self):
29         self._featureGenerator: Optional[FeatureGenerator] = None
30         self._inputTransformerChain = DataFrameTransformerChain()
31         self._outputTransformerChain = DataFrameTransformerChain()
32         self._name = None
33 
34     @abstractmethod
","Before: 30, 31
After: 30, 31",fix vector_model.py -- a/src/sensai/vector_model.py,Made DFTChain signature compatible with the Multifeaturegen signature,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,158d7ae3ee2e93bf8426915c40b7a735b09e3f6c,7088cbbe3dd309f0c5e889e7056cdee64187f33a,0,184,"{'module': 1, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'identifier': 14, 'parameters': 1, '(': 5, ')': 5, ':': 2, 'block': 1, 'assignment': 4, 'attribute': 4, '.': 4, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '=': 4, 'none': 2, 'call': 2, 'argument_list': 2, 'tuple': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 38, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8842674144015862,0.8842674144015862,"(tensor([0.9987]), tensor([0.9987]), tensor([0.9987]), tensor([0.9987]))"
"53                 result.append(x)
54         return result
55 
56     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
57         """"""
58         Makes the model use the given input transformers.
59 
60         :param inputTransformers: DataFrameTransformers for the transformation of inputs
61         :return: self
62         """"""
63         self._inputTransformerChain = DataFrameTransformerChain(self._flattened(inputTransformers))
64         return self
65 
66     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
","43     def isRegressionModel(self) -> bool:
44         pass
45 
46     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
47         """"""
48         Makes the model use the given input transformers.
49 
50         :param inputTransformers: DataFrameTransformers for the transformation of inputs
51         :return: self
52         """"""
53         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
54         return self
55 
56     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
","Before: 63
After: 53",fix vector_model.py -- a/src/sensai/vector_model.py,Made DFTChain signature compatible with the Multifeaturegen signature,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,158d7ae3ee2e93bf8426915c40b7a735b09e3f6c,7088cbbe3dd309f0c5e889e7056cdee64187f33a,0,432,"{'module': 1, 'expression_statement': 3, 'call': 3, 'attribute': 3, 'identifier': 19, '.': 3, 'argument_list': 3, '(': 4, ')': 4, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, '*': 1, ':': 2, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 38, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6538193984131555,0.6333593387089987,"(tensor([0.9520]), tensor([0.9466]), tensor([0.9493]), tensor([0.9472]))"
"63         self._inputTransformerChain = DataFrameTransformerChain(self._flattened(inputTransformers))
64         return self
65 
66     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
67         """"""
68         Makes the model use the given output transformers.
69 
70         :param outputTransformers: DataFrameTransformers for the transformation of outputs
71             (after the model has been applied)
72         :return: self
73         """"""
74         self._outputTransformerChain = DataFrameTransformerChain(self._flattened(outputTransformers))
75         return self
76 
77     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","53         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
54         return self
55 
56     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
57         """"""
58         Makes the model use the given output transformers.
59 
60         :param outputTransformers: DataFrameTransformers for the transformation of outputs
61             (after the model has been applied)
62         :return: self
63         """"""
64         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
65         return self
66 
67     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","Before: 74
After: 64",fix vector_model.py -- a/src/sensai/vector_model.py,Made DFTChain signature compatible with the Multifeaturegen signature,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,158d7ae3ee2e93bf8426915c40b7a735b09e3f6c,7088cbbe3dd309f0c5e889e7056cdee64187f33a,0,496,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 4, 'identifier': 22, '.': 4, '=': 2, 'call': 4, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, '*': 1, ':': 2, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 38, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6377991544190379,0.6241840948307378,"(tensor([0.9732]), tensor([0.9569]), tensor([0.9650]), tensor([0.9585]))"
"32     def test_ruleBasedAlwaysFitted(self):
33         assert self.RuleBasedTestDFT().isFitted()
34 
35     def test_emptyChainFitted(self):
36         assert DataFrameTransformerChain([]).isFitted()
37 
38     def test_combinationFittedIffEachMemberFitted(self):
","32     def test_ruleBasedAlwaysFitted(self):
33         assert self.RuleBasedTestDFT().isFitted()
34 
35     def test_emptyChainFitted(self):
36         assert DataFrameTransformerChain().isFitted()
37 
38     def test_combinationFittedIffEachMemberFitted(self):
","Before: 36
After: 36",fix test_emptychainfitted and test_combinationfitted,Made DFTChain signature compatible with the Multifeaturegen signature,https://github.com/opcode81/sensAI,tests/base/test_data_transformation.py,158d7ae3ee2e93bf8426915c40b7a735b09e3f6c,7088cbbe3dd309f0c5e889e7056cdee64187f33a,0,440,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 9, 'parameters': 2, '(': 6, ')': 6, ':': 2, 'block': 2, 'assert_statement': 2, 'assert': 2, 'call': 4, 'attribute': 3, '.': 3, 'argument_list': 4, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9106520295213664,0.9155859364189057,"(tensor([0.9965]), tensor([0.9965]), tensor([0.9965]), tensor([0.9965]))"
"35     def test_emptyChainFitted(self):
36         assert DataFrameTransformerChain([]).isFitted()
37 
38     def test_combinationFittedIffEachMemberFitted(self):
39         # if one of the fgens is not fitted, the combination is not fitted either
40         dftChain = DataFrameTransformerChain([self.TestDFT(), self.RuleBasedTestDFT()])
41         assert dftChain.dataFrameTransformers[1].isFitted()
42         assert not dftChain.isFitted()
43         dftChain.fit(self.testdf)
44         assert dftChain.isFitted()
45         assert dftChain.dataFrameTransformers[0].isFitted()
46 
47         # if all fgens are fitted, the combination is also fitted, even if fit was not called
48         dftChain = DataFrameTransformerChain([self.RuleBasedTestDFT(), self.RuleBasedTestDFT()])
49         assert dftChain.isFitted()
","35     def test_emptyChainFitted(self):
36         assert DataFrameTransformerChain().isFitted()
37 
38     def test_combinationFittedIffEachMemberFitted(self):
39         # if one of the fgens is not fitted, the combination is not fitted either
40         dftChain = DataFrameTransformerChain(self.TestDFT(), self.RuleBasedTestDFT())
41         assert dftChain.dataFrameTransformers[1].isFitted()
42         assert not dftChain.isFitted()
43         dftChain.fit(self.testdf)
44         assert dftChain.isFitted()
45         assert dftChain.dataFrameTransformers[0].isFitted()
46 
47         # if all fgens are fitted, the combination is also fitted, even if fit was not called
48         dftChain = DataFrameTransformerChain([self.RuleBasedTestDFT(), self.RuleBasedTestDFT()])
49         assert dftChain.isFitted()
","Before: 40
After: 40",fix test_emptychainfitted and test_combinationfitted,Made DFTChain signature compatible with the Multifeaturegen signature,https://github.com/opcode81/sensAI,tests/base/test_data_transformation.py,158d7ae3ee2e93bf8426915c40b7a735b09e3f6c,7088cbbe3dd309f0c5e889e7056cdee64187f33a,0,479,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 32, 'parameters': 2, '(': 15, ')': 15, ':': 2, 'block': 2, 'assert_statement': 5, 'assert': 5, 'call': 13, 'attribute': 13, 'argument_list': 13, 'list': 3, '[': 5, ']': 5, '.': 13, 'comment': 2, 'expression_statement': 3, 'assignment': 2, '=': 2, ',': 2, 'subscript': 2, 'integer': 2, 'not_operator': 1, 'not': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9341625609190263,0.9356850321300673,"(tensor([0.9964]), tensor([0.9964]), tensor([0.9964]), tensor([0.9964]))"
"1 import logging
2 import time
3 from typing import Sequence, TypeVar, List
4 
5 from . import cache
6 
7 T = TypeVar(""T"")
","1 import logging
2 import time
3 from typing import Sequence, TypeVar, List, Union
4 
5 from . import cache
6 
7 T = TypeVar(""T"")
","Before: 3
After: 3",add flattenarguments function,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,src/sensai/util/__init__.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,23,"{'module': 1, 'import_statement': 2, 'import': 4, 'dotted_name': 7, 'identifier': 7, 'import_from_statement': 2, 'from': 2, ',': 2, 'relative_import': 1, 'import_prefix': 1, '.': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': 'concatSequences', 'long_name': 'concatSequences( seqs : Sequence [ Sequence [ T ] ] )', 'start_line': 11, 'end_line': 15, 'full_parameters': ['seqs : Sequence [ Sequence [ T ] ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/__init__.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 35, 'name': 'concatSequences', 'long_name': 'concatSequences( seqs : Sequence [ Sequence [ T ] ] )', 'start_line': 11, 'end_line': 15, 'full_parameters': ['seqs : Sequence [ Sequence [ T ] ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/__init__.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8866029039778043,0.8851694265268515,"(tensor([0.9909]), tensor([0.9969]), tensor([0.9939]), tensor([0.9963]))"
"151             self._featureGenerator.fit(X, Y=Y, ctx=self)
152         self._inputTransformerChain.fit(X)
153 
154     def isFitted(self):
155         result = self._inputTransformerChain.isFitted()
156         if self.getFeatureGenerator() is not None:
157             result = result and self.getFeatureGenerator().isFitted()
158         return result
159 
160     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","157             self._featureGenerator.fit(X, Y=Y, ctx=self)
158         self._inputTransformerChain.fit(X)
159 
160     def isFitted(self):
161         return self._prePostProcessorsAreFitted()
162 
163     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 155, 156, 157, 158
After: 161",update vector_model.py for backwards compatibility,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,1,883,"{'module': 1, 'expression_statement': 4, 'call': 6, 'attribute': 9, 'identifier': 26, '.': 9, 'argument_list': 6, '(': 7, ',': 2, 'keyword_argument': 2, '=': 4, ')': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 2, 'block': 2, 'assignment': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'boolean_operator': 1, 'and': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.37763289809828915,0.36288664388150993,"(tensor([0.9294]), tensor([0.8893]), tensor([0.9089]), tensor([0.8931]))"
"180     """"""
181     Base class for models that map data frames to predictions and can be fitted on data frames
182     """"""
183     def __init__(self, checkInputColumns=True):
184         """"""
185         :param checkInputColumns: Whether to check if the input column list (after feature generation)
186             during inference coincides with the input column list during fit.
187             This should be disabled if feature generation is not performed by the model itself,
188             e.g. in ensemble models.
189         """"""
190         super().__init__()
191         self.__modelIsFitted = False
192         self._predictedVariableNames = None
193         self._modelInputVariableNames = None
194         self._modelOutputVariableNames = [""UNKNOWN""]
195         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = None
196         self.checkInputColumns = checkInputColumns
197 
198     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","183     """"""
184     Base class for models that map data frames to predictions and can be fitted on data frames
185     """"""
186     def __init__(self, checkInputColumns=True):
187         """"""
188         :param checkInputColumns: Whether to check if the input column list (after feature generation)
189             during inference coincides with the input column list during fit.
190             This should be disabled if feature generation is not performed by the model itself,
191             e.g. in ensemble models.
192         """"""
193         super().__init__()
194         self._modelIsFitted = False
195         self._predictedVariableNames = None
196         self._modelInputVariableNames = None
197         self._modelOutputVariableNames = [""UNKNOWN""]
198         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = None
199         self.checkInputColumns = checkInputColumns
200 
201     # for backwards compatibility with persisted models based on code prior to commit 7088cbbe
","Before: 191
After: 194, 201, 202, 203, 204, 205, 206",update vector_model.py for backwards compatibility,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,1,1117,"{'module': 1, 'expression_statement': 9, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'function_definition': 1, 'def': 1, 'identifier': 20, 'parameters': 1, '(': 3, ',': 1, 'default_parameter': 1, '=': 7, 'true': 1, ')': 3, ':': 2, 'block': 1, 'call': 2, 'attribute': 7, 'argument_list': 2, '.': 7, 'assignment': 6, 'false': 1, 'none': 3, 'list': 1, '[': 2, ']': 2, 'type': 2, 'generic_type': 1, 'type_parameter': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6182573461788271,0.6045880905767247,"(tensor([0.9404]), tensor([0.9595]), tensor([0.9499]), tensor([0.9575]))"
"213     def getTargetTransformer(self):
214         return self._targetTransformer
215 
216     def _prePostProcessorsAreFitted(self):
217         result = self._inputTransformerChain.isFitted() and self._outputTransformerChain.isFitted()
218         if self.getFeatureGenerator() is not None:
219             result = result and self.getFeatureGenerator().isFitted()
220         if self._targetTransformer is not None:
221             result = result and self._targetTransformer.isFitted()
222         return result
223 
224     def isFitted(self):
","222     def getTargetTransformer(self):
223         return self._targetTransformer
224 
225     def isFitted(self):
226         result = self._modelIsFitted and self._prePostProcessorsAreFitted()
227         if self._targetTransformer is not None:
228             result = result and self._targetTransformer.isFitted()
229         return result
230 
231     def _computeInputs(self, X: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","Before: 216, 217, 218, 219
After: 225, 226",update vector_model.py for backwards compatibility,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,1,1236,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 28, 'parameters': 2, '(': 8, ')': 8, ':': 4, 'block': 4, 'return_statement': 2, 'return': 2, 'attribute': 11, '.': 11, 'expression_statement': 3, 'assignment': 3, '=': 3, 'boolean_operator': 3, 'call': 6, 'argument_list': 6, 'and': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is not': 4, 'none': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.31016476947194216,0.2661461817164579,"(tensor([0.8542]), tensor([0.9022]), tensor([0.8776]), tensor([0.8972]))"
"258     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
259         pass
260 
261     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
262         """"""
263         Fits the model using the given data
264 
265         :param X: a data frame containing input data
266         :param Y: a data frame containing output data
267         :param fitPreprocessors: if False, the model's feature generator and input transformer will not be fitted.
268             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
269             an exception will be raised.
270         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
271             If it requires fitting, was not separately fit before and this option is set to False,
272             an exception will be raised.
273         """"""
274         log.info(f""Training {self.__class__.__name__}"")
275         self._predictedVariableNames = list(Y.columns)
276         X = self._computeInputs(X, Y=Y, fit=fitPreprocessors)
277         if self._targetTransformer is not None:
278             if fitTargetTransformer:
279                 Y = self._targetTransformer.fitApply(Y)
280             else:
281                 Y = self._targetTransformer.apply(Y)
282         self._modelInputVariableNames = list(X.columns)
283         self._modelOutputVariableNames = list(Y.columns)
284         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
285         self._fit(X, Y)
286         self.__modelIsFitted = True
287 
288     @abstractmethod
","262     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
263         pass
264 
265     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
266         """"""
267         Fits the model using the given data
268 
269         :param X: a data frame containing input data
270         :param Y: a data frame containing output data
271         :param fitPreprocessors: if False, the model's feature generator and input transformer will not be fitted.
272             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
273             an exception will be raised.
274         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
275             If it requires fitting, was not separately fit before and this option is set to False,
276             an exception will be raised.
277         """"""
278         log.info(f""Training {self.__class__.__name__}"")
279         self._predictedVariableNames = list(Y.columns)
280         X = self._computeInputs(X, Y=Y, fit=fitPreprocessors)
281         if self._targetTransformer is not None:
282             if fitTargetTransformer:
283                 Y = self._targetTransformer.fitApply(Y)
284             else:
285                 Y = self._targetTransformer.apply(Y)
286         self._modelInputVariableNames = list(X.columns)
287         self._modelOutputVariableNames = list(Y.columns)
288         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
289         self._fit(X, Y)
290         self._modelIsFitted = True
291 
292     @abstractmethod
","Before: 286
After: 290",update vector_model.py for backwards compatibility,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,1,2032,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 83, 'parameters': 2, '(': 14, ',': 8, 'typed_parameter': 3, ':': 8, 'type': 4, 'attribute': 29, '.': 29, ')': 14, '->': 1, 'block': 5, 'pass_statement': 1, 'pass': 1, 'default_parameter': 2, '=': 11, 'true': 3, 'expression_statement': 11, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 12, 'argument_list': 12, 'interpolation': 5, '{': 5, '}': 5, 'assignment': 7, 'keyword_argument': 2, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'else_clause': 1, 'else': 1, 'list_comprehension': 1, '[': 2, 'binary_operator': 2, '+': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.788164984915429,0.7866346434529092,"(tensor([0.9854]), tensor([0.9851]), tensor([0.9853]), tensor([0.9852]))"
"37     inputDf = pd.DataFrame({""a"": [np.array([1, 2])], ""b"": [np.array([5, 6])]})
38     fgen1 = flattenedFeatureGenerator(FeatureGeneratorTakeColumns(""a""))
39     assert fgen1.generate(inputDf).equals(pd.DataFrame({""a_0"": [1], ""a_1"": [2]}))
40 
41 
42 class TestFgenBasics:
43     class TestFgen(FeatureGenerator):
44         def _fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, ctx=None):
45             pass
46 
","37     inputDf = pd.DataFrame({""a"": [np.array([1, 2])], ""b"": [np.array([5, 6])]})
38     fgen1 = flattenedFeatureGenerator(FeatureGeneratorTakeColumns(""a""))
39     assert fgen1.generate(inputDf).equals(pd.DataFrame({""a_0"": [1], ""a_1"": [2]}))
40 
41 
42 class TestFgen(FeatureGenerator):
43     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, ctx=None):
44         pass
45 
46     def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:
","Before: 42, 43, 44, 45
After: 42, 43, 44, 45, 46, 47",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,731,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 28, '=': 4, 'call': 8, 'attribute': 8, '.': 8, 'argument_list': 9, '(': 10, 'dictionary': 2, '{': 2, 'pair': 4, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, ':': 9, 'list': 6, '[': 6, 'integer': 6, ',': 7, ']': 6, ')': 10, '}': 2, 'assert_statement': 1, 'assert': 1, 'class_definition': 2, 'class': 2, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 2, 'typed_default_parameter': 1, 'none': 2, 'default_parameter': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8370515142840119,0.8327105877718507,"(tensor([0.9754]), tensor([0.9787]), tensor([0.9770]), tensor([0.9784]))"
"44         def _fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, ctx=None):
45             pass
46 
47         def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:
48             return df
49 
50     class RuleBasedTestFgen(RuleBasedFeatureGenerator):
","45 
46     def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:
47         return df
48 
49 
50 class RuleBasedTestFgen(RuleBasedFeatureGenerator):
51     def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:
52         return df
53 
54 
","Before: 47, 48, 50, 51, 52
After: 50, 51, 52",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,805,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 18, 'parameters': 2, '(': 2, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 4, 'attribute': 4, '.': 4, 'typed_default_parameter': 1, '=': 3, 'none': 3, 'default_parameter': 2, ')': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, '->': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.524681934720105,0.48812720564646983,"(tensor([0.9397]), tensor([0.9324]), tensor([0.9361]), tensor([0.9332]))"
"49 
50     class RuleBasedTestFgen(RuleBasedFeatureGenerator):
51         def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:
52             return df
53 
54     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
55 
56     def test_basicProperties(self):
57         testfgen = self.TestFgen()
58 
","49 
50 class RuleBasedTestFgen(RuleBasedFeatureGenerator):
51     def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:
52         return df
53 
54 
55 class TestFgenBasics:
56     def __init__(self):
57         self.testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
58 
","Before: 54
After: 54, 55, 56, 57",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,889,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 19, 'argument_list': 3, '(': 5, ')': 5, ':': 6, 'block': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 5, 'typed_parameter': 1, 'type': 2, 'attribute': 4, '.': 4, 'default_parameter': 1, '=': 3, 'none': 1, '->': 1, 'return_statement': 1, 'return': 1, 'expression_statement': 2, 'assignment': 2, 'call': 2, 'dictionary': 1, '{': 1, 'pair': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'list': 2, '[': 2, 'integer': 4, ']': 2, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8146479804725766,0.8055369909893902,"(tensor([0.9577]), tensor([0.9529]), tensor([0.9553]), tensor([0.9534]))"
"53 
54     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
55 
56     def test_basicProperties(self):
57         testfgen = self.TestFgen()
58 
59         assert not testfgen.isFitted()
60         assert testfgen.getGeneratedColumnNames() is None
61         testfgen.fit(self.testdf)
62         assert testfgen.isFitted()
63         testfgen.generate(self.testdf)
64         assert set(testfgen.getGeneratedColumnNames()) == {""foo"", ""bar""}
65 
66     def test_ruleBasedAlwaysFitted(self):
","56     def __init__(self):
57         self.testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
58 
59     def test_basicProperties(self):
60         testfgen = TestFgen()
61 
62         assert not testfgen.isFitted()
63         assert testfgen.getGeneratedColumnNames() is None
64         testfgen.fit(self.testdf)
65         assert testfgen.isFitted()
66         testfgen.generate(self.testdf)
67         assert set(testfgen.getGeneratedColumnNames()) == {""foo"", ""bar""}
68     
69     @pytest.mark.parametrize(""fgen"", [TestFgen(), RuleBasedTestFgen(), MultiFeatureGenerator(TestFgen()),
","Before: 57
After: 60",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,910,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'identifier': 25, '=': 2, 'call': 9, 'attribute': 10, '.': 10, 'argument_list': 9, '(': 10, 'dictionary': 1, '{': 2, 'pair': 2, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ':': 3, 'list': 2, '[': 2, 'integer': 4, ',': 4, ']': 2, '}': 2, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, 'block': 1, 'assert_statement': 4, 'assert': 4, 'not_operator': 1, 'not': 1, 'comparison_operator': 2, 'is': 1, 'none': 1, '==': 1, 'set': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.565782581078727,0.5714513840766686,"(tensor([0.9180]), tensor([0.9578]), tensor([0.9375]), tensor([0.9537]))"
"63         testfgen.generate(self.testdf)
64         assert set(testfgen.getGeneratedColumnNames()) == {""foo"", ""bar""}
65 
66     def test_ruleBasedAlwaysFitted(self):
67         assert self.RuleBasedTestFgen().isFitted()
68 
69     def test_emptyCombinationsRaiseError(self):
","73         fgen.setName(""bar"")
74         assert fgen.getName() == ""bar""
75 
76     def test_ruleBasedAlwaysFitted(self):
77         assert RuleBasedTestFgen().isFitted()
78 
79     def test_emptyChainRaisesError(self):
","Before: 67
After: 77",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,1026,"{'module': 1, 'expression_statement': 1, 'call': 5, 'attribute': 5, 'identifier': 12, '.': 5, 'argument_list': 5, '(': 6, ')': 6, 'assert_statement': 2, 'assert': 2, 'comparison_operator': 1, '==': 1, 'set': 1, '{': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ',': 1, '}': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2793047373973851,0.20347140890090284,"(tensor([0.9254]), tensor([0.8889]), tensor([0.9068]), tensor([0.8924]))"
"66     def test_ruleBasedAlwaysFitted(self):
67         assert self.RuleBasedTestFgen().isFitted()
68 
69     def test_emptyCombinationsRaiseError(self):
70         with pytest.raises(ValueError):
71             MultiFeatureGenerator()
72         with pytest.raises(ValueError):
73             ChainedFeatureGenerator()
74 
75     def test_combinationFittedIffEachMemberFitted(self):
","76     def test_ruleBasedAlwaysFitted(self):
77         assert RuleBasedTestFgen().isFitted()
78 
79     def test_emptyChainRaisesError(self):
80         with pytest.raises(ValueError):
81             ChainedFeatureGenerator()
82 
83     def test_combinationFittedIffEachMemberFitted(self):
","Before: 69, 70, 71
After: 79",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,1035,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 15, 'parameters': 2, '(': 8, ')': 8, ':': 4, 'block': 4, 'assert_statement': 1, 'assert': 1, 'call': 6, 'attribute': 4, '.': 4, 'argument_list': 6, 'with_statement': 2, 'with': 2, 'with_clause': 2, 'with_item': 2, 'expression_statement': 2}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3836370241612427,0.2956451674750747,"(tensor([0.9479]), tensor([0.9366]), tensor([0.9422]), tensor([0.9377]))"
"72         with pytest.raises(ValueError):
73             ChainedFeatureGenerator()
74 
75     def test_combinationFittedIffEachMemberFitted(self):
76         # if one of the fgens is not fitted, the combination is not fitted either
77         multifgen = MultiFeatureGenerator(self.TestFgen(), self.RuleBasedTestFgen())
78         chainfgen = ChainedFeatureGenerator(self.TestFgen(), self.RuleBasedTestFgen())
79         assert chainfgen.featureGenerators[1].isFitted() and multifgen.featureGenerators[1].isFitted()
80         assert not multifgen.isFitted() and not chainfgen.isFitted()
81         chainfgen.fit(self.testdf)
82         multifgen.fit(self.testdf)
83         assert multifgen.isFitted() and chainfgen.isFitted()
84         assert chainfgen.featureGenerators[0].isFitted() and multifgen.featureGenerators[0].isFitted()
85 
86         # if all fgens are fitted, the combination is also fitted, even if fit was not called
87         multifgen = MultiFeatureGenerator(self.RuleBasedTestFgen(), self.RuleBasedTestFgen())
88         chainfgen = ChainedFeatureGenerator(self.RuleBasedTestFgen(), self.RuleBasedTestFgen())
89         assert multifgen.isFitted() and chainfgen.isFitted()
90 
","80         with pytest.raises(ValueError):
81             ChainedFeatureGenerator()
82 
83     def test_combinationFittedIffEachMemberFitted(self):
84         # if one of the fgens is not fitted, the combination is not fitted either
85         multifgen = MultiFeatureGenerator(TestFgen(), RuleBasedTestFgen())
86         chainfgen = ChainedFeatureGenerator(TestFgen(), RuleBasedTestFgen())
87         assert chainfgen.featureGenerators[1].isFitted() and multifgen.featureGenerators[1].isFitted()
88         assert not multifgen.isFitted() and not chainfgen.isFitted()
89         chainfgen.fit(self.testdf)
90         multifgen.fit(self.testdf)
91         assert multifgen.isFitted() and chainfgen.isFitted()
92         assert chainfgen.featureGenerators[0].isFitted() and multifgen.featureGenerators[0].isFitted()
93 
94         # if all fgens are fitted, the combination is also fitted, even if fit was not called
95         multifgen = MultiFeatureGenerator(RuleBasedTestFgen(), RuleBasedTestFgen())
96         chainfgen = ChainedFeatureGenerator(RuleBasedTestFgen(), RuleBasedTestFgen())
97         assert multifgen.isFitted() and chainfgen.isFitted()
98 
","Before: 77, 78
After: 85, 86",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,1113,"{'module': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'call': 26, 'attribute': 27, 'identifier': 62, '.': 27, 'argument_list': 26, '(': 27, ')': 27, ':': 2, 'block': 2, 'expression_statement': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, 'comment': 2, 'assignment': 4, '=': 4, ',': 4, 'assert_statement': 5, 'assert': 5, 'boolean_operator': 5, 'subscript': 4, '[': 4, 'integer': 4, ']': 4, 'and': 5, 'not_operator': 2, 'not': 2}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6601174538021215,0.6359069556191933,"(tensor([0.9781]), tensor([0.9740]), tensor([0.9760]), tensor([0.9744]))"
"72         with pytest.raises(ValueError):
73             ChainedFeatureGenerator()
74 
75     def test_combinationFittedIffEachMemberFitted(self):
76         # if one of the fgens is not fitted, the combination is not fitted either
77         multifgen = MultiFeatureGenerator(self.TestFgen(), self.RuleBasedTestFgen())
78         chainfgen = ChainedFeatureGenerator(self.TestFgen(), self.RuleBasedTestFgen())
79         assert chainfgen.featureGenerators[1].isFitted() and multifgen.featureGenerators[1].isFitted()
80         assert not multifgen.isFitted() and not chainfgen.isFitted()
81         chainfgen.fit(self.testdf)
82         multifgen.fit(self.testdf)
83         assert multifgen.isFitted() and chainfgen.isFitted()
84         assert chainfgen.featureGenerators[0].isFitted() and multifgen.featureGenerators[0].isFitted()
85 
86         # if all fgens are fitted, the combination is also fitted, even if fit was not called
87         multifgen = MultiFeatureGenerator(self.RuleBasedTestFgen(), self.RuleBasedTestFgen())
88         chainfgen = ChainedFeatureGenerator(self.RuleBasedTestFgen(), self.RuleBasedTestFgen())
89         assert multifgen.isFitted() and chainfgen.isFitted()
90 
","80         with pytest.raises(ValueError):
81             ChainedFeatureGenerator()
82 
83     def test_combinationFittedIffEachMemberFitted(self):
84         # if one of the fgens is not fitted, the combination is not fitted either
85         multifgen = MultiFeatureGenerator(TestFgen(), RuleBasedTestFgen())
86         chainfgen = ChainedFeatureGenerator(TestFgen(), RuleBasedTestFgen())
87         assert chainfgen.featureGenerators[1].isFitted() and multifgen.featureGenerators[1].isFitted()
88         assert not multifgen.isFitted() and not chainfgen.isFitted()
89         chainfgen.fit(self.testdf)
90         multifgen.fit(self.testdf)
91         assert multifgen.isFitted() and chainfgen.isFitted()
92         assert chainfgen.featureGenerators[0].isFitted() and multifgen.featureGenerators[0].isFitted()
93 
94         # if all fgens are fitted, the combination is also fitted, even if fit was not called
95         multifgen = MultiFeatureGenerator(RuleBasedTestFgen(), RuleBasedTestFgen())
96         chainfgen = ChainedFeatureGenerator(RuleBasedTestFgen(), RuleBasedTestFgen())
97         assert multifgen.isFitted() and chainfgen.isFitted()
98 
","Before: 87, 88
After: 95, 96",fix test_featuregen.py -- a/b/c,Added flattening for uniform signatures in featuregens and DFTs,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,b29b2cf75f11f15a3ecd22071f62d69eb44c829b,a9ee219c558211a7729d872e3190f4e826eaaf7a,0,1304,"{'module': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'call': 26, 'attribute': 27, 'identifier': 62, '.': 27, 'argument_list': 26, '(': 27, ')': 27, ':': 2, 'block': 2, 'expression_statement': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, 'comment': 2, 'assignment': 4, '=': 4, ',': 4, 'assert_statement': 5, 'assert': 5, 'boolean_operator': 5, 'subscript': 4, '[': 4, 'integer': 4, ']': 4, 'and': 5, 'not_operator': 2, 'not': 2}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6601174538021215,0.6359069556191933,"(tensor([0.9781]), tensor([0.9740]), tensor([0.9760]), tensor([0.9744]))"
"183     """"""
184     Base class for models that map data frames to predictions and can be fitted on data frames
185     """"""
186     def __init__(self, checkInputColumns=True):
187         """"""
188         :param checkInputColumns: Whether to check if the input column list (after feature generation)
189             during inference coincides with the input column list during fit.
190             This should be disabled if feature generation is not performed by the model itself,
191             e.g. in ensemble models.
192         """"""
193         super().__init__()
194         self._modelIsFitted = False
195         self._predictedVariableNames = None
196         self._modelInputVariableNames = None
197         self._modelOutputVariableNames = [""UNKNOWN""]
198         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = None
199         self.checkInputColumns = checkInputColumns
200 
201     # for backwards compatibility with persisted models based on code prior to commit 7088cbbe
","183     """"""
184     Base class for models that map data frames to predictions and can be fitted on data frames
185     """"""
186     def __init__(self, checkInputColumns=True):
187         """"""
188         :param checkInputColumns: Whether to check if the input column list (after feature generation)
189             during inference coincides with the input column list during fit.
190             This should be disabled if feature generation is not performed by the model itself,
191             e.g. in ensemble models.
192         """"""
193         super().__init__()
194         self._isFitted = False  # Note: this keeps track only of the actual model being fitted, not the pre/postprocessors
195         self._predictedVariableNames = None
196         self._modelInputVariableNames = None
197         self._modelOutputVariableNames = [""UNKNOWN""]
198         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = None
199         self.checkInputColumns = checkInputColumns
200 
201     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","Before: 194
After: 194",remove unnecessary code from vector_model,"Renamings and minor fixes, mostly across summary methods. Fixed broken log in Multifgen",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,f5a97bea3482af52a5d44b36fdfb5e9082fb7488,81daeb9deacbc17d3025f532fa63e826f7de2044,1,1149,"{'module': 1, 'expression_statement': 9, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'function_definition': 1, 'def': 1, 'identifier': 20, 'parameters': 1, '(': 3, ',': 1, 'default_parameter': 1, '=': 7, 'true': 1, ')': 3, ':': 2, 'block': 1, 'call': 2, 'attribute': 7, 'argument_list': 2, '.': 7, 'assignment': 6, 'false': 1, 'none': 3, 'list': 1, '[': 2, ']': 2, 'type': 2, 'generic_type': 1, 'type_parameter': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7844241723313548,0.7783515822129238,"(tensor([0.9392]), tensor([0.9501]), tensor([0.9446]), tensor([0.9490]))"
"222     def getTargetTransformer(self):
223         return self._targetTransformer
224 
225     def isFitted(self):
226         result = self._modelIsFitted and self._prePostProcessorsAreFitted()
227         if self._targetTransformer is not None:
228             result = result and self._targetTransformer.isFitted()
229         return result
230 
231     def _computeInputs(self, X: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","216     def getTargetTransformer(self):
217         return self._targetTransformer
218 
219     def isFitted(self):
220         result = self._isFitted and self._prePostProcessorsAreFitted()
221         if self._targetTransformer is not None:
222             result = result and self._targetTransformer.isFitted()
223         return result
224 
225     def _computeInputs(self, X: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","Before: 226
After: 220",remove unnecessary code from vector_model,"Renamings and minor fixes, mostly across summary methods. Fixed broken log in Multifgen",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,f5a97bea3482af52a5d44b36fdfb5e9082fb7488,81daeb9deacbc17d3025f532fa63e826f7de2044,1,1341,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 19, 'parameters': 2, '(': 4, ')': 4, ':': 3, 'block': 3, 'return_statement': 2, 'return': 2, 'attribute': 6, '.': 6, 'expression_statement': 2, 'assignment': 2, '=': 2, 'boolean_operator': 2, 'and': 2, 'call': 2, 'argument_list': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6716699872244011,0.6466468232660568,"(tensor([0.9773]), tensor([0.9750]), tensor([0.9762]), tensor([0.9752]))"
"262     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
263         pass
264 
265     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
266         """"""
267         Fits the model using the given data
268 
269         :param X: a data frame containing input data
270         :param Y: a data frame containing output data
271         :param fitPreprocessors: if False, the model's feature generator and input transformer will not be fitted.
272             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
273             an exception will be raised.
274         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
275             If it requires fitting, was not separately fit before and this option is set to False,
276             an exception will be raised.
277         """"""
278         log.info(f""Training {self.__class__.__name__}"")
279         self._predictedVariableNames = list(Y.columns)
280         X = self._computeInputs(X, Y=Y, fit=fitPreprocessors)
281         if self._targetTransformer is not None:
282             if fitTargetTransformer:
283                 Y = self._targetTransformer.fitApply(Y)
284             else:
285                 Y = self._targetTransformer.apply(Y)
286         self._modelInputVariableNames = list(X.columns)
287         self._modelOutputVariableNames = list(Y.columns)
288         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
289         self._fit(X, Y)
290         self._modelIsFitted = True
291 
292     @abstractmethod
","256     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
257         pass
258 
259     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
260         """"""
261         Fits the model using the given data
262 
263         :param X: a data frame containing input data
264         :param Y: a data frame containing output data
265         :param fitPreprocessors: if False, the model's feature generator and input transformer will not be fitted.
266             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
267             an exception will be raised.
268         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
269             If it requires fitting, was not separately fit before and this option is set to False,
270             an exception will be raised.
271         """"""
272         log.info(f""Training {self.__class__.__name__}"")
273         self._predictedVariableNames = list(Y.columns)
274         X = self._computeInputs(X, Y=Y, fit=fitPreprocessors)
275         if self._targetTransformer is not None:
276             if fitTargetTransformer:
277                 Y = self._targetTransformer.fitApply(Y)
278             else:
279                 Y = self._targetTransformer.apply(Y)
280         self._modelInputVariableNames = list(X.columns)
281         self._modelOutputVariableNames = list(Y.columns)
282         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
283         self._fit(X, Y)
284         self._isFitted = True
285 
286     @abstractmethod
","Before: 290
After: 284",remove unnecessary code from vector_model,"Renamings and minor fixes, mostly across summary methods. Fixed broken log in Multifgen",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,f5a97bea3482af52a5d44b36fdfb5e9082fb7488,81daeb9deacbc17d3025f532fa63e826f7de2044,1,2046,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 83, 'parameters': 2, '(': 14, ',': 8, 'typed_parameter': 3, ':': 8, 'type': 4, 'attribute': 29, '.': 29, ')': 14, '->': 1, 'block': 5, 'pass_statement': 1, 'pass': 1, 'default_parameter': 2, '=': 11, 'true': 3, 'expression_statement': 11, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 12, 'argument_list': 12, 'interpolation': 5, '{': 5, '}': 5, 'assignment': 7, 'keyword_argument': 2, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'else_clause': 1, 'else': 1, 'list_comprehension': 1, '[': 2, 'binary_operator': 2, '+': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7844624027350151,0.7823133891698932,"(tensor([0.9837]), tensor([0.9825]), tensor([0.9831]), tensor([0.9826]))"
"17 
18     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
19 
20     def test_basicProperties(self):
21         testdft = self.TestDFT()
22 
23         assert not testdft.isFitted()
24         assert testdft.summary()[""changeInColumnNames""] is None
25         testdft.fit(self.testdf)
26         assert testdft.isFitted()
27         assert all(testdft.apply(self.testdf) == pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
28         assert testdft.summary()[""changeInColumnNames""] is not None
29 
30         # testing apply with no change in columns
31         testdft.apply(pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
32         assert testdft.summary()[""changeInColumnNames""] == ""none""
33 
34     def test_ruleBasedAlwaysFitted(self):
","17 
18     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
19 
20     def test_basicProperties(self):
21         testdft = self.TestDFT()
22 
23         assert not testdft.isFitted()
24         assert testdft.info()[""changeInColumnNames""] is None
25         testdft.fit(self.testdf)
26         assert testdft.isFitted()
27         assert all(testdft.apply(self.testdf) == pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
28         assert testdft.info()[""changeInColumnNames""] is not None
29 
30         # testing apply with no change in columns
31         testdft.apply(pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
32         assert testdft.info()[""changeInColumnNames""] == ""none""
33 
34     def test_ruleBasedAlwaysFitted(self):
","Before: 24
After: 24",update test_data_transformation.py to use testdft.info,"Renamings and minor fixes, mostly across summary methods. Fixed broken log in Multifgen",https://github.com/opcode81/sensAI,tests/base/test_data_transformation.py,f5a97bea3482af52a5d44b36fdfb5e9082fb7488,81daeb9deacbc17d3025f532fa63e826f7de2044,0,249,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'identifier': 33, '=': 2, 'call': 13, 'attribute': 14, '.': 14, 'argument_list': 13, '(': 14, 'dictionary': 3, '{': 3, 'pair': 6, 'string': 10, 'string_start': 10, 'string_content': 10, 'string_end': 10, ':': 7, 'list': 6, '[': 9, 'integer': 12, ',': 9, ']': 9, '}': 3, ')': 14, 'function_definition': 1, 'def': 1, 'parameters': 1, 'block': 1, 'assert_statement': 6, 'assert': 6, 'not_operator': 1, 'not': 1, 'comparison_operator': 4, 'subscript': 3, 'is': 1, 'none': 2, '==': 2, 'is not': 2, 'comment': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9571909027220098,0.9522400573005405,"(tensor([0.9956]), tensor([0.9956]), tensor([0.9956]), tensor([0.9956]))"
"17 
18     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
19 
20     def test_basicProperties(self):
21         testdft = self.TestDFT()
22 
23         assert not testdft.isFitted()
24         assert testdft.summary()[""changeInColumnNames""] is None
25         testdft.fit(self.testdf)
26         assert testdft.isFitted()
27         assert all(testdft.apply(self.testdf) == pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
28         assert testdft.summary()[""changeInColumnNames""] is not None
29 
30         # testing apply with no change in columns
31         testdft.apply(pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
32         assert testdft.summary()[""changeInColumnNames""] == ""none""
33 
34     def test_ruleBasedAlwaysFitted(self):
","17 
18     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
19 
20     def test_basicProperties(self):
21         testdft = self.TestDFT()
22 
23         assert not testdft.isFitted()
24         assert testdft.info()[""changeInColumnNames""] is None
25         testdft.fit(self.testdf)
26         assert testdft.isFitted()
27         assert all(testdft.apply(self.testdf) == pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
28         assert testdft.info()[""changeInColumnNames""] is not None
29 
30         # testing apply with no change in columns
31         testdft.apply(pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
32         assert testdft.info()[""changeInColumnNames""] == ""none""
33 
34     def test_ruleBasedAlwaysFitted(self):
","Before: 28
After: 28",update test_data_transformation.py to use testdft.info,"Renamings and minor fixes, mostly across summary methods. Fixed broken log in Multifgen",https://github.com/opcode81/sensAI,tests/base/test_data_transformation.py,f5a97bea3482af52a5d44b36fdfb5e9082fb7488,81daeb9deacbc17d3025f532fa63e826f7de2044,0,350,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'identifier': 33, '=': 2, 'call': 13, 'attribute': 14, '.': 14, 'argument_list': 13, '(': 14, 'dictionary': 3, '{': 3, 'pair': 6, 'string': 10, 'string_start': 10, 'string_content': 10, 'string_end': 10, ':': 7, 'list': 6, '[': 9, 'integer': 12, ',': 9, ']': 9, '}': 3, ')': 14, 'function_definition': 1, 'def': 1, 'parameters': 1, 'block': 1, 'assert_statement': 6, 'assert': 6, 'not_operator': 1, 'not': 1, 'comparison_operator': 4, 'subscript': 3, 'is': 1, 'none': 2, '==': 2, 'is not': 2, 'comment': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9571909027220098,0.9522400573005405,"(tensor([0.9956]), tensor([0.9956]), tensor([0.9956]), tensor([0.9956]))"
"17 
18     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
19 
20     def test_basicProperties(self):
21         testdft = self.TestDFT()
22 
23         assert not testdft.isFitted()
24         assert testdft.summary()[""changeInColumnNames""] is None
25         testdft.fit(self.testdf)
26         assert testdft.isFitted()
27         assert all(testdft.apply(self.testdf) == pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
28         assert testdft.summary()[""changeInColumnNames""] is not None
29 
30         # testing apply with no change in columns
31         testdft.apply(pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
32         assert testdft.summary()[""changeInColumnNames""] == ""none""
33 
34     def test_ruleBasedAlwaysFitted(self):
","17 
18     testdf = pd.DataFrame({""foo"": [1, 2], ""bar"": [1, 2]})
19 
20     def test_basicProperties(self):
21         testdft = self.TestDFT()
22 
23         assert not testdft.isFitted()
24         assert testdft.info()[""changeInColumnNames""] is None
25         testdft.fit(self.testdf)
26         assert testdft.isFitted()
27         assert all(testdft.apply(self.testdf) == pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
28         assert testdft.info()[""changeInColumnNames""] is not None
29 
30         # testing apply with no change in columns
31         testdft.apply(pd.DataFrame({""foo"": [1, 2], ""baz"": [1, 2]}))
32         assert testdft.info()[""changeInColumnNames""] == ""none""
33 
34     def test_ruleBasedAlwaysFitted(self):
","Before: 32
After: 32",update test_data_transformation.py to use testdft.info,"Renamings and minor fixes, mostly across summary methods. Fixed broken log in Multifgen",https://github.com/opcode81/sensAI,tests/base/test_data_transformation.py,f5a97bea3482af52a5d44b36fdfb5e9082fb7488,81daeb9deacbc17d3025f532fa63e826f7de2044,0,419,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'identifier': 33, '=': 2, 'call': 13, 'attribute': 14, '.': 14, 'argument_list': 13, '(': 14, 'dictionary': 3, '{': 3, 'pair': 6, 'string': 10, 'string_start': 10, 'string_content': 10, 'string_end': 10, ':': 7, 'list': 6, '[': 9, 'integer': 12, ',': 9, ']': 9, '}': 3, ')': 14, 'function_definition': 1, 'def': 1, 'parameters': 1, 'block': 1, 'assert_statement': 6, 'assert': 6, 'not_operator': 1, 'not': 1, 'comparison_operator': 4, 'subscript': 3, 'is': 1, 'none': 2, '==': 2, 'is not': 2, 'comment': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 12, 'name': '_fit', 'long_name': '_fit( self , df : pd . DataFrame )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self', ' df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_data_transformation.py', 'top_nesting_level': 2, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9571909027220098,0.9522400573005405,"(tensor([0.9956]), tensor([0.9956]), tensor([0.9956]), tensor([0.9956]))"
"40     def isRegressionModel(self) -> bool:
41         pass
42 
43     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
44         """"""
45         Makes the model use the given input transformers.
46 
47         :param inputTransformers: DataFrameTransformers for the transformation of inputs
48         :return: self
49         """"""
50         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
51         return self
52 
53     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
","40     def isRegressionModel(self) -> bool:
41         pass
42 
43     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
44         """"""
45         Makes the model use the given input transformers. Call with empty input to remove existing input transformers.
46 
47         :param inputTransformers: DataFrameTransformers for the transformation of inputs
48         :return: self
49         """"""
50         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
51         return self
52 
53     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
","Before: 45
After: 45",update vector_model and predictor_model,Fixed bug in VectorModel.predict related to output transformers,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,9ea3371b03585c508fcff9ceec6c841c0be508a8,869ea5765ec00bb3ebf055139caf1e1a1d4af2a1,1,291,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 16, 'parameters': 2, '(': 3, ')': 3, '->': 2, 'type': 6, ':': 3, 'block': 2, 'pass_statement': 1, 'pass': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, '*': 2, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'attribute': 1, '.': 1, '=': 1, 'call': 1, 'argument_list': 1, 'list_splat': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9126550284774987,0.8994115898624064,"(tensor([0.9770]), tensor([0.9962]), tensor([0.9865]), tensor([0.9942]))"
"50         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
51         return self
52 
53     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
54         """"""
55         Makes the model use the given output transformers. For models that can be fitted, they are ignored during
56         the fit phase.
57 
58         :param outputTransformers: DataFrameTransformers for the transformation of outputs
59             (after the model has been applied)
60         :return: self
61         """"""
62         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
63         return self
64 
65     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","50         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
51         return self
52 
53     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
54         """"""
55         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
56         For models that can be fitted, the transformers are ignored during the fit phase.
57 
58         **Important**: The output columns names of the last output transformer should be the same
59         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
60         (fit will run through without problems, though).
61 
62         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
63         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
64         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
65         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
66         by some heuristics or by hand-crafted rules.
67 
68         **How not to use**: Output transformers are not meant to transform the predictions into something with a
69         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
70         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
71         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
72         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
73         probabilistic classifiers the output transformers will not be applied to the probability vectors
74         but to the actual label predictions, unless explicitly stated otherwise.
75 
76         :param outputTransformers: DataFrameTransformers for the transformation of outputs
77             (after the model has been applied)
78         :return: self
79         """"""
80         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
81         return self
82 
83     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","Before: 55, 56
After: 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74",update vector_model and predictor_model,Fixed bug in VectorModel.predict related to output transformers,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,9ea3371b03585c508fcff9ceec6c841c0be508a8,869ea5765ec00bb3ebf055139caf1e1a1d4af2a1,1,363,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 2, 'identifier': 18, '.': 2, '=': 2, 'call': 2, 'argument_list': 2, '(': 3, 'list_splat': 2, '*': 3, ')': 3, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, ':': 2, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.24161279708481379,0.23554922990257607,"(tensor([0.7431]), tensor([0.9182]), tensor([0.8214]), tensor([0.8971]))"
"183     """"""
184     Base class for models that map data frames to predictions and can be fitted on data frames
185     """"""
186     def __init__(self, checkInputColumns=True):
187         """"""
188         :param checkInputColumns: Whether to check if the input column list (after feature generation)
189             during inference coincides with the input column list during fit.
190             This should be disabled if feature generation is not performed by the model itself,
191             e.g. in ensemble models.
192         """"""
193         super().__init__()
194         self._isFitted = False  # Note: this keeps track only of the actual model being fitted, not the pre/postprocessors
195         self._predictedVariableNames = None
196         self._modelInputVariableNames = None
197         self._modelOutputVariableNames = [""UNKNOWN""]
198         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = None
199         self.checkInputColumns = checkInputColumns
200 
201     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","201     """"""
202     Base class for models that map data frames to predictions and can be fitted on data frames
203     """"""
204     def __init__(self, checkInputColumns=True):
205         """"""
206         :param checkInputColumns: Whether to check if the input column list (after feature generation)
207             during inference coincides with the input column list during fit.
208             This should be disabled if feature generation is not performed by the model itself,
209             e.g. in ensemble models.
210         """"""
211         super().__init__()
212         self._isFitted = False  # Note: this keeps track only of the actual model being fitted, not the pre/postprocessors
213         self._predictedVariableNames: Optional[list] = None
214         self._modelInputVariableNames: Optional[list] = None
215         self._modelOutputVariableNames: Optional[list] = None
216         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = None
217         self.checkInputColumns = checkInputColumns
218 
219     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","Before: 195, 196, 197
After: 213, 214, 215",update vector_model and predictor_model,Fixed bug in VectorModel.predict related to output transformers,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,9ea3371b03585c508fcff9ceec6c841c0be508a8,869ea5765ec00bb3ebf055139caf1e1a1d4af2a1,1,1158,"{'module': 1, 'expression_statement': 9, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'function_definition': 1, 'def': 1, 'identifier': 20, 'parameters': 1, '(': 3, ',': 1, 'default_parameter': 1, '=': 7, 'true': 1, ')': 3, ':': 2, 'block': 1, 'call': 2, 'attribute': 7, 'argument_list': 2, '.': 7, 'assignment': 6, 'false': 1, 'comment': 1, 'none': 3, 'list': 1, '[': 2, ']': 2, 'type': 2, 'generic_type': 1, 'type_parameter': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6535595486763492,0.6487131374744126,"(tensor([0.9695]), tensor([0.9720]), tensor([0.9708]), tensor([0.9718]))"
"237                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(X.columns)}"")
238         return X
239 
240     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
241         """"""
242         Performs a prediction for the given input data frame
243 
244         :param x: the input data
245         :return: a DataFrame with the same index as the input
246         """"""
247         x = self._computeInputs(x)
248         y = self._predict(x)
249         y.index = x.index
250         y = self._outputTransformerChain.apply(y)
251         if self._targetTransformer is not None:
252             y = self._targetTransformer.applyInverse(y)
253         return y
254 
255     @abstractmethod
","255                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(X.columns)}"")
256         return X
257 
258     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
259         """"""
260         Performs a prediction for the given input data frame
261 
262         :param x: the input data
263         :return: a DataFrame with the same index as the input
264         """"""
265         x = self._computeInputs(x)
266         y = self._predict(x)
267         y.index = x.index
268         if self._targetTransformer is not None:
269             y = self._targetTransformer.applyInverse(y)
270         y = self._outputTransformerChain.apply(y)
271         if list(y.columns) != self.getPredictedVariableNames():
272             raise Exception(
273                 f""The model's predicted variable names are not correct. Got ""
274                 f""{list(y.columns)} but expected {self.getPredictedVariableNames()}. ""
275                 f""This kind of error can happen if the model's outputTransformerChain changes a data frame's ""
276                 f""columns (e.g. renames them or changes order). Only output transformer chains that do not change ""
277                 f""columns are permitted in VectorModel. You can fix this by modifying this instance's outputTransformerChain, ""
278                 f""e.g. by calling .withOutputTransformers() with the correct input ""
279                 f""(which can be empty to remove existing output transformers)""
280             )
281         return y
282 
283     @abstractmethod
","Before: 250
After: 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280",update vector_model and predictor_model,Fixed bug in VectorModel.predict related to output transformers,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,9ea3371b03585c508fcff9ceec6c841c0be508a8,869ea5765ec00bb3ebf055139caf1e1a1d4af2a1,1,1629,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 6, 'identifier': 39, 'argument_list': 6, '(': 7, 'string': 2, 'string_start': 2, 'string_content': 3, 'interpolation': 2, '{': 2, 'attribute': 13, '.': 13, '}': 2, ')': 7, 'string_end': 2, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'expression_statement': 6, 'assignment': 5, '=': 5, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.31555456096592194,0.3001392658294963,"(tensor([0.8176]), tensor([0.9504]), tensor([0.8790]), tensor([0.9352]))"
"256     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
257         pass
258 
259     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
260         """"""
261         Fits the model using the given data
262 
263         :param X: a data frame containing input data
264         :param Y: a data frame containing output data
265         :param fitPreprocessors: if False, the model's feature generator and input transformer will not be fitted.
266             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
267             an exception will be raised.
268         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
269             If it requires fitting, was not separately fit before and this option is set to False,
270             an exception will be raised.
271         """"""
272         log.info(f""Training {self.__class__.__name__}"")
273         self._predictedVariableNames = list(Y.columns)
274         X = self._computeInputs(X, Y=Y, fit=fitPreprocessors)
275         if self._targetTransformer is not None:
276             if fitTargetTransformer:
277                 Y = self._targetTransformer.fitApply(Y)
278             else:
279                 Y = self._targetTransformer.apply(Y)
280         self._modelInputVariableNames = list(X.columns)
281         self._modelOutputVariableNames = list(Y.columns)
282         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
283         self._fit(X, Y)
284         self._isFitted = True
285 
286     @abstractmethod
","284     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
285         pass
286 
287     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
288         """"""
289         Fits the model using the given data
290 
291         :param X: a data frame containing input data
292         :param Y: a data frame containing output data
293         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
294             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
295             an exception will be raised.
296         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
297             If it requires fitting, was not separately fit before and this option is set to False,
298             an exception will be raised.
299         """"""
300         log.info(f""Training {self.__class__.__name__}"")
301         self._predictedVariableNames = list(Y.columns)
302         X = self._computeInputs(X, Y=Y, fit=fitPreprocessors)
303         if self._targetTransformer is not None:
304             if fitTargetTransformer:
305                 Y = self._targetTransformer.fitApply(Y)
306             else:
307                 Y = self._targetTransformer.apply(Y)
308         self._modelInputVariableNames = list(X.columns)
309         self._modelOutputVariableNames = list(Y.columns)
310         log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
311         self._fit(X, Y)
312         self._isFitted = True
313 
314     @abstractmethod
","Before: 265
After: 293",update vector_model and predictor_model,Fixed bug in VectorModel.predict related to output transformers,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,9ea3371b03585c508fcff9ceec6c841c0be508a8,869ea5765ec00bb3ebf055139caf1e1a1d4af2a1,1,1786,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 83, 'parameters': 2, '(': 14, ',': 8, 'typed_parameter': 3, ':': 8, 'type': 4, 'attribute': 29, '.': 29, ')': 14, '->': 1, 'block': 5, 'pass_statement': 1, 'pass': 1, 'default_parameter': 2, '=': 11, 'true': 3, 'expression_statement': 11, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 12, 'argument_list': 12, 'interpolation': 5, '{': 5, '}': 5, 'assignment': 7, 'keyword_argument': 2, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'else_clause': 1, 'else': 1, 'list_comprehension': 1, '[': 2, 'binary_operator': 2, '+': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7603993220742106,0.7509599472504805,"(tensor([0.9751]), tensor([0.9760]), tensor([0.9755]), tensor([0.9759]))"
"287     def _fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame]):
288         pass
289 
290     def getModelOutputVariableNames(self):
291         """"""
292         Gets the list of variable names predicted by the underlying model.
293         For the case where the final output is transformed by an output transformer which changes column names,
294         the names of the variables prior to the transformation will be returned, i.e. this method
295         always returns the variable names that are actually predicted by the model.
296         For the variable names that are ultimately output by the model (including output transformations),
297         use getPredictedVariableNames.
298         """"""
299         return self._modelOutputVariableNames
300 
301     def getPredictedVariableNames(self):
","315     def _fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame]):
316         pass
317 
318     def getModelOutputVariableNames(self):
319         """"""
320         Gets the list of variable names predicted by the underlying model.
321         For the case where at training time the ground truth is transformed by a target transformer
322         which changes column names, the names of the variables prior to the transformation will be returned.
323         Thus this method always returns the variable names that are actually predicted by the fitted model alone.
324         For the variable names that are ultimately output by the entire VectorModel instance when calling predict,
325         use getPredictedVariableNames.
326         """"""
327         return self._modelOutputVariableNames
328 
329     def getPredictedVariableNames(self):
","Before: 293, 294, 295, 296
After: 321, 322, 323, 324",update vector_model and predictor_model,Fixed bug in VectorModel.predict related to output transformers,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,9ea3371b03585c508fcff9ceec6c841c0be508a8,869ea5765ec00bb3ebf055139caf1e1a1d4af2a1,1,2076,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 13, 'parameters': 2, '(': 2, ',': 2, 'typed_parameter': 2, ':': 4, 'type': 3, 'attribute': 3, '.': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ')': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 25, 'end_line': 29, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5652376168690532,0.54307798337878,"(tensor([0.9342]), tensor([0.9467]), tensor([0.9404]), tensor([0.9455]))"
"1 from .crossval import VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, \
2     VectorClassificationModelCrossValidationData, VectorRegressionModelCrossValidationData
3 from .eval_util import RegressionEvaluationUtil, ClassificationEvaluationUtil, MultiDataEvaluationUtil, \
4     evalModelViaEvaluator, createEvaluationUtil, createVectorModelEvaluator, createVectorModelCrossValidator
5 from .evaluator import VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
6     VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData
","1 from .crossval import VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, \
2     VectorClassificationModelCrossValidationData, VectorRegressionModelCrossValidationData
3 from .eval_util import RegressionEvaluationUtil, ClassificationEvaluationUtil, MultiDataEvaluationUtil, \
4     evalModelViaEvaluator, createEvaluationUtil, createVectorModelEvaluator, createVectorModelCrossValidator
5 from .evaluator import VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
6     RegressionModelEvaluationData, ClassificationModelEvaluationData
","Before: 6
After: 6",fix typo in src/src/sensai/evaluation/__init__.py,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/__init__.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,71,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'relative_import': 3, 'import_prefix': 3, '.': 3, 'dotted_name': 16, 'identifier': 16, 'import': 3, ',': 11, 'line_continuation': 3}",{},{},0.9364799716089345,0.9199738401569129,"(tensor([0.9936]), tensor([0.9926]), tensor([0.9931]), tensor([0.9927]))"
"7 import numpy as np
8 
9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictProvider
15 from ..data_ingest import InputOutputData
16 from ..util.typing import PandasNamedTuple
","7 import numpy as np
8 
9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import RegressionModelEvaluationData, ClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictProvider
15 from ..data_ingest import InputOutputData
16 from ..util.typing import PandasNamedTuple
","Before: 12
After: 12",remove unused imports,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,113,"{'module': 1, 'import_statement': 1, 'import': 6, 'aliased_import': 1, 'dotted_name': 19, 'identifier': 23, 'as': 1, 'import_from_statement': 5, 'from': 5, 'relative_import': 5, 'import_prefix': 5, '.': 9, ',': 8, 'line_continuation': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9238344363760219,0.9147304401716583,"(tensor([0.9928]), tensor([0.9919]), tensor([0.9923]), tensor([0.9920]))"
"114     def _computeMetrics(self, model: VectorModel):
115         data = self.evalModel(model)
116         return data.getEvalStatsCollection().aggStats()
117 
118 
119 class VectorRegressionModelCrossValidationData(PredictorModelCrossValidationData[VectorRegressionModel, VectorRegressionModelEvaluationData, RegressionEvalStats, RegressionEvalStatsCollection]):
120     def _createEvalStatsCollection(self, l: List[RegressionEvalStats]) -> RegressionEvalStatsCollection:
121         return RegressionEvalStatsCollection(l)
122 
123 
","114     def _computeMetrics(self, model: VectorModel):
115         data = self.evalModel(model)
116         return data.getEvalStatsCollection().aggStats()
117 
118 
119 class VectorRegressionModelCrossValidationData(PredictorModelCrossValidationData[VectorRegressionModel, RegressionModelEvaluationData, RegressionEvalStats, RegressionEvalStatsCollection]):
120     def _createEvalStatsCollection(self, l: List[RegressionEvalStats]) -> RegressionEvalStatsCollection:
121         return RegressionEvalStatsCollection(l)
122 
123 
","Before: 119
After: 119",remove unused imports,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,1233,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 25, 'parameters': 2, '(': 7, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 4, ')': 7, 'block': 3, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 4, 'attribute': 3, '.': 3, 'argument_list': 5, 'return_statement': 2, 'return': 2, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 2, ']': 2, 'generic_type': 1, 'type_parameter': 1, '->': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9625248317849852,0.9584068650454449,"(tensor([0.9980]), tensor([0.9970]), tensor([0.9975]), tensor([0.9971]))"
"128 
129     def _createResultData(self, trainedModels, evalDataList, testIndicesList, predictedVarNames) -> VectorRegressionModelCrossValidationData:
130         return VectorRegressionModelCrossValidationData(trainedModels, evalDataList, predictedVarNames, testIndicesList)
131 
132 
133 class VectorClassificationModelCrossValidationData(PredictorModelCrossValidationData[VectorClassificationModel, VectorClassificationModelEvaluationData, ClassificationEvalStats, ClassificationEvalStatsCollection]):
134     def _createEvalStatsCollection(self, l: List[ClassificationEvalStats]) -> ClassificationEvalStatsCollection:
135         return ClassificationEvalStatsCollection(l)
136 
137 
","128 
129     def _createResultData(self, trainedModels, evalDataList, testIndicesList, predictedVarNames) -> VectorRegressionModelCrossValidationData:
130         return VectorRegressionModelCrossValidationData(trainedModels, evalDataList, predictedVarNames, testIndicesList)
131 
132 
133 class VectorClassificationModelCrossValidationData(PredictorModelCrossValidationData[VectorClassificationModel, ClassificationModelEvaluationData, ClassificationEvalStats, ClassificationEvalStatsCollection]):
134     def _createEvalStatsCollection(self, l: List[ClassificationEvalStats]) -> ClassificationEvalStatsCollection:
135         return ClassificationEvalStatsCollection(l)
136 
137 
","Before: 133
After: 133",remove unused imports,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,1376,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 26, 'parameters': 2, '(': 5, ',': 11, ')': 5, '->': 2, 'type': 4, ':': 4, 'block': 3, 'return_statement': 2, 'return': 2, 'call': 2, 'argument_list': 3, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 2, ']': 2, 'typed_parameter': 1, 'generic_type': 1, 'type_parameter': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9651391201669045,0.9616311437622332,"(tensor([0.9974]), tensor([0.9965]), tensor([0.9970]), tensor([0.9966]))"
"20     VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, VectorModelCrossValidator
21 from .eval_stats.eval_stats_base import EvalStats, EvalStatsCollection
22 from .eval_stats.eval_stats_classification import ClassificationEvalStats
23 from .eval_stats.eval_stats_regression import RegressionEvalStats
24 from .evaluator import VectorModelEvaluator, PredictorModelEvaluationData, VectorRegressionModelEvaluator, \
25     VectorRegressionModelEvaluationData, VectorClassificationModelEvaluator, VectorClassificationModelEvaluationData
26 from ..data_ingest import InputOutputData
27 from ..util.io import ResultWriter
28 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel
29 
","20     VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, VectorModelCrossValidator
21 from .eval_stats.eval_stats_base import EvalStats, EvalStatsCollection
22 from .eval_stats.eval_stats_classification import ClassificationEvalStats
23 from .eval_stats.eval_stats_regression import RegressionEvalStats
24 from .evaluator import VectorModelEvaluator, PredictorModelEvaluationData, VectorRegressionModelEvaluator, \
25     RegressionModelEvaluationData, VectorClassificationModelEvaluator, ClassificationModelEvaluationData
26 from ..data_ingest import InputOutputData
27 from ..util.io import ResultWriter
28 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel
29 
","Before: 25
After: 25",remove unused imports,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,175,"{'module': 1, 'expression_statement': 1, 'identifier': 29, ',': 10, 'import_from_statement': 7, 'from': 7, 'relative_import': 7, 'import_prefix': 7, '.': 14, 'dotted_name': 22, 'import': 7, 'line_continuation': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9172286979619474,0.908569141486438,"(tensor([0.9933]), tensor([0.9931]), tensor([0.9932]), tensor([0.9932]))"
"284         :param subtitle: the subtitle to use for generated plots (if any)
285         """"""
286         pass
287 
288 
289 class RegressionEvaluationUtil(EvaluationUtil[VectorRegressionModel, VectorRegressionModelEvaluator, VectorRegressionModelEvaluationData, VectorRegressionModelCrossValidator, VectorRegressionModelCrossValidationData, RegressionEvalStats]):
290     def _createEvalStatsPlots(self, evalStats: RegressionEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
","284         :param subtitle: the subtitle to use for generated plots (if any)
285         """"""
286         pass
287 
288 
289 class RegressionEvaluationUtil(EvaluationUtil[VectorRegressionModel, VectorRegressionModelEvaluator, RegressionModelEvaluationData, VectorRegressionModelCrossValidator, VectorRegressionModelCrossValidationData, RegressionEvalStats]):
290     def _createEvalStatsPlots(self, evalStats: RegressionEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
","Before: 289
After: 289",remove unused imports,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,2724,"{'module': 1, 'ERROR': 4, ':': 2, 'identifier': 10, 'expression_statement': 1, 'assignment': 1, 'type': 1, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1, 'string_start': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9736071380988617,0.9705541112332454,"(tensor([0.9984]), tensor([0.9989]), tensor([0.9987]), tensor([0.9988]))"
"291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
294 
295 
296 class ClassificationEvaluationUtil(EvaluationUtil[VectorClassificationModel, VectorClassificationModelEvaluator, VectorClassificationModelEvaluationData, VectorClassificationModelCrossValidator, VectorClassificationModelCrossValidationData, ClassificationEvalStats]):
297     def _createEvalStatsPlots(self, evalStats: ClassificationEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
298         resultCollector.addFigure(""confusion-matrix"", evalStats.plotConfusionMatrix(titleAdd=subtitle))
299 
300 
","291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
294 
295 
296 class ClassificationEvaluationUtil(EvaluationUtil[VectorClassificationModel, VectorClassificationModelEvaluator, ClassificationModelEvaluationData, VectorClassificationModelCrossValidator, VectorClassificationModelCrossValidationData, ClassificationEvalStats]):
297     def _createEvalStatsPlots(self, evalStats: ClassificationEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
298         resultCollector.addFigure(""confusion-matrix"", evalStats.plotConfusionMatrix(titleAdd=subtitle))
299 
300 
","Before: 296
After: 296",remove unused imports,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,2854,"{'module': 1, 'expression_statement': 4, 'call': 8, 'attribute': 9, 'identifier': 40, '.': 9, 'argument_list': 9, '(': 10, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ',': 12, 'keyword_argument': 4, '=': 5, ')': 10, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, ']': 1, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, 'type': 2, 'default_parameter': 1, 'none': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.97117159097714,0.9667077643416399,"(tensor([0.9977]), tensor([0.9981]), tensor([0.9979]), tensor([0.9981]))"
"76         evalStats = self.getEvalStats(predictedVarName)
77         for i, namedTuple in enumerate(self.inputData.itertuples()):
78             yield namedTuple, evalStats.y_predicted[i], evalStats.y_true[i]
79 
80 
81 class VectorRegressionModelEvaluationData(PredictorModelEvaluationData[RegressionEvalStats]):
82     def getEvalStatsCollection(self):
83         return RegressionEvalStatsCollection(list(self.evalStatsByVarName.values()))
84 
85 
","76         evalStats = self.getEvalStats(predictedVarName)
77         for i, namedTuple in enumerate(self.inputData.itertuples()):
78             yield namedTuple, evalStats.y_predicted[i], evalStats.y_true[i]
79 
80 
81 class RegressionModelEvaluationData(PredictorModelEvaluationData[RegressionEvalStats]):
82     def getEvalStatsCollection(self):
83         return RegressionEvalStatsCollection(list(self.evalStatsByVarName.values()))
84 
85 
","Before: 81
After: 81",fix bug in vectorregressionmodelevaluator,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,882,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 27, '=': 1, 'call': 6, 'attribute': 7, '.': 7, 'argument_list': 7, '(': 8, ')': 8, 'for_statement': 1, 'for': 1, 'pattern_list': 1, ',': 3, 'in': 1, ':': 3, 'block': 3, 'yield': 2, 'expression_list': 1, 'subscript': 3, '[': 3, ']': 3, 'class_definition': 1, 'class': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9601328622880726,0.9554427922043668,"(tensor([0.9973]), tensor([0.9941]), tensor([0.9957]), tensor([0.9944]))"
"137         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
138         self.additionalMetrics = additionalMetrics
139 
140     def evalModel(self, model: PredictorModel, onTrainingData=False) -> VectorRegressionModelEvaluationData:
141         if not model.isRegressionModel():
142             raise ValueError(f""Expected a regression model, got {model}"")
143         evalStatsByVarName = {}
144         inputOutputData = self.trainingData if onTrainingData else self.testData
145         predictions, groundTruth = self._computeOutputs(model, inputOutputData)
146         for predictedVarName in model.getPredictedVariableNames():
147             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
148                 additionalMetrics=self.additionalMetrics)
149             evalStatsByVarName[predictedVarName] = evalStats
150         return VectorRegressionModelEvaluationData(evalStatsByVarName, inputOutputData.inputs, model)
151 
152     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","137         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
138         self.additionalMetrics = additionalMetrics
139 
140     def evalModel(self, model: PredictorModel, onTrainingData=False) -> RegressionModelEvaluationData:
141         if not model.isRegressionModel():
142             raise ValueError(f""Expected a regression model, got {model}"")
143         evalStatsByVarName = {}
144         inputOutputData = self.trainingData if onTrainingData else self.testData
145         predictions, groundTruth = self._computeOutputs(model, inputOutputData)
146         for predictedVarName in model.getPredictedVariableNames():
147             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
148                 additionalMetrics=self.additionalMetrics)
149             evalStatsByVarName[predictedVarName] = evalStats
150         return RegressionModelEvaluationData(evalStatsByVarName, inputOutputData.inputs, model)
151 
152     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","Before: 140
After: 140",fix bug in vectorregressionmodelevaluator,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,1423,"{'module': 1, 'expression_statement': 7, 'call': 8, 'attribute': 9, 'identifier': 62, 'argument_list': 8, '(': 9, ')': 9, '.': 9, 'keyword_argument': 9, '=': 16, ',': 13, 'assignment': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 4, 'type': 2, 'default_parameter': 1, 'false': 1, '->': 1, 'block': 3, 'if_statement': 1, 'if': 2, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'dictionary': 1, 'conditional_expression': 1, 'else': 1, 'pattern_list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 3, '[': 3, ']': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9660933020921969,0.9633009202729679,"(tensor([0.9975]), tensor([0.9948]), tensor([0.9961]), tensor([0.9950]))"
"137         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
138         self.additionalMetrics = additionalMetrics
139 
140     def evalModel(self, model: PredictorModel, onTrainingData=False) -> VectorRegressionModelEvaluationData:
141         if not model.isRegressionModel():
142             raise ValueError(f""Expected a regression model, got {model}"")
143         evalStatsByVarName = {}
144         inputOutputData = self.trainingData if onTrainingData else self.testData
145         predictions, groundTruth = self._computeOutputs(model, inputOutputData)
146         for predictedVarName in model.getPredictedVariableNames():
147             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
148                 additionalMetrics=self.additionalMetrics)
149             evalStatsByVarName[predictedVarName] = evalStats
150         return VectorRegressionModelEvaluationData(evalStatsByVarName, inputOutputData.inputs, model)
151 
152     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","137         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
138         self.additionalMetrics = additionalMetrics
139 
140     def evalModel(self, model: PredictorModel, onTrainingData=False) -> RegressionModelEvaluationData:
141         if not model.isRegressionModel():
142             raise ValueError(f""Expected a regression model, got {model}"")
143         evalStatsByVarName = {}
144         inputOutputData = self.trainingData if onTrainingData else self.testData
145         predictions, groundTruth = self._computeOutputs(model, inputOutputData)
146         for predictedVarName in model.getPredictedVariableNames():
147             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
148                 additionalMetrics=self.additionalMetrics)
149             evalStatsByVarName[predictedVarName] = evalStats
150         return RegressionModelEvaluationData(evalStatsByVarName, inputOutputData.inputs, model)
151 
152     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","Before: 150
After: 150",fix bug in vectorregressionmodelevaluator,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,1565,"{'module': 1, 'expression_statement': 7, 'call': 8, 'attribute': 9, 'identifier': 62, 'argument_list': 8, '(': 9, ')': 9, '.': 9, 'keyword_argument': 9, '=': 16, ',': 13, 'assignment': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 4, 'type': 2, 'default_parameter': 1, 'false': 1, '->': 1, 'block': 3, 'if_statement': 1, 'if': 2, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'dictionary': 1, 'conditional_expression': 1, 'else': 1, 'pattern_list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 3, '[': 3, ']': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9660933020921969,0.9633009202729679,"(tensor([0.9975]), tensor([0.9948]), tensor([0.9961]), tensor([0.9950]))"
"169         predictions = model.predict(inputOutputData.inputs)
170         groundTruth = inputOutputData.outputs
171         return predictions, groundTruth
172 
173 
174 class VectorClassificationModelEvaluationData(PredictorModelEvaluationData[ClassificationEvalStats]):
175     pass
176 
177 
178 class VectorClassificationModelEvaluator(VectorModelEvaluator):
","169         predictions = model.predict(inputOutputData.inputs)
170         groundTruth = inputOutputData.outputs
171         return predictions, groundTruth
172 
173 
174 class ClassificationModelEvaluationData(PredictorModelEvaluationData[ClassificationEvalStats]):
175     pass
176 
177 
178 class VectorClassificationModelEvaluator(VectorModelEvaluator):
","Before: 174
After: 174",fix bug in vectorregressionmodelevaluator,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,1684,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 13, '=': 2, 'call': 1, 'attribute': 3, '.': 3, 'argument_list': 2, '(': 2, ')': 2, 'return_statement': 1, 'return': 1, 'expression_list': 1, ',': 1, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, ']': 1, ':': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.931838481115484,0.9258229600890695,"(tensor([0.9923]), tensor([0.9917]), tensor([0.9920]), tensor([0.9918]))"
"182         self.computeProbabilities = computeProbabilities
183         self.additionalMetrics = additionalMetrics
184 
185     def evalModel(self, model: VectorClassificationModel, onTrainingData=False) -> VectorClassificationModelEvaluationData:
186         if model.isRegressionModel():
187             raise ValueError(f""Expected a classification model, got {model}"")
188         inputOutputData = self.trainingData if onTrainingData else self.testData
189         predictions, predictions_proba, groundTruth = self._computeOutputs(model, inputOutputData)
190         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
191             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
192         predictedVarName = model.getPredictedVariableNames()[0]
193         return VectorClassificationModelEvaluationData({predictedVarName: evalStats}, inputOutputData.inputs, model)
194 
195     def computeTestDataOutputs(self, model: VectorClassificationModel) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","182         self.computeProbabilities = computeProbabilities
183         self.additionalMetrics = additionalMetrics
184 
185     def evalModel(self, model: VectorClassificationModel, onTrainingData=False) -> ClassificationModelEvaluationData:
186         if model.isRegressionModel():
187             raise ValueError(f""Expected a classification model, got {model}"")
188         inputOutputData = self.trainingData if onTrainingData else self.testData
189         predictions, predictions_proba, groundTruth = self._computeOutputs(model, inputOutputData)
190         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
191             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
192         predictedVarName = model.getPredictedVariableNames()[0]
193         return ClassificationModelEvaluationData({predictedVarName: evalStats}, inputOutputData.inputs, model)
194 
195     def computeTestDataOutputs(self, model: VectorClassificationModel) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","Before: 185
After: 185",fix bug in vectorregressionmodelevaluator,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,1838,"{'module': 1, 'expression_statement': 6, 'assignment': 6, 'attribute': 10, 'identifier': 52, '.': 10, '=': 12, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 11, 'typed_parameter': 1, ':': 4, 'type': 2, 'default_parameter': 1, 'false': 1, ')': 8, '->': 1, 'block': 2, 'if_statement': 1, 'if': 2, 'call': 7, 'argument_list': 7, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'conditional_expression': 1, 'else': 1, 'pattern_list': 1, 'keyword_argument': 5, 'subscript': 1, '[': 1, 'integer': 1, ']': 1, 'return_statement': 1, 'return': 1, 'dictionary': 1, 'pair': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9613775135016038,0.9582787758369394,"(tensor([0.9955]), tensor([0.9950]), tensor([0.9952]), tensor([0.9950]))"
"182         self.computeProbabilities = computeProbabilities
183         self.additionalMetrics = additionalMetrics
184 
185     def evalModel(self, model: VectorClassificationModel, onTrainingData=False) -> VectorClassificationModelEvaluationData:
186         if model.isRegressionModel():
187             raise ValueError(f""Expected a classification model, got {model}"")
188         inputOutputData = self.trainingData if onTrainingData else self.testData
189         predictions, predictions_proba, groundTruth = self._computeOutputs(model, inputOutputData)
190         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
191             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
192         predictedVarName = model.getPredictedVariableNames()[0]
193         return VectorClassificationModelEvaluationData({predictedVarName: evalStats}, inputOutputData.inputs, model)
194 
195     def computeTestDataOutputs(self, model: VectorClassificationModel) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","182         self.computeProbabilities = computeProbabilities
183         self.additionalMetrics = additionalMetrics
184 
185     def evalModel(self, model: VectorClassificationModel, onTrainingData=False) -> ClassificationModelEvaluationData:
186         if model.isRegressionModel():
187             raise ValueError(f""Expected a classification model, got {model}"")
188         inputOutputData = self.trainingData if onTrainingData else self.testData
189         predictions, predictions_proba, groundTruth = self._computeOutputs(model, inputOutputData)
190         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
191             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
192         predictedVarName = model.getPredictedVariableNames()[0]
193         return ClassificationModelEvaluationData({predictedVarName: evalStats}, inputOutputData.inputs, model)
194 
195     def computeTestDataOutputs(self, model: VectorClassificationModel) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","Before: 193
After: 193",fix bug in vectorregressionmodelevaluator,"Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c380f42a431245f8f987db5103935006d103471b,ed7f9aad6f0d37cd8ae89c9c7ce4fc1af724736e,0,1981,"{'module': 1, 'expression_statement': 6, 'assignment': 6, 'attribute': 10, 'identifier': 52, '.': 10, '=': 12, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 11, 'typed_parameter': 1, ':': 4, 'type': 2, 'default_parameter': 1, 'false': 1, ')': 8, '->': 1, 'block': 2, 'if_statement': 1, 'if': 2, 'call': 7, 'argument_list': 7, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'conditional_expression': 1, 'else': 1, 'pattern_list': 1, 'keyword_argument': 5, 'subscript': 1, '[': 1, 'integer': 1, ']': 1, 'return_statement': 1, 'return': 1, 'dictionary': 1, 'pair': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9613775135016038,0.9582787758369394,"(tensor([0.9955]), tensor([0.9950]), tensor([0.9952]), tensor([0.9950]))"
"83         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
84         return self
85 
86     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
87         """"""
88         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
89         For models that can be fitted, the transformers are ignored during the fit phase.
90 
91         **Important**: The output columns names of the last output transformer should be the same
92         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
93         (fit will run through without problems, though).
94 
95         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
96         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
97         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
98         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
99         by some heuristics or by hand-crafted rules.
100 
101         **How not to use**: Output transformers are not meant to transform the predictions into something with a
102         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
103         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
104         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
105         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
106         probabilistic classifiers the output transformers will not be applied to the probability vectors
107         but to the actual label predictions, unless explicitly stated otherwise.
108 
109         :param outputTransformers: DataFrameTransformers for the transformation of outputs
110             (after the model has been applied)
111         :return: self
112         """"""
113         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
114         return self
115 
116     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","82         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
83         return self
84 
85     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
86         """"""
87         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
88         The transformers are ignored during the fit phase. Not supported for rule based models.
89 
90         **Important**: The output columns names of the last output transformer should be the same
91         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
92         (fit will run through without problems, though).
93 
94         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
95         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
96         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
97         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
98         by some heuristics or by hand-crafted rules.
99 
100         **How not to use**: Output transformers are not meant to transform the predictions into something with a
101         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
102         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
103         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
104         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
105         probabilistic classifiers the output transformers will not be applied to the probability vectors
106         but to the actual label predictions, unless explicitly stated otherwise.
107 
108         :param outputTransformers: DataFrameTransformers for the transformation of outputs
109             (after the model has been applied)
110         :return: self
111         """"""
112         # Since we have to forbid target transformers for rule based models, we might as well forbid output transformers as well
113         # There is no reason for post processing in rule based models.
114         if not self._underlyingModelRequiresFitting():
115             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
116         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
117         return self
118 
119     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","Before: 89
After: 88",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,592,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 2, 'identifier': 18, '.': 2, '=': 2, 'call': 2, 'argument_list': 2, '(': 3, 'list_splat': 2, '*': 3, ')': 3, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, ':': 2, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7097620452218029,0.7074629058595152,"(tensor([0.9536]), tensor([0.9827]), tensor([0.9679]), tensor([0.9797]))"
"113         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
114         return self
115 
116     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
117         """"""
118         Makes the model use the given target transformers.
119 
120         NOTE: all feature generators and data frame transformers will be fit on the untransformed target.
121         The targetTransformer only affects the fit of the internal model.
122 
123         :param targetTransformer: a transformer which transforms the targets (training data outputs) prior to learning the model, such
124             that the model learns to predict the transformed outputs. When predicting, the inverse transformer is applied after applying
125             the model, i.e. the transformation is completely transparent when applying the model.
126         :return: self
127         """"""
128         self._targetTransformer = targetTransformer
129         return self
130 
131     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","116         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
117         return self
118 
119     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
120         """"""
121         Makes the model use the given target transformers. Not supported for rule based models.
122 
123         NOTE: all feature generators and data frame transformers will be fit on the untransformed target.
124         The targetTransformer only affects the fit of the internal model.
125 
126         :param targetTransformer: a transformer which transforms the targets (training data outputs) prior to learning the model, such
127             that the model learns to predict the transformed outputs. When predicting, the inverse transformer is applied after applying
128             the model, i.e. the transformation is completely transparent when applying the model.
129         :return: self
130         """"""
131         # Note: it is important to disallow targetTransformers for rule based models since we need
132         # predictedVarNames and modelOutputVarNames to coincide there.
133         if not self._underlyingModelRequiresFitting():
134             raise Exception(f""Target transformers are not supported for model of type {self.__class__.__name__}"")
135         self._targetTransformer = targetTransformer
136         return self
137 
138     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","Before: 118
After: 121",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,608,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 2, 'identifier': 15, '.': 2, '=': 2, 'call': 1, 'argument_list': 1, '(': 2, 'list_splat': 1, '*': 1, ')': 2, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5778922265629342,0.5664715647380346,"(tensor([0.8941]), tensor([0.9726]), tensor([0.9317]), tensor([0.9641]))"
"264                 X = self._featureGenerator.fitGenerate(X, Y, self)
265         self._inputTransformerChain.fit(X)
266 
267     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
268         """"""
269         Fits the model using the given data
270 
271         :param X: a data frame containing input data
272         :param Y: a data frame containing output data
273         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
274             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
275             an exception will be raised.
276         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
277             If it requires fitting, was not separately fit before and this option is set to False,
278             an exception will be raised.
279         """"""
280         log.info(f""Training {self.__class__.__name__}"")
281         self._predictedVariableNames = list(Y.columns)
282         if not self._underlyingModelRequiresFitting():
283             self.fitPreprocessors(X, Y=Y)
284         else:
285             X = self.computeProcessedInputs(X, Y=Y, fit=fitPreprocessors)
286             if self._targetTransformer is not None:
287                 if fitTargetTransformer:
288                     Y = self._targetTransformer.fitApply(Y)
289                 else:
290                     Y = self._targetTransformer.apply(Y)
291             self._modelInputVariableNames = list(X.columns)
292             self._modelOutputVariableNames = list(Y.columns)
293             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
294             self._fit(X, Y)
295             self._isFitted = True
296 
297     @abstractmethod
","271                 X = self._featureGenerator.fitGenerate(X, Y, self)
272         self._inputTransformerChain.fit(X)
273 
274     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True, fitTargetTransformer=True):
275         """"""
276         Fits the model using the given data
277 
278         :param X: a data frame containing input data
279         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
280             fitting, e.g. with rule based models
281         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
282             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
283             an exception will be raised.
284         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
285             If it requires fitting, was not separately fit before and this option is set to False,
286             an exception will be raised.
287         """"""
288         log.info(f""Training {self.__class__.__name__}"")
289         self._predictedVariableNames = list(Y.columns)
290         if not self._underlyingModelRequiresFitting():
291             self.fitPreprocessors(X, Y=Y)
292         else:
293             if Y is None:
294                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
295             X = self.computeProcessedInputs(X, Y=Y, fit=fitPreprocessors)
296             if self._targetTransformer is not None:
297                 if fitTargetTransformer:
298                     Y = self._targetTransformer.fitApply(Y)
299                 else:
300                     Y = self._targetTransformer.apply(Y)
301             self._modelInputVariableNames = list(X.columns)
302             self._modelOutputVariableNames = list(Y.columns)
303             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
304             self._fit(X, Y)
305             self._isFitted = True
306 
307     @abstractmethod
","Before: 267
After: 274",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,1603,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'identifier': 94, '=': 13, 'call': 16, 'attribute': 33, '.': 33, 'argument_list': 16, '(': 17, ',': 10, ')': 17, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 8, 'type': 2, 'default_parameter': 2, 'true': 3, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 3, 'if': 3, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 2, 'else': 2, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 2, 'binary_operator': 2, '+': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6611934734889653,0.6540643500068561,"(tensor([0.9383]), tensor([0.9586]), tensor([0.9483]), tensor([0.9565]))"
"264                 X = self._featureGenerator.fitGenerate(X, Y, self)
265         self._inputTransformerChain.fit(X)
266 
267     def fit(self, X: pd.DataFrame, Y: pd.DataFrame, fitPreprocessors=True, fitTargetTransformer=True):
268         """"""
269         Fits the model using the given data
270 
271         :param X: a data frame containing input data
272         :param Y: a data frame containing output data
273         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
274             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
275             an exception will be raised.
276         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
277             If it requires fitting, was not separately fit before and this option is set to False,
278             an exception will be raised.
279         """"""
280         log.info(f""Training {self.__class__.__name__}"")
281         self._predictedVariableNames = list(Y.columns)
282         if not self._underlyingModelRequiresFitting():
283             self.fitPreprocessors(X, Y=Y)
284         else:
285             X = self.computeProcessedInputs(X, Y=Y, fit=fitPreprocessors)
286             if self._targetTransformer is not None:
287                 if fitTargetTransformer:
288                     Y = self._targetTransformer.fitApply(Y)
289                 else:
290                     Y = self._targetTransformer.apply(Y)
291             self._modelInputVariableNames = list(X.columns)
292             self._modelOutputVariableNames = list(Y.columns)
293             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
294             self._fit(X, Y)
295             self._isFitted = True
296 
297     @abstractmethod
","271                 X = self._featureGenerator.fitGenerate(X, Y, self)
272         self._inputTransformerChain.fit(X)
273 
274     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True, fitTargetTransformer=True):
275         """"""
276         Fits the model using the given data
277 
278         :param X: a data frame containing input data
279         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
280             fitting, e.g. with rule based models
281         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
282             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
283             an exception will be raised.
284         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
285             If it requires fitting, was not separately fit before and this option is set to False,
286             an exception will be raised.
287         """"""
288         log.info(f""Training {self.__class__.__name__}"")
289         self._predictedVariableNames = list(Y.columns)
290         if not self._underlyingModelRequiresFitting():
291             self.fitPreprocessors(X, Y=Y)
292         else:
293             if Y is None:
294                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
295             X = self.computeProcessedInputs(X, Y=Y, fit=fitPreprocessors)
296             if self._targetTransformer is not None:
297                 if fitTargetTransformer:
298                     Y = self._targetTransformer.fitApply(Y)
299                 else:
300                     Y = self._targetTransformer.apply(Y)
301             self._modelInputVariableNames = list(X.columns)
302             self._modelOutputVariableNames = list(Y.columns)
303             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
304             self._fit(X, Y)
305             self._isFitted = True
306 
307     @abstractmethod
","Before: 272
After: 279, 280",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,1642,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'identifier': 94, '=': 13, 'call': 16, 'attribute': 33, '.': 33, 'argument_list': 16, '(': 17, ',': 10, ')': 17, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 8, 'type': 2, 'default_parameter': 2, 'true': 3, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 3, 'if': 3, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 2, 'else': 2, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'list_comprehension': 1, '[': 2, 'binary_operator': 2, '+': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6611934734889653,0.6540643500068561,"(tensor([0.9383]), tensor([0.9586]), tensor([0.9483]), tensor([0.9565]))"
"298     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
299         pass
300 
301     def getModelOutputVariableNames(self):
302         """"""
303         Gets the list of variable names predicted by the underlying model.
304         For the case where at training time the ground truth is transformed by a target transformer
305         which changes column names, the names of the variables prior to the transformation will be returned.
306         Thus this method always returns the variable names that are actually predicted by the fitted model alone.
307         For the variable names that are ultimately output by the entire VectorModel instance when calling predict,
308         use getPredictedVariableNames.
309         """"""
310         return self._modelOutputVariableNames
311 
312     def getPredictedVariableNames(self):
","308     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
309         pass
310 
311     def getModelOutputVariableNames(self):
312         """"""
313         Gets the list of variable names predicted by the underlying model.
314         For the case where at training time the ground truth is transformed by a target transformer
315         which changes column names, the names of the variables prior to the transformation will be returned.
316         Thus this method always returns the variable names that are actually predicted by the underlying model alone.
317         For the variable names that are ultimately output by the entire VectorModel instance when calling predict,
318         use getPredictedVariableNames.
319         """"""
320         # Note that this method is needed in RuleBasedClassificationModel, so we cannot just raise an exception
321         if not self._underlyingModelRequiresFitting():
322             return self.getPredictedVariableNames()
323         return self._modelOutputVariableNames
324 
325     def getPredictedVariableNames(self):
","Before: 306
After: 316, 320, 321, 322",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,2016,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 12, 'parameters': 2, '(': 2, ',': 2, 'typed_parameter': 2, ':': 4, 'type': 2, 'attribute': 3, '.': 3, ')': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5989549118523039,0.5867455620046154,"(tensor([0.9245]), tensor([0.9726]), tensor([0.9479]), tensor([0.9675]))"
"362     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
363         pass
364 
365     def _convertClassProbabilitiesToPredictions(self, predictedProbaDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
366         """"""
367         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
368         with predicted class labels
369 
370         :param predictedProbaDf: the output data frame from predictClassProbabilities
371         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
372         """"""
373         labels = self.getClassLabels()
374         dfCols = list(predictedProbaDf.columns)
375         if dfCols != labels:
376             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
377         yArray = predictedProbaDf.values
378         maxIndices = np.argmax(yArray, axis=1)
379         result = [labels[i] for i in maxIndices]
380         return pd.DataFrame(result, columns=[predictedVariableName])
381 
382     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","378     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
379         pass
380 
381     def _convertClassProbabilitiesToPredictions(self, predictedProbabilitiesDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
382         """"""
383         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
384         with predicted class labels
385 
386         :param predictedProbabilitiesDf: the output data frame from predictClassProbabilities
387         :param predictedVariableName:
388         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
389         """"""
390         labels = self.getClassLabels()
391         dfCols = list(predictedProbabilitiesDf.columns)
392         if dfCols != labels:
393             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
394         yArray = predictedProbabilitiesDf.values
395         maxIndices = np.argmax(yArray, axis=1)
396         result = [labels[i] for i in maxIndices]
397         return pd.DataFrame(result, columns=[predictedVariableName])
398 
399     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","Before: 365
After: 381",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,2430,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 44, 'parameters': 2, '(': 7, ',': 6, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 8, '.': 8, ')': 7, 'block': 3, 'pass_statement': 1, 'pass': 1, 'default_parameter': 1, '=': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'expression_statement': 6, 'assignment': 5, 'call': 5, 'argument_list': 5, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 3, 'subscript': 1, ']': 3, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1, 'list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6866318834839851,0.6735408689091861,"(tensor([0.9693]), tensor([0.9714]), tensor([0.9704]), tensor([0.9712]))"
"362     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
363         pass
364 
365     def _convertClassProbabilitiesToPredictions(self, predictedProbaDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
366         """"""
367         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
368         with predicted class labels
369 
370         :param predictedProbaDf: the output data frame from predictClassProbabilities
371         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
372         """"""
373         labels = self.getClassLabels()
374         dfCols = list(predictedProbaDf.columns)
375         if dfCols != labels:
376             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
377         yArray = predictedProbaDf.values
378         maxIndices = np.argmax(yArray, axis=1)
379         result = [labels[i] for i in maxIndices]
380         return pd.DataFrame(result, columns=[predictedVariableName])
381 
382     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","378     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
379         pass
380 
381     def _convertClassProbabilitiesToPredictions(self, predictedProbabilitiesDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
382         """"""
383         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
384         with predicted class labels
385 
386         :param predictedProbabilitiesDf: the output data frame from predictClassProbabilities
387         :param predictedVariableName:
388         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
389         """"""
390         labels = self.getClassLabels()
391         dfCols = list(predictedProbabilitiesDf.columns)
392         if dfCols != labels:
393             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
394         yArray = predictedProbabilitiesDf.values
395         maxIndices = np.argmax(yArray, axis=1)
396         result = [labels[i] for i in maxIndices]
397         return pd.DataFrame(result, columns=[predictedVariableName])
398 
399     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","Before: 370
After: 386, 387",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,2465,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 44, 'parameters': 2, '(': 7, ',': 6, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 8, '.': 8, ')': 7, 'block': 3, 'pass_statement': 1, 'pass': 1, 'default_parameter': 1, '=': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'expression_statement': 6, 'assignment': 5, 'call': 5, 'argument_list': 5, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 3, 'subscript': 1, ']': 3, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1, 'list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6866318834839851,0.6735408689091861,"(tensor([0.9693]), tensor([0.9714]), tensor([0.9704]), tensor([0.9712]))"
"362     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
363         pass
364 
365     def _convertClassProbabilitiesToPredictions(self, predictedProbaDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
366         """"""
367         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
368         with predicted class labels
369 
370         :param predictedProbaDf: the output data frame from predictClassProbabilities
371         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
372         """"""
373         labels = self.getClassLabels()
374         dfCols = list(predictedProbaDf.columns)
375         if dfCols != labels:
376             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
377         yArray = predictedProbaDf.values
378         maxIndices = np.argmax(yArray, axis=1)
379         result = [labels[i] for i in maxIndices]
380         return pd.DataFrame(result, columns=[predictedVariableName])
381 
382     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","378     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
379         pass
380 
381     def _convertClassProbabilitiesToPredictions(self, predictedProbabilitiesDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
382         """"""
383         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
384         with predicted class labels
385 
386         :param predictedProbabilitiesDf: the output data frame from predictClassProbabilities
387         :param predictedVariableName:
388         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
389         """"""
390         labels = self.getClassLabels()
391         dfCols = list(predictedProbabilitiesDf.columns)
392         if dfCols != labels:
393             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
394         yArray = predictedProbabilitiesDf.values
395         maxIndices = np.argmax(yArray, axis=1)
396         result = [labels[i] for i in maxIndices]
397         return pd.DataFrame(result, columns=[predictedVariableName])
398 
399     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","Before: 374
After: 391",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,2460,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 44, 'parameters': 2, '(': 7, ',': 6, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 8, '.': 8, ')': 7, 'block': 3, 'pass_statement': 1, 'pass': 1, 'default_parameter': 1, '=': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'expression_statement': 6, 'assignment': 5, 'call': 5, 'argument_list': 5, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 3, 'subscript': 1, ']': 3, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1, 'list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6866318834839851,0.6735408689091861,"(tensor([0.9693]), tensor([0.9714]), tensor([0.9704]), tensor([0.9712]))"
"362     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
363         pass
364 
365     def _convertClassProbabilitiesToPredictions(self, predictedProbaDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
366         """"""
367         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
368         with predicted class labels
369 
370         :param predictedProbaDf: the output data frame from predictClassProbabilities
371         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
372         """"""
373         labels = self.getClassLabels()
374         dfCols = list(predictedProbaDf.columns)
375         if dfCols != labels:
376             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
377         yArray = predictedProbaDf.values
378         maxIndices = np.argmax(yArray, axis=1)
379         result = [labels[i] for i in maxIndices]
380         return pd.DataFrame(result, columns=[predictedVariableName])
381 
382     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","378     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
379         pass
380 
381     def _convertClassProbabilitiesToPredictions(self, predictedProbabilitiesDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
382         """"""
383         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
384         with predicted class labels
385 
386         :param predictedProbabilitiesDf: the output data frame from predictClassProbabilities
387         :param predictedVariableName:
388         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
389         """"""
390         labels = self.getClassLabels()
391         dfCols = list(predictedProbabilitiesDf.columns)
392         if dfCols != labels:
393             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
394         yArray = predictedProbabilitiesDf.values
395         maxIndices = np.argmax(yArray, axis=1)
396         result = [labels[i] for i in maxIndices]
397         return pd.DataFrame(result, columns=[predictedVariableName])
398 
399     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","Before: 377
After: 394",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,2496,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 44, 'parameters': 2, '(': 7, ',': 6, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 8, '.': 8, ')': 7, 'block': 3, 'pass_statement': 1, 'pass': 1, 'default_parameter': 1, '=': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'expression_statement': 6, 'assignment': 5, 'call': 5, 'argument_list': 5, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 3, 'subscript': 1, ']': 3, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1, 'list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6866318834839851,0.6735408689091861,"(tensor([0.9693]), tensor([0.9714]), tensor([0.9704]), tensor([0.9712]))"
"427     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
428         pass
429 
430     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
431         return self.convertClassProbabilitiesToPredictions(self._predictClassProbabilities(x))
432 
433 
","443     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
444         pass
445 
446     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
447         modelOutputVariableName = self.getModelOutputVariableNames()[0]
448         predictedProbabilitiesDf = self._predictClassProbabilities(x)
449         return self._convertClassProbabilitiesToPredictions(predictedProbabilitiesDf, predictedVariableName=modelOutputVariableName)
450 
451 
","Before: 431
After: 447, 448, 449",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,3025,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 19, 'parameters': 2, '(': 4, ',': 2, 'typed_parameter': 2, ':': 4, 'type': 4, 'attribute': 6, '.': 6, ')': 4, '->': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'return_statement': 1, 'return': 1, 'call': 2, 'argument_list': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4700168974426528,0.3706866381788037,"(tensor([0.8724]), tensor([0.9479]), tensor([0.9085]), tensor([0.9397]))"
"429 
430     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
431         return self.convertClassProbabilitiesToPredictions(self._predictClassProbabilities(x))
432 
433 
434 class RuleBasedRegressionModel(VectorRegressionModel):
435     def __init__(self):
436         super().__init__(checkInputColumns=False)
437 
438     def _underlyingModelRequiresFitting(self):
","447         modelOutputVariableName = self.getModelOutputVariableNames()[0]
448         predictedProbabilitiesDf = self._predictClassProbabilities(x)
449         return self._convertClassProbabilitiesToPredictions(predictedProbabilitiesDf, predictedVariableName=modelOutputVariableName)
450 
451 
452 class RuleBasedRegressionModel(VectorRegressionModel, ABC):
453     def __init__(self, predictedVariableNames: list):
454         """"""
455         :param predictedVariableNames: These are typically known at init time for rule based models
456         """"""
","Before: 434, 435
After: 452, 453, 454, 455, 456, 458",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,3034,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 19, 'parameters': 2, '(': 7, ',': 1, 'typed_parameter': 1, ':': 4, 'type': 2, 'attribute': 5, '.': 5, ')': 7, '->': 1, 'block': 3, 'return_statement': 1, 'return': 1, 'call': 4, 'argument_list': 5, 'class_definition': 1, 'class': 1, 'expression_statement': 1, 'keyword_argument': 1, '=': 1, 'false': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.13270673135882702,0.09823304244541617,"(tensor([0.8038]), tensor([0.8409]), tensor([0.8219]), tensor([0.8370]))"
"443 
444 
445 class RuleBasedClassificationModel(VectorClassificationModel, ABC):
446     def __init__(self, labels: list):
447         super(RuleBasedClassificationModel, self).__init__()
448         duplicate = getFirstDuplicate(labels)
449         if duplicate is not None:
450             raise Exception(f""Found duplicate label: {duplicate}"")
451         self._labels = labels
452 
453     def _underlyingModelRequiresFitting(self):
","475 
476 
477 class RuleBasedClassificationModel(VectorClassificationModel, ABC):
478     def __init__(self, labels: list, predictedVariableName=""predictedLabel""):
479         super().__init__(checkInputColumns=False)
480 
481         duplicate = getFirstDuplicate(labels)
482         if duplicate is not None:
483             raise Exception(f""Found duplicate label: {duplicate}"")
484         self._labels = labels
485         self._predictedVariableNames = [predictedVariableName]
486 
487     def _underlyingModelRequiresFitting(self):
","Before: 446, 447
After: 478, 479, 480, 485",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,3127,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 20, 'argument_list': 5, '(': 6, ',': 3, ')': 6, ':': 4, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 1, 'expression_statement': 3, 'call': 4, 'attribute': 2, '.': 2, 'assignment': 2, '=': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 1, '}': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5250447349083026,0.4861886182999406,"(tensor([0.8932]), tensor([0.9520]), tensor([0.9217]), tensor([0.9458]))"
"453     def _underlyingModelRequiresFitting(self):
454         return False
455 
456     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
457         pass","487     def _underlyingModelRequiresFitting(self):
488         return False
489 
490     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
491         pass
492 
493     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
","Before: 457
After: 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504",fix vector_model.py to work with rule based models,"Fixes in vector model and rule based model, fixed bug in classification _predict",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,e2eabd84250d7d8b049d947492fdb9bab4ffb295,419d01c1805e9d409e5581f07767b1c3a23840c9,1,3226,"{'module': 1, 'function_definition': 1, 'def': 1, 'identifier': 2, 'parameters': 1, '(': 1, ')': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'false': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4163358357591712,0.38113018929577874,"(tensor([0.9442]), tensor([0.9629]), tensor([0.9534]), tensor([0.9610]))"
"82         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
83         return self
84 
85     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
86         """"""
87         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
88         The transformers are ignored during the fit phase. Not supported for rule based models.
89 
90         **Important**: The output columns names of the last output transformer should be the same
91         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
92         (fit will run through without problems, though).
93 
94         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
95         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
96         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
97         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
98         by some heuristics or by hand-crafted rules.
99 
100         **How not to use**: Output transformers are not meant to transform the predictions into something with a
101         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
102         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
103         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
104         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
105         probabilistic classifiers the output transformers will not be applied to the probability vectors
106         but to the actual label predictions, unless explicitly stated otherwise.
107 
108         :param outputTransformers: DataFrameTransformers for the transformation of outputs
109             (after the model has been applied)
110         :return: self
111         """"""
112         # Since we have to forbid target transformers for rule based models, we might as well forbid output transformers as well
113         # There is no reason for post processing in rule based models.
114         if not self._underlyingModelRequiresFitting():
115             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
116         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
117         return self
118 
119     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","81         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
82         return self
83 
84     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
85         """"""
86         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
87         The transformers are ignored during the fit phase. Not supported for rule-based models.
88 
89         **Important**: The output columns names of the last output transformer should be the same
90         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
91         (fit will run through without problems, though).
92 
93         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
94         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
95         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
96         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
97         by some heuristics or by hand-crafted rules.
98 
99         **How not to use**: Output transformers are not meant to transform the predictions into something with a
100         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
101         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
102         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
103         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
104         probabilistic classifiers the output transformers will not be applied to the probability vectors
105         but to the actual label predictions, unless explicitly stated otherwise.
106 
107         :param outputTransformers: DataFrameTransformers for the transformation of outputs
108             (after the model has been applied)
109         :return: self
110         """"""
111         # Since we have to forbid target transformers for rule-based models, we might as well forbid output transformers as well
112         # There is no reason for post processing in rule-based models.
113         if not self._underlyingModelRequiresFitting():
114             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
115         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
116         return self
117 
118     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","Before: 88
After: 87",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,591,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 5, 'identifier': 24, '.': 5, '=': 2, 'call': 4, 'argument_list': 4, '(': 5, 'list_splat': 2, '*': 3, ')': 5, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, ':': 3, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '->': 1, 'block': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'comment': 2, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8184311453054041,0.8177885789876015,"(tensor([0.9902]), tensor([0.9910]), tensor([0.9906]), tensor([0.9909]))"
"82         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
83         return self
84 
85     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
86         """"""
87         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
88         The transformers are ignored during the fit phase. Not supported for rule based models.
89 
90         **Important**: The output columns names of the last output transformer should be the same
91         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
92         (fit will run through without problems, though).
93 
94         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
95         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
96         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
97         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
98         by some heuristics or by hand-crafted rules.
99 
100         **How not to use**: Output transformers are not meant to transform the predictions into something with a
101         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
102         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
103         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
104         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
105         probabilistic classifiers the output transformers will not be applied to the probability vectors
106         but to the actual label predictions, unless explicitly stated otherwise.
107 
108         :param outputTransformers: DataFrameTransformers for the transformation of outputs
109             (after the model has been applied)
110         :return: self
111         """"""
112         # Since we have to forbid target transformers for rule based models, we might as well forbid output transformers as well
113         # There is no reason for post processing in rule based models.
114         if not self._underlyingModelRequiresFitting():
115             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
116         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
117         return self
118 
119     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","81         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
82         return self
83 
84     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
85         """"""
86         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
87         The transformers are ignored during the fit phase. Not supported for rule-based models.
88 
89         **Important**: The output columns names of the last output transformer should be the same
90         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
91         (fit will run through without problems, though).
92 
93         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
94         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
95         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
96         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
97         by some heuristics or by hand-crafted rules.
98 
99         **How not to use**: Output transformers are not meant to transform the predictions into something with a
100         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
101         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
102         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
103         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
104         probabilistic classifiers the output transformers will not be applied to the probability vectors
105         but to the actual label predictions, unless explicitly stated otherwise.
106 
107         :param outputTransformers: DataFrameTransformers for the transformation of outputs
108             (after the model has been applied)
109         :return: self
110         """"""
111         # Since we have to forbid target transformers for rule-based models, we might as well forbid output transformers as well
112         # There is no reason for post processing in rule-based models.
113         if not self._underlyingModelRequiresFitting():
114             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
115         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
116         return self
117 
118     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","Before: 112, 113
After: 111, 112",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,555,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 5, 'identifier': 24, '.': 5, '=': 2, 'call': 4, 'argument_list': 4, '(': 5, 'list_splat': 2, '*': 3, ')': 5, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, ':': 3, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '->': 1, 'block': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'comment': 2, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8184311453054041,0.8177885789876015,"(tensor([0.9902]), tensor([0.9910]), tensor([0.9906]), tensor([0.9909]))"
"116         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
117         return self
118 
119     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
120         """"""
121         Makes the model use the given target transformers. Not supported for rule based models.
122 
123         NOTE: all feature generators and data frame transformers will be fit on the untransformed target.
124         The targetTransformer only affects the fit of the internal model.
125 
126         :param targetTransformer: a transformer which transforms the targets (training data outputs) prior to learning the model, such
127             that the model learns to predict the transformed outputs. When predicting, the inverse transformer is applied after applying
128             the model, i.e. the transformation is completely transparent when applying the model.
129         :return: self
130         """"""
131         # Note: it is important to disallow targetTransformers for rule based models since we need
132         # predictedVarNames and modelOutputVarNames to coincide there.
133         if not self._underlyingModelRequiresFitting():
134             raise Exception(f""Target transformers are not supported for model of type {self.__class__.__name__}"")
135         self._targetTransformer = targetTransformer
136         return self
137 
138     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","115         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
116         return self
117 
118     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
119         """"""
120         Makes the model use the given target transformers. Not supported for rule-based models.
121 
122         NOTE: all feature generators and data frame transformers will be fit on the untransformed target.
123         The targetTransformer only affects the fit of the internal model.
124 
125         :param targetTransformer: a transformer which transforms the targets (training data outputs) prior to learning the model, such
126             that the model learns to predict the transformed outputs. When predicting, the inverse transformer is applied after applying
127             the model, i.e. the transformation is completely transparent when applying the model.
128         :return: self
129         """"""
130         # Note: it is important to disallow targetTransformers for rule-based models since we need
131         # predictedVarNames and modelOutputVarNames to coincide there.
132         if not self._underlyingModelRequiresFitting():
133             raise Exception(f""Target transformers are not supported for model of type {self.__class__.__name__}"")
134         self._targetTransformer = targetTransformer
135         return self
136 
137     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","Before: 121
After: 120",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,653,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 5, 'identifier': 21, '.': 5, '=': 2, 'call': 3, 'argument_list': 3, '(': 4, 'list_splat': 1, '*': 1, ')': 4, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'comment': 2, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.767908133361904,0.7658080269032905,"(tensor([0.9876]), tensor([0.9898]), tensor([0.9887]), tensor([0.9896]))"
"116         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
117         return self
118 
119     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
120         """"""
121         Makes the model use the given target transformers. Not supported for rule based models.
122 
123         NOTE: all feature generators and data frame transformers will be fit on the untransformed target.
124         The targetTransformer only affects the fit of the internal model.
125 
126         :param targetTransformer: a transformer which transforms the targets (training data outputs) prior to learning the model, such
127             that the model learns to predict the transformed outputs. When predicting, the inverse transformer is applied after applying
128             the model, i.e. the transformation is completely transparent when applying the model.
129         :return: self
130         """"""
131         # Note: it is important to disallow targetTransformers for rule based models since we need
132         # predictedVarNames and modelOutputVarNames to coincide there.
133         if not self._underlyingModelRequiresFitting():
134             raise Exception(f""Target transformers are not supported for model of type {self.__class__.__name__}"")
135         self._targetTransformer = targetTransformer
136         return self
137 
138     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","115         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
116         return self
117 
118     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
119         """"""
120         Makes the model use the given target transformers. Not supported for rule-based models.
121 
122         NOTE: all feature generators and data frame transformers will be fit on the untransformed target.
123         The targetTransformer only affects the fit of the internal model.
124 
125         :param targetTransformer: a transformer which transforms the targets (training data outputs) prior to learning the model, such
126             that the model learns to predict the transformed outputs. When predicting, the inverse transformer is applied after applying
127             the model, i.e. the transformation is completely transparent when applying the model.
128         :return: self
129         """"""
130         # Note: it is important to disallow targetTransformers for rule-based models since we need
131         # predictedVarNames and modelOutputVarNames to coincide there.
132         if not self._underlyingModelRequiresFitting():
133             raise Exception(f""Target transformers are not supported for model of type {self.__class__.__name__}"")
134         self._targetTransformer = targetTransformer
135         return self
136 
137     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","Before: 131
After: 130",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,639,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 5, 'identifier': 21, '.': 5, '=': 2, 'call': 3, 'argument_list': 3, '(': 4, 'list_splat': 1, '*': 1, ')': 4, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'comment': 2, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.767908133361904,0.7658080269032905,"(tensor([0.9876]), tensor([0.9898]), tensor([0.9887]), tensor([0.9896]))"
"270                 X = self._featureGenerator.fitGenerate(X, Y, self)
271         self._inputTransformerChain.fit(X)
272 
273     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True, fitTargetTransformer=True):
274         """"""
275         Fits the model using the given data
276 
277         :param X: a data frame containing input data
278         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
279             fitting, e.g. with rule based models
280         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
281             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
282             an exception will be raised.
283         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
284             If it requires fitting, was not separately fit before and this option is set to False,
285             an exception will be raised.
286         """"""
287         log.info(f""Training {self.__class__.__name__}"")
288         self._predictedVariableNames = list(Y.columns)
289         if not self._underlyingModelRequiresFitting():
290             self._fitPreprocessors(X, Y=Y)
291         else:
292             if Y is None:
293                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
294             X = self._computeProcessedInputs(X, Y=Y, fit=fitPreprocessors)
295             if self._targetTransformer is not None:
296                 if fitTargetTransformer:
297                     Y = self._targetTransformer.fitApply(Y)
298                 else:
299                     Y = self._targetTransformer.apply(Y)
300             self._modelInputVariableNames = list(X.columns)
301             self._modelOutputVariableNames = list(Y.columns)
302             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
303             self._fit(X, Y)
304             self._isFitted = True
305 
306     @abstractmethod
","269                 X = self._featureGenerator.fitGenerate(X, Y, self)
270         self._inputTransformerChain.fit(X)
271 
272     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True, fitTargetTransformer=True):
273         """"""
274         Fits the model using the given data
275 
276         :param X: a data frame containing input data
277         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
278             fitting, e.g. with rule-based models
279         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
280             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
281             an exception will be raised.
282         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
283             If it requires fitting, was not separately fit before and this option is set to False,
284             an exception will be raised.
285         """"""
286         log.info(f""Training {self.__class__.__name__}"")
287         self._predictedVariableNames = list(Y.columns)
288         if not self._underlyingModelRequiresFitting():
289             self._fitPreprocessors(X, Y=Y)
290         else:
291             if Y is None:
292                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
293             X = self._computeProcessedInputs(X, Y=Y, fit=fitPreprocessors)
294             if self._targetTransformer is not None:
295                 if fitTargetTransformer:
296                     Y = self._targetTransformer.fitApply(Y)
297                 else:
298                     Y = self._targetTransformer.apply(Y)
299             self._modelInputVariableNames = list(X.columns)
300             self._modelOutputVariableNames = list(Y.columns)
301             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
302             self._fit(X, Y)
303             self._isFitted = True
304 
305     @abstractmethod
","Before: 279
After: 278",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,1731,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'identifier': 97, '=': 13, 'call': 17, 'attribute': 33, '.': 33, 'argument_list': 17, '(': 18, ',': 10, ')': 18, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 9, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 2, 'true': 3, 'block': 7, 'string': 6, 'string_start': 6, 'string_content': 10, 'string_end': 6, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 4, 'if': 4, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 2, 'else': 2, 'comparison_operator': 2, 'is': 1, 'none': 2, 'raise_statement': 1, 'raise': 1, 'is not': 2, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.786764617173031,0.7862471951365867,"(tensor([0.9901]), tensor([0.9906]), tensor([0.9903]), tensor([0.9905]))"
"307     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
308         pass
309 
310     def getModelOutputVariableNames(self):
311         """"""
312         Gets the list of variable names predicted by the underlying model.
313         For the case where at training time the ground truth is transformed by a target transformer
314         which changes column names, the names of the variables prior to the transformation will be returned.
315         Thus this method always returns the variable names that are actually predicted by the underlying model alone.
316         For the variable names that are ultimately output by the entire VectorModel instance when calling predict,
317         use getPredictedVariableNames.
318         """"""
319         # Note that this method is needed in RuleBasedClassificationModel, so we cannot just raise an exception
320         if not self._underlyingModelRequiresFitting():
321             return self.getPredictedVariableNames()
322         return self._modelOutputVariableNames
323 
324     def getPredictedVariableNames(self):
","306     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
307         pass
308 
309     def getModelOutputVariableNames(self):
310         """"""
311         Gets the list of variable names predicted by the underlying model.
312         For the case where at training time the ground truth is transformed by a target transformer
313         which changes column names, the names of the variables prior to the transformation will be returned.
314         Thus this method always returns the variable names that are actually predicted by the underlying model alone.
315         For the variable names that are ultimately output by the entire VectorModel instance when calling predict,
316         use getPredictedVariableNames.
317         """"""
318         # Note that this method is needed in RuleBasedVectorClassificationModel, so we cannot just raise an exception
319         if not self._underlyingModelRequiresFitting():
320             return self.getPredictedVariableNames()
321         return self._modelOutputVariableNames
322 
323     def getPredictedVariableNames(self):
","Before: 319
After: 318",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,2036,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 16, 'parameters': 2, '(': 4, ',': 2, 'typed_parameter': 2, ':': 5, 'type': 2, 'attribute': 5, '.': 5, ')': 4, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'comment': 1, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'call': 2, 'argument_list': 2, 'return_statement': 2, 'return': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.758460995974937,0.7562808874698215,"(tensor([0.9910]), tensor([0.9917]), tensor([0.9913]), tensor([0.9916]))"
"395         result = [labels[i] for i in maxIndices]
396         return pd.DataFrame(result, columns=[predictedVariableName])
397 
398     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
399         """"""
400         Converts from a result returned by predictClassProbabilities to a result as return by the model
401         prior to application of target and output transformers.
402 
403         :param df: the output data frame from predictClassProbabilities
404         :return: an output data frame as it would be returned by predict
405         """"""
406         modelOutputVariableName = self.getModelOutputVariableNames()[0]
407         y = self._convertClassProbabilitiesToPredictions(df, predictedVariableName=modelOutputVariableName)
408         y = self._applyPostProcessing(y)
409         return y
410 
411     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","396         result = [labels[i] for i in maxIndices]
397         return pd.DataFrame(result, columns=[predictedVariableName])
398 
399     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
400         """"""
401         Converts from a result returned by predictClassProbabilities to a result as return by predict.
402 
403         :param df: the output data frame from predictClassProbabilities
404         :return: an output data frame as it would be returned by predict
405         """"""
406         modelOutputVariableName = self.getModelOutputVariableNames()[0]
407         y = self._convertClassProbabilitiesToPredictions(df, predictedVariableName=modelOutputVariableName)
408         y = self._applyPostProcessing(y)
409         return y
410 
411     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 400, 401
After: 401",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,2716,"{'module': 1, 'expression_statement': 5, 'assignment': 4, 'identifier': 29, '=': 6, 'list_comprehension': 1, '[': 4, 'subscript': 2, ']': 4, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 4, 'attribute': 5, '.': 5, 'argument_list': 4, '(': 5, ',': 3, 'keyword_argument': 2, 'list': 1, ')': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.829904283799759,0.8289612092268525,"(tensor([0.9862]), tensor([0.9667]), tensor([0.9764]), tensor([0.9687]))"
"446         modelOutputVariableName = self.getModelOutputVariableNames()[0]
447         predictedProbabilitiesDf = self._predictClassProbabilities(x)
448         return self._convertClassProbabilitiesToPredictions(predictedProbabilitiesDf, predictedVariableName=modelOutputVariableName)
449 
450 
451 class RuleBasedRegressionModel(VectorRegressionModel, ABC):
452     def __init__(self, predictedVariableNames: list):
453         """"""
454         :param predictedVariableNames: These are typically known at init time for rule based models
455         """"""
","446         modelOutputVariableName = self.getModelOutputVariableNames()[0]
447         predictedProbabilitiesDf = self._predictClassProbabilities(x)
448         return self._convertClassProbabilitiesToPredictions(predictedProbabilitiesDf, predictedVariableName=modelOutputVariableName)
449 
450 
451 class RuleBasedVectorRegressionModel(VectorRegressionModel, ABC):
452     def __init__(self, predictedVariableNames: list):
453         """"""
454         :param predictedVariableNames: These are typically known at init time for rule-based models
455         """"""
","Before: 451
After: 451",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,3188,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'identifier': 32, '=': 3, 'subscript': 1, 'call': 3, 'attribute': 3, '.': 3, 'argument_list': 4, '(': 5, ')': 5, '[': 1, 'integer': 1, ']': 1, 'return_statement': 1, 'return': 1, ',': 3, 'keyword_argument': 1, 'class_definition': 1, 'class': 1, ':': 5, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 2, 'ERROR': 2, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9114122111092942,0.905705212940665,"(tensor([0.9940]), tensor([0.9974]), tensor([0.9957]), tensor([0.9971]))"
"449 
450 
451 class RuleBasedRegressionModel(VectorRegressionModel, ABC):
452     def __init__(self, predictedVariableNames: list):
453         """"""
454         :param predictedVariableNames: These are typically known at init time for rule based models
455         """"""
456         super().__init__(checkInputColumns=False)
457         self._predictedVariableNames = predictedVariableNames
458 
459     def _underlyingModelRequiresFitting(self):
","449 
450 
451 class RuleBasedVectorRegressionModel(VectorRegressionModel, ABC):
452     def __init__(self, predictedVariableNames: list):
453         """"""
454         :param predictedVariableNames: These are typically known at init time for rule-based models
455         """"""
456         super().__init__(checkInputColumns=False)
457         self._predictedVariableNames = predictedVariableNames
458 
459     def _underlyingModelRequiresFitting(self):
","Before: 454
After: 454",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,3224,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 13, 'argument_list': 3, '(': 4, ',': 2, ')': 4, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 1, 'expression_statement': 3, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 2, 'attribute': 2, '.': 2, 'keyword_argument': 1, '=': 2, 'false': 1, 'assignment': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9058415926238057,0.8984763729733842,"(tensor([0.9908]), tensor([0.9956]), tensor([0.9932]), tensor([0.9951]))"
"471         :param kwargs: for consistency with VectorModel interface, will be ignored
472         """"""
473         super().fit(X, Y, fitPreprocessors=True, fitTargetTransformer=False)
474 
475 
476 class RuleBasedClassificationModel(VectorClassificationModel, ABC):
477     def __init__(self, labels: list, predictedVariableName=""predictedLabel""):
478         super().__init__(checkInputColumns=False)
479 
480         duplicate = getFirstDuplicate(labels)
","471         :param kwargs: for consistency with VectorModel interface, will be ignored
472         """"""
473         super().fit(X, Y, fitPreprocessors=True, fitTargetTransformer=False)
474 
475 
476 class RuleBasedVectorClassificationModel(VectorClassificationModel, ABC):
477     def __init__(self, labels: list, predictedVariableName=""predictedLabel""):
478         super().__init__(checkInputColumns=False)
479 
480         duplicate = getFirstDuplicate(labels)
","Before: 476
After: 476",fix typos in vector_model and fittablemodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,1,3349,"{'module': 1, 'ERROR': 3, ':': 2, 'identifier': 9, 'expression_statement': 1, 'assignment': 1, 'type': 1, 'with': 1, 'with_item': 1, ',': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9656187881844999,0.9623115278938064,"(tensor([0.9974]), tensor([0.9983]), tensor([0.9978]), tensor([0.9982]))"
"5 import pytest
6 
7 from sensai.data_transformation import DFTDRowFilterOnIndex, \
8     InvertibleDataFrameTransformer
9 from sensai.featuregen import FeatureGeneratorTakeColumns, FeatureGenerator
10 from sensai.vector_model import VectorModel, RuleBasedRegressionModel
11 
12 
13 class FittableFgen(FeatureGenerator):
14 
","5 import pytest
6 
7 from sensai.data_transformation import DFTDRowFilterOnIndex, \
8     InvertibleDataFrameTransformer
9 from sensai.featuregen import FeatureGeneratorTakeColumns, FeatureGenerator
10 from sensai.vector_model import VectorModel, RuleBasedVectorRegressionModel
11 
12 
13 class FittableFgen(FeatureGenerator):
14 
","Before: 10
After: 10",fix tests for samplerulebasedvectormodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,0,64,"{'module': 1, 'import_statement': 1, 'import': 4, 'dotted_name': 10, 'identifier': 15, 'import_from_statement': 3, 'from': 3, '.': 3, ',': 3, 'line_continuation': 1, 'class_definition': 1, 'class': 1, 'argument_list': 1, '(': 1, ')': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9278982724420874,0.9217324939947308,"(tensor([0.9970]), tensor([0.9984]), tensor([0.9977]), tensor([0.9982]))"
"29 
30     def _apply(self, df: pd.DataFrame) -> pd.DataFrame:
31         return df
32 
33 
34 class SampleRuleBasedModel(RuleBasedRegressionModel):
35     def __init__(self):
36         super(SampleRuleBasedModel, self).__init__(predictedVariableNames=[""prediction""])
37 
38     def _predict(self, X: pd.DataFrame) -> pd.DataFrame:
","29 
30     def _apply(self, df: pd.DataFrame) -> pd.DataFrame:
31         return df
32 
33 
34 class SampleRuleBasedVectorModel(RuleBasedVectorRegressionModel):
35     def __init__(self):
36         super(SampleRuleBasedVectorModel, self).__init__(predictedVariableNames=[""prediction""])
37 
38     def _predict(self, X: pd.DataFrame) -> pd.DataFrame:
","Before: 34
After: 34",fix tests for samplerulebasedvectormodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,0,233,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 5, ',': 2, 'typed_parameter': 1, ':': 4, 'type': 2, 'attribute': 3, '.': 3, ')': 5, '->': 1, 'block': 3, 'return_statement': 1, 'return': 1, 'class_definition': 1, 'class': 1, 'argument_list': 3, 'expression_statement': 1, 'call': 2, 'keyword_argument': 1, '=': 1, 'list': 1, '[': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.891949028188207,0.8771230008392822,"(tensor([0.9865]), tensor([0.9956]), tensor([0.9910]), tensor([0.9946]))"
"32 
33 
34 class SampleRuleBasedModel(RuleBasedRegressionModel):
35     def __init__(self):
36         super(SampleRuleBasedModel, self).__init__(predictedVariableNames=[""prediction""])
37 
38     def _predict(self, X: pd.DataFrame) -> pd.DataFrame:
","32 
33 
34 class SampleRuleBasedVectorModel(RuleBasedVectorRegressionModel):
35     def __init__(self):
36         super(SampleRuleBasedVectorModel, self).__init__(predictedVariableNames=[""prediction""])
37 
38     def _predict(self, X: pd.DataFrame) -> pd.DataFrame:
","Before: 36
After: 36",fix tests for samplerulebasedvectormodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,0,268,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 9, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, ',': 1, '.': 1, 'keyword_argument': 1, '=': 1, 'list': 1, '[': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.848043456060569,0.8291070488079563,"(tensor([0.9829]), tensor([0.9935]), tensor([0.9882]), tensor([0.9925]))"
"78 
79 
80 @pytest.fixture()
81 def ruleBasedModel():
82     return SampleRuleBasedModel()
83 
84 
","78 
79 
80 @pytest.fixture()
81 def ruleBasedModel():
82     return SampleRuleBasedVectorModel()
83 
84 
","Before: 82
After: 82",fix tests for samplerulebasedvectormodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,0,594,"{'module': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'call': 2, 'attribute': 1, 'identifier': 4, '.': 1, 'argument_list': 2, '(': 3, ')': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8578928092681435,0.8314643097834779,"(tensor([0.9854]), tensor([0.9944]), tensor([0.9899]), tensor([0.9935]))"
"96     model.fit(testX, testY)
97     return model
98 
99 
100 class TestIsFitted:
101     @pytest.mark.parametrize(""model"", [SampleRuleBasedModel(), fittedVectorModel()])
102     def test_isFittedWhenPreprocessorsRuleBased(self, model, ruleBasedDFT, ruleBasedFgen):
103         assert model.isFitted()
104         model.withFeatureGenerator(ruleBasedFgen)
105         model.withInputTransformers(ruleBasedDFT)
","96     model.fit(testX, testY)
97     return model
98 
99 
100 class TestIsFitted:
101     @pytest.mark.parametrize(""model"", [SampleRuleBasedVectorModel(), fittedVectorModel()])
102     def test_isFittedWhenPreprocessorsRuleBased(self, model, ruleBasedDFT, ruleBasedFgen):
103         assert model.isFitted()
104         model.withFeatureGenerator(ruleBasedFgen)
105         model.withInputTransformers(ruleBasedDFT)
","Before: 101
After: 101",fix tests for samplerulebasedvectormodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,0,748,"{'module': 1, 'expression_statement': 2, 'call': 6, 'attribute': 5, 'identifier': 21, '.': 5, 'argument_list': 6, '(': 7, ',': 6, ')': 7, 'return_statement': 1, 'return': 1, 'class_definition': 1, 'class': 1, ':': 2, 'block': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'list': 1, '[': 1, ']': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assert_statement': 1, 'assert': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9581282631934457,0.9531893685723627,"(tensor([0.9965]), tensor([0.9975]), tensor([0.9970]), tensor([0.9974]))"
"103         assert model.isFitted()
104         model.withFeatureGenerator(ruleBasedFgen)
105         model.withInputTransformers(ruleBasedDFT)
106         assert model.isFitted()
107 
108     @pytest.mark.parametrize(""modelConstructor"", [SampleRuleBasedModel, fittedVectorModel])
109     def test_isFittedWithFittableProcessors(self, modelConstructor, fittableDFT, fittableFgen):
110         # is fitted after fit with model
111         model = modelConstructor().withInputTransformers(fittableDFT)
112         assert not model.isFitted()
","103         assert model.isFitted()
104         model.withFeatureGenerator(ruleBasedFgen)
105         model.withInputTransformers(ruleBasedDFT)
106         assert model.isFitted()
107 
108     @pytest.mark.parametrize(""modelConstructor"", [SampleRuleBasedVectorModel, fittedVectorModel])
109     def test_isFittedWithFittableProcessors(self, modelConstructor, fittableDFT, fittableFgen):
110         # is fitted after fit with model
111         model = modelConstructor().withInputTransformers(fittableDFT)
112         assert not model.isFitted()
","Before: 108
After: 108",fix tests for samplerulebasedvectormodel,"Added Vector prefix to RuleBasedModel class names, fixed typos and docu",https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,49f5c4a2c0cf614f3cbd69b006d84143f9c2ec22,8d398b6196d44a2187f39ec2454114337d6abbf8,0,828,"{'module': 1, 'assert_statement': 2, 'assert': 2, 'call': 7, 'attribute': 7, 'identifier': 24, '.': 7, 'argument_list': 7, '(': 8, ')': 8, 'expression_statement': 3, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ',': 5, 'list': 1, '[': 1, ']': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'comment': 1, 'block': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9641384561775734,0.9598421558461958,"(tensor([0.9969]), tensor([0.9978]), tensor([0.9973]), tensor([0.9977]))"
"353 
354 
355 class VectorClassificationModel(VectorModel, ABC):
356     def __init__(self):
357         """"""
358         """"""
359         super().__init__()
360         self._labels = None
361 
362     def isRegressionModel(self) -> bool:
","353 
354 
355 class VectorClassificationModel(VectorModel, ABC):
356     def __init__(self, checkInputColumns=True):
357         """"""
358         :param checkInputColumns: Whether to check if the input column list (after feature generation)
359             during inference coincides with the input column list during fit.
360             This should be disabled if feature generation is not performed by the model itself,
361             e.g. in ensemble models.
362         """"""
363         super().__init__(checkInputColumns=checkInputColumns)
364         self._labels = None
365 
366     def isRegressionModel(self) -> bool:
","Before: 356
After: 356, 358, 359, 360, 361",update vector_model and vectorclassificationmodel,Fixed bug in VectorClassificationModel init,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,8a025a5d2441ae1ddc1809d8c94046c1030eb538,f50003e355104f699b7be4cf89830f8cd2b1118c,1,2317,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 9, 'argument_list': 3, '(': 4, ',': 1, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 3, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 2, 'attribute': 2, '.': 2, 'assignment': 1, '=': 1, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3341859126550578,0.3003029552424337,"(tensor([0.7562]), tensor([0.9427]), tensor([0.8392]), tensor([0.9200]))"
"353 
354 
355 class VectorClassificationModel(VectorModel, ABC):
356     def __init__(self):
357         """"""
358         """"""
359         super().__init__()
360         self._labels = None
361 
362     def isRegressionModel(self) -> bool:
","353 
354 
355 class VectorClassificationModel(VectorModel, ABC):
356     def __init__(self, checkInputColumns=True):
357         """"""
358         :param checkInputColumns: Whether to check if the input column list (after feature generation)
359             during inference coincides with the input column list during fit.
360             This should be disabled if feature generation is not performed by the model itself,
361             e.g. in ensemble models.
362         """"""
363         super().__init__(checkInputColumns=checkInputColumns)
364         self._labels = None
365 
366     def isRegressionModel(self) -> bool:
","Before: 359
After: 363",update vector_model and vectorclassificationmodel,Fixed bug in VectorClassificationModel init,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,8a025a5d2441ae1ddc1809d8c94046c1030eb538,f50003e355104f699b7be4cf89830f8cd2b1118c,1,2335,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 9, 'argument_list': 3, '(': 4, ',': 1, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'expression_statement': 3, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 2, 'attribute': 2, '.': 2, 'assignment': 1, '=': 1, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3341859126550578,0.3003029552424337,"(tensor([0.7562]), tensor([0.9427]), tensor([0.8392]), tensor([0.9200]))"
"170         self.setName(name)
171         return self
172 
173     def getTargetTransformer(self):
174         return self._targetTransformer
175 
176     def _applyPostProcessing(self, y: pd.DataFrame):
","118         self.setName(name)
119         return self
120 
121     def _preProcessorsAreFitted(self):
122         result = self._inputTransformerChain.isFitted()
123         if self.getFeatureGenerator() is not None:
124             result = result and self.getFeatureGenerator().isFitted()
125         return result
126 
127     def isFitted(self):
","Before: 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194
After: 121, 122",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,806,"{'module': 1, 'expression_statement': 1, 'call': 1, 'attribute': 2, 'identifier': 8, '.': 2, 'argument_list': 1, '(': 2, ')': 2, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.11653232247227953,0.07307844209030134,"(tensor([0.7641]), tensor([0.8277]), tensor([0.7946]), tensor([0.8208]))"
"196             result = result and self.getFeatureGenerator().isFitted()
197         return result
198 
199     def isFitted(self):
200         underlyingModelIsFitted = not self._underlyingModelRequiresFitting() or self._isFitted
201         if not underlyingModelIsFitted:
202             return False
203         if not self._prePostProcessorsAreFitted():
204             return False
205         if self._targetTransformer is not None and not self._targetTransformer.isFitted():
206             return False
207         return True
208 
209     def _checkModelInputColumns(self, modelInput: pd.DataFrame):
","124             result = result and self.getFeatureGenerator().isFitted()
125         return result
126 
127     def isFitted(self):
128         underlyingModelIsFitted = not self._underlyingModelRequiresFitting() or self._isFitted
129         if not underlyingModelIsFitted:
130             return False
131         if not self._preProcessorsAreFitted():
132             return False
133         return True
134 
135     def _checkModelInputColumns(self, modelInput: pd.DataFrame):
","Before: 203, 204, 205
After: 131",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,1091,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 21, '=': 2, 'boolean_operator': 3, 'and': 2, 'call': 5, 'attribute': 8, '.': 8, 'argument_list': 5, '(': 6, ')': 6, 'return_statement': 5, 'return': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 4, 'block': 4, 'not_operator': 4, 'not': 4, 'or': 1, 'if_statement': 3, 'if': 3, 'false': 3, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.432355232340772,0.39629975178342347,"(tensor([0.9633]), tensor([0.9289]), tensor([0.9458]), tensor([0.9322]))"
"269                 X = self._featureGenerator.fitGenerate(X, Y, self)
270         self._inputTransformerChain.fit(X)
271 
272     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True, fitTargetTransformer=True):
273         """"""
274         Fits the model using the given data
275 
276         :param X: a data frame containing input data
277         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
278             fitting, e.g. with rule-based models
279         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
280             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
281             an exception will be raised.
282         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
283             If it requires fitting, was not separately fit before and this option is set to False,
284             an exception will be raised.
285         """"""
286         log.info(f""Training {self.__class__.__name__}"")
287         self._predictedVariableNames = list(Y.columns)
288         if not self._underlyingModelRequiresFitting():
289             self._fitPreprocessors(X, Y=Y)
290         else:
291             if Y is None:
292                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
293             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
294             if self._targetTransformer is not None:
295                 if fitTargetTransformer:
296                     Y = self._targetTransformer.fitApply(Y)
297                 else:
298                     Y = self._targetTransformer.apply(Y)
299             self._modelInputVariableNames = list(X.columns)
300             self._modelOutputVariableNames = list(Y.columns)
301             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
302             self._fit(X, Y)
303             self._isFitted = True
304 
305     @abstractmethod
","194                 X = self._featureGenerator.fitGenerate(X, Y, self)
195         self._inputTransformerChain.fit(X)
196 
197     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
198         """"""
199         Fits the model using the given data
200 
201         :param X: a data frame containing input data
202         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
203             fitting, e.g. with rule-based models
204         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
205             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
206             an exception will be raised.
207         """"""
208         log.info(f""Training {self.__class__.__name__}"")
209         self._predictedVariableNames = list(Y.columns)
210         if not self._underlyingModelRequiresFitting():
211             self._fitPreprocessors(X, Y=Y)
212         else:
213             if Y is None:
214                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
215             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
216             self._modelInputVariableNames = list(X.columns)
217             log.info(
218                 f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
219             self._fit(X, Y)
220             self._isFitted = True
221 
222     @abstractmethod
","Before: 272
After: 197",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,1679,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'identifier': 97, '=': 13, 'call': 17, 'attribute': 33, '.': 33, 'argument_list': 17, '(': 18, ',': 10, ')': 18, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 9, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 2, 'true': 3, 'block': 7, 'string': 6, 'string_start': 6, 'string_content': 10, 'string_end': 6, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 4, 'if': 4, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 2, 'else': 2, 'comparison_operator': 2, 'is': 1, 'none': 2, 'raise_statement': 1, 'raise': 1, 'is not': 2, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5809218880860484,0.5723715835420988,"(tensor([0.9378]), tensor([0.9141]), tensor([0.9258]), tensor([0.9164]))"
"269                 X = self._featureGenerator.fitGenerate(X, Y, self)
270         self._inputTransformerChain.fit(X)
271 
272     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True, fitTargetTransformer=True):
273         """"""
274         Fits the model using the given data
275 
276         :param X: a data frame containing input data
277         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
278             fitting, e.g. with rule-based models
279         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
280             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
281             an exception will be raised.
282         :param fitTargetTransformer: if False, the model's target transformer will not be fitted.
283             If it requires fitting, was not separately fit before and this option is set to False,
284             an exception will be raised.
285         """"""
286         log.info(f""Training {self.__class__.__name__}"")
287         self._predictedVariableNames = list(Y.columns)
288         if not self._underlyingModelRequiresFitting():
289             self._fitPreprocessors(X, Y=Y)
290         else:
291             if Y is None:
292                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
293             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
294             if self._targetTransformer is not None:
295                 if fitTargetTransformer:
296                     Y = self._targetTransformer.fitApply(Y)
297                 else:
298                     Y = self._targetTransformer.apply(Y)
299             self._modelInputVariableNames = list(X.columns)
300             self._modelOutputVariableNames = list(Y.columns)
301             log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
302             self._fit(X, Y)
303             self._isFitted = True
304 
305     @abstractmethod
","194                 X = self._featureGenerator.fitGenerate(X, Y, self)
195         self._inputTransformerChain.fit(X)
196 
197     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
198         """"""
199         Fits the model using the given data
200 
201         :param X: a data frame containing input data
202         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
203             fitting, e.g. with rule-based models
204         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
205             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
206             an exception will be raised.
207         """"""
208         log.info(f""Training {self.__class__.__name__}"")
209         self._predictedVariableNames = list(Y.columns)
210         if not self._underlyingModelRequiresFitting():
211             self._fitPreprocessors(X, Y=Y)
212         else:
213             if Y is None:
214                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
215             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
216             self._modelInputVariableNames = list(X.columns)
217             log.info(
218                 f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
219             self._fit(X, Y)
220             self._isFitted = True
221 
222     @abstractmethod
","Before: 294, 295, 296, 297, 298, 300, 301
After: 217, 218",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,1810,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'identifier': 97, '=': 13, 'call': 17, 'attribute': 33, '.': 33, 'argument_list': 17, '(': 18, ',': 10, ')': 18, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 9, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 2, 'true': 3, 'block': 7, 'string': 6, 'string_start': 6, 'string_content': 10, 'string_end': 6, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 4, 'if': 4, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 2, 'else': 2, 'comparison_operator': 2, 'is': 1, 'none': 2, 'raise_statement': 1, 'raise': 1, 'is not': 2, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5809218880860484,0.5723715835420988,"(tensor([0.9378]), tensor([0.9141]), tensor([0.9258]), tensor([0.9164]))"
"379     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
380         pass
381 
382     def _convertClassProbabilitiesToPredictions(self, predictedProbabilitiesDf: pd.DataFrame, predictedVariableName=""predictedLabel""):
383         """"""
384         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
385         with predicted class labels
386 
387         :param predictedProbabilitiesDf: the output data frame from predictClassProbabilities
388         :param predictedVariableName:
389         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
390         """"""
391         labels = self.getClassLabels()
392         dfCols = list(predictedProbabilitiesDf.columns)
393         if dfCols != labels:
394             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
395         yArray = predictedProbabilitiesDf.values
396         maxIndices = np.argmax(yArray, axis=1)
397         result = [labels[i] for i in maxIndices]
398         return pd.DataFrame(result, columns=[predictedVariableName])
399 
400     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","408     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
409         pass
410 
411     def _convertClassProbabilitiesToPredictions(self, predictedProbabilitiesDf: pd.DataFrame,
412                                                 predictedVariableName=""predictedLabel""):
413         """"""
414         Converts from a data frame with probabilities as returned by predictClassProbabilities to a data frame
415         with predicted class labels
416 
417         :param predictedProbabilitiesDf: the output data frame from predictClassProbabilities
418         :param predictedVariableName:
419         :return: an output data frame with a single column named predictedVariableName and containing the predicted classes
420         """"""
421         labels = self.getClassLabels()
422         dfCols = list(predictedProbabilitiesDf.columns)
423         if dfCols != labels:
424             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
425         yArray = predictedProbabilitiesDf.values
426         maxIndices = np.argmax(yArray, axis=1)
427         result = [labels[i] for i in maxIndices]
428         return pd.DataFrame(result, columns=[predictedVariableName])
429 
430     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
","Before: 382
After: 411, 412",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,2554,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 44, 'parameters': 2, '(': 7, ',': 6, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 8, '.': 8, ')': 7, 'block': 3, 'pass_statement': 1, 'pass': 1, 'default_parameter': 1, '=': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'expression_statement': 6, 'assignment': 5, 'call': 5, 'argument_list': 5, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 3, 'subscript': 1, ']': 3, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1, 'list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7217118782071626,0.7060468748858819,"(tensor([0.9549]), tensor([0.9565]), tensor([0.9557]), tensor([0.9563]))"
"397         result = [labels[i] for i in maxIndices]
398         return pd.DataFrame(result, columns=[predictedVariableName])
399 
400     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
401         """"""
402         Converts from a result returned by predictClassProbabilities to a result as return by predict.
403 
404         :param df: the output data frame from predictClassProbabilities
405         :return: an output data frame as it would be returned by predict
406         """"""
407         modelOutputVariableName = self.getModelOutputVariableNames()[0]
408         y = self._convertClassProbabilitiesToPredictions(df, predictedVariableName=modelOutputVariableName)
409         y = self._applyPostProcessing(y)
410         return y
411 
412     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","427         result = [labels[i] for i in maxIndices]
428         return pd.DataFrame(result, columns=[predictedVariableName])
429 
430     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
431         """"""
432         Converts from a result returned by predictClassProbabilities to a result as return by predict.
433 
434         :param df: the output data frame from predictClassProbabilities
435         :return: an output data frame as it would be returned by predict
436         """"""
437         predictedVariableName = self.getPredictedVariableNames()[0]
438         y = self._convertClassProbabilitiesToPredictions(df, predictedVariableName=predictedVariableName)
439         return y
440 
441     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 407, 408, 409
After: 437, 438",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,2713,"{'module': 1, 'expression_statement': 5, 'assignment': 4, 'identifier': 29, '=': 6, 'list_comprehension': 1, '[': 4, 'subscript': 2, ']': 4, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 4, 'attribute': 5, '.': 5, 'argument_list': 4, '(': 5, ',': 3, 'keyword_argument': 2, 'list': 1, ')': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6581393199333697,0.6406002632715238,"(tensor([0.9716]), tensor([0.9564]), tensor([0.9640]), tensor([0.9579]))"
"419         self._checkPrediction(result)
420         return result
421 
422     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
423         """"""
424         Checks whether the column names are correctly set and whether the entries correspond to probabilities
425         """"""
426         labels = self.getClassLabels()
427         if list(predictionDf.columns) != labels:
428             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
429                             f""expected {labels}, got {predictionDf.columns}"")
430 
431         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
432         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
433 
434             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
435                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
436                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
437 
438             s = valueSeries.sum()
439             if not np.isclose(s, 1, rtol=1e-2, ltol=1e-2):
440                 log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
441 
442     @abstractmethod
","448         self._checkPrediction(result)
449         return result
450 
451     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
452         """"""
453         Checks whether the column names are correctly set and whether the entries correspond to probabilities
454         """"""
455         labels = self.getClassLabels()
456         if list(predictionDf.columns) != labels:
457             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
458                             f""expected {labels}, got {predictionDf.columns}"")
459 
460         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
461         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
462 
463             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
464                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
465                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
466 
467             s = valueSeries.sum()
468             if not np.isclose(s, 1, rtol=1e-2, ltol=1e-2):
469                 log.warning(
470                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
471 
472     @abstractmethod
","Before: 440
After: 469, 470",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,3085,"{'module': 1, 'expression_statement': 7, 'call': 14, 'attribute': 11, 'identifier': 57, '.': 11, 'argument_list': 14, '(': 16, ')': 16, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'typed_parameter': 1, ':': 7, 'type': 1, 'default_parameter': 1, '=': 7, 'integer': 5, 'block': 5, 'string': 6, 'string_start': 6, 'string_content': 11, 'string_end': 6, 'assignment': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '!=': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 2, 'interpolation': 9, '{': 9, '}': 9, 'subscript': 1, '[': 1, 'slice': 1, ']': 1, 'for_statement': 1, 'for': 1, 'pattern_list': 1, 'tuple_pattern': 1, 'in': 1, 'keyword_argument': 3, 'boolean_operator': 1, 'not_operator': 3, 'not': 3, '<=': 2, 'or': 1, 'float': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7803113540894564,0.765182043622075,"(tensor([0.9760]), tensor([0.9766]), tensor([0.9763]), tensor([0.9766]))"
"443     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
444         pass
445 
446     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
447         modelOutputVariableName = self.getModelOutputVariableNames()[0]
448         predictedProbabilitiesDf = self._predictClassProbabilities(x)
449         return self._convertClassProbabilitiesToPredictions(predictedProbabilitiesDf, predictedVariableName=modelOutputVariableName)
450 
451 
","473     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
474         pass
475 
476     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
477         predictedProbabilitiesDf = self._predictClassProbabilities(x)
478         return self.convertClassProbabilitiesToPredictions(predictedProbabilitiesDf)
479 
480 
","Before: 447, 449
After: 478",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,3155,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 26, 'parameters': 2, '(': 5, ',': 3, 'typed_parameter': 2, ':': 4, 'type': 4, 'attribute': 7, '.': 7, ')': 5, '->': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 2, 'assignment': 2, '=': 3, 'subscript': 1, 'call': 3, 'argument_list': 3, '[': 1, 'integer': 1, ']': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5051613823813255,0.44855083659672756,"(tensor([0.9645]), tensor([0.9114]), tensor([0.9372]), tensor([0.9165]))"
"465     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
466         pass
467 
468     def fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, **kwargs):
469         """"""
470         Fits the model using the given data
471 
472         :param X: a data frame containing input data
473         :param Y: a data frame containing output data or None. Preprocessors may require Y for fitting.
474         :param kwargs: for consistency with VectorModel interface, will be ignored
475         """"""
476         super().fit(X, Y, fitPreprocessors=True, fitTargetTransformer=False)
477 
478 
","494     def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):
495         pass
496 
497     def fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, **kwargs):
498         """"""
499         Fits the model using the given data
500 
501         :param X: a data frame containing input data
502         :param Y: a data frame containing output data or None. Preprocessors may require Y for fitting.
503         :param kwargs: for consistency with VectorModel interface, will be ignored
504         """"""
505         super().fit(X, Y, fitPreprocessors=True)
506 
507 
","Before: 476
After: 505",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,3354,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 23, 'parameters': 2, '(': 4, ',': 8, 'typed_parameter': 3, ':': 6, 'type': 4, 'attribute': 5, '.': 5, ')': 4, 'block': 2, 'pass_statement': 1, 'pass': 1, 'typed_default_parameter': 1, '=': 3, 'none': 1, 'dictionary_splat_pattern': 1, '**': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 2, 'argument_list': 2, 'keyword_argument': 2, 'true': 1, 'false': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7411770310644398,0.7193900703421462,"(tensor([0.9708]), tensor([0.9614]), tensor([0.9661]), tensor([0.9623]))"
"497     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
498         pass
499 
500     def fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, **kwargs):
501         """"""
502         Fits the model using the given data
503 
504         :param X: a data frame containing input data
505         :param Y: a data frame containing output data or None. Preprocessors may require Y for fitting.
506         :param kwargs: for consistency with VectorModel interface, will be ignored
507         """"""
508         super().fit(X, Y, fitPreprocessors=True, fitTargetTransformer=False)
","524     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
525         pass
526 
527     def fit(self, X: pd.DataFrame, Y: pd.DataFrame = None, **kwargs):
528         """"""
529         Fits the model using the given data
530 
531         :param X: a data frame containing input data
532         :param Y: a data frame containing output data or None. Preprocessors may require Y for fitting.
533         :param kwargs: for consistency with VectorModel interface, will be ignored
534         """"""
535         super().fit(X, Y, fitPreprocessors=True)
","Before: 508
After: 535",move vector_model and predictormodel to src/sensai/vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,1,3604,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 2, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 4, 'attribute': 4, '.': 4, ')': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'typed_default_parameter': 1, '=': 1, 'none': 1, 'dictionary_splat_pattern': 1, '**': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 26, 'end_line': 27, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7541266588482972,0.7351797618865504,"(tensor([0.9551]), tensor([0.9613]), tensor([0.9582]), tensor([0.9607]))"
"5 import pytest
6 
7 from sensai.data_transformation import DFTDRowFilterOnIndex, \
8     InvertibleDataFrameTransformer
9 from sensai.featuregen import FeatureGeneratorTakeColumns, FeatureGenerator
10 from sensai.vector_model import VectorModel, RuleBasedVectorRegressionModel
11 
12 
13 class FittableFgen(FeatureGenerator):
14 
","5 import pytest
6 
7 from sensai.data_transformation import DFTDRowFilterOnIndex, \
8     InvertibleDataFrameTransformer
9 from sensai.featuregen import FeatureGeneratorTakeColumns, FeatureGenerator
10 from sensai.vector_model import RuleBasedVectorRegressionModel, VectorRegressionModel
11 
12 
13 class FittableFgen(FeatureGenerator):
14 
","Before: 10
After: 10",remove unused test_vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,0,64,"{'module': 1, 'import_statement': 1, 'import': 4, 'dotted_name': 10, 'identifier': 15, 'import_from_statement': 3, 'from': 3, '.': 3, ',': 3, 'line_continuation': 1, 'class_definition': 1, 'class': 1, 'argument_list': 1, '(': 1, ')': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8839346220573184,0.8780609770647382,"(tensor([0.9944]), tensor([0.9949]), tensor([0.9947]), tensor([0.9949]))"
"43 
44     def isRegressionModel(self) -> bool:
45         return True
46 
47 
48 class SampleVectorModel(VectorModel):
49 
50     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
51         return pd.DataFrame({""prediction"": 1}, index=x.index)
52 
","43 
44     def isRegressionModel(self) -> bool:
45         return True
46 
47 
48 class SampleVectorModel(VectorRegressionModel):
49 
50     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
51         return pd.DataFrame({""prediction"": 1}, index=x.index)
52 
","Before: 48
After: 48",remove unused test_vector_model.py,Breaking change: Moved methods for post-processing from VectorModel to VectorRegressionModel,https://github.com/opcode81/sensAI,tests/base/test_vector_model.py,04d722b442c6ef771dbb6ae9ec4f9333b66225e3,8a025a5d2441ae1ddc1809d8c94046c1030eb538,0,362,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 4, ')': 4, '->': 2, 'type': 3, ':': 5, 'block': 3, 'return_statement': 2, 'return': 2, 'true': 1, 'class_definition': 1, 'class': 1, 'argument_list': 2, ',': 2, 'typed_parameter': 1, 'attribute': 4, '.': 4, 'call': 1, 'dictionary': 1, '{': 1, 'pair': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'integer': 1, '}': 1, 'keyword_argument': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 24, 'name': '_fit', 'long_name': '_fit( self , X : pd . DataFrame , Y : pd . DataFrame = None , ctx = None )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' X : pd . DataFrame', ' Y : pd . DataFrame = None', ' ctx = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9506885335787997,0.9455275676611904,"(tensor([0.9911]), tensor([0.9952]), tensor([0.9931]), tensor([0.9947]))"
"49         totalCount = sum(counts.values())
50         return (valueCount + self.pseudoCount) / (totalCount + self.pseudoCount)
51 
52     def _predict(self, X: pd.DataFrame) -> pd.DataFrame:
53         results = []
54         for _, features in X.iterrows():
55             bestCls = None
56             bestLp = None
57             for cls in self.prior:
58                 lp = log(self._probability(self.prior, cls))
59                 for idxFeature, value in enumerate(features):
60                     lp += log(self._probability(self.conditionals[cls][idxFeature], value))
61                 if bestLp is None or lp > bestLp:
62                     bestLp = lp
63                     bestCls = cls
64             results.append(bestCls)
65         return pd.DataFrame(results, columns=self.getModelOutputVariableNames())
","49         totalCount = sum(counts.values())
50         return (valueCount + self.pseudoCount) / (totalCount + self.pseudoCount)
51 
52     def _predict(self, X: pd.DataFrame) -> pd.DataFrame:
53         results = []
54         for _, features in X.iterrows():
55             bestCls = None
56             bestLp = None
57             for cls in self.prior:
58                 lp = log(self._probability(self.prior, cls))
59                 for idxFeature, value in enumerate(features):
60                     lp += log(self._probability(self.conditionals[cls][idxFeature], value))
61                 if bestLp is None or lp > bestLp:
62                     bestLp = lp
63                     bestCls = cls
64             results.append(bestCls)
65         return pd.DataFrame(results, columns=self.getPredictedVariableNames())
","Before: 65
After: 65",fix typo in categoricalnaivedbayesvectorclassificationmodel,Fix in naive bayes classifier: use predictedVarNames instead of modelOutputVarNames,https://github.com/opcode81/sensAI,src/sensai/naive_bayes.py,8d61eebd6f8cd213ad456793dd2f1ff7d3a35bdf,2119a317e97e892c90cc3196bc83a3568fe2a06f,0,778,"{'module': 1, 'expression_statement': 9, 'assignment': 7, 'identifier': 57, '=': 7, 'call': 9, 'argument_list': 9, '(': 12, 'attribute': 12, '.': 12, ')': 12, 'return_statement': 1, 'return': 1, 'binary_operator': 3, 'parenthesized_expression': 2, '+': 2, '/': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'typed_parameter': 1, ':': 6, 'type': 2, '->': 1, 'block': 5, 'list': 1, '[': 3, ']': 3, 'for_statement': 3, 'for': 3, 'pattern_list': 2, 'in': 3, 'none': 3, 'augmented_assignment': 1, '+=': 1, 'subscript': 2, 'if_statement': 1, 'if': 1, 'boolean_operator': 1, 'comparison_operator': 2, 'is': 1, 'or': 1, '>': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self , pseudoCount = 0 . 1 )', 'start_line': 14, 'end_line': 21, 'full_parameters': ['self', ' pseudoCount = 0 . 1'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/naive_bayes.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 34, 'name': '__init__', 'long_name': '__init__( self , pseudoCount = 0 . 1 )', 'start_line': 14, 'end_line': 21, 'full_parameters': ['self', ' pseudoCount = 0 . 1'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/naive_bayes.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9818487054407757,0.9803359697000784,"(tensor([0.9956]), tensor([0.9952]), tensor([0.9954]), tensor([0.9952]))"
"266     def isRegressionModel(self) -> bool:
267         return True
268 
269     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
270         """"""
271         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
272         The transformers are ignored during the fit phase. Not supported for rule-based models.
273 
274         **Important**: The output columns names of the last output transformer should be the same
275         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
276         (fit will run through without problems, though).
277 
278         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
279         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
280         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
281         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
282         by some heuristics or by hand-crafted rules.
283 
284         **How not to use**: Output transformers are not meant to transform the predictions into something with a
285         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
286         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
287         post processing, when this is desired. E.g. if your model predicts labels, make sure that the output
288         transformers predict the same labels or you might get nasty behaviour during evaluation. Note that for
289         probabilistic classifiers the output transformers will not be applied to the probability vectors
290         but to the actual label predictions, unless explicitly stated otherwise.
291 
292         :param outputTransformers: DataFrameTransformers for the transformation of outputs
293             (after the model has been applied)
294         :return: self
295         """"""
296         # There is no reason for post processing in rule-based models
297         if not self._underlyingModelRequiresFitting():
298             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
299         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
300         return self
301 
302     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","266     def isRegressionModel(self) -> bool:
267         return True
268 
269     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
270         """"""
271         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
272         The transformers are ignored during the fit phase. Not supported for rule-based models.
273 
274         **Important**: The output columns names of the last output transformer should be the same
275         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
276         (fit will run through without problems, though).
277 
278         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
279         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
280         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
281         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
282         by some heuristics or by hand-crafted rules.
283 
284         **How not to use**: Output transformers are not meant to transform the predictions into something with a
285         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
286         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
287         post processing, when this is desired.
288 
289         :param outputTransformers: DataFrameTransformers for the transformation of outputs
290             (after the model has been applied)
291         :return: self
292         """"""
293         # There is no reason for post processing in rule-based models
294         if not self._underlyingModelRequiresFitting():
295             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
296         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
297         return self
298 
299     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","Before: 287, 288, 289, 290
After: 287",update vector_model.py and vectorclassifier.py,Removed None check for classProbabilities in classification evaluator,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,c278fc14ded97b36a38316367bb234b5df57b2df,1,2217,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 22, 'parameters': 2, '(': 5, ')': 5, '->': 2, 'type': 6, ':': 4, 'block': 3, 'return_statement': 2, 'return': 2, 'true': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, '*': 2, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'expression_statement': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'comment': 1, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'call': 3, 'attribute': 4, '.': 4, 'argument_list': 3, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1, 'assignment': 1, '=': 1, 'list_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8127066060043123,0.8127066060043123,"(tensor([0.9853]), tensor([0.9605]), tensor([0.9728]), tensor([0.9629]))"
"408     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
409         pass
410 
411     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
412         """"""
413         Converts from a result returned by predictClassProbabilities to a result as return by predict.
414 
415         :param df: the output data frame from predictClassProbabilities
416         :return: an output data frame as it would be returned by predict
417         """"""
418         labels = self.getClassLabels()
419         dfCols = list(df.columns)
420         if dfCols != labels:
421             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
422         yArray = df.values
423         maxIndices = np.argmax(yArray, axis=1)
424         result = [labels[i] for i in maxIndices]
425         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
426 
427     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","405     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
406         pass
407 
408     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
409         """"""
410         Converts from a result returned by predictClassProbabilities to a result as return by predict.
411 
412         :param df: the output data frame from predictClassProbabilities
413         :return: an output data frame as it would be returned by predict
414         """"""
415         labels = self.getClassLabels()
416         dfCols = list(df.columns)
417         if sorted(dfCols) != labels:
418             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
419         yArray = df.values
420         maxIndices = np.argmax(yArray, axis=1)
421         result = [labels[i] for i in maxIndices]
422         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
423 
424     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 420
After: 417",update vector_model.py and vectorclassifier.py,Removed None check for classProbabilities in classification evaluator,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,c278fc14ded97b36a38316367bb234b5df57b2df,1,2738,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 44, 'parameters': 2, '(': 8, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 9, '.': 9, ')': 8, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'assignment': 5, '=': 7, 'call': 6, 'argument_list': 6, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7433382591862692,0.7417139628662578,"(tensor([0.9842]), tensor([0.9866]), tensor([0.9854]), tensor([0.9864]))"
"424         result = [labels[i] for i in maxIndices]
425         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
426 
427     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
428         """"""
429         :param x: the input data
430         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
431         """"""
432         x = self._computeModelInputs(x)
433         result = self._predictClassProbabilities(x)
434         self._checkPrediction(result)
435         return result
436 
437     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","421         result = [labels[i] for i in maxIndices]
422         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
423 
424     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
425         """"""
426         :param x: the input data
427         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
428             Returns None if the classifier cannot predict probabilities.
429         """"""
430         x = self._computeModelInputs(x)
431         result = self._predictClassProbabilities(x)
432         self._checkPrediction(result)
433         return result
434 
435     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","Before: 430
After: 427, 428",update vector_model.py and vectorclassifier.py,Removed None check for classProbabilities in classification evaluator,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,c278fc14ded97b36a38316367bb234b5df57b2df,1,2895,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 30, '=': 4, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 5, 'attribute': 7, '.': 7, 'argument_list': 5, '(': 6, ',': 2, 'keyword_argument': 1, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6700582550228131,0.6572285457755138,"(tensor([0.9658]), tensor([0.9820]), tensor([0.9738]), tensor([0.9804]))"
"456                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
457 
458     @abstractmethod
459     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
460         pass
461 
462     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
","454                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
455 
456     @abstractmethod
457     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
458         """"""
459         If you are implementing a probabilistic classifier, this method has to return a data frame with probabilities
460         (one column per label). The default implementation of _predict will then use the output of
461         this method and convert it to predicted labels (via argmax).
462 
463         In case you want to predict labels only or have a more efficient implementation of predicting labels than
464         using argmax, your will have to override _predict in your implementation. In the former case of a
465         non-probabilistic classifier, the implementation of this method should raise an exception, like the one below.
466         """"""
467         raise NotImplementedError(f""Model {self.__class__.__name__} does not support prediction of probabilities"")
468 
469     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 460
After: 458, 459, 460, 461, 462, 463, 464, 465, 466, 467",update vector_model.py and vectorclassifier.py,Removed None check for classProbabilities in classification evaluator,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,c278fc14ded97b36a38316367bb234b5df57b2df,1,3192,"{'module': 1, 'ERROR': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 3, 'interpolation': 3, '{': 3, 'identifier': 12, '}': 3, 'call': 1, 'argument_list': 1, '(': 2, ')': 3, 'string_end': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 2, 'attribute': 2, '.': 2, '->': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2649102830628097,0.2473554698297872,"(tensor([0.7269]), tensor([0.9449]), tensor([0.8217]), tensor([0.9174]))"
"459     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
460         pass
461 
462     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
463         predictedProbabilitiesDf = self._predictClassProbabilities(x)
464         return self.convertClassProbabilitiesToPredictions(predictedProbabilitiesDf)
465 
466 
","466         """"""
467         raise NotImplementedError(f""Model {self.__class__.__name__} does not support prediction of probabilities"")
468 
469     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
470         try:
471             predictedProbabilitiesDf = self._predictClassProbabilities(x)
472         except Exception:
473             raise Exception(f""Wrong implementation of {self.__class__.__name__}. For non-probabilistic classifiers ""
474                             ""_predict has to be overrode!"")
475         return self.convertClassProbabilitiesToPredictions(predictedProbabilitiesDf)
476 
477 
","Before: 463
After: 470, 471, 472, 473, 474",update vector_model.py and vectorclassifier.py,Removed None check for classProbabilities in classification evaluator,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,c278fc14ded97b36a38316367bb234b5df57b2df,1,3229,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 21, 'parameters': 2, '(': 4, ',': 2, 'typed_parameter': 2, ':': 4, 'type': 4, 'attribute': 6, '.': 6, ')': 4, '->': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 2, 'argument_list': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.23832882691859483,0.19142800421731,"(tensor([0.8015]), tensor([0.9237]), tensor([0.8582]), tensor([0.9098]))"
"492 
493 
494 class RuleBasedVectorClassificationModel(VectorClassificationModel, ABC):
495     def __init__(self, labels: list, predictedVariableName=""predictedLabel""):
496         """"""
497         :param labels:
498         :param predictedVariableName:
499         """"""
500         super().__init__(checkInputColumns=False)
501 
502         duplicate = getFirstDuplicate(labels)
503         if duplicate is not None:
504             raise Exception(f""Found duplicate label: {duplicate}"")
505         self._labels = labels
506         self._predictedVariableNames = [predictedVariableName]
507 
508     def _underlyingModelRequiresFitting(self):
","503 
504 
505 class RuleBasedVectorClassificationModel(VectorClassificationModel, ABC):
506     def __init__(self, labels: list, predictedVariableName=""predictedLabel""):
507         """"""
508         :param labels:
509         :param predictedVariableName:
510         """"""
511         super().__init__(checkInputColumns=False)
512 
513         duplicate = getFirstDuplicate(labels)
514         if duplicate is not None:
515             raise Exception(f""Found duplicate label: {duplicate}"")
516         self._labels = sorted(labels)
517         self._predictedVariableNames = [predictedVariableName]
518 
519     def _underlyingModelRequiresFitting(self):
","Before: 505
After: 516",update vector_model.py and vectorclassifier.py,Removed None check for classProbabilities in classification evaluator,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,c278fc14ded97b36a38316367bb234b5df57b2df,1,3503,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 23, 'argument_list': 5, '(': 6, ',': 3, ')': 6, ':': 4, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 1, 'default_parameter': 1, '=': 5, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'expression_statement': 5, 'call': 4, 'attribute': 3, '.': 3, 'keyword_argument': 1, 'false': 1, 'assignment': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6185986278558845,0.6001696631332518,"(tensor([0.9652]), tensor([0.9680]), tensor([0.9666]), tensor([0.9678]))"
"27     """"""
28     log = log.getChild(__qualname__)
29 
30     def __init__(self, cuda=True):
31         self.cuda = cuda
32         self.module: torch.nn.Module = None
33         self.outputScaler: Optional[TensorScaler] = None
34         self.inputScaler: Optional[TensorScaler] = None
35         self.bestEpoch = None
36         self._gpu = None
37 
38     def setTorchModule(self, module: torch.nn.Module):
","27     """"""
28     log = log.getChild(__qualname__)
29 
30     def __init__(self, cuda=True):
31         self.cuda = cuda
32         self.module: Optional[torch.nn.Module] = None
33         self.outputScaler: Optional[TensorScaler] = None
34         self.inputScaler: Optional[TensorScaler] = None
35         self.bestEpoch = None
36         self._gpu = None
37 
38     def setTorchModule(self, module: torch.nn.Module):
","Before: 32
After: 32",fix a typo in torch_base.py,Log a warning in vector model if predict is called before fit,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,a7e85f5da2d662ecf3366acd363acdfc421c0e9c,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,0,226,"{'module': 1, 'ERROR': 1, 'string_start': 1, 'expression_statement': 7, 'assignment': 7, 'identifier': 27, '=': 8, 'call': 1, 'attribute': 9, '.': 9, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'default_parameter': 1, 'true': 1, ':': 4, 'block': 1, 'type': 5, 'none': 5, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 55, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 36, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 36, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9249253412226136,0.9365565353233203,"(tensor([0.9915]), tensor([0.9952]), tensor([0.9933]), tensor([0.9948]))"
"351 class NNOptimiser:
352     log = log.getChild(__qualname__)
353 
354     def __init__(self, lossEvaluator: NNLossEvaluator = None, cuda=True, gpu=None, optimiser=""adam"", optimiserClip=10., optimiserLR=0.001,
355             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False, optimiserLRDecay=1, startLRDecayAtEpoch=None,
356             useShrinkage=True, **optimiserArgs):
357         """"""
358         :param cuda: whether to use CUDA
359         :param lossEvaluator: the loss evaluator to use
360         :param gpu: index of the gpu to be used, if parameter cuda is True
361         :param optimiser: the optimizer to be used; defaults to ""adam""
362         :param optimiserClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
363         :param optimiserLR: the optimizer's learning rate decay
364         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
365             If no validation is to be performed, pass 1.0.
366         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
367             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
368         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
369         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
370         """"""
371         if optimiser == 'lbfgs':
372             largeBatchSize = 1e12
373             if batchSize is not None:
374                 self.log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
375             batchSize = largeBatchSize
376         else:
377             if batchSize is None:
378                 batchSize = 64
379 
380         if lossEvaluator is None:
381             raise ValueError(""Must provide a loss evaluator"")
382 
383         self.epochs = epochs
384         self.batchSize = batchSize
385         self.optimiserLR = optimiserLR
386         self.optimiserClip = optimiserClip
387         self.optimiser = optimiser
388         self.cuda = cuda
389         self.gpu = gpu
390         self.trainFraction = trainFraction
391         self.scaledOutputs = scaledOutputs
392         self.lossEvaluator = lossEvaluator
393         self.startLRDecayAtEpoch = startLRDecayAtEpoch
394         self.optimiserLRDecay = optimiserLRDecay
395         self.optimiserArgs = optimiserArgs
396         self.useShrinkage = useShrinkage
397 
398         self.lossEvaluatorState = None
399         self.trainingLog = None
400         self.bestEpoch = None
401 
402     def __str__(self):
","351 class NNOptimiser:
352     log = log.getChild(__qualname__)
353 
354     def __init__(self, lossEvaluator: NNLossEvaluator = None, cuda=True, gpu=None, optimiser=""adam"", optimiserClip=10., optimiserLR=0.001,
355             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False, optimiserLRDecay=1, startLRDecayAtEpoch=None,
356             useShrinkage=True, **optimiserArgs):
357         """"""
358         :param cuda: whether to use CUDA
359         :param lossEvaluator: the loss evaluator to use
360         :param gpu: index of the gpu to be used, if parameter cuda is True
361         :param optimiser: the optimizer to be used; defaults to ""adam""
362         :param optimiserClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
363         :param optimiserLR: the optimiser's learning rate decay
364         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
365             If no validation is to be performed, pass 1.0.
366         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
367             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
368         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
369         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
370         """"""
371         if optimiser == 'lbfgs':
372             largeBatchSize = 1e12
373             if batchSize is not None:
374                 self.log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
375             batchSize = largeBatchSize
376         else:
377             if batchSize is None:
378                 batchSize = 64
379 
380         if lossEvaluator is None:
381             raise ValueError(""Must provide a loss evaluator"")
382 
383         self.epochs = epochs
384         self.batchSize = batchSize
385         self.optimiserLR = optimiserLR
386         self.optimiserClip = optimiserClip
387         self.optimiser = optimiser
388         self.cuda = cuda
389         self.gpu = gpu
390         self.trainFraction = trainFraction
391         self.scaledOutputs = scaledOutputs
392         self.lossEvaluator = lossEvaluator
393         self.startLRDecayAtEpoch = startLRDecayAtEpoch
394         self.optimiserLRDecay = optimiserLRDecay
395         self.optimiserArgs = optimiserArgs
396         self.useShrinkage = useShrinkage
397 
398         self.lossEvaluatorState = None
399         self.trainingLog = None
400         self.bestEpoch = None
401 
402     def __str__(self):
","Before: 363
After: 363",fix typo in nnoptimiser doc,Log a warning in vector model if predict is called before fit,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,a7e85f5da2d662ecf3366acd363acdfc421c0e9c,dd4f08bfaeeb25b0a6e7c6e110f1a5af6315b568,0,3223,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 83, ':': 8, 'block': 7, 'expression_statement': 23, 'assignment': 21, '=': 34, 'call': 3, 'attribute': 20, '.': 20, 'argument_list': 3, '(': 4, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 14, 'typed_default_parameter': 1, 'type': 1, 'none': 10, 'default_parameter': 12, 'true': 2, 'string': 5, 'string_start': 5, 'string_content': 6, 'string_end': 5, 'float': 4, 'integer': 3, 'false': 1, 'dictionary_splat_pattern': 1, '**': 1, 'if_statement': 4, 'if': 4, 'comparison_operator': 4, '==': 1, 'is not': 2, 'interpolation': 1, '{': 1, '}': 1, 'else_clause': 1, 'else': 1, 'is': 2, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 29, 'end_line': 54, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 29, 'end_line': 54, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9941002105402976,0.9938262093477218,"(tensor([0.9998]), tensor([0.9998]), tensor([0.9998]), tensor([0.9998]))"
"409     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
410         pass
411 
412     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
413         """"""
414         Converts from a result returned by predictClassProbabilities to a result as return by predict.
415 
416         :param df: the output data frame from predictClassProbabilities
417         :return: an output data frame as it would be returned by predict
418         """"""
419         labels = self.getClassLabels()
420         dfCols = list(df.columns)
421         if sorted(dfCols) != labels:
422             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
423         yArray = df.values
424         maxIndices = np.argmax(yArray, axis=1)
425         result = [labels[i] for i in maxIndices]
426         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
427 
428     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","409     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
410         pass
411 
412     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
413         """"""
414         Converts from a result returned by predictClassProbabilities to a result as return by predict.
415 
416         :param df: the output data frame from predictClassProbabilities
417         :return: an output data frame as it would be returned by predict
418         """"""
419         labels = self.getClassLabels()
420         dfCols = list(df.columns)
421         if sorted(dfCols) != labels:
422             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
423         yArray = df.values
424         maxIndices = np.argmax(yArray, axis=1)
425         result = [dfCols[i] for i in maxIndices]
426         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
427 
428     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 425
After: 425",fix typo in vector_model.py,Fixed a bug due to ordering of columns in VectorClassificationMode,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,9752a44e81c8795dd87b3cd90d8747ff3ddd45eb,0893dd10a0ef48388dcf078cef3359cebeab76f2,1,2839,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 45, 'parameters': 2, '(': 9, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 9, '.': 9, ')': 9, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'assignment': 5, '=': 7, 'call': 7, 'argument_list': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9855269679826361,0.9846421660551068,"(tensor([0.9965]), tensor([0.9964]), tensor([0.9965]), tensor([0.9964]))"
"409     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
410         pass
411 
412     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
413         """"""
414         Converts from a result returned by predictClassProbabilities to a result as return by predict.
415 
416         :param df: the output data frame from predictClassProbabilities
417         :return: an output data frame as it would be returned by predict
418         """"""
419         labels = self.getClassLabels()
420         dfCols = list(df.columns)
421         if sorted(dfCols) != labels:
422             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
423         yArray = df.values
424         maxIndices = np.argmax(yArray, axis=1)
425         result = [dfCols[i] for i in maxIndices]
426         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
427 
428     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","409     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
410         pass
411 
412     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
413         """"""
414         Converts from a data frame as returned by predictClassProbabilities to a result as return by predict.
415 
416         :param df: the output data frame from predictClassProbabilities
417         :return: an output data frame as it would be returned by predict
418         """"""
419         labels = self.getClassLabels()
420         dfCols = list(df.columns)
421         if sorted(dfCols) != labels:
422             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
423         yArray = df.values
424         maxIndices = np.argmax(yArray, axis=1)
425         result = [dfCols[i] for i in maxIndices]
426         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
427 
428     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 414
After: 414",fix typos in vector_model.py,Minor fixes/adjustments in docstrings in VectorClassificationModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7487f5f1e222409f5d0f4abd60e67f3e8b37606e,9752a44e81c8795dd87b3cd90d8747ff3ddd45eb,1,2753,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 45, 'parameters': 2, '(': 9, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 9, '.': 9, ')': 9, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'assignment': 5, '=': 7, 'call': 7, 'argument_list': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9742954329635839,0.9717767307770249,"(tensor([0.9963]), tensor([0.9971]), tensor([0.9967]), tensor([0.9970]))"
"425         result = [dfCols[i] for i in maxIndices]
426         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
427 
428     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
429         """"""
430         :param x: the input data
431         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
432             Returns None if the classifier cannot predict probabilities.
433         """"""
434         if not self.isFitted():
435             # TODO: raise an Exception instead?
436             log.warning(f""Calling predict with unfitted model. ""
437                         f""This might lead to errors down the line, especially if input/output checks are enabled"")
438         x = self._computeModelInputs(x)
439         result = self._predictClassProbabilities(x)
440         self._checkPrediction(result)
441         return result
442 
443     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","425         result = [dfCols[i] for i in maxIndices]
426         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
427 
428     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
429         """"""
430         :param x: the input data
431         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
432             Raises an exception if the classifier cannot predict probabilities.
433         """"""
434         if not self.isFitted():
435             # TODO: raise an Exception instead?
436             log.warning(f""Calling predict with unfitted model. ""
437                         f""This might lead to errors down the line, especially if input/output checks are enabled"")
438         x = self._computeModelInputs(x)
439         result = self._predictClassProbabilities(x)
440         self._checkPrediction(result)
441         return result
442 
443     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","Before: 432
After: 432",fix typos in vector_model.py,Minor fixes/adjustments in docstrings in VectorClassificationModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7487f5f1e222409f5d0f4abd60e67f3e8b37606e,9752a44e81c8795dd87b3cd90d8747ff3ddd45eb,1,2941,"{'module': 1, 'expression_statement': 6, 'assignment': 3, 'identifier': 34, '=': 4, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 7, 'attribute': 9, '.': 9, 'argument_list': 7, '(': 8, ',': 2, 'keyword_argument': 1, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'comment': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9736943262081637,0.9712246757362689,"(tensor([0.9947]), tensor([0.9953]), tensor([0.9950]), tensor([0.9952]))"
"440         self._checkPrediction(result)
441         return result
442 
443     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
444         """"""
445         Checks whether the column names are correctly set and whether the entries correspond to probabilities
446         """"""
447         labels = self.getClassLabels()
448         if list(predictionDf.columns) != labels:
449             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
450                             f""expected {labels}, got {predictionDf.columns}"")
451 
452         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
453         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
454 
455             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
456                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
457                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
458 
459             s = valueSeries.sum()
460             if not np.isclose(s, 1, rtol=1e-2, ltol=1e-2):
461                 log.warning(
462                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
463 
464     @abstractmethod
","440         self._checkPrediction(result)
441         return result
442 
443     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
444         """"""
445         Checks whether the column names are correctly set, sorted and whether the entries correspond to probabilities
446         """"""
447         labels = self.getClassLabels()
448         if list(predictionDf.columns) != labels:
449             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
450                             f""expected {labels}, got {predictionDf.columns}"")
451 
452         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
453         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
454 
455             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
456                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
457                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
458 
459             s = valueSeries.sum()
460             if not np.isclose(s, 1, rtol=1e-2, ltol=1e-2):
461                 log.warning(
462                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
463 
464     @abstractmethod
","Before: 445
After: 445",fix typos in vector_model.py,Minor fixes/adjustments in docstrings in VectorClassificationModel,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7487f5f1e222409f5d0f4abd60e67f3e8b37606e,9752a44e81c8795dd87b3cd90d8747ff3ddd45eb,1,3006,"{'module': 1, 'expression_statement': 7, 'call': 14, 'attribute': 11, 'identifier': 57, '.': 11, 'argument_list': 14, '(': 16, ')': 16, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'typed_parameter': 1, ':': 7, 'type': 1, 'default_parameter': 1, '=': 7, 'integer': 5, 'block': 5, 'string': 6, 'string_start': 6, 'string_content': 11, 'string_end': 6, 'assignment': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '!=': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 2, 'interpolation': 9, '{': 9, '}': 9, 'subscript': 1, '[': 1, 'slice': 1, ']': 1, 'for_statement': 1, 'for': 1, 'pattern_list': 1, 'tuple_pattern': 1, 'in': 1, 'keyword_argument': 3, 'boolean_operator': 1, 'not_operator': 3, 'not': 3, '<=': 2, 'or': 1, 'float': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9858272448466256,0.9861552169937786,"(tensor([0.9946]), tensor([0.9958]), tensor([0.9952]), tensor([0.9957]))"
"440         self._checkPrediction(result)
441         return result
442 
443     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
444         """"""
445         Checks whether the column names are correctly set, sorted and whether the entries correspond to probabilities
446         """"""
447         labels = self.getClassLabels()
448         if list(predictionDf.columns) != labels:
449             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
450                             f""expected {labels}, got {predictionDf.columns}"")
451 
452         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
453         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
454 
455             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
456                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
457                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
458 
459             s = valueSeries.sum()
460             if not np.isclose(s, 1, rtol=1e-2, ltol=1e-2):
461                 log.warning(
462                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
463 
464     @abstractmethod
","440         self._checkPrediction(result)
441         return result
442 
443     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
444         """"""
445         Checks whether the column names are correctly set, sorted and whether the entries correspond to probabilities
446         """"""
447         labels = self.getClassLabels()
448         if list(predictionDf.columns) != labels:
449             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
450                             f""expected {labels}, got {predictionDf.columns}"")
451 
452         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
453         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
454 
455             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
456                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
457                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
458 
459             s = valueSeries.sum()
460             if not np.isclose(s, 1, atol=1e-2):
461                 log.warning(
462                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
463 
464     @abstractmethod
","Before: 460
After: 460",use atol instead of rtol,Fixed small bug in _checkPrediction in tensor model,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,05bfb634f07b8f5b94dc6e7f850b956cc51b16f8,909556e5ee26c16da9004b13864663d9a5ccb059,1,3201,"{'module': 1, 'expression_statement': 7, 'call': 14, 'attribute': 11, 'identifier': 57, '.': 11, 'argument_list': 14, '(': 16, ')': 16, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'typed_parameter': 1, ':': 7, 'type': 1, 'default_parameter': 1, '=': 7, 'integer': 5, 'block': 5, 'string': 6, 'string_start': 6, 'string_content': 11, 'string_end': 6, 'assignment': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '!=': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 2, 'interpolation': 9, '{': 9, '}': 9, 'subscript': 1, '[': 1, 'slice': 1, ']': 1, 'for_statement': 1, 'for': 1, 'pattern_list': 1, 'tuple_pattern': 1, 'in': 1, 'keyword_argument': 3, 'boolean_operator': 1, 'not_operator': 3, 'not': 3, '<=': 2, 'or': 1, 'float': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.981802962043794,0.9811348550108784,"(tensor([0.9959]), tensor([0.9919]), tensor([0.9939]), tensor([0.9923]))"
"18 from typing import Optional, Tuple
19 
20 import numpy as np
21 import pandas as pd
22 
23 from sensai import VectorRegressionModel, VectorClassificationModel, VectorModel
24 from sensai.util.pandas import extractArray
25 
26 log = logging.getLogger(__name__)
27 # we set the default level to debug because it is often interesting for the user to receive
","18 from typing import Optional, Tuple
19 
20 import numpy as np
21 import pandas as pd
22 
23 from .util.pandas import extractArray
24 from .vector_model import VectorRegressionModel, VectorClassificationModel, VectorModel
25 
26 log = logging.getLogger(__name__)
27 # we set the default level to debug because it is often interesting for the user to receive
","Before: 23, 24
After: 23, 24",fix typo in tensor_model.py,Fixed imports in tensor_model (absolute to relative),https://github.com/opcode81/sensAI,src/sensai/tensor_model.py,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,66137ed3989df1635bf0597deae335741042a0ab,0,58,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 11, 'identifier': 19, 'import': 5, ',': 3, 'import_statement': 2, 'aliased_import': 2, 'as': 2, '.': 3, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 25, 'name': '_getDatapointShape', 'long_name': '_getDatapointShape( df : pd . DataFrame )', 'start_line': 35, 'end_line': 38, 'full_parameters': ['df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tensor_model.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 25, 'name': '_getDatapointShape', 'long_name': '_getDatapointShape( df : pd . DataFrame )', 'start_line': 35, 'end_line': 38, 'full_parameters': ['df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tensor_model.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8678863857057113,0.8638293766704643,"(tensor([0.9890]), tensor([0.9770]), tensor([0.9830]), tensor([0.9782]))"
"63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape (N_rows, N_columns, tensorShape). Thus, N_rows can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     (N_rows, tensorShape).
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     | ----|-----R-----|-----G-----|-----B------
80     | =========================================
81     | 0---|--channel--|--channel--|--channel
82     | 1---| ...
83 
84      or as df of the type
85 
86     | ----|----image----|
87     | ====================
88     | 0---|--RGBArray--|
89     | 1---| ...
90 
91     In both cases the returned array will have shape (N_images, 3, width, height)
92 
93     :param df: data frame where each entry is an array of shape tensorShape
94     :return: array of shape N_rows, N_columns, tensorShape with stripped empty dimensions
95     """"""
96     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
97     try:
98         return np.stack(df.apply(np.stack, axis=1)).squeeze()
99     except ValueError:
100         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
101                          f""Make sure that all entries have the same shape"")
","63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape `(N_rows, N_columns, *tensorShape)`. Thus, `N_rows` can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     `(N_rows, *tensorShape)`.
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     +------------------+------------------+------------------+
80     | R                | G                | B                |
81     +==================+==================+==================+
82     | channel          | channel          | channel          |
83     +------------------+------------------+------------------+
84     | channel          | channel          | channel          |
85     +------------------+------------------+------------------+
86     | ...              | ...              | ...              |
87     +------------------+------------------+------------------+
88 
89     or as data frame of type
90 
91     +------------------+
92     | image            |
93     +==================+
94     | RGB-array        |
95     +------------------+
96     | RGB-array        |
97     +------------------+
98     | ...              |
99     +------------------+
100 
101     In both cases the returned array will have shape `(N_images, 3, width, height)`
102 
103     :param df: data frame where each entry is an array of shape tensorShape
104     :return: array of shape `(N_rows, N_columns, *tensorShape)` with stripped empty dimensions
105     """"""
106     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
107     try:
108         return np.stack(df.apply(np.stack, axis=1)).squeeze()
109     except ValueError:
110         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
111                          f""Make sure that all entries have the same shape"")
","Before: 72
After: 72",fix typos in src/sensai/util.pandas.py,Minor fix in docu,https://github.com/opcode81/sensAI,src/sensai/util/pandas.py,f553d28486728202de5facf11faac8384f430928,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,0,477,"{'module': 1, 'ERROR': 2, 'string': 4, 'string_start': 4, 'string_content': 6, 'string_end': 4, ')': 9, 'function_definition': 1, 'def': 1, 'identifier': 26, 'parameters': 1, '(': 9, 'typed_parameter': 1, ':': 4, 'type': 1, 'attribute': 10, '.': 10, 'block': 3, 'expression_statement': 2, 'call': 7, 'argument_list': 7, 'interpolation': 3, '{': 3, 'subscript': 1, '[': 1, 'integer': 3, ',': 2, ']': 1, '}': 3, 'try_statement': 1, 'try': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1, '=': 1, 'except_clause': 1, 'except': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5344232994320375,0.6622044366656624,"(tensor([0.9045]), tensor([0.9260]), tensor([0.9152]), tensor([0.9238]))"
"63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape (N_rows, N_columns, tensorShape). Thus, N_rows can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     (N_rows, tensorShape).
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     | ----|-----R-----|-----G-----|-----B------
80     | =========================================
81     | 0---|--channel--|--channel--|--channel
82     | 1---| ...
83 
84      or as df of the type
85 
86     | ----|----image----|
87     | ====================
88     | 0---|--RGBArray--|
89     | 1---| ...
90 
91     In both cases the returned array will have shape (N_images, 3, width, height)
92 
93     :param df: data frame where each entry is an array of shape tensorShape
94     :return: array of shape N_rows, N_columns, tensorShape with stripped empty dimensions
95     """"""
96     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
97     try:
98         return np.stack(df.apply(np.stack, axis=1)).squeeze()
99     except ValueError:
100         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
101                          f""Make sure that all entries have the same shape"")
","63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape `(N_rows, N_columns, *tensorShape)`. Thus, `N_rows` can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     `(N_rows, *tensorShape)`.
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     +------------------+------------------+------------------+
80     | R                | G                | B                |
81     +==================+==================+==================+
82     | channel          | channel          | channel          |
83     +------------------+------------------+------------------+
84     | channel          | channel          | channel          |
85     +------------------+------------------+------------------+
86     | ...              | ...              | ...              |
87     +------------------+------------------+------------------+
88 
89     or as data frame of type
90 
91     +------------------+
92     | image            |
93     +==================+
94     | RGB-array        |
95     +------------------+
96     | RGB-array        |
97     +------------------+
98     | ...              |
99     +------------------+
100 
101     In both cases the returned array will have shape `(N_images, 3, width, height)`
102 
103     :param df: data frame where each entry is an array of shape tensorShape
104     :return: array of shape `(N_rows, N_columns, *tensorShape)` with stripped empty dimensions
105     """"""
106     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
107     try:
108         return np.stack(df.apply(np.stack, axis=1)).squeeze()
109     except ValueError:
110         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
111                          f""Make sure that all entries have the same shape"")
","Before: 75
After: 75",fix typos in src/sensai/util.pandas.py,Minor fix in docu,https://github.com/opcode81/sensAI,src/sensai/util/pandas.py,f553d28486728202de5facf11faac8384f430928,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,0,540,"{'module': 1, 'ERROR': 2, 'string': 4, 'string_start': 4, 'string_content': 6, 'string_end': 4, ')': 9, 'function_definition': 1, 'def': 1, 'identifier': 26, 'parameters': 1, '(': 9, 'typed_parameter': 1, ':': 4, 'type': 1, 'attribute': 10, '.': 10, 'block': 3, 'expression_statement': 2, 'call': 7, 'argument_list': 7, 'interpolation': 3, '{': 3, 'subscript': 1, '[': 1, 'integer': 3, ',': 2, ']': 1, '}': 3, 'try_statement': 1, 'try': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1, '=': 1, 'except_clause': 1, 'except': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5344232994320375,0.6622044366656624,"(tensor([0.9045]), tensor([0.9260]), tensor([0.9152]), tensor([0.9238]))"
"63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape (N_rows, N_columns, tensorShape). Thus, N_rows can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     (N_rows, tensorShape).
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     | ----|-----R-----|-----G-----|-----B------
80     | =========================================
81     | 0---|--channel--|--channel--|--channel
82     | 1---| ...
83 
84      or as df of the type
85 
86     | ----|----image----|
87     | ====================
88     | 0---|--RGBArray--|
89     | 1---| ...
90 
91     In both cases the returned array will have shape (N_images, 3, width, height)
92 
93     :param df: data frame where each entry is an array of shape tensorShape
94     :return: array of shape N_rows, N_columns, tensorShape with stripped empty dimensions
95     """"""
96     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
97     try:
98         return np.stack(df.apply(np.stack, axis=1)).squeeze()
99     except ValueError:
100         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
101                          f""Make sure that all entries have the same shape"")
","63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape `(N_rows, N_columns, *tensorShape)`. Thus, `N_rows` can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     `(N_rows, *tensorShape)`.
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     +------------------+------------------+------------------+
80     | R                | G                | B                |
81     +==================+==================+==================+
82     | channel          | channel          | channel          |
83     +------------------+------------------+------------------+
84     | channel          | channel          | channel          |
85     +------------------+------------------+------------------+
86     | ...              | ...              | ...              |
87     +------------------+------------------+------------------+
88 
89     or as data frame of type
90 
91     +------------------+
92     | image            |
93     +==================+
94     | RGB-array        |
95     +------------------+
96     | RGB-array        |
97     +------------------+
98     | ...              |
99     +------------------+
100 
101     In both cases the returned array will have shape `(N_images, 3, width, height)`
102 
103     :param df: data frame where each entry is an array of shape tensorShape
104     :return: array of shape `(N_rows, N_columns, *tensorShape)` with stripped empty dimensions
105     """"""
106     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
107     try:
108         return np.stack(df.apply(np.stack, axis=1)).squeeze()
109     except ValueError:
110         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
111                          f""Make sure that all entries have the same shape"")
","Before: 79, 80, 81, 82
After: 79, 80, 81, 82, 83, 84, 85, 86, 87",fix typos in src/sensai/util.pandas.py,Minor fix in docu,https://github.com/opcode81/sensAI,src/sensai/util/pandas.py,f553d28486728202de5facf11faac8384f430928,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,0,637,"{'module': 1, 'ERROR': 2, 'string': 4, 'string_start': 4, 'string_content': 6, 'string_end': 4, ')': 9, 'function_definition': 1, 'def': 1, 'identifier': 26, 'parameters': 1, '(': 9, 'typed_parameter': 1, ':': 4, 'type': 1, 'attribute': 10, '.': 10, 'block': 3, 'expression_statement': 2, 'call': 7, 'argument_list': 7, 'interpolation': 3, '{': 3, 'subscript': 1, '[': 1, 'integer': 3, ',': 2, ']': 1, '}': 3, 'try_statement': 1, 'try': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1, '=': 1, 'except_clause': 1, 'except': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5344232994320375,0.6622044366656624,"(tensor([0.9045]), tensor([0.9260]), tensor([0.9152]), tensor([0.9238]))"
"63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape (N_rows, N_columns, tensorShape). Thus, N_rows can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     (N_rows, tensorShape).
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     | ----|-----R-----|-----G-----|-----B------
80     | =========================================
81     | 0---|--channel--|--channel--|--channel
82     | 1---| ...
83 
84      or as df of the type
85 
86     | ----|----image----|
87     | ====================
88     | 0---|--RGBArray--|
89     | 1---| ...
90 
91     In both cases the returned array will have shape (N_images, 3, width, height)
92 
93     :param df: data frame where each entry is an array of shape tensorShape
94     :return: array of shape N_rows, N_columns, tensorShape with stripped empty dimensions
95     """"""
96     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
97     try:
98         return np.stack(df.apply(np.stack, axis=1)).squeeze()
99     except ValueError:
100         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
101                          f""Make sure that all entries have the same shape"")
","63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape `(N_rows, N_columns, *tensorShape)`. Thus, `N_rows` can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     `(N_rows, *tensorShape)`.
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     +------------------+------------------+------------------+
80     | R                | G                | B                |
81     +==================+==================+==================+
82     | channel          | channel          | channel          |
83     +------------------+------------------+------------------+
84     | channel          | channel          | channel          |
85     +------------------+------------------+------------------+
86     | ...              | ...              | ...              |
87     +------------------+------------------+------------------+
88 
89     or as data frame of type
90 
91     +------------------+
92     | image            |
93     +==================+
94     | RGB-array        |
95     +------------------+
96     | RGB-array        |
97     +------------------+
98     | ...              |
99     +------------------+
100 
101     In both cases the returned array will have shape `(N_images, 3, width, height)`
102 
103     :param df: data frame where each entry is an array of shape tensorShape
104     :return: array of shape `(N_rows, N_columns, *tensorShape)` with stripped empty dimensions
105     """"""
106     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
107     try:
108         return np.stack(df.apply(np.stack, axis=1)).squeeze()
109     except ValueError:
110         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
111                          f""Make sure that all entries have the same shape"")
","Before: 84
After: 89",fix typos in src/sensai/util.pandas.py,Minor fix in docu,https://github.com/opcode81/sensAI,src/sensai/util/pandas.py,f553d28486728202de5facf11faac8384f430928,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,0,723,"{'module': 1, 'ERROR': 2, 'string': 4, 'string_start': 4, 'string_content': 6, 'string_end': 4, ')': 9, 'function_definition': 1, 'def': 1, 'identifier': 26, 'parameters': 1, '(': 9, 'typed_parameter': 1, ':': 4, 'type': 1, 'attribute': 10, '.': 10, 'block': 3, 'expression_statement': 2, 'call': 7, 'argument_list': 7, 'interpolation': 3, '{': 3, 'subscript': 1, '[': 1, 'integer': 3, ',': 2, ']': 1, '}': 3, 'try_statement': 1, 'try': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1, '=': 1, 'except_clause': 1, 'except': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5344232994320375,0.6622044366656624,"(tensor([0.9045]), tensor([0.9260]), tensor([0.9152]), tensor([0.9238]))"
"63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape (N_rows, N_columns, tensorShape). Thus, N_rows can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     (N_rows, tensorShape).
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     | ----|-----R-----|-----G-----|-----B------
80     | =========================================
81     | 0---|--channel--|--channel--|--channel
82     | 1---| ...
83 
84      or as df of the type
85 
86     | ----|----image----|
87     | ====================
88     | 0---|--RGBArray--|
89     | 1---| ...
90 
91     In both cases the returned array will have shape (N_images, 3, width, height)
92 
93     :param df: data frame where each entry is an array of shape tensorShape
94     :return: array of shape N_rows, N_columns, tensorShape with stripped empty dimensions
95     """"""
96     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
97     try:
98         return np.stack(df.apply(np.stack, axis=1)).squeeze()
99     except ValueError:
100         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
101                          f""Make sure that all entries have the same shape"")
","63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape `(N_rows, N_columns, *tensorShape)`. Thus, `N_rows` can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     `(N_rows, *tensorShape)`.
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     +------------------+------------------+------------------+
80     | R                | G                | B                |
81     +==================+==================+==================+
82     | channel          | channel          | channel          |
83     +------------------+------------------+------------------+
84     | channel          | channel          | channel          |
85     +------------------+------------------+------------------+
86     | ...              | ...              | ...              |
87     +------------------+------------------+------------------+
88 
89     or as data frame of type
90 
91     +------------------+
92     | image            |
93     +==================+
94     | RGB-array        |
95     +------------------+
96     | RGB-array        |
97     +------------------+
98     | ...              |
99     +------------------+
100 
101     In both cases the returned array will have shape `(N_images, 3, width, height)`
102 
103     :param df: data frame where each entry is an array of shape tensorShape
104     :return: array of shape `(N_rows, N_columns, *tensorShape)` with stripped empty dimensions
105     """"""
106     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
107     try:
108         return np.stack(df.apply(np.stack, axis=1)).squeeze()
109     except ValueError:
110         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
111                          f""Make sure that all entries have the same shape"")
","Before: 86, 87, 88, 89
After: 91, 92, 93, 94, 95, 96, 97, 98, 99",fix typos in src/sensai/util.pandas.py,Minor fix in docu,https://github.com/opcode81/sensAI,src/sensai/util/pandas.py,f553d28486728202de5facf11faac8384f430928,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,0,754,"{'module': 1, 'ERROR': 2, 'string': 4, 'string_start': 4, 'string_content': 6, 'string_end': 4, ')': 9, 'function_definition': 1, 'def': 1, 'identifier': 26, 'parameters': 1, '(': 9, 'typed_parameter': 1, ':': 4, 'type': 1, 'attribute': 10, '.': 10, 'block': 3, 'expression_statement': 2, 'call': 7, 'argument_list': 7, 'interpolation': 3, '{': 3, 'subscript': 1, '[': 1, 'integer': 3, ',': 2, ']': 1, '}': 3, 'try_statement': 1, 'try': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1, '=': 1, 'except_clause': 1, 'except': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5344232994320375,0.6622044366656624,"(tensor([0.9045]), tensor([0.9260]), tensor([0.9152]), tensor([0.9238]))"
"63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape (N_rows, N_columns, tensorShape). Thus, N_rows can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     (N_rows, tensorShape).
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     | ----|-----R-----|-----G-----|-----B------
80     | =========================================
81     | 0---|--channel--|--channel--|--channel
82     | 1---| ...
83 
84      or as df of the type
85 
86     | ----|----image----|
87     | ====================
88     | 0---|--RGBArray--|
89     | 1---| ...
90 
91     In both cases the returned array will have shape (N_images, 3, width, height)
92 
93     :param df: data frame where each entry is an array of shape tensorShape
94     :return: array of shape N_rows, N_columns, tensorShape with stripped empty dimensions
95     """"""
96     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
97     try:
98         return np.stack(df.apply(np.stack, axis=1)).squeeze()
99     except ValueError:
100         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
101                          f""Make sure that all entries have the same shape"")
","63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape `(N_rows, N_columns, *tensorShape)`. Thus, `N_rows` can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     `(N_rows, *tensorShape)`.
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     +------------------+------------------+------------------+
80     | R                | G                | B                |
81     +==================+==================+==================+
82     | channel          | channel          | channel          |
83     +------------------+------------------+------------------+
84     | channel          | channel          | channel          |
85     +------------------+------------------+------------------+
86     | ...              | ...              | ...              |
87     +------------------+------------------+------------------+
88 
89     or as data frame of type
90 
91     +------------------+
92     | image            |
93     +==================+
94     | RGB-array        |
95     +------------------+
96     | RGB-array        |
97     +------------------+
98     | ...              |
99     +------------------+
100 
101     In both cases the returned array will have shape `(N_images, 3, width, height)`
102 
103     :param df: data frame where each entry is an array of shape tensorShape
104     :return: array of shape `(N_rows, N_columns, *tensorShape)` with stripped empty dimensions
105     """"""
106     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
107     try:
108         return np.stack(df.apply(np.stack, axis=1)).squeeze()
109     except ValueError:
110         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
111                          f""Make sure that all entries have the same shape"")
","Before: 91
After: 101",fix typos in src/sensai/util.pandas.py,Minor fix in docu,https://github.com/opcode81/sensAI,src/sensai/util/pandas.py,f553d28486728202de5facf11faac8384f430928,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,0,814,"{'module': 1, 'ERROR': 2, 'string': 4, 'string_start': 4, 'string_content': 6, 'string_end': 4, ')': 9, 'function_definition': 1, 'def': 1, 'identifier': 26, 'parameters': 1, '(': 9, 'typed_parameter': 1, ':': 4, 'type': 1, 'attribute': 10, '.': 10, 'block': 3, 'expression_statement': 2, 'call': 7, 'argument_list': 7, 'interpolation': 3, '{': 3, 'subscript': 1, '[': 1, 'integer': 3, ',': 2, ']': 1, '}': 3, 'try_statement': 1, 'try': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1, '=': 1, 'except_clause': 1, 'except': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5344232994320375,0.6622044366656624,"(tensor([0.9045]), tensor([0.9260]), tensor([0.9152]), tensor([0.9238]))"
"63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape (N_rows, N_columns, tensorShape). Thus, N_rows can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     (N_rows, tensorShape).
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     | ----|-----R-----|-----G-----|-----B------
80     | =========================================
81     | 0---|--channel--|--channel--|--channel
82     | 1---| ...
83 
84      or as df of the type
85 
86     | ----|----image----|
87     | ====================
88     | 0---|--RGBArray--|
89     | 1---| ...
90 
91     In both cases the returned array will have shape (N_images, 3, width, height)
92 
93     :param df: data frame where each entry is an array of shape tensorShape
94     :return: array of shape N_rows, N_columns, tensorShape with stripped empty dimensions
95     """"""
96     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
97     try:
98         return np.stack(df.apply(np.stack, axis=1)).squeeze()
99     except ValueError:
100         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
101                          f""Make sure that all entries have the same shape"")
","63                             f""Did you forget to call trackChange on the resulting data frame?"")
64 
65 
66 def extractArray(df: pd.DataFrame):
67     """"""
68     Extracts array from data frame. It is expected that each row corresponds to a data point and
69     each column corresponds to a ""channel"". Moreover, all entries are expected to be arrays of the same shape
70     (or scalars or sequences of the same length). We will refer to that shape as tensorShape.
71 
72     The output will be of shape `(N_rows, N_columns, *tensorShape)`. Thus, `N_rows` can be interpreted as dataset length
73     (or batch size, if a single batch is passed) and N_columns can be interpreted as number of channels.
74     Empty dimensions will be stripped, thus if the data frame has only one column, the array will have shape
75     `(N_rows, *tensorShape)`.
76     E.g. an image with three channels could equally be passed as data frame of the type
77 
78 
79     +------------------+------------------+------------------+
80     | R                | G                | B                |
81     +==================+==================+==================+
82     | channel          | channel          | channel          |
83     +------------------+------------------+------------------+
84     | channel          | channel          | channel          |
85     +------------------+------------------+------------------+
86     | ...              | ...              | ...              |
87     +------------------+------------------+------------------+
88 
89     or as data frame of type
90 
91     +------------------+
92     | image            |
93     +==================+
94     | RGB-array        |
95     +------------------+
96     | RGB-array        |
97     +------------------+
98     | ...              |
99     +------------------+
100 
101     In both cases the returned array will have shape `(N_images, 3, width, height)`
102 
103     :param df: data frame where each entry is an array of shape tensorShape
104     :return: array of shape `(N_rows, N_columns, *tensorShape)` with stripped empty dimensions
105     """"""
106     log.debug(f""Stacking tensors of shape {np.array(df.iloc[0, 0]).shape}"")
107     try:
108         return np.stack(df.apply(np.stack, axis=1)).squeeze()
109     except ValueError:
110         raise ValueError(f""No array can be extracted from frame of length {len(df)} with columns {list(df.columns)}. ""
111                          f""Make sure that all entries have the same shape"")
","Before: 94
After: 104",fix typos in src/sensai/util.pandas.py,Minor fix in docu,https://github.com/opcode81/sensAI,src/sensai/util/pandas.py,f553d28486728202de5facf11faac8384f430928,a42c5cf7b6997618c380695d5d8d0aaeb25f854f,0,850,"{'module': 1, 'ERROR': 2, 'string': 4, 'string_start': 4, 'string_content': 6, 'string_end': 4, ')': 9, 'function_definition': 1, 'def': 1, 'identifier': 26, 'parameters': 1, '(': 9, 'typed_parameter': 1, ':': 4, 'type': 1, 'attribute': 10, '.': 10, 'block': 3, 'expression_statement': 2, 'call': 7, 'argument_list': 7, 'interpolation': 3, '{': 3, 'subscript': 1, '[': 1, 'integer': 3, ',': 2, ']': 1, '}': 3, 'try_statement': 1, 'try': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 1, '=': 1, 'except_clause': 1, 'except': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 26, 'name': '__init__', 'long_name': '__init__( self , initialDF : pd . DataFrame )', 'start_line': 29, 'end_line': 31, 'full_parameters': ['self', ' initialDF : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pandas.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5344232994320375,0.6622044366656624,"(tensor([0.9045]), tensor([0.9260]), tensor([0.9152]), tensor([0.9238]))"
"1 from setuptools import find_packages, setup
2 
3 tf_requirements = ['tensorflow==1.15.0']
4 torch_requirements = ['torch==1.4.0', 'torchtext==0.5.0', 'pytorch_lightning~=1.1']
5 lightgbm_requirements = ['lightgbm==2.3.0']
6 setup(
7     name='sensai',
8     package_dir={"""": ""src""},
","1 from setuptools import find_packages, setup
2 
3 tf_requirements = ['tensorflow==1.15.0']
4 torch_requirements = ['torch==1.4.0', 'torchtext==0.5.0']
5 lightgbm_requirements = ['lightgbm==2.3.0']
6 setup(
7     name='sensai',
8     package_dir={"""": ""src""},
","Before: 4
After: 4",remove pytorch_lightning from setup.py,Build: moved pytorch-lightning away from setup.py,https://github.com/opcode81/sensAI,setup.py,606c35c14b2609d451dcb54556bab826604b9ab1,f553d28486728202de5facf11faac8384f430928,0,44,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'dotted_name': 3, 'identifier': 8, 'import': 1, ',': 4, 'expression_statement': 3, 'assignment': 3, '=': 4, 'list': 3, '[': 3, 'string': 6, 'string_start': 6, 'string_content': 6, 'string_end': 6, ']': 3, 'ERROR': 1, '(': 1, 'keyword_argument': 1}",{},{},0.9266914374289533,0.9266914374289533,"(tensor([0.9934]), tensor([0.9705]), tensor([0.9818]), tensor([0.9728]))"
"218     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
219         return self._predictDFThroughArray(x, self.getPredictedVariableNames())
220 
221     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
222         if not self.isFitted():
223             # TODO: raise an Exception instead?
224             log.warning(f""Calling predict with unfitted model. ""
225                         f""This might lead to errors down the line, especially if input/output checks are enabled"")
226         if self.checkInputShape:
227             _checkDfShape(x, self.getModelInputShape())
228         y = super().predict(x)
229         if self.checkOutputShape:
230             _checkDfShape(y, self.getModelOutputShape())
231         return y
232 
233 
","218     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
219         return self._predictDFThroughArray(x, self.getPredictedVariableNames())
220 
221     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
222         if not self.isFitted():
223             raise Exception(f""Calling predict with unfitted model. ""
224                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
225         if self.checkInputShape:
226             _checkDfShape(x, self.getModelInputShape())
227         y = super().predict(x)
228         if self.checkOutputShape:
229             _checkDfShape(y, self.getModelOutputShape())
230         return y
231 
232 
","Before: 223, 224, 225
After: 223, 224",fix typo in tensor_model.py,Minor improvements in data ingest and vector model,https://github.com/opcode81/sensAI,src/sensai/tensor_model.py,4518f2c83680f57dc7994abdbf45fee50b3d7515,165bf3811595a886c1ca369e73dd84a400315aa2,0,1487,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 40, 'parameters': 2, '(': 12, ',': 5, 'typed_parameter': 2, ':': 7, 'type': 4, 'attribute': 13, '.': 13, ')': 12, '->': 2, 'block': 5, 'return_statement': 2, 'return': 2, 'call': 10, 'argument_list': 10, 'if_statement': 3, 'if': 3, 'not_operator': 1, 'not': 1, 'comment': 1, 'expression_statement': 4, 'concatenated_string': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 25, 'name': '_getDatapointShape', 'long_name': '_getDatapointShape( df : pd . DataFrame )', 'start_line': 35, 'end_line': 38, 'full_parameters': ['df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tensor_model.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 25, 'name': '_getDatapointShape', 'long_name': '_getDatapointShape( df : pd . DataFrame )', 'start_line': 35, 'end_line': 38, 'full_parameters': ['df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tensor_model.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7703469336288958,0.7688084543205165,"(tensor([0.9847]), tensor([0.9697]), tensor([0.9771]), tensor([0.9712]))"
"374         return self.convertClassProbabilitiesToPredictions(predictedProbabilitiesDf)
375 
376     # TODO or not TODO: I don't see how to reduce the code duplication here...
377     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
378         """"""
379         Returns an array of integers. If the model was fitted on binary ground truth arrays of
380         shape `(*tensorShape, numLabels)`, predictions will have the shape `tensorShape` and contain integers
381         0, 1, ..., numLabels - 1. They correspond to the predicted labels
382         """"""
383         if not self.isFitted():
384             # TODO: raise an Exception instead?
385             log.warning(f""Calling predict with unfitted model. ""
386                         f""This might lead to errors down the line, especially if input/output checks are enabled"")
387         if self.checkInputShape:
388             _checkDfShape(x, self.getModelInputShape())
389         y = super().predict(x)
390         if self.checkOutputShape:
391             _checkDfShape(y, self.getModelOutputShape())
392         return y
","373         return self.convertClassProbabilitiesToPredictions(predictedProbabilitiesDf)
374 
375     # TODO or not TODO: I don't see how to reduce the code duplication here...
376     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
377         """"""
378         Returns an array of integers. If the model was fitted on binary ground truth arrays of
379         shape `(*tensorShape, numLabels)`, predictions will have the shape `tensorShape` and contain integers
380         0, 1, ..., numLabels - 1. They correspond to the predicted labels
381         """"""
382         if not self.isFitted():
383             raise Exception(f""Calling predict with unfitted model. ""
384                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
385         if self.checkInputShape:
386             _checkDfShape(x, self.getModelInputShape())
387         y = super().predict(x)
388         if self.checkOutputShape:
389             _checkDfShape(y, self.getModelOutputShape())
390         return y
","Before: 384, 385, 386
After: 383, 384",fix typo in tensor_model.py,Minor improvements in data ingest and vector model,https://github.com/opcode81/sensAI,src/sensai/tensor_model.py,4518f2c83680f57dc7994abdbf45fee50b3d7515,165bf3811595a886c1ca369e73dd84a400315aa2,0,2665,"{'module': 1, 'return_statement': 1, 'return': 1, 'call': 9, 'attribute': 10, 'identifier': 30, '.': 10, 'argument_list': 9, '(': 10, ')': 10, 'comment': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, 'typed_parameter': 1, ':': 5, 'type': 2, '->': 1, 'block': 4, 'expression_statement': 5, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 3, 'if': 3, 'not_operator': 1, 'not': 1, 'concatenated_string': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 25, 'name': '_getDatapointShape', 'long_name': '_getDatapointShape( df : pd . DataFrame )', 'start_line': 35, 'end_line': 38, 'full_parameters': ['df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tensor_model.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 25, 'name': '_getDatapointShape', 'long_name': '_getDatapointShape( df : pd . DataFrame )', 'start_line': 35, 'end_line': 38, 'full_parameters': ['df : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tensor_model.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7424143774396339,0.7409570244186782,"(tensor([0.9884]), tensor([0.9798]), tensor([0.9841]), tensor([0.9806]))"
"168             X = self._inputTransformerChain.apply(X)
169         return X
170 
171     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
172         """"""
173         Performs a prediction for the given input data frame
174 
175         :param x: the input data
176         :return: a DataFrame with the same index as the input
177         """"""
178         if not self.isFitted():
179             # TODO: raise an Exception instead?
180             log.warning(f""Calling predict with unfitted model. ""
181                         f""This might lead to errors down the line, especially if input/output checks are enabled"")
182         x = self._computeModelInputs(x)
183         self._checkModelInputColumns(x)
184         y = self._predict(x)
185         y.index = x.index
186         return y
187 
188     @abstractmethod
","168             X = self._inputTransformerChain.apply(X)
169         return X
170 
171     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
172         """"""
173         Performs a prediction for the given input data frame
174 
175         :param x: the input data
176         :return: a DataFrame with the same index as the input
177         """"""
178         if not self.isFitted():
179             raise Exception(f""Calling predict with unfitted model. ""
180                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
181         x = self._computeModelInputs(x)
182         self._checkModelInputColumns(x)
183         y = self._predict(x)
184         y.index = x.index
185         return y
186 
187     @abstractmethod
","Before: 179, 180, 181
After: 179, 180",fix typo in vector_model.py,Minor improvements in data ingest and vector model,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,4518f2c83680f57dc7994abdbf45fee50b3d7515,165bf3811595a886c1ca369e73dd84a400315aa2,1,1076,"{'module': 1, 'expression_statement': 7, 'assignment': 4, 'identifier': 33, '=': 4, 'call': 6, 'attribute': 11, '.': 11, 'argument_list': 6, '(': 7, ')': 7, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'comment': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8148066004751712,0.8148066004751712,"(tensor([0.9879]), tensor([0.9734]), tensor([0.9806]), tensor([0.9748]))"
"425         result = [dfCols[i] for i in maxIndices]
426         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
427 
428     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
429         """"""
430         :param x: the input data
431         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
432             Raises an exception if the classifier cannot predict probabilities.
433         """"""
434         if not self.isFitted():
435             # TODO: raise an Exception instead?
436             log.warning(f""Calling predict with unfitted model. ""
437                         f""This might lead to errors down the line, especially if input/output checks are enabled"")
438         x = self._computeModelInputs(x)
439         result = self._predictClassProbabilities(x)
440         self._checkPrediction(result)
441         return result
442 
443     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","424         result = [dfCols[i] for i in maxIndices]
425         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
426 
427     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
428         """"""
429         :param x: the input data
430         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
431             Raises an exception if the classifier cannot predict probabilities.
432         """"""
433         if not self.isFitted():
434             raise Exception(f""Calling predict with unfitted model. ""
435                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
436         x = self._computeModelInputs(x)
437         result = self._predictClassProbabilities(x)
438         self._checkPrediction(result)
439         return result
440 
441     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","Before: 435, 436, 437
After: 434, 435",fix typo in vector_model.py,Minor improvements in data ingest and vector model,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,4518f2c83680f57dc7994abdbf45fee50b3d7515,165bf3811595a886c1ca369e73dd84a400315aa2,1,2906,"{'module': 1, 'expression_statement': 6, 'assignment': 3, 'identifier': 34, '=': 4, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 7, 'attribute': 9, '.': 9, 'argument_list': 7, '(': 8, ',': 2, 'keyword_argument': 1, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'comment': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 27, 'end_line': 28, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7280183047981333,0.7273493580392425,"(tensor([0.9851]), tensor([0.9745]), tensor([0.9798]), tensor([0.9756]))"
"1 import numpy as np
2 from pytorch_lightning import Trainer, LightningModule
3 from torch import tensor
4 
5 import sensai.tensor_model as tm
6 from sensai.data_ingest import InputOutputArrays, DataSplitterFractional
7 
8 
9 def _fitModelWithTrainer(model: LightningModule, trainer: Trainer, inputOutputData,
","1 import numpy as np
2 from pytorch_lightning import Trainer, LightningModule
3 from torch import tensor
4 
5 from .. import tensor_model as tm
6 from ..data import InputOutputArrays, DataSplitterFractional
7 
8 
9 def _fitModelWithTrainer(model: LightningModule, trainer: Trainer, inputOutputData,
","Before: 5, 6
After: 5, 6",fix typo in pytorch_lightning/pl_models.py,Fixed absolute imports,https://github.com/opcode81/sensAI,src/sensai/pytorch_lightning/pl_models.py,18b04822bfcc3b811d703627371bb90819087d7c,755426b0bad0bf661bb39732071bf64f95cb1015,0,35,"{'module': 1, 'import_statement': 2, 'import': 5, 'aliased_import': 2, 'dotted_name': 10, 'identifier': 14, 'as': 2, 'import_from_statement': 3, 'from': 3, ',': 2, '.': 2}","{'cyclomatic_complexity': 2, 'nloc': 10, 'token_count': 86, 'name': '_fitModelWithTrainer', 'long_name': '_fitModelWithTrainer( model : LightningModule , trainer : Trainer , inputOutputData , batchSize : int , splitter : DataSplitterFractional = None )', 'start_line': 9, 'end_line': 18, 'full_parameters': ['model : LightningModule', ' trainer : Trainer', ' inputOutputData', ' batchSize : int', ' splitter : DataSplitterFractional = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/pytorch_lightning/pl_models.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 10, 'token_count': 86, 'name': '_fitModelWithTrainer', 'long_name': '_fitModelWithTrainer( model : LightningModule , trainer : Trainer , inputOutputData , batchSize : int , splitter : DataSplitterFractional = None )', 'start_line': 9, 'end_line': 18, 'full_parameters': ['model : LightningModule', ' trainer : Trainer', ' inputOutputData', ' batchSize : int', ' splitter : DataSplitterFractional = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/pytorch_lightning/pl_models.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.812316127112615,0.7971361423988464,"(tensor([0.9849]), tensor([0.9569]), tensor([0.9707]), tensor([0.9597]))"
"12 from .ensemble import AveragingVectorRegressionModel
13 from .evaluation.eval_stats import eval_stats_classification, eval_stats_regression
14 from .normalisation import NormalisationMode
15 from .tensor_model import TensorToTensorRegressionModel, TensorToScalarRegressionModel, \
16     TensorToTensorClassificationModel, TensorToScalarClassificationModel
17 from .vector_model import VectorModel, VectorRegressionModel, VectorClassificationModel
18 
19 __version__ = ""0.0.5.dev0""
20 
21 # The following submodules are not imported by default to avoid necessarily requiring their dependencies:
","12 from .ensemble import AveragingVectorRegressionModel
13 from .evaluation.eval_stats import eval_stats_classification, eval_stats_regression
14 from .normalisation import NormalisationMode
15 from .tensor_model import TensorToTensorRegressionModel, TensorToScalarRegressionModel, \
16     TensorToTensorClassificationModel, TensorToScalarClassificationModel
17 from .vector_model import PredictorModel, VectorModel, VectorRegressionModel, VectorClassificationModel
18 
19 __version__ = ""0.0.5.dev0""
20 
21 # The following submodules are not imported by default to avoid necessarily requiring their dependencies:
","Before: 17
After: 17",fix typo in src/src/sensai/__init__.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/__init__.py,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,18b04822bfcc3b811d703627371bb90819087d7c,0,168,"{'module': 1, 'import_from_statement': 5, 'from': 5, 'relative_import': 5, 'import_prefix': 5, '.': 6, 'dotted_name': 16, 'identifier': 18, 'import': 5, ',': 6, 'line_continuation': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}",{},{},0.9452425713908255,0.9468210504282143,"(tensor([0.9922]), tensor([0.9967]), tensor([0.9944]), tensor([0.9962]))"
"1 import collections
2 import logging
3 import math
4 import random
5 import time
6 from abc import ABC, abstractmethod
7 from typing import Optional, Tuple, Callable, Type, Sequence, TypeVar, Generic
","1 import collections
2 import logging
3 import random
4 import time
5 from abc import ABC, abstractmethod
6 import math
7 from typing import Optional, Tuple, Callable, Type, Sequence, Dict
8 
9 import numpy as np
10 import pandas as pd
","Before: 3, 7
After: 6, 7",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,14,"{'module': 1, 'import_statement': 5, 'import': 6, 'dotted_name': 8, 'identifier': 8, 'import_from_statement': 1, 'from': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5528939464199152,0.5192251465264609,"(tensor([0.8987]), tensor([0.9675]), tensor([0.9318]), tensor([0.9602]))"
"8 
9 import numpy as np
10 import pandas as pd
11 from matplotlib import pyplot as plt
12 
13 log = logging.getLogger(__name__)
14 
15 
16 class SATemperatureSchedule(ABC):
17     """"""
","8 
9 import numpy as np
10 import pandas as pd
11 from matplotlib import pyplot as plt
12 
13 
14 _log = logging.getLogger(__name__)
15 
16 
17 class SATemperatureSchedule(ABC):
","Before: 13
After: 13, 14",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,94,"{'module': 1, 'import_statement': 2, 'import': 3, 'aliased_import': 3, 'dotted_name': 4, 'identifier': 13, 'as': 3, 'import_from_statement': 1, 'from': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.729331759210232,0.7251279428720682,"(tensor([0.9822]), tensor([0.9825]), tensor([0.9824]), tensor([0.9825]))"
"319         :param representation: a representation as returned by getStateRepresentation
320         """"""
321         pass
322 
323 
324 TSAState = TypeVar(""TSAState"", bound=SAState)
325 
326 
327 class SAOperator(Generic[TSAState]):
328     """"""
","320         :param representation: a representation as returned by getStateRepresentation
321         """"""
322         pass
323 
324 
325 class SAOperator:
326     """"""
327     An operator which, when applied with appropriately chosen parameters, can transform a state into another
328     state during simulated annealing
329     """"""
","Before: 324, 325, 326, 327
After: 325",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2138,"{'module': 1, 'ERROR': 4, ':': 3, 'identifier': 12, 'expression_statement': 1, 'assignment': 1, 'type': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, 'string_start': 1, ',': 1, '=': 1, ')': 2, 'class_definition': 1, 'class': 1, 'argument_list': 1, '(': 1, 'subscript': 1, '[': 1, ']': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.25794316955477087,0.22308262937199444,"(tensor([0.8203]), tensor([0.8657]), tensor([0.8424]), tensor([0.8609]))"
"330     state during simulated annealing
331     """"""
332 
333     def __init__(self, state: TSAState):
334         """"""
335         :param state: the state to which the operator is applied
336         """"""
337         self.state = state
338 
339     def applyCostChange(self, costDelta: SACostValue):
","328     state during simulated annealing
329     """"""
330 
331     def __init__(self, state: SAState):
332         """"""
333         :param state: the state to which the operator is applied
334         """"""
335         self.state = state
336 
337     def applyCostChange(self, costDelta: SACostValue, params):
","Before: 333
After: 331",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2171,"{'module': 1, 'ERROR': 4, 'identifier': 16, 'expression_statement': 4, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 2, 'assignment': 2, 'type': 1, 'comparison_operator': 1, 'is': 1, 'attribute': 1, '.': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5780090249338843,0.555609044063741,"(tensor([0.9628]), tensor([0.9706]), tensor([0.9667]), tensor([0.9698]))"
"336         """"""
337         self.state = state
338 
339     def applyCostChange(self, costDelta: SACostValue):
340         """"""
341         Applies the cost change to the state given at construction
342 
343         :param costDelta: the cost change to apply
344         """"""
345         self.state.cost = self.state.cost.add(costDelta)
346 
347     @abstractmethod
","334         """"""
335         self.state = state
336 
337     def applyCostChange(self, costDelta: SACostValue, params):
338         """"""
339         Applies the cost change to the state given at construction
340 
341         :param costDelta: the cost change to apply
342         :param params: the operator parameters
343         """"""
344         self.state.cost = self.state.cost.add(costDelta)
345 
346     @abstractmethod
","Before: 339
After: 337, 342",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2199,"{'module': 1, 'expression_statement': 4, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 4, 'identifier': 25, ':': 2, 'assignment': 2, 'type': 1, 'attribute': 5, '.': 5, '=': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4971983622979822,0.4765579760742204,"(tensor([0.9543]), tensor([0.9741]), tensor([0.9641]), tensor([0.9721]))"
"355         """"""
356         pass
357 
358     def apply(self, params: Tuple, costDelta: SACostValue):
359         """"""
360         Applies the operator to the state given at construction, changing the state and updating the costs appropriately
361 
362         :param params: the parameters with which the operator is to be applied
363         :param costDelta: the cost change that results from the application
364         :return:
365         """"""
366         self.applyCostChange(costDelta)
367         self.applyStateChange(*params)
368 
369     @abstractmethod
","354         """"""
355         pass
356 
357     def apply(self, params: Tuple, costDelta: SACostValue):
358         """"""
359         Applies the operator to the state given at construction, changing the state and updating the costs appropriately
360 
361         :param params: the parameters with which the operator is to be applied
362         :param costDelta: the cost change that results from the application
363         :return:
364         """"""
365         self.applyCostChange(costDelta, params)
366         self.applyStateChange(*params)
367 
368     @abstractmethod
","Before: 366
After: 365",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2289,"{'module': 1, 'expression_statement': 7, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 9, 'identifier': 44, ',': 1, 'boolean_operator': 1, 'and': 1, ':': 6, 'assignment': 3, 'type': 5, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'comparison_operator': 1, 'is': 1, 'block': 1, 'constrained_type': 1, 'call': 2, 'attribute': 2, '.': 2, 'argument_list': 2, '(': 2, ')': 2, 'list_splat': 1, '*': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6822572905788068,0.6824763517895739,"(tensor([0.9844]), tensor([0.9865]), tensor([0.9854]), tensor([0.9863]))"
"405 
406 
407 class SAChain:
408     """"""Manages the progression of one state during simulated annealing""""""
409 
410     log = log.getChild(__qualname__)
411 
412     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
413             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]], randomSeed, collectStats=False):
414         self.schedule = schedule
","404 
405 
406 class SAChain:
407     """"""Manages the progression of one state during simulated annealing""""""
408 
409     _log = _log.getChild(__qualname__)
410 
411     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
412             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator]]], randomSeed, collectStats=False):
413         self.schedule = schedule
","Before: 410
After: 409",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2537,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 23, ':': 5, 'block': 2, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 2, 'call': 1, 'attribute': 2, '.': 2, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'typed_parameter': 3, 'type': 10, 'generic_type': 4, 'type_parameter': 4, '[': 6, 'list': 2, ']': 6, 'default_parameter': 1, 'false': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7191075941448168,0.7080603301665918,"(tensor([0.9805]), tensor([0.9776]), tensor([0.9790]), tensor([0.9779]))"
"409 
410     log = log.getChild(__qualname__)
411 
412     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
413             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]], randomSeed, collectStats=False):
414         self.schedule = schedule
415         self.r = random.Random(randomSeed)
416         self.state = stateFactory(self.r)
417         self.collectStats = collectStats
418         operators, weights = zip(*opsAndWeights)
419         cumWeights, s = [], 0
420         for weight in weights:
421             s += weight
422             cumWeights.append(s)
423         self.ops = [cons(self.state) for cons in operators]
424         self.opCumWeights = cumWeights
425         self.stepsTaken = 0
426         self.countNoneParams = 0
427         self.countBestUpdates = -1
428         self.bestCost = None
429         self.bestStateRepr = None
430         self.loggedSeries = collections.defaultdict(lambda: [])
431         self._updateBestState()
432 
433         if self.collectStats:
434             self.operatorInapplicabilityCounters = {}
435             for op in self.ops:
436                 self.operatorInapplicabilityCounters[op] = RelativeFrequencyCounter()
437 
438     def _updateBestState(self):
","408 
409     _log = _log.getChild(__qualname__)
410 
411     def __init__(self, stateFactory: Callable[[random.Random], SAState], schedule: SATemperatureSchedule,
412             opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator]]], randomSeed, collectStats=False):
413         self.schedule = schedule
414         self.r = random.Random(randomSeed)
415         self.state = stateFactory(self.r)
416         self.collectStats = collectStats
417         operators, weights = zip(*opsAndWeights)
418         cumWeights, s = [], 0
419         for weight in weights:
420             s += weight
421             cumWeights.append(s)
422         self.ops = [cons(self.state) for cons in operators]
423         self.opCumWeights = cumWeights
424         self.stepsTaken = 0
425         self.countNoneParams = 0
426         self.countBestUpdates = -1
427         self.bestCost = None
428         self.bestStateRepr = None
429         self.loggedSeries = collections.defaultdict(lambda: [])
430         self._updateBestState()
431 
432         if self.collectStats:
433             self.operatorInapplicabilityCounters = {}
434             for op in self.ops:
435                 self.operatorInapplicabilityCounters[op] = RelativeFrequencyCounter()
436 
437     def _updateBestState(self):
","Before: 413
After: 412",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2613,"{'module': 1, 'expression_statement': 20, 'assignment': 17, 'identifier': 88, '=': 18, 'call': 9, 'attribute': 24, '.': 24, 'argument_list': 9, '(': 10, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 11, 'typed_parameter': 3, ':': 8, 'type': 10, 'generic_type': 4, 'type_parameter': 4, '[': 10, 'list': 4, ']': 10, 'default_parameter': 1, 'false': 1, 'block': 4, 'pattern_list': 2, 'list_splat': 1, '*': 1, 'expression_list': 1, 'integer': 4, 'for_statement': 2, 'for': 3, 'in': 3, 'augmented_assignment': 1, '+=': 1, 'list_comprehension': 1, 'for_in_clause': 1, 'unary_operator': 1, '-': 1, 'none': 2, 'lambda': 2, 'if_statement': 1, 'if': 1, 'dictionary': 1, '{': 1, '}': 1, 'subscript': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6309129130241906,0.6272900974128615,"(tensor([0.9913]), tensor([0.9899]), tensor([0.9906]), tensor([0.9900]))"
"442             self.bestStateRepr = self.state.getStateRepresentation()
443             self.countBestUpdates += 1
444 
445     def step(self, degreeOfCompletion):
446         r = self.r
447 
448         # make move
449         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
450         paramChoice = op.chooseParams()
451         if paramChoice is None:
452             self.countNoneParams += 1
453         else:
454             params, costChange = paramChoice
455             if costChange is None:
456                 costChange = op.costDelta(*params)
457             if costChange.value() < 0:
458                 makeMove = True
459             else:
460                 costChangeValue = costChange.value()
461                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
462                 makeMove = r.random() <= p
463                 self.log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
464                 if self.collectStats:
465                     self.loggedSeries[""temperatures""].append(T)
466                     self.loggedSeries[""probabilities""].append(p)
467             if makeMove:
468                 op.apply(params, costChange)
469                 self._updateBestState()
470             if self.collectStats:
471                 self.loggedSeries[""costDeltas""].append(costChange.value())
472         if self.collectStats:
473             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
474             self.loggedSeries[""costValues""].append(self.state.cost.value())
475             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
476 
477         self.stepsTaken += 1
478 
479         if self.log.isEnabledFor(logging.DEBUG):
480             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
481 
482     def logStats(self):
","441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self._log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","Before: 463
After: 462",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3170,"{'module': 1, 'expression_statement': 23, 'assignment': 10, 'attribute': 51, 'identifier': 121, '.': 51, '=': 12, 'call': 22, 'argument_list': 22, '(': 23, ')': 23, 'augmented_assignment': 3, '+=': 3, 'integer': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, ':': 11, 'block': 11, 'comment': 1, 'subscript': 7, 'keyword_argument': 2, '[': 7, ']': 7, 'if_statement': 8, 'if': 8, 'comparison_operator': 5, 'is': 3, 'none': 3, 'else_clause': 2, 'else': 2, 'pattern_list': 2, 'list_splat': 1, '*': 1, '<': 1, 'true': 1, '<=': 1, 'string': 7, 'string_start': 7, 'string_content': 12, 'interpolation': 7, '{': 7, '}': 7, 'string_end': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6929714439535254,0.6896347711708015,"(tensor([0.9913]), tensor([0.9913]), tensor([0.9913]), tensor([0.9913]))"
"442             self.bestStateRepr = self.state.getStateRepresentation()
443             self.countBestUpdates += 1
444 
445     def step(self, degreeOfCompletion):
446         r = self.r
447 
448         # make move
449         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
450         paramChoice = op.chooseParams()
451         if paramChoice is None:
452             self.countNoneParams += 1
453         else:
454             params, costChange = paramChoice
455             if costChange is None:
456                 costChange = op.costDelta(*params)
457             if costChange.value() < 0:
458                 makeMove = True
459             else:
460                 costChangeValue = costChange.value()
461                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
462                 makeMove = r.random() <= p
463                 self.log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
464                 if self.collectStats:
465                     self.loggedSeries[""temperatures""].append(T)
466                     self.loggedSeries[""probabilities""].append(p)
467             if makeMove:
468                 op.apply(params, costChange)
469                 self._updateBestState()
470             if self.collectStats:
471                 self.loggedSeries[""costDeltas""].append(costChange.value())
472         if self.collectStats:
473             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
474             self.loggedSeries[""costValues""].append(self.state.cost.value())
475             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
476 
477         self.stepsTaken += 1
478 
479         if self.log.isEnabledFor(logging.DEBUG):
480             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
481 
482     def logStats(self):
","441             self.bestStateRepr = self.state.getStateRepresentation()
442             self.countBestUpdates += 1
443 
444     def step(self, degreeOfCompletion):
445         r = self.r
446 
447         # make move
448         op = r.choices(self.ops, cum_weights=self.opCumWeights, k=1)[0]
449         paramChoice = op.chooseParams()
450         if paramChoice is None:
451             self.countNoneParams += 1
452         else:
453             params, costChange = paramChoice
454             if costChange is None:
455                 costChange = op.costDelta(*params)
456             if costChange.value() < 0:
457                 makeMove = True
458             else:
459                 costChangeValue = costChange.value()
460                 p, T = self.schedule.probability(degreeOfCompletion, costChangeValue)
461                 makeMove = r.random() <= p
462                 self._log.debug(f'p: {p}, T: {T}, costDelta: {costChangeValue}, move: {makeMove}')
463                 if self.collectStats:
464                     self.loggedSeries[""temperatures""].append(T)
465                     self.loggedSeries[""probabilities""].append(p)
466             if makeMove:
467                 op.apply(params, costChange)
468                 self._updateBestState()
469             if self.collectStats:
470                 self.loggedSeries[""costDeltas""].append(costChange.value())
471         if self.collectStats:
472             self.loggedSeries[""bestCostValues""].append(self.bestCost.value())
473             self.loggedSeries[""costValues""].append(self.state.cost.value())
474             self.operatorInapplicabilityCounters[op].count(paramChoice is None)
475 
476         self.stepsTaken += 1
477 
478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
","Before: 479, 480
After: 478, 479",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3397,"{'module': 1, 'expression_statement': 23, 'assignment': 10, 'attribute': 51, 'identifier': 121, '.': 51, '=': 12, 'call': 22, 'argument_list': 22, '(': 23, ')': 23, 'augmented_assignment': 3, '+=': 3, 'integer': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, ':': 11, 'block': 11, 'comment': 1, 'subscript': 7, 'keyword_argument': 2, '[': 7, ']': 7, 'if_statement': 8, 'if': 8, 'comparison_operator': 5, 'is': 3, 'none': 3, 'else_clause': 2, 'else': 2, 'pattern_list': 2, 'list_splat': 1, '*': 1, '<': 1, 'true': 1, '<=': 1, 'string': 7, 'string_start': 7, 'string_content': 12, 'interpolation': 7, '{': 7, '}': 7, 'string_end': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6929714439535254,0.6896347711708015,"(tensor([0.9913]), tensor([0.9913]), tensor([0.9913]), tensor([0.9913]))"
"479         if self.log.isEnabledFor(logging.DEBUG):
480             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
481 
482     def logStats(self):
483         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
484         if self.collectStats:
485             for op, counter in self.operatorInapplicabilityCounters.items():
486                 stats[f""useless moves of {op}""] = str(counter)
487             loggedCostDeltas = self.loggedSeries[""costDeltas""]
488             if loggedCostDeltas:
489                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
490                 absCostDeltas = np.abs(loggedCostDeltas)
491                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
492                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
493                 if positiveCostDeltas:
494                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
495                                                    f"" max={np.max(positiveCostDeltas):.3f}""
496         statsJoin = ""\n    "" if self.collectStats else ""; ""
497         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
498         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
499 
500     def applyBestState(self):
","478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","Before: 489
After: 488",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3591,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 18, ')': 18, ':': 14, 'block': 6, 'expression_statement': 12, 'string': 17, 'string_start': 17, 'string_content': 23, 'interpolation': 16, '{': 17, '}': 17, 'string_end': 17, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 2, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'format_specifier': 7, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'concatenated_string': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, 'binary_operator': 2, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6598176688168239,0.656775758866981,"(tensor([0.9776]), tensor([0.9758]), tensor([0.9767]), tensor([0.9760]))"
"479         if self.log.isEnabledFor(logging.DEBUG):
480             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
481 
482     def logStats(self):
483         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
484         if self.collectStats:
485             for op, counter in self.operatorInapplicabilityCounters.items():
486                 stats[f""useless moves of {op}""] = str(counter)
487             loggedCostDeltas = self.loggedSeries[""costDeltas""]
488             if loggedCostDeltas:
489                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
490                 absCostDeltas = np.abs(loggedCostDeltas)
491                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
492                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
493                 if positiveCostDeltas:
494                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
495                                                    f"" max={np.max(positiveCostDeltas):.3f}""
496         statsJoin = ""\n    "" if self.collectStats else ""; ""
497         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
498         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
499 
500     def applyBestState(self):
","478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","Before: 491
After: 490",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3647,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 18, ')': 18, ':': 14, 'block': 6, 'expression_statement': 12, 'string': 17, 'string_start': 17, 'string_content': 23, 'interpolation': 16, '{': 17, '}': 17, 'string_end': 17, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 2, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'format_specifier': 7, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'concatenated_string': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, 'binary_operator': 2, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6598176688168239,0.656775758866981,"(tensor([0.9776]), tensor([0.9758]), tensor([0.9767]), tensor([0.9760]))"
"479         if self.log.isEnabledFor(logging.DEBUG):
480             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
481 
482     def logStats(self):
483         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
484         if self.collectStats:
485             for op, counter in self.operatorInapplicabilityCounters.items():
486                 stats[f""useless moves of {op}""] = str(counter)
487             loggedCostDeltas = self.loggedSeries[""costDeltas""]
488             if loggedCostDeltas:
489                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
490                 absCostDeltas = np.abs(loggedCostDeltas)
491                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
492                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
493                 if positiveCostDeltas:
494                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
495                                                    f"" max={np.max(positiveCostDeltas):.3f}""
496         statsJoin = ""\n    "" if self.collectStats else ""; ""
497         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
498         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
499 
500     def applyBestState(self):
","478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","Before: 494, 495
After: 493",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3717,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 18, ')': 18, ':': 14, 'block': 6, 'expression_statement': 12, 'string': 17, 'string_start': 17, 'string_content': 23, 'interpolation': 16, '{': 17, '}': 17, 'string_end': 17, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 2, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'format_specifier': 7, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'concatenated_string': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, 'binary_operator': 2, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6598176688168239,0.656775758866981,"(tensor([0.9776]), tensor([0.9758]), tensor([0.9767]), tensor([0.9760]))"
"479         if self.log.isEnabledFor(logging.DEBUG):
480             self.log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
481 
482     def logStats(self):
483         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
484         if self.collectStats:
485             for op, counter in self.operatorInapplicabilityCounters.items():
486                 stats[f""useless moves of {op}""] = str(counter)
487             loggedCostDeltas = self.loggedSeries[""costDeltas""]
488             if loggedCostDeltas:
489                 stats[""mean cost delta""] = f""{np.mean(loggedCostDeltas):.3f} +- { np.std(loggedCostDeltas):.3f}""
490                 absCostDeltas = np.abs(loggedCostDeltas)
491                 stats[""mean absolute cost delta""] = f""{np.mean(absCostDeltas):.3f} +- {np.std(absCostDeltas):.3f}""
492                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
493                 if positiveCostDeltas:
494                     stats[""positive cost delta""] = f""mean={np.mean(positiveCostDeltas):.3f} +- {np.std(positiveCostDeltas):.3f},"" \
495                                                    f"" max={np.max(positiveCostDeltas):.3f}""
496         statsJoin = ""\n    "" if self.collectStats else ""; ""
497         self.log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
498         self.log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
499 
500     def applyBestState(self):
","478         if self._log.isEnabledFor(logging.DEBUG):
479             self._log.debug(f""Step {self.stepsTaken}: cost={self.state.cost}; best cost={self.bestCost}"")
480 
481     def logStats(self):
482         stats = {""useless moves total (None params)"": f""{self.countNoneParams}/{self.stepsTaken}""}
483         if self.collectStats:
484             for op, counter in self.operatorInapplicabilityCounters.items():
485                 stats[f""useless moves of {op}""] = str(counter)
486             loggedCostDeltas = self.loggedSeries[""costDeltas""]
487             if loggedCostDeltas:
488                 stats[""mean cost delta""] = ""%.3f +- %.3f"" % (np.mean(loggedCostDeltas), np.std(loggedCostDeltas))
489                 absCostDeltas = np.abs(loggedCostDeltas)
490                 stats[""mean absolute cost delta""] = ""%.3f +- %.3f"" % (np.mean(absCostDeltas), np.std(absCostDeltas))
491                 positiveCostDeltas = [cd for cd in loggedCostDeltas if cd > 0]
492                 if positiveCostDeltas:
493                     stats[""positive cost delta""] = ""mean=%.3f +- %.3f, max=%.3f"" % (np.mean(positiveCostDeltas), np.std(positiveCostDeltas), np.max(positiveCostDeltas))
494         statsJoin = ""\n    "" if self.collectStats else ""; ""
495         self._log.info(f""Stats: {statsJoin.join([key + ': ' + value for (key, value) in stats.items()])}"")
496         self._log.info(f""Best solution has {self.bestCost} after {self.countBestUpdates} updates of best state"")
497 
498     def applyBestState(self):
","Before: 497, 498
After: 495, 496",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3812,"{'module': 1, 'if_statement': 4, 'if': 6, 'call': 16, 'attribute': 32, 'identifier': 92, '.': 32, 'argument_list': 16, '(': 18, ')': 18, ':': 14, 'block': 6, 'expression_statement': 12, 'string': 17, 'string_start': 17, 'string_content': 23, 'interpolation': 16, '{': 17, '}': 17, 'string_end': 17, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 9, '=': 9, 'dictionary': 1, 'pair': 1, 'for_statement': 1, 'for': 3, 'pattern_list': 1, ',': 2, 'in': 3, 'subscript': 5, '[': 7, ']': 7, 'format_specifier': 7, 'list_comprehension': 2, 'for_in_clause': 2, 'if_clause': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'concatenated_string': 1, 'conditional_expression': 1, 'escape_sequence': 1, 'else': 1, 'binary_operator': 2, '+': 2, 'tuple_pattern': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6598176688168239,0.656775758866981,"(tensor([0.9776]), tensor([0.9758]), tensor([0.9767]), tensor([0.9760]))"
"523             raise Exception(""Unknown series"")
524         return pd.Series(self.loggedSeries[seriesName])
525 
526 
527 class SimulatedAnnealing:
528     log = log.getChild(__qualname__)
529 
530     """"""
531     The simulated annealing algorithm for discrete optimisation (cost minimisation)
532     """"""
","521             raise Exception(""Unknown series"")
522         return pd.Series(self.loggedSeries[seriesName])
523 
524 
525 class SimulatedAnnealing:
526     _log = _log.getChild(__qualname__)
527 
528     """"""
529     The simulated annealing algorithm for discrete optimisation (cost minimisation)
530     """"""
","Before: 528
After: 526",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,4021,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 4, 'identifier': 19, 'argument_list': 4, '(': 4, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ')': 4, 'return_statement': 1, 'return': 1, 'attribute': 3, '.': 3, 'subscript': 1, '[': 1, ']': 1, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'ERROR': 3, 'for': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5890953986367728,0.5694137246762522,"(tensor([0.9646]), tensor([0.9651]), tensor([0.9648]), tensor([0.9650]))"
"530     """"""
531     The simulated annealing algorithm for discrete optimisation (cost minimisation)
532     """"""
533     def __init__(self, scheduleFactory: Callable[[], SATemperatureSchedule], opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]],
534             maxSteps: int = None, duration: float = None, randomSeed=42, collectStats=False):
535         """"""
536         :param scheduleFactory: a factory for the creation of the temperature schedule for the annealing process
537         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
538         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
539         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
540         :param randomSeed: the random seed to use for all random choices
541         :param collectStats: flag indicating whether to collect additional statics which will be logged
542         """"""
543         if maxSteps is not None and maxSteps <= 0:
544             raise ValueError(""The number of iterations should be greater than 0."")
545         if maxSteps is None and duration is None or (maxSteps is not None and duration is not None):
546             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
547         if duration is not None and duration <= 0:
548             raise ValueError(""Duration must be greater than 0 if provided"")
549         self.scheduleFactory = scheduleFactory
550         self.maxSteps = maxSteps
551         self.duration = duration
552         self.randomSeed = randomSeed
553         self.opsAndWeights = opsAndWeights
554         self.collectStats = collectStats
555         self._chain = None
556 
557     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
","528     """"""
529     The simulated annealing algorithm for discrete optimisation (cost minimisation)
530     """"""
531     def __init__(self, scheduleFactory: Callable[[], SATemperatureSchedule], opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]], maxSteps=None, duration=None,
532                  randomSeed=42, collectStats=False):
533         """"""
534         :param scheduleFactory: a factory for the creation of the temperature schedule for the annealing process
535         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
536         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
537         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
538         :param randomSeed: the random seed to use for all random choices
539         :param collectStats: flag indicating whether to collect additional statics which will be logged
540         """"""
541         if maxSteps is not None and maxSteps <= 0:
542             raise ValueError(""The number of iterations should be greater than 0."")
543         if maxSteps is None and duration is None or (maxSteps is not None and duration is not None):
544             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
545         self.scheduleFactory = scheduleFactory
546         self.maxSteps = maxSteps
547         self.duration = duration
548         self.randomSeed = randomSeed
549         self.opsAndWeights = opsAndWeights
550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
","Before: 533, 534
After: 531, 532",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,4082,"{'module': 1, 'expression_statement': 9, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'function_definition': 1, 'def': 1, 'identifier': 49, 'parameters': 1, '(': 5, ',': 9, 'typed_parameter': 2, ':': 8, 'type': 11, 'generic_type': 4, 'type_parameter': 4, '[': 6, 'list': 2, ']': 6, 'typed_default_parameter': 2, '=': 11, 'none': 9, 'default_parameter': 2, 'integer': 3, 'false': 1, ')': 5, 'block': 4, 'if_statement': 3, 'if': 3, 'boolean_operator': 5, 'comparison_operator': 8, 'is not': 8, 'and': 4, '<=': 2, 'raise_statement': 3, 'raise': 3, 'call': 3, 'argument_list': 3, 'is': 2, 'or': 1, 'parenthesized_expression': 1, 'assignment': 7, 'attribute': 7, '.': 7}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7461008259874171,0.7449961807696915,"(tensor([0.9840]), tensor([0.9784]), tensor([0.9812]), tensor([0.9790]))"
"554         self.collectStats = collectStats
555         self._chain = None
556 
557     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
558         """"""
559         Applies the annealing process starting with a state created via the given factory.
560         The result of the optimisation (i.e. the final best state representation) is written via the state's
561         applyStateRepresentation method, which should write to an object the state receives at construction.
562 
563         :param stateFactory: the factory with which to create the (initial) state
564         """"""
565         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
566         self.log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
567         startTime = time.time()
568         while True:
569             timeElapsed = time.time() - startTime
570             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
571                 break
572             if self.maxSteps is not None:
573                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
574             else:
575                 degreeOfCompletion = timeElapsed / self.duration
576             chain.step(degreeOfCompletion)
577         self.log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
578         chain.logStats()
579         chain.applyBestState()
580         if self.collectStats:
581             self._chain = chain
582 
583     def getChain(self) -> Optional[SAChain]:
","550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self._log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self._log.info(f""Simulated annealing completed after {time.time()-startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","Before: 566
After: 562",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,4408,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'attribute': 33, 'identifier': 87, '.': 33, '=': 11, 'none': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 14, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ')': 14, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 11, 'argument_list': 11, 'keyword_argument': 3, 'interpolation': 4, '{': 4, '}': 4, 'conditional_expression': 1, 'binary_operator': 6, '%': 2, 'if': 4, 'comparison_operator': 6, 'is not': 8, 'else': 2, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 3, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>=': 2, 'or': 1, 'break_statement': 1, 'break': 1, '/': 2, 'else_clause': 1, 'format_specifier': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.758447439845996,0.7553194965583492,"(tensor([0.9872]), tensor([0.9877]), tensor([0.9875]), tensor([0.9877]))"
"554         self.collectStats = collectStats
555         self._chain = None
556 
557     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
558         """"""
559         Applies the annealing process starting with a state created via the given factory.
560         The result of the optimisation (i.e. the final best state representation) is written via the state's
561         applyStateRepresentation method, which should write to an object the state receives at construction.
562 
563         :param stateFactory: the factory with which to create the (initial) state
564         """"""
565         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
566         self.log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
567         startTime = time.time()
568         while True:
569             timeElapsed = time.time() - startTime
570             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
571                 break
572             if self.maxSteps is not None:
573                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
574             else:
575                 degreeOfCompletion = timeElapsed / self.duration
576             chain.step(degreeOfCompletion)
577         self.log.info(f""Simulated annealing completed after {time.time() - startTime:.1f} seconds, {chain.stepsTaken} steps"")
578         chain.logStats()
579         chain.applyBestState()
580         if self.collectStats:
581             self._chain = chain
582 
583     def getChain(self) -> Optional[SAChain]:
","550         self.collectStats = collectStats
551         self._chain = None
552 
553     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
554         """"""
555         Applies the annealing process starting with a state created via the given factory.
556         The result of the optimisation (i.e. the final best state representation) is written via the state's
557         applyStateRepresentation method, which should write to an object the state receives at construction.
558 
559         :param stateFactory: the factory with which to create the (initial) state
560         """"""
561         chain = SAChain(stateFactory, self.scheduleFactory(), opsAndWeights=self.opsAndWeights, randomSeed=self.randomSeed, collectStats=self.collectStats)
562         self._log.info(f""Running simulated annealing with {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
563         startTime = time.time()
564         while True:
565             timeElapsed = time.time() - startTime
566             if (self.maxSteps is not None and chain.stepsTaken >= self.maxSteps) or (self.duration is not None and timeElapsed >= self.duration):
567                 break
568             if self.maxSteps is not None:
569                 degreeOfCompletion = chain.stepsTaken / self.maxSteps
570             else:
571                 degreeOfCompletion = timeElapsed / self.duration
572             chain.step(degreeOfCompletion)
573         self._log.info(f""Simulated annealing completed after {time.time()-startTime:.1f} seconds, {chain.stepsTaken} steps"")
574         chain.logStats()
575         chain.applyBestState()
576         if self.collectStats:
577             self._chain = chain
578 
579     def getChain(self) -> Optional[SAChain]:
","Before: 577
After: 573",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,4583,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'attribute': 33, 'identifier': 87, '.': 33, '=': 11, 'none': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 14, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, ')': 14, 'block': 6, 'string': 5, 'string_start': 5, 'string_content': 9, 'string_end': 5, 'call': 11, 'argument_list': 11, 'keyword_argument': 3, 'interpolation': 4, '{': 4, '}': 4, 'conditional_expression': 1, 'binary_operator': 6, '%': 2, 'if': 4, 'comparison_operator': 6, 'is not': 8, 'else': 2, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 3, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>=': 2, 'or': 1, 'break_statement': 1, 'break': 1, '/': 2, 'else_clause': 1, 'format_specifier': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.758447439845996,0.7553194965583492,"(tensor([0.9872]), tensor([0.9877]), tensor([0.9875]), tensor([0.9877]))"
"588         """"""
589         return self._chain
590 
591 
592 class ParallelTempering:
593     log = log.getChild(__qualname__)
594 
595     """"""
596     The parallel tempering algorithm for discrete optimisation (cost minimisation)
597     """"""
","584         """"""
585         return self._chain
586 
587 
588 class ParallelTempering:
589     _log = _log.getChild(__qualname__)
590 
591     """"""
592     The parallel tempering algorithm for discrete optimisation (cost minimisation)
593     """"""
","Before: 593
After: 589",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,4664,"{'module': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'ERROR': 3, 'identifier': 8, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4340144349710596,0.3894599539514586,"(tensor([0.9621]), tensor([0.9642]), tensor([0.9631]), tensor([0.9640]))"
"595     """"""
596     The parallel tempering algorithm for discrete optimisation (cost minimisation)
597     """"""
598     def __init__(self, numChains, opsAndWeights: Sequence[Tuple[Type[SAOperator], float]],
599                  schedule: SATemperatureSchedule = None, probabilityFunction: SAProbabilityFunction = None,
600                  maxSteps: int = None, duration: float = None, randomSeed=42, logCostProgression=False):
601         """"""
602         Creates a parallel tempering optimiser with the given number of chains and operators for each chain.
603         To determine the schedule to use for each chain, either schedule or probabilityFunction must be provided.
604         It is usually more robust to use adaptive schedules and therefore to provide probabilityFunction.
605 
606         :param numChains: the number of parallel chains
607         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
608         :param schedule: the temperature schedule from which numChains temperatures of chains are drawn (using equidistant degrees of completion); if None, must provide probabilityFunction
609         :param probabilityFunction: the probability function from which numChains probabilities for adaptive probability schedules, each using a constant probability, are to be drawn; if None, must provide schedule
610         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
611         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
612         :param randomSeed: the random seed to use for all random choices
613         :param logCostProgression: whether to log cost progression of all chains (such that it can be plotted after the fact via plotCostProgression)
614         """"""
615         if maxSteps is not None and maxSteps <= 0:
616             raise ValueError(""The number of iterations should be greater than 0."")
617         if (maxSteps is None and duration is None) or (maxSteps is not None and duration is not None):
618             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
619         if duration is not None and duration <= 0:
620             raise ValueError(""duration should be greater than 0 if provided."")
621         if numChains < 2:
622             raise ValueError(""Number of chains must be at least 2."")
623         if (schedule is None and probabilityFunction is None) or (schedule is not None and probabilityFunction is not None):
624             raise ValueError(""Exactly one of {schedule, probabilityFunction} must be given"")
625         self.maxSteps = maxSteps
626         self.duration = duration
627         self.randomSeed = randomSeed
628         self.numChains = numChains
629         self.baseSchedule = schedule
630         self.baseProbabilityFunction = probabilityFunction
631         self.opsAndWeights = opsAndWeights
632         self.logCostProgression = logCostProgression
633 
634         # transient members
635         self._costProgressions = None
636         self._scheduleParamStrings = None
637 
638     def _createSchedules(self):
","591     """"""
592     The parallel tempering algorithm for discrete optimisation (cost minimisation)
593     """"""
594     def __init__(self, numChains, opsAndWeights: Sequence[Tuple[Type[SAOperator], float]],
595                  schedule: SATemperatureSchedule = None, probabilityFunction: SAProbabilityFunction = None,
596                  maxSteps=None, duration=None, randomSeed=42, logCostProgression=False):
597         """"""
598         Creates a parallel tempering optimiser with the given number of chains and operators for each chain.
599         To determine the schedule to use for each chain, either schedule or probabilityFunction must be provided.
600         It is usually more robust to use adaptive schedules and therefore to provide probabilityFunction.
601 
602         :param numChains: the number of parallel chains
603         :param opsAndWeights: a list of operators with associated weights (which are to indicate the non-normalised probability of chosing the associated operator)
604         :param schedule: the temperature schedule from which numChains temperatures of chains are drawn (using equidistant degrees of completion); if None, must provide probabilityFunction
605         :param probabilityFunction: the probability function from which numChains probabilities for adaptive probability schedules, each using a constant probability, are to be drawn; if None, must provide schedule
606         :param maxSteps: the number of steps for which to run the optimisation; may be None (if not given, duration must be provided)
607         :param duration: the duration, in seconds, for which to run the optimisation; may be None (if not given, maxSteps must be provided)
608         :param randomSeed: the random seed to use for all random choices
609         :param logCostProgression: whether to log cost progression of all chains (such that it can be plotted after the fact via plotCostProgression)
610         """"""
611         if maxSteps is not None and maxSteps <= 0:
612             raise ValueError(""The number of iterations should be greater than 0."")
613         if (maxSteps is None and duration is None) or (maxSteps is not None and duration is not None):
614             raise ValueError(""Exactly one of {maxSteps, duration} must be specified."")
615         if numChains < 2:
616             raise ValueError(""Number of chains must be at least 2."")
617         if (schedule is None and probabilityFunction is None) or (schedule is not None and probabilityFunction is not None):
618             raise ValueError(""Exactly one of {schedule, probabilityFunction} must be given"")
619         self.maxSteps = maxSteps
620         self.duration = duration
621         self.randomSeed = randomSeed
622         self.numChains = numChains
623         self.baseSchedule = schedule
624         self.baseProbabilityFunction = probabilityFunction
625         self.opsAndWeights = opsAndWeights
626         self.logCostProgression = logCostProgression
627 
628         # transient members
629         self._costProgressions = None
630         self._scheduleParamStrings = None
631 
632     def _createSchedules(self):
","Before: 600
After: 596",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,4749,"{'module': 1, 'expression_statement': 12, 'string': 7, 'string_start': 7, 'string_content': 7, 'string_end': 7, 'function_definition': 1, 'def': 1, 'identifier': 65, 'parameters': 1, '(': 10, ',': 9, 'typed_parameter': 1, ':': 11, 'type': 9, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'typed_default_parameter': 4, '=': 16, 'none': 16, 'default_parameter': 2, 'integer': 4, 'false': 1, ')': 10, 'block': 6, 'if_statement': 5, 'if': 5, 'boolean_operator': 8, 'comparison_operator': 13, 'is not': 12, 'and': 6, '<=': 2, 'raise_statement': 5, 'raise': 5, 'call': 5, 'argument_list': 5, 'parenthesized_expression': 4, 'is': 4, 'or': 2, '<': 1, 'assignment': 10, 'attribute': 10, '.': 10, 'comment': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.779504366292285,0.7785227710236932,"(tensor([0.9758]), tensor([0.9707]), tensor([0.9732]), tensor([0.9712]))"
"649             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
650             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
651 
652     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
653         """"""
654         Applies the optimisation process starting, in each chain, with a state created via the given factory.
655         The result of the optimisation (i.e. the final best state representation) is written by calling the
656         applyStateRepresentation method on one of the states, which should write to a suitable object each
657         state receives at construction.
658 
659         :param stateFactory: the factory with which to create the states for all chains
660         """"""
661         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
662 
663         r = random.Random(self.randomSeed)
664         chains = []
665         costProgressions = []
666         for i, schedule in enumerate(self._createSchedules(), start=1):
667             self.log.info(f""Chain {i} uses {schedule}"")
668             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
669             costProgressions.append([])
670 
671         startTime = time.time()
672         step = 0
673         numChainSwaps = 0
674         while True:
675             timeElapsed = time.time() - startTime
676             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
677                 break
678 
679             # take one step in each chain
680             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
681             for chain in chains:
682                 chain.step(degreeOfCompletion)
683 
684             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
685             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
686             # in the chains array, which shall always be in descending order of temperature)
687             for idxHighTempChain in range(0, self.numChains-1):
688                 idxLowTempChain = idxHighTempChain+1
689                 highTempChain = chains[idxHighTempChain]
690                 lowTempChain = chains[idxLowTempChain]
691                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
692                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
693                     chains[idxLowTempChain] = highTempChain
694                     chains[idxHighTempChain] = lowTempChain
695                     numChainSwaps += 1
696 
697             if self.logCostProgression:
698                 for idxChain, chain in enumerate(chains):
699                     costProgressions[idxChain].append(chain.state.cost.value())
700 
701             step += 1
702 
703         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
704         if self.logCostProgression: self._costProgressions = costProgressions
705 
706         # apply best solution
707         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
708         chains[bestChainIdx].applyBestState()
709 
710     def plotCostProgression(self):
","643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","Before: 661
After: 655",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,5299,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 165, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 26, 'argument_list': 26, '(': 29, 'none': 5, ',': 13, ')': 29, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.763302047772842,0.7618454914443433,"(tensor([0.9858]), tensor([0.9851]), tensor([0.9854]), tensor([0.9852]))"
"649             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
650             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
651 
652     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
653         """"""
654         Applies the optimisation process starting, in each chain, with a state created via the given factory.
655         The result of the optimisation (i.e. the final best state representation) is written by calling the
656         applyStateRepresentation method on one of the states, which should write to a suitable object each
657         state receives at construction.
658 
659         :param stateFactory: the factory with which to create the states for all chains
660         """"""
661         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
662 
663         r = random.Random(self.randomSeed)
664         chains = []
665         costProgressions = []
666         for i, schedule in enumerate(self._createSchedules(), start=1):
667             self.log.info(f""Chain {i} uses {schedule}"")
668             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
669             costProgressions.append([])
670 
671         startTime = time.time()
672         step = 0
673         numChainSwaps = 0
674         while True:
675             timeElapsed = time.time() - startTime
676             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
677                 break
678 
679             # take one step in each chain
680             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
681             for chain in chains:
682                 chain.step(degreeOfCompletion)
683 
684             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
685             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
686             # in the chains array, which shall always be in descending order of temperature)
687             for idxHighTempChain in range(0, self.numChains-1):
688                 idxLowTempChain = idxHighTempChain+1
689                 highTempChain = chains[idxHighTempChain]
690                 lowTempChain = chains[idxLowTempChain]
691                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
692                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
693                     chains[idxLowTempChain] = highTempChain
694                     chains[idxHighTempChain] = lowTempChain
695                     numChainSwaps += 1
696 
697             if self.logCostProgression:
698                 for idxChain, chain in enumerate(chains):
699                     costProgressions[idxChain].append(chain.state.cost.value())
700 
701             step += 1
702 
703         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
704         if self.logCostProgression: self._costProgressions = costProgressions
705 
706         # apply best solution
707         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
708         chains[bestChainIdx].applyBestState()
709 
710     def plotCostProgression(self):
","643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","Before: 667
After: 661",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,5381,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 165, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 26, 'argument_list': 26, '(': 29, 'none': 5, ',': 13, ')': 29, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.763302047772842,0.7618454914443433,"(tensor([0.9858]), tensor([0.9851]), tensor([0.9854]), tensor([0.9852]))"
"649             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
650             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
651 
652     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
653         """"""
654         Applies the optimisation process starting, in each chain, with a state created via the given factory.
655         The result of the optimisation (i.e. the final best state representation) is written by calling the
656         applyStateRepresentation method on one of the states, which should write to a suitable object each
657         state receives at construction.
658 
659         :param stateFactory: the factory with which to create the states for all chains
660         """"""
661         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
662 
663         r = random.Random(self.randomSeed)
664         chains = []
665         costProgressions = []
666         for i, schedule in enumerate(self._createSchedules(), start=1):
667             self.log.info(f""Chain {i} uses {schedule}"")
668             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
669             costProgressions.append([])
670 
671         startTime = time.time()
672         step = 0
673         numChainSwaps = 0
674         while True:
675             timeElapsed = time.time() - startTime
676             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
677                 break
678 
679             # take one step in each chain
680             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
681             for chain in chains:
682                 chain.step(degreeOfCompletion)
683 
684             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
685             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
686             # in the chains array, which shall always be in descending order of temperature)
687             for idxHighTempChain in range(0, self.numChains-1):
688                 idxLowTempChain = idxHighTempChain+1
689                 highTempChain = chains[idxHighTempChain]
690                 lowTempChain = chains[idxLowTempChain]
691                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
692                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
693                     chains[idxLowTempChain] = highTempChain
694                     chains[idxHighTempChain] = lowTempChain
695                     numChainSwaps += 1
696 
697             if self.logCostProgression:
698                 for idxChain, chain in enumerate(chains):
699                     costProgressions[idxChain].append(chain.state.cost.value())
700 
701             step += 1
702 
703         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
704         if self.logCostProgression: self._costProgressions = costProgressions
705 
706         # apply best solution
707         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
708         chains[bestChainIdx].applyBestState()
709 
710     def plotCostProgression(self):
","643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","Before: 703
After: 697",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,5774,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 165, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 26, 'argument_list': 26, '(': 29, 'none': 5, ',': 13, ')': 29, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.763302047772842,0.7618454914443433,"(tensor([0.9858]), tensor([0.9851]), tensor([0.9854]), tensor([0.9852]))"
"649             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
650             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
651 
652     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
653         """"""
654         Applies the optimisation process starting, in each chain, with a state created via the given factory.
655         The result of the optimisation (i.e. the final best state representation) is written by calling the
656         applyStateRepresentation method on one of the states, which should write to a suitable object each
657         state receives at construction.
658 
659         :param stateFactory: the factory with which to create the states for all chains
660         """"""
661         self.log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
662 
663         r = random.Random(self.randomSeed)
664         chains = []
665         costProgressions = []
666         for i, schedule in enumerate(self._createSchedules(), start=1):
667             self.log.info(f""Chain {i} uses {schedule}"")
668             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
669             costProgressions.append([])
670 
671         startTime = time.time()
672         step = 0
673         numChainSwaps = 0
674         while True:
675             timeElapsed = time.time() - startTime
676             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
677                 break
678 
679             # take one step in each chain
680             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
681             for chain in chains:
682                 chain.step(degreeOfCompletion)
683 
684             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
685             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
686             # in the chains array, which shall always be in descending order of temperature)
687             for idxHighTempChain in range(0, self.numChains-1):
688                 idxLowTempChain = idxHighTempChain+1
689                 highTempChain = chains[idxHighTempChain]
690                 lowTempChain = chains[idxLowTempChain]
691                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
692                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
693                     chains[idxLowTempChain] = highTempChain
694                     chains[idxHighTempChain] = lowTempChain
695                     numChainSwaps += 1
696 
697             if self.logCostProgression:
698                 for idxChain, chain in enumerate(chains):
699                     costProgressions[idxChain].append(chain.state.cost.value())
700 
701             step += 1
702 
703         self.log.info(f""Number of chain swaps: {numChainSwaps}"")
704         if self.logCostProgression: self._costProgressions = costProgressions
705 
706         # apply best solution
707         bestChainIdx = int(np.argmin([chain.bestCost.value() for chain in chains]))
708         chains[bestChainIdx].applyBestState()
709 
710     def plotCostProgression(self):
","643             self._scheduleParamStrings = [""p=%.3f"" % p for p in probabilities]
644             return [SAProbabilitySchedule(None, SAProbabilityFunctionConstant(p)) for p in probabilities]
645 
646     def optimise(self, stateFactory: Callable[[random.Random], SAState]):
647         """"""
648         Applies the optimisation process starting, in each chain, with a state created via the given factory.
649         The result of the optimisation (i.e. the final best state representation) is written by calling the
650         applyStateRepresentation method on one of the states, which should write to a suitable object each
651         state receives at construction.
652 
653         :param stateFactory: the factory with which to create the states for all chains
654         """"""
655         self._log.info(f""Running parallel tempering with {self.numChains} chains, {len(self.opsAndWeights)} operators for {'%d steps' % self.maxSteps if self.maxSteps is not None else '%d seconds' % self.duration} ..."")
656 
657         r = random.Random(self.randomSeed)
658         chains = []
659         costProgressions = []
660         for i, schedule in enumerate(self._createSchedules(), start=1):
661             self._log.info(f""Chain {i} uses {schedule}"")
662             chains.append(SAChain(stateFactory, schedule, opsAndWeights=self.opsAndWeights, randomSeed=r.randint(0, 1000)))
663             costProgressions.append([])
664 
665         startTime = time.time()
666         step = 0
667         numChainSwaps = 0
668         while True:
669             timeElapsed = time.time() - startTime
670             if (self.maxSteps is not None and step > self.maxSteps) or (self.duration is not None and timeElapsed > self.duration):
671                 break
672 
673             # take one step in each chain
674             degreeOfCompletion = step / self.maxSteps if self.maxSteps is not None else timeElapsed / self.duration
675             for chain in chains:
676                 chain.step(degreeOfCompletion)
677 
678             # check if neighbouring chains can be ""swapped"": if a high-temperature chain has a better state
679             # than a low-temperature chain, swap them (by exchanging their schedules and swapping them
680             # in the chains array, which shall always be in descending order of temperature)
681             for idxHighTempChain in range(0, self.numChains-1):
682                 idxLowTempChain = idxHighTempChain+1
683                 highTempChain = chains[idxHighTempChain]
684                 lowTempChain = chains[idxLowTempChain]
685                 if highTempChain.state.cost.value() < lowTempChain.state.cost.value():
686                     highTempChain.schedule, lowTempChain.schedule = lowTempChain.schedule, highTempChain.schedule
687                     chains[idxLowTempChain] = highTempChain
688                     chains[idxHighTempChain] = lowTempChain
689                     numChainSwaps += 1
690 
691             if self.logCostProgression:
692                 for idxChain, chain in enumerate(chains):
693                     costProgressions[idxChain].append(chain.state.cost.value())
694 
695             step += 1
696 
697         self._log.info(f""Number of chain swaps: {numChainSwaps}"")
698         if self.logCostProgression: self._costProgressions = costProgressions
699 
700         # apply best solution
701         bestChainIdx = np.argmin([chain.bestCost.value() for chain in chains])
702         chains[bestChainIdx].applyBestState()
703 
704     def plotCostProgression(self):
","Before: 707
After: 701",update local_search.py for new local_search.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/local_search.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,5827,"{'module': 1, 'expression_statement': 28, 'assignment': 17, 'attribute': 52, 'identifier': 165, '.': 52, '=': 20, 'list_comprehension': 3, '[': 14, 'binary_operator': 8, 'string': 7, 'string_start': 7, 'string_content': 11, 'string_end': 7, '%': 3, 'for_in_clause': 3, 'for': 7, 'in': 7, ']': 14, 'return_statement': 1, 'return': 1, 'call': 26, 'argument_list': 26, '(': 29, 'none': 5, ',': 13, ')': 29, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'list': 4, 'block': 10, 'interpolation': 6, '{': 6, '}': 6, 'conditional_expression': 2, 'if': 6, 'comparison_operator': 7, 'is not': 8, 'else': 2, 'for_statement': 4, 'pattern_list': 3, 'keyword_argument': 3, 'integer': 10, 'while_statement': 1, 'while': 1, 'true': 1, '-': 2, 'if_statement': 4, 'boolean_operator': 3, 'parenthesized_expression': 2, 'and': 2, '>': 2, 'or': 1, 'break_statement': 1, 'break': 1, 'comment': 5, '/': 2, '+': 1, 'subscript': 6, '<': 1, 'expression_list': 1, 'augmented_assignment': 2, '+=': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 24, 'end_line': 30, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.763302047772842,0.7618454914443433,"(tensor([0.9858]), tensor([0.9851]), tensor([0.9854]), tensor([0.9852]))"
"1 from abc import ABC
2 from azureml.core import Experiment, Workspace
3 from typing import Dict, Any
4 
5 from .tracking_base import TrackedExperiment
","1 from typing import Union, Dict, Any
2 from azureml.core import Experiment, Workspace
3 
4 from .. import VectorModel, evaluation
5 from ..evaluation import VectorModelEvaluator, VectorModelCrossValidator
","Before: 1
After: 1",update azure_tracking.py for python 3,Sync faz,https://github.com/opcode81/sensAI,src/sensai/tracking/azure_tracking.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,9,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 8, 'identifier': 9, 'import': 3, '.': 1, ',': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 42, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : MetricsDictProvider )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : MetricsDictProvider'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3353883156001707,0.28313938578766423,"(tensor([0.8790]), tensor([0.9143]), tensor([0.8963]), tensor([0.9107]))"
"1 from abc import ABC
2 from azureml.core import Experiment, Workspace
3 from typing import Dict, Any
4 
5 from .tracking_base import TrackedExperiment
6 from ..evaluation.evaluator import MetricsDictProvider
7 
","1 from typing import Union, Dict, Any
2 from azureml.core import Experiment, Workspace
3 
4 from .. import VectorModel, evaluation
5 from ..evaluation import VectorModelEvaluator, VectorModelCrossValidator
6 from .tracking_base import TrackedExperiment
7 
8 
","Before: 3
After: 4, 5",update azure_tracking.py for python 3,Sync faz,https://github.com/opcode81/sensAI,src/sensai/tracking/azure_tracking.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,31,"{'module': 1, 'import_from_statement': 5, 'from': 5, 'dotted_name': 12, 'identifier': 14, 'import': 5, '.': 5, ',': 2, 'relative_import': 2, 'import_prefix': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 42, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : MetricsDictProvider )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : MetricsDictProvider'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3518093193099913,0.33445379731088526,"(tensor([0.8865]), tensor([0.9036]), tensor([0.8950]), tensor([0.9019]))"
"1 from abc import ABC
2 from azureml.core import Experiment, Workspace
3 from typing import Dict, Any
4 
5 from .tracking_base import TrackedExperiment
6 from ..evaluation.evaluator import MetricsDictProvider
7 
8 
9 class TrackedAzureMLEvaluation(ABC):
10     """"""
","4 from .. import VectorModel, evaluation
5 from ..evaluation import VectorModelEvaluator, VectorModelCrossValidator
6 from .tracking_base import TrackedExperiment
7 
8 
9 class TrackedAzureMLEvaluation:
10     """"""
11     Class to automatically track parameters, metrics and artifacts for a single model with azureml-sdk
12     """"""
13     def __init__(self, experimentName: str, workspace: Workspace,
","Before: 6, 9
After: 9",update azure_tracking.py for python 3,Sync faz,https://github.com/opcode81/sensAI,src/sensai/tracking/azure_tracking.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,54,"{'module': 1, 'import_from_statement': 5, 'from': 5, 'dotted_name': 12, 'identifier': 16, 'import': 5, '.': 5, ',': 2, 'relative_import': 2, 'import_prefix': 2, 'class_definition': 1, 'class': 1, 'argument_list': 1, '(': 1, ')': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 42, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : MetricsDictProvider )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : MetricsDictProvider'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.17578419794738684,0.15949752488407026,"(tensor([0.7951]), tensor([0.8455]), tensor([0.8195]), tensor([0.8402]))"
"10     """"""
11     Class to automatically track parameters, metrics and artifacts for a single model with azureml-sdk
12     """"""
13     def __init__(self, experimentName: str, workspace: Workspace,
14             evaluator: MetricsDictProvider):
15         """"""
16         :param experimentName:
17         :param workspace:
18         :param evaluator:
19         """"""
20         self.experimentName = experimentName
21         self.evaluator = evaluator
22         self.experiment = Experiment(workspace=workspace, name=experimentName)
23 
24     def evalModel(self, model, additionalLoggingValuesDict: dict = None):
","10     """"""
11     Class to automatically track parameters, metrics and artifacts for a single model with azureml-sdk
12     """"""
13     def __init__(self, experimentName: str, workspace: Workspace,
14             evaluator: Union[VectorModelEvaluator, VectorModelCrossValidator]):
15         """"""
16         :param experimentName:
17         :param workspace:
18         :param evaluator:
19         """"""
20         self.experimentName = experimentName
21         self.evaluator = evaluator
22         self.experiment = Experiment(workspace=workspace, name=experimentName)
23 
24     def evalModel(self, model: VectorModel, additionalLoggingValuesDict: dict = None, **startLoggingKwargs):
","Before: 14
After: 14",update azure_tracking.py for python 3,Sync faz,https://github.com/opcode81/sensAI,src/sensai/tracking/azure_tracking.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,95,"{'module': 1, 'expression_statement': 5, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'function_definition': 1, 'def': 1, 'identifier': 21, 'parameters': 1, '(': 2, ',': 4, 'typed_parameter': 3, ':': 4, 'type': 3, ')': 2, 'block': 1, 'assignment': 3, 'attribute': 3, '.': 3, '=': 5, 'call': 1, 'argument_list': 1, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 42, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : MetricsDictProvider )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : MetricsDictProvider'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8508034391311924,0.8402630873915339,"(tensor([0.9472]), tensor([0.9787]), tensor([0.9627]), tensor([0.9754]))"
"21         self.evaluator = evaluator
22         self.experiment = Experiment(workspace=workspace, name=experimentName)
23 
24     def evalModel(self, model, additionalLoggingValuesDict: dict = None):
25         with self.experiment.start_logging() as run:
26             valuesDict = self.evaluator.computeMetrics(model)
27             valuesDict['str(model)'] = str(model)
28             if additionalLoggingValuesDict is not None:
29                 valuesDict.update(additionalLoggingValuesDict)
30             for name, value in valuesDict.items():
31                 run.log(name, value)
32 
33 
","21         self.evaluator = evaluator
22         self.experiment = Experiment(workspace=workspace, name=experimentName)
23 
24     def evalModel(self, model: VectorModel, additionalLoggingValuesDict: dict = None, **startLoggingKwargs):
25         with self.experiment.start_logging(**startLoggingKwargs) as run:
26             valuesDict = evaluation.computeEvaluationMetricsDict(model, self.evaluator)
27             valuesDict['str(model)'] = str(model)
28             if additionalLoggingValuesDict is not None:
29                 valuesDict.update(additionalLoggingValuesDict)
30             for name, value in valuesDict.items():
31                 run.log(name, value)
32 
33 
","Before: 24, 25, 26
After: 24, 25, 26",update azure_tracking.py for python 3,Sync faz,https://github.com/opcode81/sensAI,src/sensai/tracking/azure_tracking.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,156,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'attribute': 9, 'identifier': 39, '.': 9, '=': 7, 'call': 7, 'argument_list': 7, '(': 8, 'keyword_argument': 2, ',': 5, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 1, ':': 5, 'type': 1, 'none': 2, 'block': 4, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, 'subscript': 1, '[': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ']': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'for_statement': 1, 'for': 1, 'pattern_list': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 42, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : MetricsDictProvider )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : MetricsDictProvider'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , experimentName : str , workspace : Workspace , evaluator : Union [ VectorModelEvaluator , VectorModelCrossValidator ] )', 'start_line': 13, 'end_line': 22, 'full_parameters': ['self', ' experimentName : str', ' workspace : Workspace', ' evaluator : Union [ VectorModelEvaluator', ' VectorModelCrossValidator ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/tracking/azure_tracking.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8044516100773526,0.7924010785360094,"(tensor([0.9481]), tensor([0.9844]), tensor([0.9659]), tensor([0.9807]))"
"17         DOUBLE = (""DOUBLE"", False)  # (SQL data type, isCachedValuePickled)
18         BLOB = (""BLOB"", True)
19 
20     def __init__(self, host, db, user, pw, valueType: ValueType, tableName=""cache"", deferredCommitDelaySecs=1.0):
21         self.conn = MySQLdb.connect(host, db, user, pw)
22         self.tableName = tableName
23         self.maxKeyLength = 255
24         self.deferredCommitDelaySecs = deferredCommitDelaySecs
25         self._commitThread = None
26         self._commitThreadSemaphore = threading.Semaphore()
27         self._numEntriesToBeCommitted = 0
28 
29         cacheValueSqlType, self.isCacheValuePickled = valueType.value
30 
31         cursor = self.conn.cursor()
32         cursor.execute(f""SHOW TABLES;"")
33         if tableName not in [r[0] for r in cursor.fetchall()]:
34             cursor.execute(f""CREATE TABLE {tableName} (cache_key VARCHAR({self.maxKeyLength}) PRIMARY KEY, cache_value {cacheValueSqlType});"")
35         cursor.close()
36 
37     def set(self, key, value):
","18         DOUBLE = (""DOUBLE"", False)  # (SQL data type, isCachedValuePickled)
19         BLOB = (""BLOB"", True)
20 
21     def __init__(self, host, db, user, pw, valueType: ValueType, tableName=""cache"", deferredCommitDelaySecs=1.0, inMemory=False):
22 
23         self.conn = MySQLdb.connect(host=host, database=db, user=user, password=pw)
24         self.tableName = tableName
25         self.maxKeyLength = 255
26         self.deferredCommitDelaySecs = deferredCommitDelaySecs
27         self._commitThread = None
28         self._commitThreadSemaphore = threading.Semaphore()
29         self._numEntriesToBeCommitted = 0
30 
31         cacheValueSqlType, self.isCacheValuePickled = valueType.value
32 
33         cursor = self.conn.cursor()
34         cursor.execute(f""SHOW TABLES;"")
35         if tableName not in [r[0] for r in cursor.fetchall()]:
36             cursor.execute(f""CREATE TABLE {tableName} (cache_key VARCHAR({self.maxKeyLength}) PRIMARY KEY, cache_value {cacheValueSqlType});"")
37         cursor.close()
38 
39         self._inMemoryDf = None if not inMemory else self._loadTableToDataFrame()
40 
41     def _loadTableToDataFrame(self):
","Before: 20, 21
After: 21, 22, 23",add inmemory support to mysql persistent key value cache,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_mysql.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,133,"{'module': 1, 'expression_statement': 14, 'assignment': 11, 'identifier': 60, '=': 13, 'tuple': 2, '(': 10, 'string': 5, 'string_start': 5, 'string_content': 8, 'string_end': 5, ',': 13, 'false': 1, ')': 10, 'comment': 1, 'true': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 1, 'default_parameter': 2, 'float': 1, 'block': 2, 'attribute': 18, '.': 18, 'call': 7, 'argument_list': 7, 'integer': 3, 'none': 1, 'pattern_list': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'not in': 2, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'interpolation': 3, '{': 3, '}': 3}","{'cyclomatic_complexity': 3, 'nloc': 14, 'token_count': 133, 'name': '__init__', 'long_name': '__init__( self , host , db , user , pw , valueType : ValueType , tableName = ""cache"" , deferredCommitDelaySecs = 1 . 0 )', 'start_line': 20, 'end_line': 35, 'full_parameters': ['self', ' host', ' db', ' user', ' pw', ' valueType : ValueType', ' tableName = ""cache""', ' deferredCommitDelaySecs = 1 . 0'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_mysql.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 15, 'token_count': 159, 'name': '__init__', 'long_name': '__init__( self , host , db , user , pw , valueType : ValueType , tableName = ""cache"" , deferredCommitDelaySecs = 1 . 0 , inMemory = False )', 'start_line': 21, 'end_line': 39, 'full_parameters': ['self', ' host', ' db', ' user', ' pw', ' valueType : ValueType', ' tableName = ""cache""', ' deferredCommitDelaySecs = 1 . 0', ' inMemory = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_mysql.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6485486655391054,0.6392889164210829,"(tensor([0.9413]), tensor([0.9764]), tensor([0.9586]), tensor([0.9728]))"
"1 """"""
2 This module defines base classes for models that use pandas.DataFrames for inputs and outputs, where each data frame row represents
3 a single model input or output. Since every row contains a vector of data (one-dimensional array), we refer to them as vector-based
4 models. Hence the name of the module and of the central base class VectorModel.
5 """"""
","1 import logging
2 from abc import ABC, abstractmethod
3 from typing import Sequence, List, Any, Optional, Union, TypeVar
4 
5 import numpy as np
6 import pandas as pd
7 import scipy.stats
","Before: 1, 2, 3, 4, 5, 6, 9
After: 3, 7",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,4,"{'module': 1, 'ERROR': 5, 'string_start': 1, 'identifier': 56, 'for': 1, 'attribute': 3, '.': 4, ',': 2, 'call': 1, 'argument_list': 1, '(': 1, 'binary_operator': 1, '-': 2, ')': 1, 'and': 1, 'class': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",4.997619763922748e-232,4.462777348814873e-232,"(tensor([0.6935]), tensor([0.6478]), tensor([0.6699]), tensor([0.6521]))"
"11 import numpy as np
12 import pandas as pd
13 
14 from .data_transformation import DataFrameTransformer, DataFrameTransformerChain, InvertibleDataFrameTransformer
15 from .featuregen import FeatureGenerator, FeatureCollector
16 from .util.cache import PickleLoadSaveMixin
17 from .util.sequences import getFirstDuplicate
18 
19 # imports for backward compatibility (and mark as used)
20 from .data import InputOutputData
","7 import scipy.stats
8 
9 from .data_transformation import DataFrameTransformer, DataFrameTransformerChain, InvertibleDataFrameTransformer
10 from .featuregen import FeatureGenerator, FeatureCollector
11 
12 _log = logging.getLogger(__name__)
13 
14 
15 T = TypeVar('T')
16 
","Before: 16, 17, 19, 20, 21, 22
After: 12",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,95,"{'module': 1, 'import_statement': 2, 'import': 6, 'aliased_import': 2, 'dotted_name': 13, 'identifier': 17, 'as': 2, 'import_from_statement': 4, 'from': 4, 'relative_import': 4, 'import_prefix': 4, '.': 6, ',': 3, 'comment': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2201304348843128,0.19867487693645683,"(tensor([0.8773]), tensor([0.8251]), tensor([0.8504]), tensor([0.8301]))"
"19 # imports for backward compatibility (and mark as used)
20 from .data import InputOutputData
21 if InputOutputData:
22     pass
23 
24 log = logging.getLogger(__name__)
25 
26 
27 class PredictorModel(ABC):
28     """"""
","10 from .featuregen import FeatureGenerator, FeatureCollector
11 
12 _log = logging.getLogger(__name__)
13 
14 
15 T = TypeVar('T')
16 
17 
18 class InputOutputData:
19     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 24
After: 15",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,138,"{'module': 1, 'comment': 1, 'import_from_statement': 1, 'from': 1, 'relative_import': 1, 'import_prefix': 1, '.': 2, 'dotted_name': 2, 'identifier': 9, 'import': 1, 'if_statement': 1, 'if': 1, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.11012419619306525,0.07551902523551578,"(tensor([0.7898]), tensor([0.7960]), tensor([0.7929]), tensor([0.7954]))"
"22     pass
23 
24 log = logging.getLogger(__name__)
25 
26 
27 class PredictorModel(ABC):
28     """"""
29     Base class for models that map data frames to predictions
30     """"""
31 
","13 
14 
15 T = TypeVar('T')
16 
17 
18 class InputOutputData:
19     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
20         if len(inputs) != len(outputs):
21             raise ValueError(""Lengths do not match"")
22         self.inputs = inputs
","Before: 27, 28, 29, 30, 32, 33
After: 18, 19, 20, 21, 22, 23",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,147,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 6, '=': 1, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",3.4681846712773402e-155,0,"(tensor([0.6919]), tensor([0.7011]), tensor([0.6964]), tensor([0.7002]))"
"30     """"""
31 
32     def __init__(self):
33         self._name = None
34 
35     @abstractmethod
36     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
37         pass
38 
39     @abstractmethod
","22         self.inputs = inputs
23         self.outputs = outputs
24 
25     def __len__(self):
26         return len(self.inputs)
27 
28     @property
","Before: 35, 36, 37
After: 25, 26",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,173,"{'module': 1, 'ERROR': 1, 'string_start': 1, 'function_definition': 2, 'def': 2, 'identifier': 12, 'parameters': 2, '(': 2, ')': 2, ':': 3, 'block': 2, 'expression_statement': 1, 'assignment': 1, 'attribute': 3, '.': 3, '=': 1, 'none': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, ',': 1, 'typed_parameter': 1, 'type': 2, '->': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.06464492806072165,0,"(tensor([0.7552]), tensor([0.7154]), tensor([0.7347]), tensor([0.7192]))"
"34 
35     @abstractmethod
36     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
37         pass
38 
39     @abstractmethod
40     def isRegressionModel(self) -> bool:
41         pass
42 
43     @abstractmethod
","23         self.outputs = outputs
24 
25     def __len__(self):
26         return len(self.inputs)
27 
28     @property
29     def inputDim(self):
30         return self.inputs.shape[1]
31 
32     @property
","Before: 39, 40, 41
After: 28, 29, 30",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,203,"{'module': 1, 'decorated_definition': 2, 'decorator': 2, '@': 2, 'identifier': 12, 'function_definition': 2, 'def': 2, 'parameters': 2, '(': 2, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 3, 'attribute': 2, '.': 2, ')': 2, '->': 2, 'block': 2, 'pass_statement': 2, 'pass': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",1.9055199287487325e-78,0,"(tensor([0.7512]), tensor([0.7175]), tensor([0.7340]), tensor([0.7208]))"
"38 
39     @abstractmethod
40     def isRegressionModel(self) -> bool:
41         pass
42 
43     @abstractmethod
44     def getPredictedVariableNames(self) -> list:
45         pass
46 
47     def withName(self, name: str):
","27 
28     @property
29     def inputDim(self):
30         return self.inputs.shape[1]
31 
32     @property
33     def outputDim(self):
34         return self.outputs.shape[1]
35 
36     def filterIndices(self, indices: Sequence[int]) -> 'InputOutputData':
","Before: 43, 44, 45
After: 32, 33, 34",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,221,"{'module': 1, 'decorated_definition': 2, 'decorator': 2, '@': 2, 'identifier': 8, 'function_definition': 2, 'def': 2, 'parameters': 2, '(': 2, ')': 2, '->': 2, 'type': 2, ':': 2, 'block': 2, 'pass_statement': 2, 'pass': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",3.3075331786679814e-78,1.4602354574163774e-78,"(tensor([0.7554]), tensor([0.7596]), tensor([0.7575]), tensor([0.7591]))"
"44     def getPredictedVariableNames(self) -> list:
45         pass
46 
47     def withName(self, name: str):
48         """"""
49         Sets the model's name.
50 
51         :param name: the name
52         :return: self
53         """"""
54         self.setName(name)
55         return self
56 
57     def setName(self, name):
","33     def outputDim(self):
34         return self.outputs.shape[1]
35 
36     def filterIndices(self, indices: Sequence[int]) -> 'InputOutputData':
37         inputs = self.inputs.iloc[indices]
38         outputs = self.outputs.iloc[indices]
39         return InputOutputData(inputs, outputs)
40 
41     def computeInputOutputCorrelation(self):
","Before: 47, 48, 49
After: 36, 37, 38, 39",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,250,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 11, 'parameters': 2, '(': 3, ')': 3, '->': 1, 'type': 2, ':': 3, 'block': 2, 'pass_statement': 1, 'pass': 1, ',': 1, 'typed_parameter': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",2.4815536446235247e-78,1.3613316026225275e-78,"(tensor([0.7042]), tensor([0.7255]), tensor([0.7147]), tensor([0.7233]))"
"44     def getPredictedVariableNames(self) -> list:
45         pass
46 
47     def withName(self, name: str):
48         """"""
49         Sets the model's name.
50 
51         :param name: the name
52         :return: self
53         """"""
54         self.setName(name)
55         return self
56 
57     def setName(self, name):
","38         outputs = self.outputs.iloc[indices]
39         return InputOutputData(inputs, outputs)
40 
41     def computeInputOutputCorrelation(self):
42         correlations = {}
43         for outputCol in self.outputs.columns:
44             correlations[outputCol] = {}
45             outputSeries = self.outputs[outputCol]
46             for inputCol in self.inputs.columns:
47                 inputSeries = self.inputs[inputCol]
48                 pcc, pvalue = scipy.stats.pearsonr(inputSeries, outputSeries)
49                 correlations[outputCol][inputCol] = pcc
50         return correlations
51 
52 
","Before: 51, 52, 53, 54, 55
After: 41, 42, 43, 44, 45, 46, 47, 48, 49, 50",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,270,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 11, 'parameters': 2, '(': 3, ')': 3, '->': 1, 'type': 2, ':': 3, 'block': 2, 'pass_statement': 1, 'pass': 1, ',': 1, 'typed_parameter': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",1.2489473704140778e-78,1.155764882590139e-78,"(tensor([0.6502]), tensor([0.7096]), tensor([0.6786]), tensor([0.7032]))"
"54         self.setName(name)
55         return self
56 
57     def setName(self, name):
58         self._name = name
59 
60     def getName(self):
","48                 pcc, pvalue = scipy.stats.pearsonr(inputSeries, outputSeries)
49                 correlations[outputCol][inputCol] = pcc
50         return correlations
51 
52 
53 class PredictorModel(ABC):
54     """"""
55     Base class for models that map vectors to predictions
56     """"""
57     @abstractmethod
","Before: 57, 58, 59, 60, 61, 62, 63
After: 53, 54, 55, 56, 57, 58, 59",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,279,"{'module': 1, 'expression_statement': 2, 'call': 1, 'attribute': 2, 'identifier': 10, '.': 2, 'argument_list': 1, '(': 2, ')': 2, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 1, 'block': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",3.795772017625871e-155,1.0244914152188952e-231,"(tensor([0.6489]), tensor([0.7026]), tensor([0.6747]), tensor([0.6968]))"
"61         if self._name is None:
62             return ""unnamed-%s-%x"" % (self.__class__.__name__, id(self))
63         return self._name
64 
65 
66 class FittableModel(PredictorModel, ABC):
67     @abstractmethod
68     def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
69         pass
70 
","59         pass
60 
61     @abstractmethod
62     def getPredictedVariableNames(self):
63         pass
64 
65     @abstractmethod
","Before: 66, 68
After: 62",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,349,"{'module': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'attribute': 6, 'identifier': 21, '.': 6, 'is': 1, 'none': 1, ':': 5, 'block': 3, 'return_statement': 2, 'return': 2, 'binary_operator': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '%': 1, 'tuple': 1, '(': 4, ',': 4, 'call': 1, 'argument_list': 2, ')': 4, 'class_definition': 1, 'class': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, 'type': 2, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",4.8377287921049054e-79,4.611048155548615e-79,"(tensor([0.8478]), tensor([0.7110]), tensor([0.7734]), tensor([0.7227]))"
"69         pass
70 
71     @abstractmethod
72     def isFitted(self) -> bool:
73         pass
74 
75 
","63         pass
64 
65     @abstractmethod
66     def isRegressionModel(self) -> bool:
67         pass
68 
69 
","Before: 72
After: 66",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,398,"{'module': 1, 'pass_statement': 2, 'pass': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ')': 1, '->': 1, 'type': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3537053193768173,0.2704644091060903,"(tensor([0.9045]), tensor([0.9136]), tensor([0.9090]), tensor([0.9127]))"
"71     @abstractmethod
72     def isFitted(self) -> bool:
73         pass
74 
75 
76 class VectorModel(FittableModel, PickleLoadSaveMixin, ABC):
77     """"""
78     Base class for models that map data frames to predictions and can be fitted on data frames
79     """"""
80 
","65     @abstractmethod
66     def isRegressionModel(self) -> bool:
67         pass
68 
69 
70 class VectorModel(PredictorModel, ABC):
71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self, checkInputColumns=True):
","Before: 76
After: 70",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,413,"{'module': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, '->': 1, 'type': 1, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'class_definition': 1, 'class': 1, 'argument_list': 1, ',': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.31903742773416593,0.28945263455819636,"(tensor([0.8765]), tensor([0.8558]), tensor([0.8661]), tensor([0.8579]))"
"73         pass
74 
75 
76 class VectorModel(FittableModel, PickleLoadSaveMixin, ABC):
77     """"""
78     Base class for models that map data frames to predictions and can be fitted on data frames
79     """"""
80 
81     def __init__(self, checkInputColumns=True):
82         """"""
","67         pass
68 
69 
70 class VectorModel(PredictorModel, ABC):
71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self, checkInputColumns=True):
75         """"""
76 
","Before: 78
After: 72",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,433,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'class_definition': 1, 'class': 1, 'identifier': 7, 'argument_list': 1, '(': 2, ',': 3, ')': 2, ':': 2, 'block': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, '=': 1, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3421705188153878,0.30251330801800663,"(tensor([0.9195]), tensor([0.8557]), tensor([0.8864]), tensor([0.8617]))"
"75 
76 class VectorModel(FittableModel, PickleLoadSaveMixin, ABC):
77     """"""
78     Base class for models that map data frames to predictions and can be fitted on data frames
79     """"""
80 
81     def __init__(self, checkInputColumns=True):
82         """"""
83         :param checkInputColumns: Whether to check if the input column list (after feature generation)
84             during inference coincides with the input column list during fit.
","71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self, checkInputColumns=True):
75         """"""
76 
77         :param checkInputColumns: Whether to check if the input column list (after feature generation) during inference coincides
78             with the input column list during fit. This should be disabled if feature generation is not performed by the model itself,
79             e.g. in ensemble models.
80         """"""
81         self._featureGenerator: Optional[""FeatureGenerator""] = None
82         self._inputTransformerChain = DataFrameTransformerChain(())
83         self._outputTransformerChain = DataFrameTransformerChain(())
84         self._predictedVariableNames = None
85         self._modelInputVariableNames = None
86         self._modelOutputVariableNames = None
87         self._targetTransformer = None
88         self._name = None
89         self._isFitted = False
90         self.checkInputColumns = checkInputColumns
91 
92     @staticmethod
","Before: 80, 83, 84, 85
After: 76, 77, 78",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,418,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 19, 'argument_list': 2, '(': 3, ',': 3, ')': 3, ':': 4, 'block': 2, 'expression_statement': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, '=': 1, 'true': 1, 'ERROR': 4, 'if': 1, 'call': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.28079518134776105,0.2731738527757919,"(tensor([0.7751]), tensor([0.8568]), tensor([0.8139]), tensor([0.8479]))"
"78     Base class for models that map data frames to predictions and can be fitted on data frames
79     """"""
80 
81     def __init__(self, checkInputColumns=True):
82         """"""
83         :param checkInputColumns: Whether to check if the input column list (after feature generation)
84             during inference coincides with the input column list during fit.
85             This should be disabled if feature generation is not performed by the model itself,
86             e.g. in ensemble models.
87         """"""
88         super().__init__()
89         self._featureGenerator: Optional[FeatureGenerator] = None
90         self._inputTransformerChain = DataFrameTransformerChain()
91         self._isFitted = False  # Note: this keeps track only of the actual model being fitted, not the pre/postprocessors
92         self._predictedVariableNames: Optional[list] = None
93         self._modelInputVariableNames: Optional[list] = None
94         self.checkInputColumns = checkInputColumns
95 
96     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
","71     """"""
72     Base class for models that map vectors to vectors
73     """"""
74     def __init__(self, checkInputColumns=True):
75         """"""
76 
77         :param checkInputColumns: Whether to check if the input column list (after feature generation) during inference coincides
78             with the input column list during fit. This should be disabled if feature generation is not performed by the model itself,
79             e.g. in ensemble models.
80         """"""
81         self._featureGenerator: Optional[""FeatureGenerator""] = None
82         self._inputTransformerChain = DataFrameTransformerChain(())
83         self._outputTransformerChain = DataFrameTransformerChain(())
84         self._predictedVariableNames = None
85         self._modelInputVariableNames = None
86         self._modelOutputVariableNames = None
87         self._targetTransformer = None
88         self._name = None
89         self._isFitted = False
90         self.checkInputColumns = checkInputColumns
91 
92     @staticmethod
","Before: 88, 89, 90, 91, 92, 93
After: 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,450,"{'module': 1, 'ERROR': 5, 'identifier': 55, 'for': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 2, 'call': 1, 'argument_list': 1, '(': 1, ')': 1, 'with': 1, 'attribute': 3, '.': 4, 'if': 1, 'not': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.38072909448291026,0.3838099046590163,"(tensor([0.9379]), tensor([0.8833]), tensor([0.9098]), tensor([0.8885]))"
"93         self._modelInputVariableNames: Optional[list] = None
94         self.checkInputColumns = checkInputColumns
95 
96     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
97         """"""
98         Makes the model use the given input transformers. Call with empty input to remove existing input transformers.
99 
100         :param inputTransformers: DataFrameTransformers for the transformation of inputs
101         :return: self
102         """"""
103         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
104         return self
105 
106     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","99                 result.append(x)
100         return result
101 
102     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
103         """"""
104         Makes the model use the given input transformers.
105 
106         :param inputTransformers: DataFrameTransformers for the transformation of inputs
107         :return: self
108         """"""
109         self._inputTransformerChain = DataFrameTransformerChain(self._flattened(inputTransformers))
110         return self
111 
112     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
","Before: 98
After: 104",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,588,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'attribute': 3, 'identifier': 20, '.': 3, ':': 3, 'type': 7, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, '=': 3, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, '*': 2, ')': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 1, 'argument_list': 1, 'list_splat': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4865427031516376,0.4798174094285222,"(tensor([0.9196]), tensor([0.8905]), tensor([0.9048]), tensor([0.8933]))"
"93         self._modelInputVariableNames: Optional[list] = None
94         self.checkInputColumns = checkInputColumns
95 
96     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
97         """"""
98         Makes the model use the given input transformers. Call with empty input to remove existing input transformers.
99 
100         :param inputTransformers: DataFrameTransformers for the transformation of inputs
101         :return: self
102         """"""
103         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
104         return self
105 
106     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","99                 result.append(x)
100         return result
101 
102     def withInputTransformers(self, *inputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
103         """"""
104         Makes the model use the given input transformers.
105 
106         :param inputTransformers: DataFrameTransformers for the transformation of inputs
107         :return: self
108         """"""
109         self._inputTransformerChain = DataFrameTransformerChain(self._flattened(inputTransformers))
110         return self
111 
112     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
","Before: 103
After: 109",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,585,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'attribute': 3, 'identifier': 20, '.': 3, ':': 3, 'type': 7, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, '=': 3, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, '*': 2, ')': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 1, 'argument_list': 1, 'list_splat': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4865427031516376,0.4798174094285222,"(tensor([0.9196]), tensor([0.8905]), tensor([0.9048]), tensor([0.8933]))"
"103         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
104         return self
105 
106     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
107         """"""
108         Makes the model use the given feature generator, which shall be used to compute
109         the actual inputs of the model from the data frame that is given.
110         Cannot be used in conjunction with withFeatureCollector
111 
112         Note: Feature computation takes place before input transformation.
113 
114         :param featureGenerator: the feature generator to use for input computation
115         :return: self
116         """"""
117         self._featureGenerator = featureGenerator
118         return self
119 
120     def withFeatureCollector(self, featureCollector: FeatureCollector) -> __qualname__:
","109         self._inputTransformerChain = DataFrameTransformerChain(self._flattened(inputTransformers))
110         return self
111 
112     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
113         """"""
114         Makes the model use the given output transformers.
115 
116         :param outputTransformers: DataFrameTransformers for the transformation of outputs (after the model has been applied)
117         :return: self
118         """"""
119         self._outputTransformerChain = DataFrameTransformerChain(self._flattened(outputTransformers))
120         return self
121 
122     def withTargetTransformer(self, targetTransformer: InvertibleDataFrameTransformer) -> __qualname__:
","Before: 106
After: 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,612,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 2, 'identifier': 15, '.': 2, '=': 2, 'call': 1, 'argument_list': 1, '(': 2, 'list_splat': 1, '*': 1, ')': 2, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.20072236976199942,0.192469309106854,"(tensor([0.8517]), tensor([0.8211]), tensor([0.8361]), tensor([0.8241]))"
"103         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
104         return self
105 
106     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
107         """"""
108         Makes the model use the given feature generator, which shall be used to compute
109         the actual inputs of the model from the data frame that is given.
110         Cannot be used in conjunction with withFeatureCollector
111 
112         Note: Feature computation takes place before input transformation.
113 
114         :param featureGenerator: the feature generator to use for input computation
115         :return: self
116         """"""
117         self._featureGenerator = featureGenerator
118         return self
119 
120     def withFeatureCollector(self, featureCollector: FeatureCollector) -> __qualname__:
","119         self._outputTransformerChain = DataFrameTransformerChain(self._flattened(outputTransformers))
120         return self
121 
122     def withTargetTransformer(self, targetTransformer: InvertibleDataFrameTransformer) -> __qualname__:
123         """"""
124         Makes the model use the given target transformers.
125 
126         :param targetTransformer: a transformer which transforms the targets (training data outputs) prior to learning the model, such
127             that the model learns to predict the transformed outputs. When predicting, the inverse transformer is applied after applying
128             the model, i.e. the transformation is completely transparent when applying the model.
129         :return: self
130         """"""
131         self._targetTransformer = targetTransformer
132         return self
133 
134     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
","Before: 108, 109, 110
After: 131, 132",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,631,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 2, 'identifier': 15, '.': 2, '=': 2, 'call': 1, 'argument_list': 1, '(': 2, 'list_splat': 1, '*': 1, ')': 2, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2547918719793683,0.23653326289404755,"(tensor([0.8410]), tensor([0.8465]), tensor([0.8437]), tensor([0.8459]))"
"103         self._inputTransformerChain = DataFrameTransformerChain(*inputTransformers)
104         return self
105 
106     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
107         """"""
108         Makes the model use the given feature generator, which shall be used to compute
109         the actual inputs of the model from the data frame that is given.
110         Cannot be used in conjunction with withFeatureCollector
111 
112         Note: Feature computation takes place before input transformation.
113 
114         :param featureGenerator: the feature generator to use for input computation
115         :return: self
116         """"""
117         self._featureGenerator = featureGenerator
118         return self
119 
120     def withFeatureCollector(self, featureCollector: FeatureCollector) -> __qualname__:
","131         self._targetTransformer = targetTransformer
132         return self
133 
134     def withFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]) -> __qualname__:
135         """"""
136         Makes the model use the given feature generator, which shall be used to compute the actual inputs of the model from the data
137         frame that is given. Feature computation takes place before input transformation.
138 
139         :param featureGenerator: the feature generator to use for input computation
140         :return: self
141         """"""
142         self._featureGenerator = featureGenerator
143         return self
144 
145     def withFeatureCollector(self, featureCollector: FeatureCollector) -> __qualname__:
","Before: 112
After: 134, 135, 136, 137",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,665,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 2, 'identifier': 15, '.': 2, '=': 2, 'call': 1, 'argument_list': 1, '(': 2, 'list_splat': 1, '*': 1, ')': 2, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5780976073695828,0.5646699246992233,"(tensor([0.9425]), tensor([0.9124]), tensor([0.9272]), tensor([0.9153]))"
"117         self._featureGenerator = featureGenerator
118         return self
119 
120     def withFeatureCollector(self, featureCollector: FeatureCollector) -> __qualname__:
121         """"""
122         Makes the model use the given feature collector's multi-feature generator
123         in order compute the actual inputs of the model from the data frame that is given.
124         Cannot be used in conjunction with withFeatureGenerator.
125 
126         Note: Feature computation takes place before input transformation.
127 
128         :param featureCollector: the feature collector whose feature generator shall be used for input computation
129         :return: self
130         """"""
131         self._featureGenerator = featureCollector.getMultiFeatureGenerator()
132         return self
133 
134     def _preProcessorsAreFitted(self):
","142         self._featureGenerator = featureGenerator
143         return self
144 
145     def withFeatureCollector(self, featureCollector: FeatureCollector) -> __qualname__:
146         """"""
147         Makes the model use the given feature collector's multi-feature generator in order compute the actual inputs of the model from
148         the data frame that is given.
149         Feature computation takes place before input transformation.
150 
151         :param featureCollector: the feature collector whose feature generator shall be used for input computation
152         :return: self
153         """"""
154         self._featureGenerator = featureCollector.getMultiFeatureGenerator()
155         return self
156 
157     def withName(self, name: str):
","Before: 122, 123, 124, 125, 126
After: 147, 148, 149",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,663,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 3, 'identifier': 14, '.': 3, '=': 2, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 2, ')': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5643600152491653,0.5476405781599969,"(tensor([0.9544]), tensor([0.9285]), tensor([0.9413]), tensor([0.9310]))"
"131         self._featureGenerator = featureCollector.getMultiFeatureGenerator()
132         return self
133 
134     def _preProcessorsAreFitted(self):
135         result = self._inputTransformerChain.isFitted()
136         if self.getFeatureGenerator() is not None:
137             result = result and self.getFeatureGenerator().isFitted()
138         return result
139 
140     def isFitted(self):
","154         self._featureGenerator = featureCollector.getMultiFeatureGenerator()
155         return self
156 
157     def withName(self, name: str):
158         """"""
159         Sets the model's name.
160 
161         :param name: the name
162         :return: self
163         """"""
164         self.setName(name)
165         return self
166 
167     @abstractmethod
","Before: 134, 135, 136, 137, 138
After: 157, 158, 159",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,678,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'attribute': 7, 'identifier': 19, '.': 7, '=': 3, 'call': 5, 'argument_list': 5, '(': 6, ')': 6, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'boolean_operator': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.08814347660995407,0.06394188573619068,"(tensor([0.7931]), tensor([0.7842]), tensor([0.7886]), tensor([0.7851]))"
"137             result = result and self.getFeatureGenerator().isFitted()
138         return result
139 
140     def isFitted(self):
141         underlyingModelIsFitted = not self._underlyingModelRequiresFitting() or self._isFitted
142         if not underlyingModelIsFitted:
143             return False
144         if not self._preProcessorsAreFitted():
145             return False
146         return True
147 
148     def _checkModelInputColumns(self, modelInput: pd.DataFrame):
","154         self._featureGenerator = featureCollector.getMultiFeatureGenerator()
155         return self
156 
157     def withName(self, name: str):
158         """"""
159         Sets the model's name.
160 
161         :param name: the name
162         :return: self
163         """"""
164         self.setName(name)
165         return self
166 
167     @abstractmethod
","Before: 140, 141, 142, 143, 144, 145, 146
After: 161, 162, 163, 164, 165",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,743,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 16, '=': 2, 'boolean_operator': 2, 'and': 1, 'call': 4, 'attribute': 5, '.': 5, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 4, 'return': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'not_operator': 3, 'not': 3, 'or': 1, 'if_statement': 2, 'if': 2, 'false': 2, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",1.5925661723500243e-78,9.926969537205993e-79,"(tensor([0.7683]), tensor([0.7129]), tensor([0.7395]), tensor([0.7181]))"
"145             return False
146         return True
147 
148     def _checkModelInputColumns(self, modelInput: pd.DataFrame):
149         if self.checkInputColumns and list(modelInput.columns) != self._modelInputVariableNames:
150             raise Exception(f""Inadmissible input data frame: ""
151                             f""expected columns {self._modelInputVariableNames}, got {list(modelInput.columns)}"")
152 
153     def computeModelInputs(self, X):
","162         :return: self
163         """"""
164         self.setName(name)
165         return self
166 
167     @abstractmethod
168     def isRegressionModel(self) -> bool:
169         pass
170 
171     def isFitted(self):
","Before: 148, 149, 150, 151
After: 167, 168, 169",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,811,"{'module': 1, 'return_statement': 2, 'return': 2, 'false': 1, 'true': 1, 'function_definition': 1, 'def': 1, 'identifier': 18, 'parameters': 1, '(': 4, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 1, 'attribute': 6, '.': 6, ')': 4, 'block': 2, 'if_statement': 1, 'if': 1, 'boolean_operator': 1, 'and': 1, 'comparison_operator': 1, 'call': 3, 'argument_list': 3, '!=': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'interpolation': 2, '{': 2, '}': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",3.1750170789542522e-155,4.195363524664677e-232,"(tensor([0.7359]), tensor([0.6769]), tensor([0.7051]), tensor([0.6824]))"
"150             raise Exception(f""Inadmissible input data frame: ""
151                             f""expected columns {self._modelInputVariableNames}, got {list(modelInput.columns)}"")
152 
153     def computeModelInputs(self, X):
154         """"""
155         Returns the dataframe that is passed to the model, i.e. the result of applying preprocessors to X.
156         """"""
157         return self._computeModelInputs(X)
158 
159     def _computeModelInputs(self, X: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","168     def isRegressionModel(self) -> bool:
169         pass
170 
171     def isFitted(self):
172         return self._isFitted
173 
174     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
","Before: 153, 154, 155, 156, 157
After: 171, 172",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,883,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 3, 'identifier': 12, 'argument_list': 3, '(': 4, 'concatenated_string': 1, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'interpolation': 2, '{': 2, 'attribute': 3, '.': 3, '}': 2, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.04126200669231376,0.02910692704759613,"(tensor([0.8105]), tensor([0.7527]), tensor([0.7805]), tensor([0.7581]))"
"156         """"""
157         return self._computeModelInputs(X)
158 
159     def _computeModelInputs(self, X: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
160         """"""
161         :param X:
162         :param Y: Only has to be provided if fit is True and preprocessors require Y for fitting
163         :param fit: if True, preprocessors will be fitted before being applied to X
164         :return:
165         """"""
166         if fit:
167             if self._featureGenerator is not None:
168                 X = self._featureGenerator.fitGenerate(X, Y, self)
169             X = self._inputTransformerChain.fitApply(X)
170         else:
171             if self._featureGenerator is not None:
172                 X = self._featureGenerator.generate(X, self)
173             X = self._inputTransformerChain.apply(X)
174         return X
175 
176     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","171     def isFitted(self):
172         return self._isFitted
173 
174     def _computeInputs(self, x: pd.DataFrame, y=None) -> pd.DataFrame:
175         fit = y is not None
176         if self._featureGenerator is not None:
177             if fit:
178                 x = self._featureGenerator.fitGenerate(x, y, self)
179             else:
180                 x = self._featureGenerator.generate(x, self)
181         x = self._inputTransformerChain.apply(x, fit=fit)
182         if not fit:
183             if not self.isFitted():
184                 raise Exception(f""Model has not been fitted"")
185             if self.checkInputColumns and list(x.columns) != self._modelInputVariableNames:
186                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(x.columns)}"")
187         return x
188 
189     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174
After: 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,939,"{'module': 1, 'expression_statement': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 5, ':': 8, 'identifier': 25, 'type': 1, 'if': 2, 'boolean_operator': 1, 'comparison_operator': 1, 'is': 1, 'true': 2, 'and': 1, 'for': 1, ',': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2213211522499449,0.20431236446164266,"(tensor([0.8300]), tensor([0.8497]), tensor([0.8397]), tensor([0.8477]))"
"173             X = self._inputTransformerChain.apply(X)
174         return X
175 
176     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
177         """"""
178         Performs a prediction for the given input data frame
179 
180         :param x: the input data
181         :return: a DataFrame with the same index as the input
182         """"""
183         if not self.isFitted():
184             raise Exception(f""Calling predict with unfitted model. ""
185                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
186         x = self._computeModelInputs(x)
187         self._checkModelInputColumns(x)
188         y = self._predict(x)
189         y.index = x.index
190         return y
191 
192     @abstractmethod
","186                 raise Exception(f""Inadmissible input data frame: expected columns {self._modelInputVariableNames}, got {list(x.columns)}"")
187         return x
188 
189     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
190         """"""
191         Performs a prediction for the given input data frame
192 
193         :param x: the input data
194         :return: a DataFrame with the same index as the input
195         """"""
196         x = self._computeInputs(x)
197         y = self._predict(x)
198         y.index = x.index
199         y = self._outputTransformerChain.apply(y)
200         if self._targetTransformer is not None:
201             y = self._targetTransformer.applyInverse(y)
202         return y
203 
204     @abstractmethod
","Before: 183, 184, 185, 186, 187
After: 196, 199, 200, 201",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1093,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 32, '=': 4, 'call': 6, 'attribute': 10, '.': 10, 'argument_list': 6, '(': 7, ')': 7, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4028499023343192,0.38347877192702484,"(tensor([0.8731]), tensor([0.8632]), tensor([0.8681]), tensor([0.8642]))"
"193     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
194         pass
195 
196     def _underlyingModelRequiresFitting(self) -> bool:
197         """"""
198         Designed to be overridden for rule-based models.
199 
200         :return: True iff the underlying model requires fitting
201         """"""
202         return True
203 
204     def _fitPreprocessors(self, X: pd.DataFrame, Y: pd.DataFrame = None):
","205     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
206         pass
207 
208     def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
209         """"""
210         Fits the model using the given data
211 
212         :param X: a data frame containing input data
213         :param Y: a data frame containing output data
214         """"""
215         _log.info(f""Training {self.__class__.__name__}"")
216         self._predictedVariableNames = list(Y.columns)
217         X = self._computeInputs(X, y=Y)
218         if self._targetTransformer is not None:
219             self._targetTransformer.fit(Y)
220             Y = self._targetTransformer.apply(Y)
221         self._modelInputVariableNames = list(X.columns)
222         self._modelOutputVariableNames = list(Y.columns)
223         _log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]={self._modelInputVariableNames}"")
224         self._fit(X, Y)
225         self._isFitted = True
226 
227     @abstractmethod
","Before: 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213
After: 208",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1201,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 10, 'parameters': 2, '(': 2, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 3, 'attribute': 2, '.': 2, ')': 2, '->': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.13925430501928762,0.11847557324482892,"(tensor([0.7202]), tensor([0.8066]), tensor([0.7610]), tensor([0.7971]))"
"210                 X = self._featureGenerator.fitGenerate(X, Y, self)
211         self._inputTransformerChain.fit(X)
212 
213     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
214         """"""
215         Fits the model using the given data
216 
217         :param X: a data frame containing input data
218         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
219             fitting, e.g. with rule-based models
220         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
221             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
222             an exception will be raised.
223         """"""
224         log.info(f""Training {self.__class__.__name__}"")
225         self._predictedVariableNames = list(Y.columns)
226         if not self._underlyingModelRequiresFitting():
227             self._fitPreprocessors(X, Y=Y)
228         else:
229             if Y is None:
230                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
231             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
232             self._modelInputVariableNames = list(X.columns)
233             log.info(
234                 f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
235             self._fit(X, Y)
236             self._isFitted = True
237 
238     @abstractmethod
","205     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
206         pass
207 
208     def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
209         """"""
210         Fits the model using the given data
211 
212         :param X: a data frame containing input data
213         :param Y: a data frame containing output data
214         """"""
215         _log.info(f""Training {self.__class__.__name__}"")
216         self._predictedVariableNames = list(Y.columns)
217         X = self._computeInputs(X, y=Y)
218         if self._targetTransformer is not None:
219             self._targetTransformer.fit(Y)
220             Y = self._targetTransformer.apply(Y)
221         self._modelInputVariableNames = list(X.columns)
222         self._modelOutputVariableNames = list(Y.columns)
223         _log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]={self._modelInputVariableNames}"")
224         self._fit(X, Y)
225         self._isFitted = True
226 
227     @abstractmethod
","Before: 218, 219, 220, 221, 222
After: 213",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1405,"{'module': 1, 'expression_statement': 11, 'assignment': 5, 'identifier': 79, '=': 9, 'call': 15, 'attribute': 26, '.': 26, 'argument_list': 15, '(': 16, ',': 9, ')': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 1, 'true': 2, 'block': 4, 'string': 6, 'string_start': 6, 'string_content': 10, 'string_end': 6, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 2, 'if': 2, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 1, 'else': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'raise_statement': 1, 'raise': 1, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.26785537837229767,0.2609194747771008,"(tensor([0.8982]), tensor([0.8108]), tensor([0.8523]), tensor([0.8188]))"
"210                 X = self._featureGenerator.fitGenerate(X, Y, self)
211         self._inputTransformerChain.fit(X)
212 
213     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
214         """"""
215         Fits the model using the given data
216 
217         :param X: a data frame containing input data
218         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
219             fitting, e.g. with rule-based models
220         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
221             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
222             an exception will be raised.
223         """"""
224         log.info(f""Training {self.__class__.__name__}"")
225         self._predictedVariableNames = list(Y.columns)
226         if not self._underlyingModelRequiresFitting():
227             self._fitPreprocessors(X, Y=Y)
228         else:
229             if Y is None:
230                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
231             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
232             self._modelInputVariableNames = list(X.columns)
233             log.info(
234                 f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
235             self._fit(X, Y)
236             self._isFitted = True
237 
238     @abstractmethod
","205     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
206         pass
207 
208     def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
209         """"""
210         Fits the model using the given data
211 
212         :param X: a data frame containing input data
213         :param Y: a data frame containing output data
214         """"""
215         _log.info(f""Training {self.__class__.__name__}"")
216         self._predictedVariableNames = list(Y.columns)
217         X = self._computeInputs(X, y=Y)
218         if self._targetTransformer is not None:
219             self._targetTransformer.fit(Y)
220             Y = self._targetTransformer.apply(Y)
221         self._modelInputVariableNames = list(X.columns)
222         self._modelOutputVariableNames = list(Y.columns)
223         _log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]={self._modelInputVariableNames}"")
224         self._fit(X, Y)
225         self._isFitted = True
226 
227     @abstractmethod
","Before: 224
After: 215",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1385,"{'module': 1, 'expression_statement': 11, 'assignment': 5, 'identifier': 79, '=': 9, 'call': 15, 'attribute': 26, '.': 26, 'argument_list': 15, '(': 16, ',': 9, ')': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 1, 'true': 2, 'block': 4, 'string': 6, 'string_start': 6, 'string_content': 10, 'string_end': 6, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 2, 'if': 2, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 1, 'else': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'raise_statement': 1, 'raise': 1, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.26785537837229767,0.2609194747771008,"(tensor([0.8982]), tensor([0.8108]), tensor([0.8523]), tensor([0.8188]))"
"210                 X = self._featureGenerator.fitGenerate(X, Y, self)
211         self._inputTransformerChain.fit(X)
212 
213     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
214         """"""
215         Fits the model using the given data
216 
217         :param X: a data frame containing input data
218         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
219             fitting, e.g. with rule-based models
220         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
221             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
222             an exception will be raised.
223         """"""
224         log.info(f""Training {self.__class__.__name__}"")
225         self._predictedVariableNames = list(Y.columns)
226         if not self._underlyingModelRequiresFitting():
227             self._fitPreprocessors(X, Y=Y)
228         else:
229             if Y is None:
230                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
231             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
232             self._modelInputVariableNames = list(X.columns)
233             log.info(
234                 f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
235             self._fit(X, Y)
236             self._isFitted = True
237 
238     @abstractmethod
","205     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
206         pass
207 
208     def fit(self, X: pd.DataFrame, Y: pd.DataFrame):
209         """"""
210         Fits the model using the given data
211 
212         :param X: a data frame containing input data
213         :param Y: a data frame containing output data
214         """"""
215         _log.info(f""Training {self.__class__.__name__}"")
216         self._predictedVariableNames = list(Y.columns)
217         X = self._computeInputs(X, y=Y)
218         if self._targetTransformer is not None:
219             self._targetTransformer.fit(Y)
220             Y = self._targetTransformer.apply(Y)
221         self._modelInputVariableNames = list(X.columns)
222         self._modelOutputVariableNames = list(Y.columns)
223         _log.info(f""Training with outputs[{len(self._modelOutputVariableNames)}]={self._modelOutputVariableNames}, inputs[{len(self._modelInputVariableNames)}]={self._modelInputVariableNames}"")
224         self._fit(X, Y)
225         self._isFitted = True
226 
227     @abstractmethod
","Before: 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236
After: 217, 218, 219, 220, 221, 222, 223, 224, 225",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1415,"{'module': 1, 'expression_statement': 11, 'assignment': 5, 'identifier': 79, '=': 9, 'call': 15, 'attribute': 26, '.': 26, 'argument_list': 15, '(': 16, ',': 9, ')': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 1, 'true': 2, 'block': 4, 'string': 6, 'string_start': 6, 'string_content': 10, 'string_end': 6, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 2, 'if': 2, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 1, 'else': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'raise_statement': 1, 'raise': 1, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.26785537837229767,0.2609194747771008,"(tensor([0.8982]), tensor([0.8108]), tensor([0.8523]), tensor([0.8188]))"
"242     def getPredictedVariableNames(self):
243         return self._predictedVariableNames
244 
245     def getInputTransformer(self, cls: Type[DataFrameTransformer]):
246         for it in self._inputTransformerChain.dataFrameTransformers:
247             if isinstance(it, cls):
248                 return it
249         return None
250 
251     def getInputTransformerChain(self):
","231     def getPredictedVariableNames(self):
232         return self._predictedVariableNames
233 
234     def getModelOutputVariableNames(self):
235         """"""
236         Gets the list of variable names predicted by the underlying model.
237         For the case where the final output is transformed by an output transformer which changes column names,
238         the names of the variables prior to the transformation will be returned, i.e. this method
239         always returns the variable names that are actually predicted by the model.
240         For the variable names that are ultimately output by the model (including output transformations),
241         use getPredictedVariabaleNames.
242         """"""
243         return self._modelOutputVariableNames
244 
245     def getInputTransformer(self, cls):
","Before: 245
After: 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1679,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 3, ')': 3, ':': 5, 'block': 4, 'return_statement': 3, 'return': 3, 'attribute': 3, '.': 3, ',': 2, 'typed_parameter': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'if_statement': 1, 'if': 1, 'call': 1, 'argument_list': 1, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.13494933492828592,0.09749745027612539,"(tensor([0.7414]), tensor([0.8105]), tensor([0.7744]), tensor([0.8030]))"
"248                 return it
249         return None
250 
251     def getInputTransformerChain(self):
252         return self._inputTransformerChain
253 
254     def setFeatureGenerator(self, featureGenerator: Optional[FeatureGenerator]):
","248                 return it
249         return None
250 
251     def setName(self, name):
252         self._name = name
253 
254     def getName(self):
","Before: 251, 252
After: 251, 252, 253, 254, 255, 256, 257",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1719,"{'module': 1, 'return_statement': 3, 'return': 3, 'identifier': 5, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ')': 1, ':': 1, 'block': 1, 'attribute': 1, '.': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4358999961544052,0.39182647795591663,"(tensor([0.9110]), tensor([0.8321]), tensor([0.8698]), tensor([0.8393]))"
"259 
260 
261 class VectorRegressionModel(VectorModel, ABC):
262     def __init__(self, checkInputColumns=True):
263         """"""
264         :param checkInputColumns: Whether to check if the input column list (after feature generation)
265             during inference coincides with the input column list during fit.
266             This should be disabled if feature generation is not performed by the model itself,
267             e.g. in ensemble models.
268         """"""
269         super().__init__(checkInputColumns=checkInputColumns)
270         self._outputTransformerChain = DataFrameTransformerChain()
271         self._modelOutputVariableNames: Optional[list] = None
272         self._targetTransformer: Optional[InvertibleDataFrameTransformer] = None
273 
274     def isRegressionModel(self) -> bool:
","264 
265 
266 class VectorRegressionModel(VectorModel, ABC):
267     def __init__(self):
268         super().__init__()
269 
270     def isRegressionModel(self) -> bool:
","Before: 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272
After: 267, 268",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1805,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 21, 'argument_list': 4, '(': 5, ',': 2, ')': 5, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'default_parameter': 1, '=': 5, 'true': 1, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 3, 'attribute': 4, '.': 4, 'keyword_argument': 1, 'assignment': 3, 'type': 4, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'none': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.05435994226028343,0.05302269698451791,"(tensor([0.8634]), tensor([0.6655]), tensor([0.7516]), tensor([0.6811]))"
"274     def isRegressionModel(self) -> bool:
275         return True
276 
277     def withOutputTransformers(self, *outputTransformers: Union[DataFrameTransformer, List[DataFrameTransformer]]) -> __qualname__:
278         """"""
279         Makes the model use the given output transformers. Call with empty input to remove existing output transformers.
280         The transformers are ignored during the fit phase. Not supported for rule-based models.
281 
282         **Important**: The output columns names of the last output transformer should be the same
283         as the first one's input column names. If this fails to hold, an exception will be raised when .predict() is called
284         (fit will run through without problems, though).
285 
286         **Note**: Output transformers perform post-processing after the actual predictions have been made. Contrary
287         to invertible target transformers, they are not invoked during the fit phase. Therefore, any losses computed there,
288         including the losses on validation sets (e.g. for early stopping), will be computed on the non-post-processed data.
289         A possible use case for such post-processing is if you know how improve the predictions of your fittable model
290         by some heuristics or by hand-crafted rules.
291 
292         **How not to use**: Output transformers are not meant to transform the predictions into something with a
293         different semantic meaning (e.g. normalized into non-normalized or something like that) - you should consider
294         using a targetTransformer for this purpose. Instead, they give the possibility to improve predictions through
295         post processing, when this is desired.
296 
297         :param outputTransformers: DataFrameTransformers for the transformation of outputs
298             (after the model has been applied)
299         :return: self
300         """"""
301         # There is no reason for post processing in rule-based models
302         if not self._underlyingModelRequiresFitting():
303             raise Exception(f""Output transformers are not supported for model of type {self.__class__.__name__}"")
304         self._outputTransformerChain = DataFrameTransformerChain(*outputTransformers)
305         return self
306 
307     def withTargetTransformer(self, targetTransformer: Optional[InvertibleDataFrameTransformer]) -> __qualname__:
","270     def isRegressionModel(self) -> bool:
271         return True
272 
273 
274 class VectorClassificationModel(VectorModel, ABC):
275 
276     def __init__(self):
277         self._labels = None
278         super().__init__()
279 
","Before: 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 390, 391, 392, 393, 394, 395, 396, 397
After: 275, 276, 278",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,1923,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 22, 'parameters': 2, '(': 5, ')': 5, '->': 2, 'type': 6, ':': 4, 'block': 3, 'return_statement': 2, 'return': 2, 'true': 1, ',': 2, 'typed_parameter': 1, 'list_splat_pattern': 1, '*': 2, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'expression_statement': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'comment': 1, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'call': 3, 'attribute': 4, '.': 4, 'argument_list': 3, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1, 'assignment': 1, '=': 1, 'list_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",8.377395389787555e-05,7.822851136722417e-05,"(tensor([0.7948]), tensor([0.5620]), tensor([0.6584]), tensor([0.5790]))"
"413     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
414         pass
415 
416     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
417         """"""
418         Converts from a data frame as returned by predictClassProbabilities to a result as return by predict.
419 
420         :param df: the output data frame from predictClassProbabilities
421         :return: an output data frame as it would be returned by predict
422         """"""
423         labels = self.getClassLabels()
424         dfCols = list(df.columns)
425         if sorted(dfCols) != labels:
426             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
427         yArray = df.values
428         maxIndices = np.argmax(yArray, axis=1)
429         result = [dfCols[i] for i in maxIndices]
430         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
431 
432     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","293     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
294         pass
295 
296     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
297         """"""
298         Converts from a result returned by predictClassProbabilities to a result as return by predict
299 
300         :param df: the output data frame from predictClassProbabilities
301         :return: an output data frame as it would be returned by predict
302         """"""
303         dfCols = list(df.columns)
304         if dfCols != self._labels:
305             raise ValueError(f""Expected data frame with columns {self._labels}, got {dfCols}"")
306         yArray = df.values
307         maxIndices = np.argmax(yArray, axis=1)
308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 418
After: 298",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2771,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 45, 'parameters': 2, '(': 9, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 9, '.': 9, ')': 9, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'assignment': 5, '=': 7, 'call': 7, 'argument_list': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6112412811539902,0.5951674086639608,"(tensor([0.9568]), tensor([0.9524]), tensor([0.9546]), tensor([0.9528]))"
"413     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
414         pass
415 
416     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
417         """"""
418         Converts from a data frame as returned by predictClassProbabilities to a result as return by predict.
419 
420         :param df: the output data frame from predictClassProbabilities
421         :return: an output data frame as it would be returned by predict
422         """"""
423         labels = self.getClassLabels()
424         dfCols = list(df.columns)
425         if sorted(dfCols) != labels:
426             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
427         yArray = df.values
428         maxIndices = np.argmax(yArray, axis=1)
429         result = [dfCols[i] for i in maxIndices]
430         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
431 
432     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","293     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
294         pass
295 
296     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
297         """"""
298         Converts from a result returned by predictClassProbabilities to a result as return by predict
299 
300         :param df: the output data frame from predictClassProbabilities
301         :return: an output data frame as it would be returned by predict
302         """"""
303         dfCols = list(df.columns)
304         if dfCols != self._labels:
305             raise ValueError(f""Expected data frame with columns {self._labels}, got {dfCols}"")
306         yArray = df.values
307         maxIndices = np.argmax(yArray, axis=1)
308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 423, 425, 426
After: 304, 305",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2765,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 45, 'parameters': 2, '(': 9, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 9, '.': 9, ')': 9, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'assignment': 5, '=': 7, 'call': 7, 'argument_list': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6112412811539902,0.5951674086639608,"(tensor([0.9568]), tensor([0.9524]), tensor([0.9546]), tensor([0.9528]))"
"413     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
414         pass
415 
416     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
417         """"""
418         Converts from a data frame as returned by predictClassProbabilities to a result as return by predict.
419 
420         :param df: the output data frame from predictClassProbabilities
421         :return: an output data frame as it would be returned by predict
422         """"""
423         labels = self.getClassLabels()
424         dfCols = list(df.columns)
425         if sorted(dfCols) != labels:
426             raise ValueError(f""Expected data frame with columns {labels}, got {dfCols}"")
427         yArray = df.values
428         maxIndices = np.argmax(yArray, axis=1)
429         result = [dfCols[i] for i in maxIndices]
430         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
431 
432     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","293     def _fitClassifier(self, X: pd.DataFrame, y: pd.DataFrame):
294         pass
295 
296     def convertClassProbabilitiesToPredictions(self, df: pd.DataFrame):
297         """"""
298         Converts from a result returned by predictClassProbabilities to a result as return by predict
299 
300         :param df: the output data frame from predictClassProbabilities
301         :return: an output data frame as it would be returned by predict
302         """"""
303         dfCols = list(df.columns)
304         if dfCols != self._labels:
305             raise ValueError(f""Expected data frame with columns {self._labels}, got {dfCols}"")
306         yArray = df.values
307         maxIndices = np.argmax(yArray, axis=1)
308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 429, 430
After: 308, 309",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2854,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 45, 'parameters': 2, '(': 9, ',': 5, 'typed_parameter': 3, ':': 6, 'type': 3, 'attribute': 9, '.': 9, ')': 9, 'block': 3, 'pass_statement': 1, 'pass': 1, 'expression_statement': 6, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'assignment': 5, '=': 7, 'call': 7, 'argument_list': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'keyword_argument': 2, 'integer': 1, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6112412811539902,0.5951674086639608,"(tensor([0.9568]), tensor([0.9524]), tensor([0.9546]), tensor([0.9528]))"
"429         result = [dfCols[i] for i in maxIndices]
430         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
431 
432     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
433         """"""
434         :param x: the input data
435         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
436             Raises an exception if the classifier cannot predict probabilities.
437         """"""
438         if not self.isFitted():
439             raise Exception(f""Calling predict with unfitted model. ""
440                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
441         x = self._computeModelInputs(x)
442         result = self._predictClassProbabilities(x)
443         self._checkPrediction(result)
444         return result
445 
446     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
312         """"""
313         :param x: the input data
314         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
315         """"""
316         x = self._computeInputs(x)
317         result = self._predictClassProbabilities(x)
318 
319         # check for correct columns
320         if list(result.columns) != self._labels:
321             raise Exception(f""_predictClassProbabilities returned DataFrame with incorrect columns: expected {self._labels}, got {result.columns}"")
322 
323         # check for normalisation
324         maxRowsToCheck = 5
325         dfToCheck = result.iloc[:maxRowsToCheck]
326         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
327             s = valueSeries.sum()
328             if abs(s-1.0) > 0.01:
329                 _log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
330 
331         return result
332 
333     @abstractmethod
","Before: 435, 436
After: 314",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2944,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 33, '=': 4, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 7, 'attribute': 8, '.': 8, 'argument_list': 7, '(': 8, ',': 2, 'keyword_argument': 1, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.301307505926712,0.2738228971903217,"(tensor([0.8259]), tensor([0.8637]), tensor([0.8444]), tensor([0.8597]))"
"429         result = [dfCols[i] for i in maxIndices]
430         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
431 
432     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
433         """"""
434         :param x: the input data
435         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
436             Raises an exception if the classifier cannot predict probabilities.
437         """"""
438         if not self.isFitted():
439             raise Exception(f""Calling predict with unfitted model. ""
440                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
441         x = self._computeModelInputs(x)
442         result = self._predictClassProbabilities(x)
443         self._checkPrediction(result)
444         return result
445 
446     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
312         """"""
313         :param x: the input data
314         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
315         """"""
316         x = self._computeInputs(x)
317         result = self._predictClassProbabilities(x)
318 
319         # check for correct columns
320         if list(result.columns) != self._labels:
321             raise Exception(f""_predictClassProbabilities returned DataFrame with incorrect columns: expected {self._labels}, got {result.columns}"")
322 
323         # check for normalisation
324         maxRowsToCheck = 5
325         dfToCheck = result.iloc[:maxRowsToCheck]
326         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
327             s = valueSeries.sum()
328             if abs(s-1.0) > 0.01:
329                 _log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
330 
331         return result
332 
333     @abstractmethod
","Before: 438, 439, 440, 441
After: 316",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2920,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 33, '=': 4, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 7, 'attribute': 8, '.': 8, 'argument_list': 7, '(': 8, ',': 2, 'keyword_argument': 1, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.301307505926712,0.2738228971903217,"(tensor([0.8259]), tensor([0.8637]), tensor([0.8444]), tensor([0.8597]))"
"429         result = [dfCols[i] for i in maxIndices]
430         return pd.DataFrame(result, columns=self.getPredictedVariableNames())
431 
432     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
433         """"""
434         :param x: the input data
435         :return: a data frame where the list of columns is the list of class labels and the values are probabilities.
436             Raises an exception if the classifier cannot predict probabilities.
437         """"""
438         if not self.isFitted():
439             raise Exception(f""Calling predict with unfitted model. ""
440                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
441         x = self._computeModelInputs(x)
442         result = self._predictClassProbabilities(x)
443         self._checkPrediction(result)
444         return result
445 
446     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
","308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
312         """"""
313         :param x: the input data
314         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
315         """"""
316         x = self._computeInputs(x)
317         result = self._predictClassProbabilities(x)
318 
319         # check for correct columns
320         if list(result.columns) != self._labels:
321             raise Exception(f""_predictClassProbabilities returned DataFrame with incorrect columns: expected {self._labels}, got {result.columns}"")
322 
323         # check for normalisation
324         maxRowsToCheck = 5
325         dfToCheck = result.iloc[:maxRowsToCheck]
326         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
327             s = valueSeries.sum()
328             if abs(s-1.0) > 0.01:
329                 _log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
330 
331         return result
332 
333     @abstractmethod
","Before: 443, 444, 446, 447, 448, 449, 450, 451, 452, 453
After: 319, 320, 321",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,2972,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 33, '=': 4, 'list_comprehension': 1, '[': 2, 'subscript': 1, ']': 2, 'for_in_clause': 1, 'for': 1, 'in': 1, 'return_statement': 2, 'return': 2, 'call': 7, 'attribute': 8, '.': 8, 'argument_list': 7, '(': 8, ',': 2, 'keyword_argument': 1, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.301307505926712,0.2738228971903217,"(tensor([0.8259]), tensor([0.8637]), tensor([0.8444]), tensor([0.8597]))"
"443         self._checkPrediction(result)
444         return result
445 
446     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
447         """"""
448         Checks whether the column names are correctly set, sorted and whether the entries correspond to probabilities
449         """"""
450         labels = self.getClassLabels()
451         if list(predictionDf.columns) != labels:
452             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
453                             f""expected {labels}, got {predictionDf.columns}"")
454 
455         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
456         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
457 
458             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
459                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
460                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
461 
462             s = valueSeries.sum()
463             if not np.isclose(s, 1, atol=1e-2):
464                 log.warning(
465                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
466 
467     @abstractmethod
","308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
312         """"""
313         :param x: the input data
314         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
315         """"""
316         x = self._computeInputs(x)
317         result = self._predictClassProbabilities(x)
318 
319         # check for correct columns
320         if list(result.columns) != self._labels:
321             raise Exception(f""_predictClassProbabilities returned DataFrame with incorrect columns: expected {self._labels}, got {result.columns}"")
322 
323         # check for normalisation
324         maxRowsToCheck = 5
325         dfToCheck = result.iloc[:maxRowsToCheck]
326         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
327             s = valueSeries.sum()
328             if abs(s-1.0) > 0.01:
329                 _log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
330 
331         return result
332 
333     @abstractmethod
","Before: 455
After: 323, 324, 325",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3077,"{'module': 1, 'expression_statement': 7, 'call': 14, 'attribute': 11, 'identifier': 56, '.': 11, 'argument_list': 14, '(': 16, ')': 16, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_parameter': 1, ':': 7, 'type': 1, 'default_parameter': 1, '=': 6, 'integer': 5, 'block': 5, 'string': 6, 'string_start': 6, 'string_content': 11, 'string_end': 6, 'assignment': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '!=': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 2, 'interpolation': 9, '{': 9, '}': 9, 'subscript': 1, '[': 1, 'slice': 1, ']': 1, 'for_statement': 1, 'for': 1, 'pattern_list': 1, 'tuple_pattern': 1, 'in': 1, 'keyword_argument': 2, 'boolean_operator': 1, 'not_operator': 3, 'not': 3, '<=': 2, 'or': 1, 'float': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3227797568633245,0.29696243527816,"(tensor([0.8574]), tensor([0.8561]), tensor([0.8568]), tensor([0.8563]))"
"443         self._checkPrediction(result)
444         return result
445 
446     def _checkPrediction(self, predictionDf: pd.DataFrame, maxRowsToCheck=5):
447         """"""
448         Checks whether the column names are correctly set, sorted and whether the entries correspond to probabilities
449         """"""
450         labels = self.getClassLabels()
451         if list(predictionDf.columns) != labels:
452             raise Exception(f""{self} _predictClassProbabilities returned DataFrame with incorrect columns: ""
453                             f""expected {labels}, got {predictionDf.columns}"")
454 
455         dfToCheck = predictionDf.iloc[:maxRowsToCheck]
456         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
457 
458             if not all(0 <= valueSeries) or not all(valueSeries <= 1):
459                 log.warning(f""Probabilities data frame may not be correctly normalised, ""
460                             f""got probabilities outside the range [0, 1]: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
461 
462             s = valueSeries.sum()
463             if not np.isclose(s, 1, atol=1e-2):
464                 log.warning(
465                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
466 
467     @abstractmethod
","308         result = [self._labels[i] for i in maxIndices]
309         return pd.DataFrame(result, columns=self.getModelOutputVariableNames())
310 
311     def predictClassProbabilities(self, x: pd.DataFrame) -> pd.DataFrame:
312         """"""
313         :param x: the input data
314         :return: a data frame where the list of columns is the list of class labels and the values are probabilities
315         """"""
316         x = self._computeInputs(x)
317         result = self._predictClassProbabilities(x)
318 
319         # check for correct columns
320         if list(result.columns) != self._labels:
321             raise Exception(f""_predictClassProbabilities returned DataFrame with incorrect columns: expected {self._labels}, got {result.columns}"")
322 
323         # check for normalisation
324         maxRowsToCheck = 5
325         dfToCheck = result.iloc[:maxRowsToCheck]
326         for i, (_, valueSeries) in enumerate(dfToCheck.iterrows(), start=1):
327             s = valueSeries.sum()
328             if abs(s-1.0) > 0.01:
329                 _log.warning(f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
330 
331         return result
332 
333     @abstractmethod
","Before: 457, 458, 459, 460, 461, 463, 464, 465
After: 328, 329, 330, 331",update vector_model.py to use typevar,Sync faz,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,7c8e7dc94bcc252c788e0bc856a8a284f729e7da,ab417f71628f43f8398ea4bbbfb4bbc63a7f7409,0,3109,"{'module': 1, 'expression_statement': 7, 'call': 14, 'attribute': 11, 'identifier': 56, '.': 11, 'argument_list': 14, '(': 16, ')': 16, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_parameter': 1, ':': 7, 'type': 1, 'default_parameter': 1, '=': 6, 'integer': 5, 'block': 5, 'string': 6, 'string_start': 6, 'string_content': 11, 'string_end': 6, 'assignment': 3, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '!=': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 2, 'interpolation': 9, '{': 9, '}': 9, 'subscript': 1, '[': 1, 'slice': 1, ']': 1, 'for_statement': 1, 'for': 1, 'pattern_list': 1, 'tuple_pattern': 1, 'in': 1, 'keyword_argument': 2, 'boolean_operator': 1, 'not_operator': 3, 'not': 3, '<=': 2, 'or': 1, 'float': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 43, 'name': '__init__', 'long_name': '__init__( self , inputs : pd . DataFrame , outputs : pd . DataFrame )', 'start_line': 19, 'end_line': 23, 'full_parameters': ['self', ' inputs : pd . DataFrame', ' outputs : pd . DataFrame'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3227797568633245,0.29696243527816,"(tensor([0.8574]), tensor([0.8561]), tensor([0.8568]), tensor([0.8563]))"
"1 from .crossval import VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, \
2     VectorClassificationModelCrossValidationData, VectorRegressionModelCrossValidationData
3 from .eval_util import RegressionEvaluationUtil, ClassificationEvaluationUtil, MultiDataEvaluationUtil, \
4     evalModelViaEvaluator, createEvaluationUtil, createVectorModelEvaluator, createVectorModelCrossValidator
5 from .evaluator import VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
6     RegressionModelEvaluationData, ClassificationModelEvaluationData, RuleBasedClassificationModelEvaluator, RuleBasedRegressionModelEvaluator
","1 from .crossval import VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, \
2     VectorClassificationModelCrossValidationData, VectorRegressionModelCrossValidationData
3 from .eval_util import RegressionEvaluationUtil, ClassificationEvaluationUtil, MultiDataEvaluationUtil, \
4     evalModelViaEvaluator, createEvaluationUtil, createVectorModelEvaluator, createVectorModelCrossValidator
5 from .evaluator import VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
6     VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData, RuleBasedClassificationModelEvaluator, RuleBasedRegressionModelEvaluator
7 
8 # imports required for backward compatibility
9 from ..data import DataSplitter, DataSplitterFractional","Before: 6
After: 6, 7, 8, 9",add imports required for backward compatibility,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/__init__.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,77,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'relative_import': 3, 'import_prefix': 3, '.': 3, 'dotted_name': 16, 'identifier': 16, 'import': 3, ',': 11, 'line_continuation': 3}",{},{},0.6799994503190993,0.6339812839973821,"(tensor([0.9553]), tensor([0.9901]), tensor([0.9724]), tensor([0.9865]))"
"7 import numpy as np
8 
9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import RegressionModelEvaluationData, ClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictProvider
15 from ..data_ingest import InputOutputData
16 from ..util.typing import PandasNamedTuple
","7 import numpy as np
8 
9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictProvider
15 from ..data import InputOutputData
16 from ..util.typing import PandasNamedTuple
","Before: 12
After: 12",remove unused imports from src/sensai/evaluation/crossval.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,113,"{'module': 1, 'import_statement': 1, 'import': 6, 'aliased_import': 1, 'dotted_name': 19, 'identifier': 23, 'as': 1, 'import_from_statement': 5, 'from': 5, 'relative_import': 5, 'import_prefix': 5, '.': 9, ',': 8, 'line_continuation': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8782805451069394,0.8649517528158809,"(tensor([0.9912]), tensor([0.9860]), tensor([0.9886]), tensor([0.9865]))"
"10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import RegressionModelEvaluationData, ClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictProvider
15 from ..data_ingest import InputOutputData
16 from ..util.typing import PandasNamedTuple
17 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel, PredictorModel
18 
19 log = logging.getLogger(__name__)
","10 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationEvalStatsCollection
11 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection
12 from .evaluator import VectorRegressionModelEvaluationData, VectorClassificationModelEvaluationData, \
13     PredictorModelEvaluationData, VectorClassificationModelEvaluator, VectorRegressionModelEvaluator, \
14     MetricsDictProvider
15 from ..data import InputOutputData
16 from ..util.typing import PandasNamedTuple
17 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel, PredictorModel
18 
19 log = logging.getLogger(__name__)
","Before: 15
After: 15",remove unused imports from src/sensai/evaluation/crossval.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,136,"{'module': 1, 'import_from_statement': 6, 'from': 6, 'relative_import': 6, 'import_prefix': 6, '.': 12, 'dotted_name': 22, 'identifier': 25, 'import': 6, ',': 10, 'line_continuation': 2}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.891949028188207,0.8806126027917818,"(tensor([0.9909]), tensor([0.9854]), tensor([0.9882]), tensor([0.9860]))"
"114     def _computeMetrics(self, model: VectorModel):
115         data = self.evalModel(model)
116         return data.getEvalStatsCollection().aggStats()
117 
118 
119 class VectorRegressionModelCrossValidationData(PredictorModelCrossValidationData[VectorRegressionModel, RegressionModelEvaluationData, RegressionEvalStats, RegressionEvalStatsCollection]):
120     def _createEvalStatsCollection(self, l: List[RegressionEvalStats]) -> RegressionEvalStatsCollection:
121         return RegressionEvalStatsCollection(l)
122 
123 
","114     def _computeMetrics(self, model: VectorModel):
115         data = self.evalModel(model)
116         return data.getEvalStatsCollection().aggStats()
117 
118 
119 class VectorRegressionModelCrossValidationData(PredictorModelCrossValidationData[VectorRegressionModel, VectorRegressionModelEvaluationData, RegressionEvalStats, RegressionEvalStatsCollection]):
120     def _createEvalStatsCollection(self, l: List[RegressionEvalStats]) -> RegressionEvalStatsCollection:
121         return RegressionEvalStatsCollection(l)
122 
123 
","Before: 119
After: 119",remove unused imports from src/sensai/evaluation/crossval.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1233,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 25, 'parameters': 2, '(': 7, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 4, ')': 7, 'block': 3, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 4, 'attribute': 3, '.': 3, 'argument_list': 5, 'return_statement': 2, 'return': 2, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 2, ']': 2, 'generic_type': 1, 'type_parameter': 1, '->': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9625248317849852,0.9584068650454449,"(tensor([0.9970]), tensor([0.9980]), tensor([0.9975]), tensor([0.9979]))"
"128 
129     def _createResultData(self, trainedModels, evalDataList, testIndicesList, predictedVarNames) -> VectorRegressionModelCrossValidationData:
130         return VectorRegressionModelCrossValidationData(trainedModels, evalDataList, predictedVarNames, testIndicesList)
131 
132 
133 class VectorClassificationModelCrossValidationData(PredictorModelCrossValidationData[VectorClassificationModel, ClassificationModelEvaluationData, ClassificationEvalStats, ClassificationEvalStatsCollection]):
134     def _createEvalStatsCollection(self, l: List[ClassificationEvalStats]) -> ClassificationEvalStatsCollection:
135         return ClassificationEvalStatsCollection(l)
136 
137 
","128 
129     def _createResultData(self, trainedModels, evalDataList, testIndicesList, predictedVarNames) -> VectorRegressionModelCrossValidationData:
130         return VectorRegressionModelCrossValidationData(trainedModels, evalDataList, predictedVarNames, testIndicesList)
131 
132 
133 class VectorClassificationModelCrossValidationData(PredictorModelCrossValidationData[VectorClassificationModel, VectorClassificationModelEvaluationData, ClassificationEvalStats, ClassificationEvalStatsCollection]):
134     def _createEvalStatsCollection(self, l: List[ClassificationEvalStats]) -> ClassificationEvalStatsCollection:
135         return ClassificationEvalStatsCollection(l)
136 
137 
","Before: 133
After: 133",remove unused imports from src/sensai/evaluation/crossval.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1376,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 26, 'parameters': 2, '(': 5, ',': 11, ')': 5, '->': 2, 'type': 4, ':': 4, 'block': 3, 'return_statement': 2, 'return': 2, 'call': 2, 'argument_list': 3, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 2, ']': 2, 'typed_parameter': 1, 'generic_type': 1, 'type_parameter': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9651391201669045,0.9616311437622332,"(tensor([0.9965]), tensor([0.9974]), tensor([0.9970]), tensor([0.9974]))"
"20     VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, VectorModelCrossValidator
21 from .eval_stats.eval_stats_base import EvalStats, EvalStatsCollection
22 from .eval_stats.eval_stats_classification import ClassificationEvalStats
23 from .eval_stats.eval_stats_regression import RegressionEvalStats
24 from .evaluator import PredictorModelEvaluator, PredictorModelEvaluationData, VectorRegressionModelEvaluator, \
25     RegressionModelEvaluationData, VectorClassificationModelEvaluator, ClassificationModelEvaluationData
26 from ..data_ingest import InputOutputData
27 from ..util.io import ResultWriter
28 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel
29 
","20     VectorClassificationModelCrossValidator, VectorRegressionModelCrossValidator, VectorModelCrossValidator
21 from .eval_stats.eval_stats_base import EvalStats, EvalStatsCollection
22 from .eval_stats.eval_stats_classification import ClassificationEvalStats
23 from .eval_stats.eval_stats_regression import RegressionEvalStats
24 from .evaluator import PredictorModelEvaluator, PredictorModelEvaluationData, VectorRegressionModelEvaluator, \
25     VectorRegressionModelEvaluationData, VectorClassificationModelEvaluator, VectorClassificationModelEvaluationData
26 from ..data import InputOutputData
27 from ..util.io import ResultWriter
28 from ..vector_model import VectorClassificationModel, VectorRegressionModel, VectorModel
29 
","Before: 25, 26
After: 25, 26",remove unused imports,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,175,"{'module': 1, 'expression_statement': 1, 'identifier': 29, ',': 10, 'import_from_statement': 7, 'from': 7, 'relative_import': 7, 'import_prefix': 7, '.': 14, 'dotted_name': 22, 'import': 7, 'line_continuation': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8751704261509159,0.8624738232498728,"(tensor([0.9934]), tensor([0.9871]), tensor([0.9902]), tensor([0.9877]))"
"284         :param subtitle: the subtitle to use for generated plots (if any)
285         """"""
286         pass
287 
288 
289 class RegressionEvaluationUtil(EvaluationUtil[VectorRegressionModel, VectorRegressionModelEvaluator, RegressionModelEvaluationData, VectorRegressionModelCrossValidator, VectorRegressionModelCrossValidationData, RegressionEvalStats]):
290     def _createEvalStatsPlots(self, evalStats: RegressionEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
","284         :param subtitle: the subtitle to use for generated plots (if any)
285         """"""
286         pass
287 
288 
289 class RegressionEvaluationUtil(EvaluationUtil[VectorRegressionModel, VectorRegressionModelEvaluator, VectorRegressionModelEvaluationData, VectorRegressionModelCrossValidator, VectorRegressionModelCrossValidationData, RegressionEvalStats]):
290     def _createEvalStatsPlots(self, evalStats: RegressionEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
","Before: 289
After: 289",remove unused imports,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,2724,"{'module': 1, 'ERROR': 4, ':': 2, 'identifier': 10, 'expression_statement': 1, 'assignment': 1, 'type': 1, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1, 'string_start': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9736071380988617,0.9705541112332454,"(tensor([0.9989]), tensor([0.9984]), tensor([0.9987]), tensor([0.9985]))"
"291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
294 
295 
296 class ClassificationEvaluationUtil(EvaluationUtil[VectorClassificationModel, VectorClassificationModelEvaluator, ClassificationModelEvaluationData, VectorClassificationModelCrossValidator, VectorClassificationModelCrossValidationData, ClassificationEvalStats]):
297     def _createEvalStatsPlots(self, evalStats: ClassificationEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
298         resultCollector.addFigure(""confusion-matrix"", evalStats.plotConfusionMatrix(titleAdd=subtitle))
299 
300 
","291         resultCollector.addFigure(""error-dist"", evalStats.plotErrorDistribution(titleAdd=subtitle))
292         resultCollector.addFigure(""heatmap-gt-pred"", evalStats.plotHeatmapGroundTruthPredictions(titleAdd=subtitle))
293         resultCollector.addFigure(""scatter-gt-pred"", evalStats.plotScatterGroundTruthPredictions(titleAdd=subtitle))
294 
295 
296 class ClassificationEvaluationUtil(EvaluationUtil[VectorClassificationModel, VectorClassificationModelEvaluator, VectorClassificationModelEvaluationData, VectorClassificationModelCrossValidator, VectorClassificationModelCrossValidationData, ClassificationEvalStats]):
297     def _createEvalStatsPlots(self, evalStats: ClassificationEvalStats, resultCollector: EvaluationUtil.ResultCollector, subtitle=None):
298         resultCollector.addFigure(""confusion-matrix"", evalStats.plotConfusionMatrix(titleAdd=subtitle))
299 
300 
","Before: 296
After: 296",remove unused imports,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,2854,"{'module': 1, 'expression_statement': 4, 'call': 8, 'attribute': 9, 'identifier': 40, '.': 9, 'argument_list': 9, '(': 10, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ',': 12, 'keyword_argument': 4, '=': 5, ')': 10, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, ']': 1, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, 'type': 2, 'default_parameter': 1, 'none': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.97117159097714,0.9667077643416399,"(tensor([0.9981]), tensor([0.9977]), tensor([0.9979]), tensor([0.9978]))"
"6 import pandas as pd
7 
8 from .eval_stats.eval_stats_base import EvalStats, EvalStatsCollection
9 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationMetric
10 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection, RegressionMetric
11 from ..data_ingest import DataSplitter, DataSplitterFractional, InputOutputData
12 from ..tracking import TrackingMixin
13 from ..util.typing import PandasNamedTuple
14 from ..vector_model import VectorClassificationModel, VectorModel, PredictorModel, FittableModel
15 
","6 import pandas as pd
7 
8 from .eval_stats.eval_stats_base import EvalStats, EvalStatsCollection
9 from .eval_stats.eval_stats_classification import ClassificationEvalStats, ClassificationMetric
10 from .eval_stats.eval_stats_regression import RegressionEvalStats, RegressionEvalStatsCollection, RegressionMetric
11 from ..data import DataSplitter, DataSplitterFractional, InputOutputData
12 from ..tracking import TrackingMixin
13 from ..util.typing import PandasNamedTuple
14 from ..vector_model import VectorClassificationModel, VectorModel, PredictorModel, FittableModel
15 
","Before: 11
After: 11",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,120,"{'module': 1, 'import_statement': 1, 'import': 8, 'aliased_import': 1, 'dotted_name': 24, 'identifier': 29, 'as': 1, 'import_from_statement': 7, 'from': 7, 'relative_import': 7, 'import_prefix': 7, '.': 15, ',': 9}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9594863506034305,0.9548759100226059,"(tensor([0.9962]), tensor([0.9895]), tensor([0.9928]), tensor([0.9902]))"
"76         evalStats = self.getEvalStats(predictedVarName)
77         for i, namedTuple in enumerate(self.inputData.itertuples()):
78             yield namedTuple, evalStats.y_predicted[i], evalStats.y_true[i]
79 
80 
81 class RegressionModelEvaluationData(PredictorModelEvaluationData[RegressionEvalStats]):
82     def getEvalStatsCollection(self):
83         return RegressionEvalStatsCollection(list(self.evalStatsByVarName.values()))
84 
85 
","76         evalStats = self.getEvalStats(predictedVarName)
77         for i, namedTuple in enumerate(self.inputData.itertuples()):
78             yield namedTuple, evalStats.y_predicted[i], evalStats.y_true[i]
79 
80 
81 class VectorRegressionModelEvaluationData(PredictorModelEvaluationData[RegressionEvalStats]):
82     def getEvalStatsCollection(self):
83         return RegressionEvalStatsCollection(list(self.evalStatsByVarName.values()))
84 
85 
","Before: 81
After: 81",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,882,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 27, '=': 1, 'call': 6, 'attribute': 7, '.': 7, 'argument_list': 7, '(': 8, ')': 8, 'for_statement': 1, 'for': 1, 'pattern_list': 1, ',': 3, 'in': 1, ':': 3, 'block': 3, 'yield': 2, 'expression_list': 1, 'subscript': 3, '[': 3, ']': 3, 'class_definition': 1, 'class': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9601328622880726,0.9554427922043668,"(tensor([0.9941]), tensor([0.9973]), tensor([0.9957]), tensor([0.9969]))"
"141         startTime = time.time()
142         model.fit(self.trainingData.inputs, self.trainingData.outputs)
143         log.info(f""Training of {model.__class__.__name__} completed in {time.time() - startTime:.1f} seconds"")
144 
145 
146 class VectorRegressionModelEvaluator(PredictorModelEvaluator[RegressionModelEvaluationData]):
147     def __init__(self, data: InputOutputData, testData: InputOutputData = None, dataSplitter=None, testFraction=None, randomSeed=42, shuffle=True,
148             additionalMetrics: Sequence[RegressionMetric] = None):
149         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
150         self.additionalMetrics = additionalMetrics
","141         startTime = time.time()
142         model.fit(self.trainingData.inputs, self.trainingData.outputs)
143         log.info(f""Training of {model.__class__.__name__} completed in {time.time() - startTime:.1f} seconds"")
144 
145 
146 class VectorRegressionModelEvaluator(PredictorModelEvaluator[VectorRegressionModelEvaluationData]):
147     def __init__(self, data: InputOutputData, testData: InputOutputData = None, dataSplitter=None, testFraction=None, randomSeed=42, shuffle=True,
148             additionalMetrics: Sequence[RegressionMetric] = None):
149         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
150         self.additionalMetrics = additionalMetrics
","Before: 146
After: 146",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1451,"{'module': 1, 'expression_statement': 4, 'assignment': 1, 'identifier': 49, '=': 13, 'call': 6, 'attribute': 11, '.': 11, 'argument_list': 7, '(': 8, ')': 8, ',': 13, 'string': 1, 'string_start': 1, 'string_content': 3, 'interpolation': 2, '{': 2, '}': 2, 'binary_operator': 1, '-': 1, 'format_specifier': 1, ':': 6, 'string_end': 1, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 2, ']': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 4, 'typed_default_parameter': 2, 'none': 4, 'default_parameter': 4, 'integer': 1, 'true': 1, 'generic_type': 1, 'type_parameter': 1, 'keyword_argument': 6}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9758978044863993,0.9739024393813431,"(tensor([0.9964]), tensor([0.9976]), tensor([0.9970]), tensor([0.9974]))"
"149         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
150         self.additionalMetrics = additionalMetrics
151 
152     def _evalModel(self, model: PredictorModel, data: InputOutputData) -> RegressionModelEvaluationData:
153         if not model.isRegressionModel():
154             raise ValueError(f""Expected a regression model, got {model}"")
155         evalStatsByVarName = {}
156         predictions, groundTruth = self._computeOutputs(model, data)
157         for predictedVarName in model.getPredictedVariableNames():
158             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
159                 additionalMetrics=self.additionalMetrics)
160             evalStatsByVarName[predictedVarName] = evalStats
161         return RegressionModelEvaluationData(evalStatsByVarName, data.inputs, model)
162 
163     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","149         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
150         self.additionalMetrics = additionalMetrics
151 
152     def _evalModel(self, model: PredictorModel, data: InputOutputData) -> VectorRegressionModelEvaluationData:
153         if not model.isRegressionModel():
154             raise ValueError(f""Expected a regression model, got {model}"")
155         evalStatsByVarName = {}
156         predictions, groundTruth = self._computeOutputs(model, data)
157         for predictedVarName in model.getPredictedVariableNames():
158             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
159                 additionalMetrics=self.additionalMetrics)
160             evalStatsByVarName[predictedVarName] = evalStats
161         return VectorRegressionModelEvaluationData(evalStatsByVarName, data.inputs, model)
162 
163     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","Before: 152
After: 152",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1582,"{'module': 1, 'expression_statement': 6, 'call': 8, 'attribute': 7, 'identifier': 57, 'argument_list': 8, '(': 9, ')': 9, '.': 7, 'keyword_argument': 9, '=': 14, ',': 13, 'assignment': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 3, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'dictionary': 1, 'pattern_list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 3, '[': 3, ']': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.964654793793552,0.9615145553631412,"(tensor([0.9943]), tensor([0.9974]), tensor([0.9958]), tensor([0.9971]))"
"149         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
150         self.additionalMetrics = additionalMetrics
151 
152     def _evalModel(self, model: PredictorModel, data: InputOutputData) -> RegressionModelEvaluationData:
153         if not model.isRegressionModel():
154             raise ValueError(f""Expected a regression model, got {model}"")
155         evalStatsByVarName = {}
156         predictions, groundTruth = self._computeOutputs(model, data)
157         for predictedVarName in model.getPredictedVariableNames():
158             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
159                 additionalMetrics=self.additionalMetrics)
160             evalStatsByVarName[predictedVarName] = evalStats
161         return RegressionModelEvaluationData(evalStatsByVarName, data.inputs, model)
162 
163     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","149         super().__init__(data=data, dataSplitter=dataSplitter, testFraction=testFraction, testData=testData, randomSeed=randomSeed, shuffle=shuffle)
150         self.additionalMetrics = additionalMetrics
151 
152     def _evalModel(self, model: PredictorModel, data: InputOutputData) -> VectorRegressionModelEvaluationData:
153         if not model.isRegressionModel():
154             raise ValueError(f""Expected a regression model, got {model}"")
155         evalStatsByVarName = {}
156         predictions, groundTruth = self._computeOutputs(model, data)
157         for predictedVarName in model.getPredictedVariableNames():
158             evalStats = RegressionEvalStats(y_predicted=predictions[predictedVarName], y_true=groundTruth[predictedVarName],
159                 additionalMetrics=self.additionalMetrics)
160             evalStatsByVarName[predictedVarName] = evalStats
161         return VectorRegressionModelEvaluationData(evalStatsByVarName, data.inputs, model)
162 
163     def computeTestDataOutputs(self, model: PredictorModel) -> Tuple[pd.DataFrame, pd.DataFrame]:
","Before: 161
After: 161",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1708,"{'module': 1, 'expression_statement': 6, 'call': 8, 'attribute': 7, 'identifier': 57, 'argument_list': 8, '(': 9, ')': 9, '.': 7, 'keyword_argument': 9, '=': 14, ',': 13, 'assignment': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 3, '->': 1, 'block': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'dictionary': 1, 'pattern_list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 3, '[': 3, ']': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.964654793793552,0.9615145553631412,"(tensor([0.9943]), tensor([0.9974]), tensor([0.9958]), tensor([0.9971]))"
"180         predictions = model.predict(inputOutputData.inputs)
181         groundTruth = inputOutputData.outputs
182         return predictions, groundTruth
183 
184 
185 class ClassificationModelEvaluationData(PredictorModelEvaluationData[ClassificationEvalStats]):
186     pass
187 
188 
189 class VectorClassificationModelEvaluator(PredictorModelEvaluator[ClassificationModelEvaluationData]):
","180         predictions = model.predict(inputOutputData.inputs)
181         groundTruth = inputOutputData.outputs
182         return predictions, groundTruth
183 
184 
185 class VectorClassificationModelEvaluationData(PredictorModelEvaluationData[ClassificationEvalStats]):
186     pass
187 
188 
189 class VectorClassificationModelEvaluator(PredictorModelEvaluator[VectorClassificationModelEvaluationData]):
","Before: 185
After: 185",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1831,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 13, '=': 2, 'call': 1, 'attribute': 3, '.': 3, 'argument_list': 2, '(': 2, ')': 2, 'return_statement': 1, 'return': 1, 'expression_list': 1, ',': 1, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, ']': 1, ':': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8729435194251833,0.8616515848841428,"(tensor([0.9851]), tensor([0.9879]), tensor([0.9865]), tensor([0.9876]))"
"184 
185 class ClassificationModelEvaluationData(PredictorModelEvaluationData[ClassificationEvalStats]):
186     pass
187 
188 
189 class VectorClassificationModelEvaluator(PredictorModelEvaluator[ClassificationModelEvaluationData]):
190     def __init__(self, data: InputOutputData, testData: InputOutputData = None, dataSplitter=None, testFraction=None,
191             randomSeed=42, computeProbabilities=False, shuffle=True, additionalMetrics: Sequence[ClassificationMetric] = None):
192         super().__init__(data=data, testData=testData, dataSplitter=dataSplitter, testFraction=testFraction, randomSeed=randomSeed, shuffle=shuffle)
193         self.computeProbabilities = computeProbabilities
","184 
185 class VectorClassificationModelEvaluationData(PredictorModelEvaluationData[ClassificationEvalStats]):
186     pass
187 
188 
189 class VectorClassificationModelEvaluator(PredictorModelEvaluator[VectorClassificationModelEvaluationData]):
190     def __init__(self, data: InputOutputData, testData: InputOutputData = None, dataSplitter=None, testFraction=None,
191             randomSeed=42, computeProbabilities=False, shuffle=True, additionalMetrics: Sequence[ClassificationMetric] = None):
192         super().__init__(data=data, testData=testData, dataSplitter=dataSplitter, testFraction=testFraction, randomSeed=randomSeed, shuffle=shuffle)
193         self.computeProbabilities = computeProbabilities
","Before: 189
After: 189",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1846,"{'module': 1, 'class_definition': 2, 'class': 2, 'identifier': 34, 'argument_list': 4, '(': 5, 'subscript': 2, '[': 3, ']': 3, ')': 5, ':': 6, 'block': 3, 'pass_statement': 1, 'pass': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 13, 'typed_parameter': 1, 'type': 4, 'typed_default_parameter': 2, '=': 13, 'none': 4, 'default_parameter': 5, 'integer': 1, 'false': 1, 'true': 1, 'generic_type': 1, 'type_parameter': 1, 'expression_statement': 1, 'call': 2, 'attribute': 1, '.': 1, 'keyword_argument': 6}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9385994309919137,0.9323800487702835,"(tensor([0.9892]), tensor([0.9910]), tensor([0.9901]), tensor([0.9908]))"
"193         self.computeProbabilities = computeProbabilities
194         self.additionalMetrics = additionalMetrics
195 
196     def _evalModel(self, model: VectorClassificationModel, data: InputOutputData) -> ClassificationModelEvaluationData:
197         if model.isRegressionModel():
198             raise ValueError(f""Expected a classification model, got {model}"")
199         predictions, predictions_proba, groundTruth = self._computeOutputs(model, data)
200         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
201             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
202         predictedVarName = model.getPredictedVariableNames()[0]
203         return ClassificationModelEvaluationData({predictedVarName: evalStats}, data.inputs, model)
204 
205     def computeTestDataOutputs(self, model) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","193         self.computeProbabilities = computeProbabilities
194         self.additionalMetrics = additionalMetrics
195 
196     def _evalModel(self, model: VectorClassificationModel, data: InputOutputData) -> VectorClassificationModelEvaluationData:
197         if model.isRegressionModel():
198             raise ValueError(f""Expected a classification model, got {model}"")
199         predictions, predictions_proba, groundTruth = self._computeOutputs(model, data)
200         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
201             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
202         predictedVarName = model.getPredictedVariableNames()[0]
203         return VectorClassificationModelEvaluationData({predictedVarName: evalStats}, data.inputs, model)
204 
205     def computeTestDataOutputs(self, model) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","Before: 196
After: 196",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,1990,"{'module': 1, 'expression_statement': 5, 'assignment': 5, 'attribute': 8, 'identifier': 47, '.': 8, '=': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 11, 'typed_parameter': 2, ':': 5, 'type': 3, ')': 8, '->': 1, 'block': 2, 'if_statement': 1, 'if': 1, 'call': 7, 'argument_list': 7, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'pattern_list': 1, 'keyword_argument': 5, 'subscript': 1, '[': 1, 'integer': 1, ']': 1, 'return_statement': 1, 'return': 1, 'dictionary': 1, 'pair': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9588326338032187,0.955272732549966,"(tensor([0.9920]), tensor([0.9947]), tensor([0.9934]), tensor([0.9945]))"
"193         self.computeProbabilities = computeProbabilities
194         self.additionalMetrics = additionalMetrics
195 
196     def _evalModel(self, model: VectorClassificationModel, data: InputOutputData) -> ClassificationModelEvaluationData:
197         if model.isRegressionModel():
198             raise ValueError(f""Expected a classification model, got {model}"")
199         predictions, predictions_proba, groundTruth = self._computeOutputs(model, data)
200         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
201             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
202         predictedVarName = model.getPredictedVariableNames()[0]
203         return ClassificationModelEvaluationData({predictedVarName: evalStats}, data.inputs, model)
204 
205     def computeTestDataOutputs(self, model) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","193         self.computeProbabilities = computeProbabilities
194         self.additionalMetrics = additionalMetrics
195 
196     def _evalModel(self, model: VectorClassificationModel, data: InputOutputData) -> VectorClassificationModelEvaluationData:
197         if model.isRegressionModel():
198             raise ValueError(f""Expected a classification model, got {model}"")
199         predictions, predictions_proba, groundTruth = self._computeOutputs(model, data)
200         evalStats = ClassificationEvalStats(y_predictedClassProbabilities=predictions_proba, y_predicted=predictions, y_true=groundTruth,
201             labels=model.getClassLabels(), additionalMetrics=self.additionalMetrics)
202         predictedVarName = model.getPredictedVariableNames()[0]
203         return VectorClassificationModelEvaluationData({predictedVarName: evalStats}, data.inputs, model)
204 
205     def computeTestDataOutputs(self, model) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
","Before: 203
After: 203",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,2117,"{'module': 1, 'expression_statement': 5, 'assignment': 5, 'attribute': 8, 'identifier': 47, '.': 8, '=': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 11, 'typed_parameter': 2, ':': 5, 'type': 3, ')': 8, '->': 1, 'block': 2, 'if_statement': 1, 'if': 1, 'call': 7, 'argument_list': 7, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 2, '}': 2, 'string_end': 1, 'pattern_list': 1, 'keyword_argument': 5, 'subscript': 1, '[': 1, 'integer': 1, ']': 1, 'return_statement': 1, 'return': 1, 'dictionary': 1, 'pair': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9588326338032187,0.955272732549966,"(tensor([0.9920]), tensor([0.9947]), tensor([0.9934]), tensor([0.9945]))"
"233     def __init__(self, data: InputOutputData):
234         super().__init__(data, testData=data)
235 
236     def evalModel(self, model: PredictorModel, onTrainingData=False) -> ClassificationModelEvaluationData:
237         """"""
238         Evaluate the rule based model. The training data and test data coincide, thus fitting the model
239         will fit the model's preprocessors on the full data set and evaluating it will evaluate the model on the
240         same data set.
241 
242         :param model:
243         :param onTrainingData: has to be False here. Setting to True is not supported and will lead to an
244             exception
245         :return:
246         """"""
247         if onTrainingData:
248             raise Exception(""Evaluating rule based models on training data is not supported. In this evaluator""
249                             ""training and test data coincide."")
250         return super().evalModel(model)
251 
252 
","233     def __init__(self, data: InputOutputData):
234         super().__init__(data, testData=data)
235 
236     def evalModel(self, model: PredictorModel, onTrainingData=False) -> VectorClassificationModelEvaluationData:
237         """"""
238         Evaluate the rule based model. The training data and test data coincide, thus fitting the model
239         will fit the model's preprocessors on the full data set and evaluating it will evaluate the model on the
240         same data set.
241 
242         :param model:
243         :param onTrainingData: has to be False here. Setting to True is not supported and will lead to an
244             exception
245         :return:
246         """"""
247         if onTrainingData:
248             raise Exception(""Evaluating rule based models on training data is not supported. In this evaluator""
249                             ""training and test data coincide."")
250         return super().evalModel(model)
251 
252 
","Before: 236
After: 236",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,2363,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 20, 'parameters': 2, '(': 7, ',': 4, 'typed_parameter': 2, ':': 5, 'type': 3, ')': 7, 'block': 3, 'expression_statement': 2, 'call': 5, 'attribute': 2, 'argument_list': 5, '.': 2, 'keyword_argument': 1, '=': 2, 'default_parameter': 1, 'false': 1, '->': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.985095502717434,0.9841694094898907,"(tensor([0.9931]), tensor([0.9967]), tensor([0.9949]), tensor([0.9963]))"
"255     def __init__(self, data: InputOutputData):
256         super().__init__(data, testData=data)
257 
258     def evalModel(self, model: PredictorModel, onTrainingData=False) -> RegressionModelEvaluationData:
259         """"""
260         Evaluate the rule based model. The training data and test data coincide, thus fitting the model
261         will fit the model's preprocessors on the full data set and evaluating it will evaluate the model on the
262         same data set.
263 
264         :param model:
265         :param onTrainingData: has to be False here. Setting to True is not supported and will lead to an
266             exception
267         :return:
268         """"""
269         if onTrainingData:
270             raise Exception(""Evaluating rule based models on training data is not supported. In this evaluator""
271                             ""training and test data coincide."")
272         return super().evalModel(model)
","255     def __init__(self, data: InputOutputData):
256         super().__init__(data, testData=data)
257 
258     def evalModel(self, model: PredictorModel, onTrainingData=False) -> VectorRegressionModelEvaluationData:
259         """"""
260         Evaluate the rule based model. The training data and test data coincide, thus fitting the model
261         will fit the model's preprocessors on the full data set and evaluating it will evaluate the model on the
262         same data set.
263 
264         :param model:
265         :param onTrainingData: has to be False here. Setting to True is not supported and will lead to an
266             exception
267         :return:
268         """"""
269         if onTrainingData:
270             raise Exception(""Evaluating rule based models on training data is not supported. In this evaluator""
271                             ""training and test data coincide."")
272         return super().evalModel(model)
","Before: 258
After: 258",fix typos in src/sensai/evaluation/evaluator.py,"Revert ""Renaming: Removed ""Vector"" prefix from ModelEvaluationData classes""",https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,c2a92c41e10a95cdd4f5c03da3206e94de26b56d,74263f0f14db68c91475ea2bf91d0a8f3f8584d8,0,2471,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 5, ',': 4, 'typed_parameter': 2, ':': 5, 'type': 3, ')': 5, 'block': 3, 'expression_statement': 2, 'call': 3, 'attribute': 1, 'argument_list': 3, '.': 1, 'keyword_argument': 1, '=': 2, 'default_parameter': 1, 'false': 1, '->': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 18, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 26, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9849156258720441,0.9839601970934723,"(tensor([0.9962]), tensor([0.9981]), tensor([0.9971]), tensor([0.9979]))"
"17     assert fgen2.generate(inputDf).equals(inputDf)
18     assert fgen3.generate(inputDf).equals(inputDf[[""a"", ""b""]])
19     with pytest.raises(Exception):
20         fgen4.generate(inputDf)
21 
22 
23 def test_flatten_columns():
24     inputDf = pd.DataFrame({""a"": [np.array([1, 2])], ""b"": [np.array([5, 6])]})
25     fgen1 = FeatureGeneratorFlattenColumns(""a"")
26     fgen2 = FeatureGeneratorFlattenColumns()
","19     with pytest.raises(Exception):
20         fgen4.generate(inputDf)
21 
22 def test_flatten_columns():
23     inputDf = pd.DataFrame({""a"": [np.array([1, 2])], ""b"": [np.array([5, 6])]})
24     fgen1 = FeatureGeneratorFlattenColumns(""a"")
25     fgen2 = FeatureGeneratorFlattenColumns()
26     fgen3 = FeatureGeneratorFlattenColumns([""a""])
27     fgen4 = FeatureGeneratorFlattenColumns(""c"")
28     assert fgen1.generate(inputDf).equals(pd.DataFrame({""a_0"": np.array([1]), ""a_1"": np.array([2])}))
29     assert fgen2.generate(inputDf).equals(pd.DataFrame({""a_0"": np.array([1]), ""a_1"": np.array([2]), ""b_0"": np.array([5]), ""b_1"": np.array([6])}))
30     assert fgen3.generate(inputDf).equals(pd.DataFrame({""a_0"": np.array([1]), ""a_1"": np.array([2])}))
31     with pytest.raises(Exception):
32         assert fgen4.generate(inputDf)
33 
34 
","Before: 22, 29, 30, 31
After: 28, 29, 30",remove redundant test_take_columns and test_flatten_columns,Fixed test_flatten_columns and test_getFlattenedFeatureGenerator failing,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,932d53cf66d775a77f3dd60919c57e6475966f6f,932ef256a7b150a18c3ffae92cb0a707eb27fd5b,0,269,"{'module': 1, 'assert_statement': 2, 'assert': 2, 'call': 10, 'attribute': 9, 'identifier': 26, '.': 9, 'argument_list': 10, '(': 11, ')': 11, 'subscript': 1, '[': 6, 'list': 5, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, ',': 4, ']': 6, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, ':': 4, 'block': 2, 'expression_statement': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'assignment': 2, '=': 2, 'dictionary': 1, '{': 1, 'pair': 2, 'integer': 4, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.33883728398178387,0.340995635924227,"(tensor([0.8863]), tensor([0.9378]), tensor([0.9113]), tensor([0.9323]))"
"33         assert fgen4.generate(inputDf)
34 
35 
36 def test_getFlattenedFeatureGenerator():
37     inputDf = pd.DataFrame({""a"": [np.array([1, 2])], ""b"": [np.array([5, 6])]})
38     fgen1 = flattenedFeatureGenerator(FeatureGeneratorTakeColumns(""a""))
39     assert fgen1.generate(inputDf).equals(pd.DataFrame({""a_0"": [1], ""a_1"": [2]}))
40 
41 
","32         assert fgen4.generate(inputDf)
33 
34 
35 def test_getFlattenedFeatureGenerator():
36     inputDf = pd.DataFrame({""a"": [np.array([1, 2])], ""b"": [np.array([5, 6])]})
37     fgen1 = flattenedFeatureGenerator(FeatureGeneratorTakeColumns(""a""))
38     assert fgen1.generate(inputDf).equals(pd.DataFrame({""a_0"": np.array([1]), ""a_1"": np.array([2])}))
39 
40 
","Before: 39
After: 38",remove redundant test_take_columns and test_flatten_columns,Fixed test_flatten_columns and test_getFlattenedFeatureGenerator failing,https://github.com/opcode81/sensAI,tests/base/test_featuregen.py,932d53cf66d775a77f3dd60919c57e6475966f6f,932ef256a7b150a18c3ffae92cb0a707eb27fd5b,0,726,"{'module': 1, 'assert_statement': 2, 'assert': 2, 'call': 9, 'attribute': 7, 'identifier': 20, '.': 7, 'argument_list': 9, '(': 10, ')': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 5, 'block': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'dictionary': 2, '{': 2, 'pair': 4, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'list': 6, '[': 6, 'integer': 6, ',': 4, ']': 6, '}': 2}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 121, 'name': 'test_take_columns', 'long_name': 'test_take_columns( )', 'start_line': 9, 'end_line': 20, 'full_parameters': [], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/base/test_featuregen.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7449094527005242,0.742630903438386,"(tensor([0.9842]), tensor([0.9885]), tensor([0.9863]), tensor([0.9881]))"
"523             raise Exception(""Unknown series"")
524         return pd.Series(self.loggedSeries[seriesName])
525 
526 
527 class SimulatedAnnealing:
528     log = log.getChild(__qualname__)
529 
530     """"""
531     The simulated annealing algorithm for discrete optimisation (cost minimisation)
532     """"""
","526 
527 class SimulatedAnnealing:
528     """"""
529     The simulated annealing algorithm for discrete optimisation (cost minimisation)
530     """"""
531 
532     log = log.getChild(__qualname__)
533 
534     def __init__(self, scheduleFactory: Callable[[], SATemperatureSchedule], opsAndWeights: Sequence[Tuple[Callable[[SAState], SAOperator], float]],
535             maxSteps: int = None, duration: float = None, randomSeed=42, collectStats=False):
","Before: 528, 529
After: 531, 532, 533",fix typos in local_search.py,Fixed class docstrings in wrong location,https://github.com/opcode81/sensAI,src/sensai/local_search.py,e14d3a2e48a4ddce03559a35e51074c3f3e84d62,7578377b8fb513e90a02b143b5f1077f796e48bd,0,4021,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 4, 'identifier': 19, 'argument_list': 4, '(': 4, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ')': 4, 'return_statement': 1, 'return': 1, 'attribute': 3, '.': 3, 'subscript': 1, '[': 1, ']': 1, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'ERROR': 3, 'for': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2789007026742326,0.2897570716509676,"(tensor([0.7772]), tensor([0.8546]), tensor([0.8141]), tensor([0.8462]))"
"588         """"""
589         return self._chain
590 
591 
592 class ParallelTempering:
593     log = log.getChild(__qualname__)
594 
595     """"""
596     The parallel tempering algorithm for discrete optimisation (cost minimisation)
597     """"""
","592 
593 class ParallelTempering:
594     """"""
595     The parallel tempering algorithm for discrete optimisation (cost minimisation)
596     """"""
597 
598     log = log.getChild(__qualname__)
599 
600     def __init__(self, numChains, opsAndWeights: Sequence[Tuple[Type[SAOperator], float]],
601                  schedule: SATemperatureSchedule = None, probabilityFunction: SAProbabilityFunction = None,
","Before: 593, 594
After: 597, 598, 599",fix typos in local_search.py,Fixed class docstrings in wrong location,https://github.com/opcode81/sensAI,src/sensai/local_search.py,e14d3a2e48a4ddce03559a35e51074c3f3e84d62,7578377b8fb513e90a02b143b5f1077f796e48bd,0,4664,"{'module': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'ERROR': 3, 'identifier': 8, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 9, 'name': 'temperature', 'long_name': 'temperature( self , degreeOfCompletion )', 'start_line': 23, 'end_line': 29, 'full_parameters': ['self', ' degreeOfCompletion'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/local_search.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.27076203759272344,0.2508576952195461,"(tensor([0.7851]), tensor([0.9023]), tensor([0.8396]), tensor([0.8891]))"
"1 import copy
2 import logging
3 import warnings
4 from abc import ABC, abstractmethod
5 from typing import Tuple, Any, Generator, Generic, TypeVar, List
6 
7 import numpy as np
8 
9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
","1 import copy
2 import logging
3 import warnings
4 from abc import ABC, abstractmethod
5 from typing import Tuple, Any, Generator, Generic, TypeVar, List, Union
6 
7 import numpy as np
8 
9 from .eval_stats.eval_stats_base import PredictionEvalStats, EvalStatsCollection
","Before: 5
After: 5",add vectorclassificationmodelcrossvalidator for regressionmodels,Sync faz,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,ca9e82e81e4d1e399a927dc6ad466189db650d57,ab66d1a9b763d37ac39dbf906140c34ffb4ab46c,0,46,"{'module': 1, 'import_statement': 4, 'import': 6, 'dotted_name': 14, 'identifier': 15, 'import_from_statement': 2, 'from': 2, ',': 6, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9220450449751959,0.9213457308241076,"(tensor([0.9945]), tensor([0.9980]), tensor([0.9963]), tensor([0.9977]))"
"95     Ensures that a given function is executed after an update happens, but delay the execution until
96     there are no further updates for a certain time period
97     """"""
98     def __init__(self, fn: Callable[[], Any], timePeriodSecs):
99         """"""
100         :param fn: the function to eventually call after an update
101         :param timePeriodSecs: the time that must pass while not receiving further updates for fn to be called
102         """"""
103         self.fn = fn
104         self.timePeriodSecs = timePeriodSecs
105         self._lastUpdateTime = None
106         self._thread = None
107         self._threadLock = threading.Lock()
108 
109     def handleUpdate(self):
","95     Ensures that a given function is executed after an update happens, but delay the execution until
96     there are no further updates for a certain time period
97     """"""
98     def __init__(self, fn: Callable[[], Any], timePeriodSecs, periodicallyExecutedFn: Optional[Callable[[], Any]] = None):
99         """"""
100         :param fn: the function to eventually call after an update
101         :param timePeriodSecs: the time that must pass while not receiving further updates for fn to be called
102         :param periodicallyExecutedFn: a function to execute periodically (every timePeriodSecs seconds) in the busy waiting loop,
103             which may, for example, log information or apply additional executions, which must not interfere with the correctness of
104             the execution of fn
105         """"""
106         self.periodicallyExecutedFn = periodicallyExecutedFn
107         self.fn = fn
108         self.timePeriodSecs = timePeriodSecs
109         self._lastUpdateTime = None
110         self._thread = None
111         self._threadLock = threading.Lock()
112 
113     def handleUpdate(self):
","Before: 98
After: 98, 102, 103, 104, 106",add periodicexecutedfn option to delayedupdatehook,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,563,"{'module': 1, 'ERROR': 6, 'identifier': 48, 'expression_statement': 1, 'comparison_operator': 1, 'is': 1, ',': 1, 'for': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 4, 'pass_statement': 1, 'pass': 1, 'not': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5491266580960313,0.5510004824504426,"(tensor([0.8794]), tensor([0.9701]), tensor([0.9225]), tensor([0.9602]))"
"1 import collections
2 import functools
3 import pickle
4 from abc import ABC, abstractmethod
5 from concurrent.futures.thread import ThreadPoolExecutor
6 from typing import Callable, Dict, Union, Any, List, Sequence
7 import json
8 import logging
9 import re
10 import threading
","2 import collections
3 import functools
4 import pickle
5 from abc import ABC, abstractmethod
6 from concurrent.futures.thread import ThreadPoolExecutor
7 from typing import Callable, Dict, Union, Any, List, Sequence, Generator, Optional
8 import json
9 import logging
10 import re
11 import threading
","Before: 6
After: 7",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,57,"{'module': 1, 'import_statement': 6, 'import': 9, 'dotted_name': 18, 'identifier': 20, 'import_from_statement': 3, 'from': 3, ',': 6, '.': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5113255752185716,0.5071960939456881,"(tensor([0.9796]), tensor([0.9837]), tensor([0.9816]), tensor([0.9833]))"
"6 from typing import Callable, Dict, Union, Any, List, Sequence
7 import json
8 import logging
9 import re
10 import threading
11 import time
12 
13 from azure.storage.table import TableService, TableBatch, Entity
14 from azure.storage.blob import BlockBlobService
15 import pandas as pd
","7 from typing import Callable, Dict, Union, Any, List, Sequence, Generator, Optional
8 import json
9 import logging
10 import re
11 import threading
12 
13 
14 from azure.storage.table import TableService, TableBatch, Entity
15 from azure.storage.blob import BlockBlobService
16 import pandas as pd
","Before: 11
After: 12",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,77,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 18, 'identifier': 22, 'import': 8, ',': 7, 'import_statement': 5, '.': 4}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5666814351840956,0.5629061902433995,"(tensor([0.9793]), tensor([0.9800]), tensor([0.9797]), tensor([0.9800]))"
"12 
13 from azure.storage.table import TableService, TableBatch, Entity
14 from azure.storage.blob import BlockBlobService
15 import pandas as pd
16 
17 from .cache import PersistentKeyValueCache
18 
19 
20 AZURE_ALLOWED_TABLE_NAME_PATTERN = re.compile(""^[A-Za-z][A-Za-z0-9]{2,62}$"")
21 AZURE_ALLOWED_TABLE_BATCH_SIZE = 100
","13 
14 from azure.storage.table import TableService, TableBatch, Entity
15 from azure.storage.blob import BlockBlobService
16 import pandas as pd
17 
18 from .cache import PersistentKeyValueCache, DelayedUpdateHook
19 
20 AZURE_ALLOWED_TABLE_NAME_PATTERN = re.compile(""^[A-Za-z][A-Za-z0-9]{2,62}$"")
21 AZURE_ALLOWED_TABLE_BATCH_SIZE = 100
22 
","Before: 17, 18
After: 18",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,122,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 9, 'identifier': 17, '.': 6, 'import': 4, ',': 2, 'import_statement': 1, 'aliased_import': 1, 'as': 1, 'relative_import': 1, 'import_prefix': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 1, '(': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7278164701320545,0.7270433124597997,"(tensor([0.9718]), tensor([0.9856]), tensor([0.9787]), tensor([0.9842]))"
"52                             command(batch)
53 
54         def __init__(self):
55             self.partitionCommandsQueue = []
56             self.partitionKey2Commands = {}
57 
58         def addCommand(self, partitionKey, command: Callable[[TableBatch], Any]):
59             if partitionKey not in self.partitionKey2Commands:
60                 commands = self.PartitionCommands(partitionKey)
61                 self.partitionCommandsQueue.append(commands)
","51                         for command in _slice:
52                             command(batch)
53 
54         def __init__(self):
55             self.partitionCommandsQueue = []
56             self.partitionKey2Commands = {}
57             self._threadLock = threading.Lock()
58 
59         def addCommand(self, partitionKey, command: Union[Callable[[TableBatch], Any], functools.partial[TableBatch]]):
","Before: 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69
After: 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,426,"{'module': 1, 'expression_statement': 4, 'call': 2, 'identifier': 22, 'argument_list': 2, '(': 4, ')': 4, 'function_definition': 2, 'def': 2, 'parameters': 2, ':': 4, 'block': 3, 'assignment': 3, 'attribute': 4, '.': 4, '=': 3, 'list': 2, '[': 3, ']': 3, 'dictionary': 1, '{': 1, '}': 1, ',': 3, 'typed_parameter': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'not in': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.62346216986067,0.6096500169242008,"(tensor([0.8730]), tensor([0.9002]), tensor([0.8864]), tensor([0.8974]))"
"71         def isEmpty(self):
72             return len(self.partitionCommandsQueue) == 0
73 
74         def _getIndexOfMaxPriority(self):
75             lengthsList = list(map(len, self.partitionCommandsQueue))
76             return lengthsList.index(max(lengthsList))
77 
78     def __init__(self, tableName: str, tableService: TableService):
","100         def _isEmpty(self):
101             return len(self.partitionCommandsQueue) == 0
102 
103         def _getMaxPriorityInfo(self):
104             lengthsList = list(map(len, self.partitionCommandsQueue))
105             maxLength = max(lengthsList)
106             return maxLength, lengthsList.index(maxLength)
107 
108     def __init__(self, tableName: str, tableService: TableService):
","Before: 74
After: 103",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,607,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 7, ')': 7, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'comparison_operator': 1, 'call': 5, 'argument_list': 5, 'attribute': 3, '.': 3, '==': 1, 'integer': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5534074871684472,0.4940327673116284,"(tensor([0.9338]), tensor([0.9467]), tensor([0.9402]), tensor([0.9454]))"
"71         def isEmpty(self):
72             return len(self.partitionCommandsQueue) == 0
73 
74         def _getIndexOfMaxPriority(self):
75             lengthsList = list(map(len, self.partitionCommandsQueue))
76             return lengthsList.index(max(lengthsList))
77 
78     def __init__(self, tableName: str, tableService: TableService):
","100         def _isEmpty(self):
101             return len(self.partitionCommandsQueue) == 0
102 
103         def _getMaxPriorityInfo(self):
104             lengthsList = list(map(len, self.partitionCommandsQueue))
105             maxLength = max(lengthsList)
106             return maxLength, lengthsList.index(maxLength)
107 
108     def __init__(self, tableName: str, tableService: TableService):
","Before: 76
After: 105, 106",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,643,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 17, 'parameters': 2, '(': 7, ')': 7, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'comparison_operator': 1, 'call': 5, 'argument_list': 5, 'attribute': 3, '.': 3, '==': 1, 'integer': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5534074871684472,0.4940327673116284,"(tensor([0.9338]), tensor([0.9467]), tensor([0.9402]), tensor([0.9454]))"
"119         """"""
120         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
121 
122     def commit(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE, minSecsBetweenPartitionCommits: Union[int, float] = None):
123         """"""
124         Commit insertion commands. Commands are executed batch-wise per partition
125         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
126         :param minSecsBetweenPartitionCommits: min seconds to wait between two partitions to be committed.
127                In case the number of commands to be committed is dynamic, increasing this can lead to larger batches and
128                fewer requests
129         """"""
130 
131         if maxBatchSize > AZURE_ALLOWED_TABLE_BATCH_SIZE:
132             _log.warning(f""Provided maxBatchSize is larger than allowed size {AZURE_ALLOWED_TABLE_BATCH_SIZE}. Will use maxBatchSize {AZURE_ALLOWED_TABLE_BATCH_SIZE} instead."")
133 
134         while not self._partitionQueues.isEmpty():
135             if minSecsBetweenPartitionCommits is not None:
136                 time.sleep(minSecsBetweenPartitionCommits)
137             commands = self._partitionQueues.pop()
138             commands.execute(self._contextManager, maxBatchSize)
139 
140     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
","149         """"""
150         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
151 
152     def commitBlockingUntilEmpty(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
153         """"""
154         Commit insertion commands. Commands are executed batch-wise per partition until partition queue is empty in a
155         blocking manner.
156         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
157         """"""
158 
159         maxBatchSize = self._validateMaxBatchSize(maxBatchSize)
160 
161         while not self._partitionQueues.isEmpty():
162             commands = self._partitionQueues.pop()
163             commands.execute(self._contextManager, maxBatchSize)
164 
165     def commitNonBlockingCurrentQueueState(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
","Before: 122
After: 152",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1014,"{'module': 1, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 61, 'attribute': 2, '.': 2, 'binary_operator': 1, '-': 1, ':': 4, 'assignment': 1, 'type': 1, 'for': 1, ',': 2, 'or': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2321140916434626,0.2199478423319654,"(tensor([0.8945]), tensor([0.8162]), tensor([0.8535]), tensor([0.8234]))"
"119         """"""
120         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
121 
122     def commit(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE, minSecsBetweenPartitionCommits: Union[int, float] = None):
123         """"""
124         Commit insertion commands. Commands are executed batch-wise per partition
125         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
126         :param minSecsBetweenPartitionCommits: min seconds to wait between two partitions to be committed.
127                In case the number of commands to be committed is dynamic, increasing this can lead to larger batches and
128                fewer requests
129         """"""
130 
131         if maxBatchSize > AZURE_ALLOWED_TABLE_BATCH_SIZE:
132             _log.warning(f""Provided maxBatchSize is larger than allowed size {AZURE_ALLOWED_TABLE_BATCH_SIZE}. Will use maxBatchSize {AZURE_ALLOWED_TABLE_BATCH_SIZE} instead."")
133 
134         while not self._partitionQueues.isEmpty():
135             if minSecsBetweenPartitionCommits is not None:
136                 time.sleep(minSecsBetweenPartitionCommits)
137             commands = self._partitionQueues.pop()
138             commands.execute(self._contextManager, maxBatchSize)
139 
140     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
","149         """"""
150         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
151 
152     def commitBlockingUntilEmpty(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
153         """"""
154         Commit insertion commands. Commands are executed batch-wise per partition until partition queue is empty in a
155         blocking manner.
156         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
157         """"""
158 
159         maxBatchSize = self._validateMaxBatchSize(maxBatchSize)
160 
161         while not self._partitionQueues.isEmpty():
162             commands = self._partitionQueues.pop()
163             commands.execute(self._contextManager, maxBatchSize)
164 
165     def commitNonBlockingCurrentQueueState(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
","Before: 124
After: 154, 155",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1032,"{'module': 1, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 61, 'attribute': 2, '.': 2, 'binary_operator': 1, '-': 1, ':': 4, 'assignment': 1, 'type': 1, 'for': 1, ',': 2, 'or': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2321140916434626,0.2199478423319654,"(tensor([0.8945]), tensor([0.8162]), tensor([0.8535]), tensor([0.8234]))"
"119         """"""
120         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
121 
122     def commit(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE, minSecsBetweenPartitionCommits: Union[int, float] = None):
123         """"""
124         Commit insertion commands. Commands are executed batch-wise per partition
125         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
126         :param minSecsBetweenPartitionCommits: min seconds to wait between two partitions to be committed.
127                In case the number of commands to be committed is dynamic, increasing this can lead to larger batches and
128                fewer requests
129         """"""
130 
131         if maxBatchSize > AZURE_ALLOWED_TABLE_BATCH_SIZE:
132             _log.warning(f""Provided maxBatchSize is larger than allowed size {AZURE_ALLOWED_TABLE_BATCH_SIZE}. Will use maxBatchSize {AZURE_ALLOWED_TABLE_BATCH_SIZE} instead."")
133 
134         while not self._partitionQueues.isEmpty():
135             if minSecsBetweenPartitionCommits is not None:
136                 time.sleep(minSecsBetweenPartitionCommits)
137             commands = self._partitionQueues.pop()
138             commands.execute(self._contextManager, maxBatchSize)
139 
140     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
","149         """"""
150         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
151 
152     def commitBlockingUntilEmpty(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
153         """"""
154         Commit insertion commands. Commands are executed batch-wise per partition until partition queue is empty in a
155         blocking manner.
156         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
157         """"""
158 
159         maxBatchSize = self._validateMaxBatchSize(maxBatchSize)
160 
161         while not self._partitionQueues.isEmpty():
162             commands = self._partitionQueues.pop()
163             commands.execute(self._contextManager, maxBatchSize)
164 
165     def commitNonBlockingCurrentQueueState(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
","Before: 126, 127, 128, 131, 132
After: 159",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1077,"{'module': 1, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 61, 'attribute': 2, '.': 2, 'binary_operator': 1, '-': 1, ':': 4, 'assignment': 1, 'type': 1, 'for': 1, ',': 2, 'or': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2321140916434626,0.2199478423319654,"(tensor([0.8945]), tensor([0.8162]), tensor([0.8535]), tensor([0.8234]))"
"119         """"""
120         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
121 
122     def commit(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE, minSecsBetweenPartitionCommits: Union[int, float] = None):
123         """"""
124         Commit insertion commands. Commands are executed batch-wise per partition
125         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
126         :param minSecsBetweenPartitionCommits: min seconds to wait between two partitions to be committed.
127                In case the number of commands to be committed is dynamic, increasing this can lead to larger batches and
128                fewer requests
129         """"""
130 
131         if maxBatchSize > AZURE_ALLOWED_TABLE_BATCH_SIZE:
132             _log.warning(f""Provided maxBatchSize is larger than allowed size {AZURE_ALLOWED_TABLE_BATCH_SIZE}. Will use maxBatchSize {AZURE_ALLOWED_TABLE_BATCH_SIZE} instead."")
133 
134         while not self._partitionQueues.isEmpty():
135             if minSecsBetweenPartitionCommits is not None:
136                 time.sleep(minSecsBetweenPartitionCommits)
137             commands = self._partitionQueues.pop()
138             commands.execute(self._contextManager, maxBatchSize)
139 
140     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
","162             commands = self._partitionQueues.pop()
163             commands.execute(self._contextManager, maxBatchSize)
164 
165     def commitNonBlockingCurrentQueueState(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
166         """"""
167         Commit insertion commands. Empties the current PartitionCommandsQueue in a non blocking way.
168         Commands are executed batch-wise per partition.
169         :param maxBatchSize: maximal batch size to use for batch insertion, must be less or equal to batch size allowed by Azure
170         """"""
171 
172         maxBatchSize = self._validateMaxBatchSize(maxBatchSize)
173 
174         def commit():
175             commandsList = self._partitionQueues.popAll()
176             for commands in commandsList:
177                 commands.execute(self._contextManager, maxBatchSize)
178 
179         thread = threading.Thread(target=commit, daemon=False)
180         thread.start()
181 
182     def commitBlockingLargestPartitionFromQueue(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE, minLength=None):
","Before: 135, 136
After: 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1076,"{'module': 1, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 61, 'attribute': 2, '.': 2, 'binary_operator': 1, '-': 1, ':': 4, 'assignment': 1, 'type': 1, 'for': 1, ',': 2, 'or': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.21637266178338724,0.19834743448506353,"(tensor([0.8403]), tensor([0.8098]), tensor([0.8248]), tensor([0.8128]))"
"155                     break
156         return pd.DataFrame(records, columns=columns)
157 
158     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
159         """"""
160 
161         :param rowFilterQuery:
162         :param chunkSize:
163         :param columns:
164         :return:
165         """"""
166         records = []
167         for record in self.iterRecords(columns, rowFilterQuery):
168             records.append(record)
169             if len(records) >= chunkSize:
170                 yield pd.DataFrame(records, columns=columns)
171                 records = []
172 
173     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None):
","216                     break
217         return pd.DataFrame(records, columns=columns)
218 
219     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
220         """"""
221         Get a generator of dataframe chunks
222         :param rowFilterQuery:
223         :param chunkSize:
224         :param columns:
225         :return:
226         """"""
227         records = []
228         for record in self.iterRecords(columns, rowFilterQuery):
229             records.append(record)
230             if len(records) >= chunkSize:
231                 yield pd.DataFrame(records, columns=columns)
232                 records = []
233 
234     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None) -> Generator[Entity, Any, None]:
","Before: 160
After: 221",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1294,"{'module': 1, 'break_statement': 1, 'break': 1, 'return_statement': 1, 'return': 1, 'call': 5, 'attribute': 4, 'identifier': 32, '.': 4, 'argument_list': 5, '(': 6, ',': 6, 'keyword_argument': 2, '=': 6, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 6, 'type': 4, 'typed_default_parameter': 2, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'none': 2, 'block': 3, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 2, 'list': 2, 'for_statement': 1, 'for': 1, 'in': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>=': 1, 'yield': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5956891374495621,0.5620844341710984,"(tensor([0.9280]), tensor([0.9549]), tensor([0.9413]), tensor([0.9521]))"
"170                 yield pd.DataFrame(records, columns=columns)
171                 records = []
172 
173     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None):
174         """"""
175 
176         :param numResults:
177         :param rowFilterQuery:
178         :param columns:
179         :return:
180         """"""
181         columnNamesAsCommaSeparatedString = None
182         if columns is not None:
183             columnNamesAsCommaSeparatedString = "","".join(columns)
184         for record in self.tableService.query_entities(self.tableName, select=columnNamesAsCommaSeparatedString,
185                 filter=rowFilterQuery):
186             yield record
187 
188     def insertDataFrameToTable(self, df: pd.DataFrame, partitionKeyGenerator: Callable[[str], str] = None, numRecords: int = None):
","231                 yield pd.DataFrame(records, columns=columns)
232                 records = []
233 
234     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None) -> Generator[Entity, Any, None]:
235         """"""
236         Get a generator of table entities
237         :param rowFilterQuery:
238         :param columns:
239         :return:
240         """"""
241         columnNamesAsCommaSeparatedString = None
242         if columns is not None:
243             columnNamesAsCommaSeparatedString = "","".join(columns)
244         for record in self.tableService.query_entities(self.tableName, select=columnNamesAsCommaSeparatedString,
245                 filter=rowFilterQuery):
246             yield record
247 
248     def insertDataFrameToTable(self, df: pd.DataFrame, partitionKeyGenerator: Callable[[str], str] = None, numRecords: int = None):
","Before: 173
After: 234",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1399,"{'module': 1, 'expression_statement': 6, 'yield': 4, 'call': 3, 'attribute': 5, 'identifier': 29, '.': 5, 'argument_list': 3, '(': 4, ',': 5, 'keyword_argument': 3, '=': 8, ')': 4, 'assignment': 3, 'list': 1, '[': 2, ']': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 2, ':': 5, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'none': 4, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'for_statement': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5990509345478814,0.5677554961822961,"(tensor([0.9458]), tensor([0.9621]), tensor([0.9539]), tensor([0.9605]))"
"170                 yield pd.DataFrame(records, columns=columns)
171                 records = []
172 
173     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None):
174         """"""
175 
176         :param numResults:
177         :param rowFilterQuery:
178         :param columns:
179         :return:
180         """"""
181         columnNamesAsCommaSeparatedString = None
182         if columns is not None:
183             columnNamesAsCommaSeparatedString = "","".join(columns)
184         for record in self.tableService.query_entities(self.tableName, select=columnNamesAsCommaSeparatedString,
185                 filter=rowFilterQuery):
186             yield record
187 
188     def insertDataFrameToTable(self, df: pd.DataFrame, partitionKeyGenerator: Callable[[str], str] = None, numRecords: int = None):
","231                 yield pd.DataFrame(records, columns=columns)
232                 records = []
233 
234     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None) -> Generator[Entity, Any, None]:
235         """"""
236         Get a generator of table entities
237         :param rowFilterQuery:
238         :param columns:
239         :return:
240         """"""
241         columnNamesAsCommaSeparatedString = None
242         if columns is not None:
243             columnNamesAsCommaSeparatedString = "","".join(columns)
244         for record in self.tableService.query_entities(self.tableName, select=columnNamesAsCommaSeparatedString,
245                 filter=rowFilterQuery):
246             yield record
247 
248     def insertDataFrameToTable(self, df: pd.DataFrame, partitionKeyGenerator: Callable[[str], str] = None, numRecords: int = None):
","Before: 175, 176
After: 236",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1401,"{'module': 1, 'expression_statement': 6, 'yield': 4, 'call': 3, 'attribute': 5, 'identifier': 29, '.': 5, 'argument_list': 3, '(': 4, ',': 5, 'keyword_argument': 3, '=': 8, ')': 4, 'assignment': 3, 'list': 1, '[': 2, ']': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 2, ':': 5, 'type': 3, 'generic_type': 1, 'type_parameter': 1, 'none': 4, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'for_statement': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5990509345478814,0.5677554961822961,"(tensor([0.9458]), tensor([0.9621]), tensor([0.9539]), tensor([0.9605]))"
"232 
233 
234 class BlobPerKeyAzureTableBlobBackend(AzureTableBlobBackend, ABC):
235 
236     """"""
237     Backend stores serialised values in the structure /tableName/partitionKey/rowKey/valueName.<fileExtension>
238     """"""
239 
240     def __init__(self, blockBlobService: BlockBlobService, containerName: str):
241         """"""
","292 
293 
294 class BlobPerKeyAzureTableBlobBackend(AzureTableBlobBackend, ABC):
295 
296     """"""
297     Backend stores serialised values as /tableName/partitionKey/rowKey/valueName.<fileExtension>
298     or /tableName/rowKey/valueName.<fileExtension>, if partitionKey equals tableName
299     """"""
300 
301     def __init__(self, blockBlobService: BlockBlobService, containerName: str):
","Before: 237
After: 297, 298",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,1850,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 9, 'argument_list': 1, '(': 2, ',': 3, ')': 2, ':': 4, 'block': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, 'type': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4710012314144844,0.4241201896657468,"(tensor([0.9373]), tensor([0.9453]), tensor([0.9413]), tensor([0.9445]))"
"376 class BlobBackedAzureLazyCommitTable(AzureLazyBatchCommitTable):
377 
378     """"""
379     Wrapper of an Azure table, which allow for convenient insertion via lazy batch execution per partition.
380     Uses a priority queue to manage order of partitions to be committed.
381     Can be equipped with a blob storage backend, to allow storage of entity properties, which are too large for table storage.
382     To execute insertions, call :func:`LazyBatchCommitTable.commit`
383     """"""
384 
385     def __init__(self, tableName, tableService: TableService, blobBackedProperties: Sequence[BlobBackedProperty] = ()):
","450 class BlobBackedAzureLazyCommitTable(AzureLazyBatchCommitTable):
451 
452     """"""
453     Wrapper of an Azure table, which allow for convenient insertion via lazy batch execution per partition.
454     Uses a priority queue to manage order of partitions to be committed.
455     Can be equipped with blob backed properties, to allow storage, which do not match table storage model, via reference to a blob.
456     To execute insertions, call :func:`LazyBatchCommitTable.commit`
457     """"""
458 
459     def __init__(self, tableName, tableService: TableService, blobBackedProperties: Sequence[BlobBackedProperty] = ()):
","Before: 381
After: 455",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3218,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 2, 'argument_list': 1, '(': 1, ')': 1, ':': 1, 'block': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6544378079924097,0.6418437672009638,"(tensor([0.9546]), tensor([0.9571]), tensor([0.9558]), tensor([0.9568]))"
"382     To execute insertions, call :func:`LazyBatchCommitTable.commit`
383     """"""
384 
385     def __init__(self, tableName, tableService: TableService, blobBackedProperties: Sequence[BlobBackedProperty] = ()):
386         """"""
387 
388         :param tableName:
389         :param tableService:
390         :param blobBackedProperties:
391         """"""
392         self.blobBackedProperties = blobBackedProperties
393 
394         super().__init__(tableName, tableService)
395 
396     def getEntity(self, partitionKey: str, rowKey: str):
","456     To execute insertions, call :func:`LazyBatchCommitTable.commit`
457     """"""
458 
459     def __init__(self, tableName, tableService: TableService, blobBackedProperties: Sequence[BlobBackedProperty] = ()):
460         """"""
461 
462         :param tableName: name of table
463         :param tableService: instance of :class:`azure.storage.table.TableService` to connect to Azure table storage
464         :param blobBackedProperties:
465         """"""
466         self.blobBackedProperties = blobBackedProperties
467 
468         super().__init__(tableName, tableService)
469 
470     def getEntity(self, partitionKey: str, rowKey: str):
","Before: 388, 389
After: 462, 463",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3196,"{'module': 1, 'ERROR': 5, 'identifier': 18, 'expression_statement': 4, 'assignment': 2, 'pattern_list': 1, ',': 2, ':': 8, 'type': 8, 'constrained_type': 3, 'string': 2, 'string_start': 3, 'string_content': 2, 'string_end': 2, 'attribute': 2, '.': 2, '=': 1, 'call': 2, 'argument_list': 2, '(': 2, ')': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5636716418791115,0.5178321144768367,"(tensor([0.9152]), tensor([0.9528]), tensor([0.9336]), tensor([0.9489]))"
"426     """"""
427     CACHE_VALUE_IDENTIFIER = ""cache_value""
428 
429     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
430             maxBatchSize=100, deferredCommitDelaySecs=10.0, inMemory=False, blobBackend: AzureTableBlobBackend = None):
431         """"""
432 
433         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
434         :param jsonSerialisedValues: boolean flag, to indicate if values must be serialised via json
435         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
436         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
437         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
438         :param blobBackend: if not None, blob storage will be used to store actual value and filed cache_value in table only contains a reference
439         """"""
440 
441         self._deferredCommitDelaySecs = deferredCommitDelaySecs
442         self._partitionKeyGenerator = partitionKeyGenerator
443         self._batchCommitTable = self._getBatchCommitTable(tableName, tableService, blobBackend)
444         self._maxBatchSize = maxBatchSize
445         self._commitThread = None
446         self._commitThreadSemaphore = threading.Semaphore()
447         self._numEntriesToBeCommitted = 0
448         self._lastUpdateTime = None
449         self._inMemoryDf = None
450         self._lastUpdateTime = time.time()
451 
452         if inMemory:
453             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
454             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
455             self._inMemoryDf = df
456 
457     def _getBatchCommitTable(self, tableName, tableService, blobBackend: AzureTableBlobBackend):
","500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","Before: 430
After: 504, 505, 508",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3575,"{'ERROR': 19, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 14, 'identifier': 106, 'assignment': 2, 'type': 4, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 8, '.': 8, '/': 10, '-': 9, '=': 1, ',': 7, 'if': 4, 'for': 1, 'none': 2, 'comparison_operator': 2, 'in': 3, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.45100459902607026,0.43106686037175723,"(tensor([0.8684]), tensor([0.9066]), tensor([0.8871]), tensor([0.9026]))"
"426     """"""
427     CACHE_VALUE_IDENTIFIER = ""cache_value""
428 
429     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
430             maxBatchSize=100, deferredCommitDelaySecs=10.0, inMemory=False, blobBackend: AzureTableBlobBackend = None):
431         """"""
432 
433         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
434         :param jsonSerialisedValues: boolean flag, to indicate if values must be serialised via json
435         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
436         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
437         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
438         :param blobBackend: if not None, blob storage will be used to store actual value and filed cache_value in table only contains a reference
439         """"""
440 
441         self._deferredCommitDelaySecs = deferredCommitDelaySecs
442         self._partitionKeyGenerator = partitionKeyGenerator
443         self._batchCommitTable = self._getBatchCommitTable(tableName, tableService, blobBackend)
444         self._maxBatchSize = maxBatchSize
445         self._commitThread = None
446         self._commitThreadSemaphore = threading.Semaphore()
447         self._numEntriesToBeCommitted = 0
448         self._lastUpdateTime = None
449         self._inMemoryDf = None
450         self._lastUpdateTime = time.time()
451 
452         if inMemory:
453             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
454             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
455             self._inMemoryDf = df
456 
457     def _getBatchCommitTable(self, tableName, tableService, blobBackend: AzureTableBlobBackend):
","500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","Before: 434
After: 512, 513, 514, 515",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3665,"{'ERROR': 19, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 14, 'identifier': 106, 'assignment': 2, 'type': 4, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 8, '.': 8, '/': 10, '-': 9, '=': 1, ',': 7, 'if': 4, 'for': 1, 'none': 2, 'comparison_operator': 2, 'in': 3, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.45100459902607026,0.43106686037175723,"(tensor([0.8684]), tensor([0.9066]), tensor([0.8871]), tensor([0.9026]))"
"426     """"""
427     CACHE_VALUE_IDENTIFIER = ""cache_value""
428 
429     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
430             maxBatchSize=100, deferredCommitDelaySecs=10.0, inMemory=False, blobBackend: AzureTableBlobBackend = None):
431         """"""
432 
433         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
434         :param jsonSerialisedValues: boolean flag, to indicate if values must be serialised via json
435         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
436         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
437         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
438         :param blobBackend: if not None, blob storage will be used to store actual value and filed cache_value in table only contains a reference
439         """"""
440 
441         self._deferredCommitDelaySecs = deferredCommitDelaySecs
442         self._partitionKeyGenerator = partitionKeyGenerator
443         self._batchCommitTable = self._getBatchCommitTable(tableName, tableService, blobBackend)
444         self._maxBatchSize = maxBatchSize
445         self._commitThread = None
446         self._commitThreadSemaphore = threading.Semaphore()
447         self._numEntriesToBeCommitted = 0
448         self._lastUpdateTime = None
449         self._inMemoryDf = None
450         self._lastUpdateTime = time.time()
451 
452         if inMemory:
453             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
454             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
455             self._inMemoryDf = df
456 
457     def _getBatchCommitTable(self, tableName, tableService, blobBackend: AzureTableBlobBackend):
","500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","Before: 438
After: 517, 518",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3806,"{'ERROR': 19, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 14, 'identifier': 106, 'assignment': 2, 'type': 4, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 8, '.': 8, '/': 10, '-': 9, '=': 1, ',': 7, 'if': 4, 'for': 1, 'none': 2, 'comparison_operator': 2, 'in': 3, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.45100459902607026,0.43106686037175723,"(tensor([0.8684]), tensor([0.9066]), tensor([0.8871]), tensor([0.9026]))"
"426     """"""
427     CACHE_VALUE_IDENTIFIER = ""cache_value""
428 
429     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
430             maxBatchSize=100, deferredCommitDelaySecs=10.0, inMemory=False, blobBackend: AzureTableBlobBackend = None):
431         """"""
432 
433         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
434         :param jsonSerialisedValues: boolean flag, to indicate if values must be serialised via json
435         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
436         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
437         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
438         :param blobBackend: if not None, blob storage will be used to store actual value and filed cache_value in table only contains a reference
439         """"""
440 
441         self._deferredCommitDelaySecs = deferredCommitDelaySecs
442         self._partitionKeyGenerator = partitionKeyGenerator
443         self._batchCommitTable = self._getBatchCommitTable(tableName, tableService, blobBackend)
444         self._maxBatchSize = maxBatchSize
445         self._commitThread = None
446         self._commitThreadSemaphore = threading.Semaphore()
447         self._numEntriesToBeCommitted = 0
448         self._lastUpdateTime = None
449         self._inMemoryDf = None
450         self._lastUpdateTime = time.time()
451 
452         if inMemory:
453             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
454             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
455             self._inMemoryDf = df
456 
457     def _getBatchCommitTable(self, tableName, tableService, blobBackend: AzureTableBlobBackend):
","500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","Before: 443
After: 523, 524, 525, 526, 527, 528, 529",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3616,"{'ERROR': 19, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 14, 'identifier': 106, 'assignment': 2, 'type': 4, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 8, '.': 8, '/': 10, '-': 9, '=': 1, ',': 7, 'if': 4, 'for': 1, 'none': 2, 'comparison_operator': 2, 'in': 3, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.45100459902607026,0.43106686037175723,"(tensor([0.8684]), tensor([0.9066]), tensor([0.8871]), tensor([0.9026]))"
"426     """"""
427     CACHE_VALUE_IDENTIFIER = ""cache_value""
428 
429     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
430             maxBatchSize=100, deferredCommitDelaySecs=10.0, inMemory=False, blobBackend: AzureTableBlobBackend = None):
431         """"""
432 
433         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
434         :param jsonSerialisedValues: boolean flag, to indicate if values must be serialised via json
435         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
436         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
437         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
438         :param blobBackend: if not None, blob storage will be used to store actual value and filed cache_value in table only contains a reference
439         """"""
440 
441         self._deferredCommitDelaySecs = deferredCommitDelaySecs
442         self._partitionKeyGenerator = partitionKeyGenerator
443         self._batchCommitTable = self._getBatchCommitTable(tableName, tableService, blobBackend)
444         self._maxBatchSize = maxBatchSize
445         self._commitThread = None
446         self._commitThreadSemaphore = threading.Semaphore()
447         self._numEntriesToBeCommitted = 0
448         self._lastUpdateTime = None
449         self._inMemoryDf = None
450         self._lastUpdateTime = time.time()
451 
452         if inMemory:
453             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
454             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
455             self._inMemoryDf = df
456 
457     def _getBatchCommitTable(self, tableName, tableService, blobBackend: AzureTableBlobBackend):
","500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","Before: 445, 446, 447, 448
After: 531, 532",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3632,"{'ERROR': 19, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 14, 'identifier': 106, 'assignment': 2, 'type': 4, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 8, '.': 8, '/': 10, '-': 9, '=': 1, ',': 7, 'if': 4, 'for': 1, 'none': 2, 'comparison_operator': 2, 'in': 3, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.45100459902607026,0.43106686037175723,"(tensor([0.8684]), tensor([0.9066]), tensor([0.8871]), tensor([0.9026]))"
"426     """"""
427     CACHE_VALUE_IDENTIFIER = ""cache_value""
428 
429     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
430             maxBatchSize=100, deferredCommitDelaySecs=10.0, inMemory=False, blobBackend: AzureTableBlobBackend = None):
431         """"""
432 
433         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
434         :param jsonSerialisedValues: boolean flag, to indicate if values must be serialised via json
435         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
436         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
437         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
438         :param blobBackend: if not None, blob storage will be used to store actual value and filed cache_value in table only contains a reference
439         """"""
440 
441         self._deferredCommitDelaySecs = deferredCommitDelaySecs
442         self._partitionKeyGenerator = partitionKeyGenerator
443         self._batchCommitTable = self._getBatchCommitTable(tableName, tableService, blobBackend)
444         self._maxBatchSize = maxBatchSize
445         self._commitThread = None
446         self._commitThreadSemaphore = threading.Semaphore()
447         self._numEntriesToBeCommitted = 0
448         self._lastUpdateTime = None
449         self._inMemoryDf = None
450         self._lastUpdateTime = time.time()
451 
452         if inMemory:
453             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
454             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
455             self._inMemoryDf = df
456 
457     def _getBatchCommitTable(self, tableName, tableService, blobBackend: AzureTableBlobBackend):
","537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
541         keyAsString = str(key)
542         partitionKey = self._getPartitionKeyForRowKey(keyAsString)
543         entity = {'PartitionKey': partitionKey, 'RowKey': keyAsString, self.CACHE_VALUE_IDENTIFIER: value}
544         self._batchCommitTable.insertOrReplaceEntity(entity)
545         self._updateHook.handleUpdate()
546 
547         if self._inMemoryDf is not None:
548             self._inMemoryDf.loc[keyAsString] = [value]
549 
550     def get(self, key):
","Before: 450, 457, 458, 459, 460, 461, 467, 468
After: 545",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,3686,"{'ERROR': 19, 'expression_statement': 3, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 14, 'identifier': 106, 'assignment': 2, 'type': 4, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 8, '.': 8, '/': 10, '-': 9, '=': 1, ',': 7, 'if': 4, 'for': 1, 'none': 2, 'comparison_operator': 2, 'in': 3, 'not_operator': 1, 'not': 1, 'and': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.04837141040331034,0.04479565861029128,"(tensor([0.7919]), tensor([0.6953]), tensor([0.7405]), tensor([0.7039]))"
"498     def _getPartitionKeyForRowKey(self, key: str):
499         return self._batchCommitTable.tableName if self._partitionKeyGenerator is None else self._partitionKeyGenerator(key)
500 
501     def _commitDeferred(self):
502 
503         def doCommit():
504             self._batchCommitTable.commit(self._maxBatchSize, self._deferredCommitDelaySecs)
505 
506         if self._commitThread is None or not self._commitThread.is_alive():
507             self._commitThreadSemaphore.acquire()
508             if self._commitThread is None or not self._commitThread.is_alive():
509                 self._commitThread = threading.Thread(target=doCommit, daemon=False)
510                 self._commitThread.start()
511             self._commitThreadSemaphore.release()","575     def _getPartitionKeyForRowKey(self, key: str):
576         return self._batchCommitTable.tableName if self._partitionKeyGenerator is None else self._partitionKeyGenerator(key)
577 
578     def _commit(self):
579         self._batchCommitTable.commitNonBlockingCurrentQueueState(self._maxBatchSize)
580 
581     def _periodicallyCommit(self):
","Before: 501, 502, 503, 504
After: 578, 579",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,4254,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 42, 'parameters': 3, '(': 9, ',': 3, 'typed_parameter': 1, ':': 6, 'type': 1, ')': 9, 'block': 5, 'return_statement': 1, 'return': 1, 'conditional_expression': 1, 'attribute': 18, '.': 18, 'if': 3, 'comparison_operator': 3, 'is': 3, 'none': 3, 'else': 1, 'call': 6, 'argument_list': 6, 'expression_statement': 3, 'if_statement': 2, 'boolean_operator': 2, 'or': 2, 'not_operator': 2, 'not': 2, 'assignment': 1, '=': 3, 'keyword_argument': 2, 'false': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.18749785791842716,0.16787136234183356,"(tensor([0.9179]), tensor([0.7833]), tensor([0.8453]), tensor([0.7950]))"
"498     def _getPartitionKeyForRowKey(self, key: str):
499         return self._batchCommitTable.tableName if self._partitionKeyGenerator is None else self._partitionKeyGenerator(key)
500 
501     def _commitDeferred(self):
502 
503         def doCommit():
504             self._batchCommitTable.commit(self._maxBatchSize, self._deferredCommitDelaySecs)
505 
506         if self._commitThread is None or not self._commitThread.is_alive():
507             self._commitThreadSemaphore.acquire()
508             if self._commitThread is None or not self._commitThread.is_alive():
509                 self._commitThread = threading.Thread(target=doCommit, daemon=False)
510                 self._commitThread.start()
511             self._commitThreadSemaphore.release()","578     def _commit(self):
579         self._batchCommitTable.commitNonBlockingCurrentQueueState(self._maxBatchSize)
580 
581     def _periodicallyCommit(self):
582         self._batchCommitTable.commitBlockingLargestPartitionFromQueue(self._maxBatchSize, self._minSizeForPeriodicCommit)
","Before: 506, 507, 508, 509, 510, 511
After: 581, 582",add a thread-safe lock to cache azure table commands,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,4309,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 42, 'parameters': 3, '(': 9, ',': 3, 'typed_parameter': 1, ':': 6, 'type': 1, ')': 9, 'block': 5, 'return_statement': 1, 'return': 1, 'conditional_expression': 1, 'attribute': 18, '.': 18, 'if': 3, 'comparison_operator': 3, 'is': 3, 'none': 3, 'else': 1, 'call': 6, 'argument_list': 6, 'expression_statement': 3, 'if_statement': 2, 'boolean_operator': 2, 'or': 2, 'not_operator': 2, 'not': 2, 'assignment': 1, '=': 3, 'keyword_argument': 2, 'false': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.016911959051671206,0.008736812646770894,"(tensor([0.8265]), tensor([0.7016]), tensor([0.7589]), tensor([0.7123]))"
"72         value = pickle.loads(storedValue) if self.isCacheValuePickled else storedValue
73         return value
74 
75     def _getFromInMemoryDf(self, key):
76         if self._inMemoryDf is None:
77             return None
78         try:
79             return self._inMemoryDf[""cache_value""][str(key)]
80         except Exception as e:
81             _log.debug(f""Unable to load value for key {str(key)} from in-memory dataframe: {e}"")
82             return None
83 
84     def _commit(self):
","72         value = pickle.loads(storedValue) if self.isCacheValuePickled else storedValue
73         return value
74 
75     def _getFromInMemoryDf(self, key):
76         if self._inMemoryDf is None:
77             return None
78         try:
79             return self._inMemoryDf[""cache_value""][str(key)]
80         except Exception as e:
81             log.debug(f""Unable to load value for key {str(key)} from in-memory dataframe: {e}"")
82             return None
83 
84     def _commit(self):
","Before: 81
After: 81",fix typo in mysql/util/cache_mysql.py,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_mysql.py,998b0e3a0aa4f6aeb5ca0736e462f5afd7cf095a,ca9e82e81e4d1e399a927dc6ad466189db650d57,0,951,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 24, '=': 1, 'conditional_expression': 1, 'call': 4, 'attribute': 5, '.': 5, 'argument_list': 4, '(': 5, ')': 5, 'if': 2, 'else': 1, 'return_statement': 4, 'return': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 4, 'block': 4, 'if_statement': 1, 'comparison_operator': 1, 'is': 1, 'none': 3, 'try_statement': 1, 'try': 1, 'subscript': 2, '[': 2, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, ']': 2, 'except_clause': 1, 'except': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, 'interpolation': 2, '{': 2, '}': 2}","{'cyclomatic_complexity': 4, 'nloc': 14, 'token_count': 154, 'name': '__init__', 'long_name': '__init__( self , host , db , user , pw , valueType : ValueType , tableName = ""cache"" , deferredCommitDelaySecs = 1 . 0 , inMemory = False )', 'start_line': 17, 'end_line': 33, 'full_parameters': ['self', ' host', ' db', ' user', ' pw', ' valueType : ValueType', ' tableName = ""cache""', ' deferredCommitDelaySecs = 1 . 0', ' inMemory = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_mysql.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 14, 'token_count': 154, 'name': '__init__', 'long_name': '__init__( self , host , db , user , pw , valueType : ValueType , tableName = ""cache"" , deferredCommitDelaySecs = 1 . 0 , inMemory = False )', 'start_line': 17, 'end_line': 33, 'full_parameters': ['self', ' host', ' db', ' user', ' pw', ' valueType : ValueType', ' tableName = ""cache""', ' deferredCommitDelaySecs = 1 . 0', ' inMemory = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_mysql.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9724430921855302,0.9705105985964099,"(tensor([0.9986]), tensor([0.9986]), tensor([0.9986]), tensor([0.9986]))"
"2 import collections
3 import functools
4 import pickle
5 from abc import ABC, abstractmethod
6 from concurrent.futures.thread import ThreadPoolExecutor
7 from typing import Callable, Dict, Union, Any, List, Sequence, Generator, Optional
8 import json
9 import logging
10 import re
11 import threading
","3 import functools
4 import pickle
5 import sys
6 from abc import ABC, abstractmethod
7 from concurrent.futures.thread import ThreadPoolExecutor
8 from typing import Callable, Dict, Union, Any, List, Sequence, Optional
9 import json
10 import logging
11 import re
12 import threading
","Before: 7
After: 8",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,69,"{'module': 1, 'import_statement': 6, 'import': 9, 'dotted_name': 20, 'identifier': 22, 'import_from_statement': 3, 'from': 3, ',': 8, '.': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6061337180503953,0.5985817892111068,"(tensor([0.9791]), tensor([0.9762]), tensor([0.9776]), tensor([0.9764]))"
"13 
14 from azure.storage.table import TableService, TableBatch, Entity
15 from azure.storage.blob import BlockBlobService
16 import pandas as pd
17 
18 from .cache import PersistentKeyValueCache, PeriodicUpdateHook
19 
20 AZURE_ALLOWED_TABLE_NAME_PATTERN = re.compile(""^[A-Za-z][A-Za-z0-9]{2,62}$"")
21 AZURE_ALLOWED_TABLE_BATCH_SIZE = 100
22 
","15 from azure.storage.table import TableService, TableBatch, Entity
16 from azure.storage.blob import BlockBlobService
17 import pandas as pd
18 import numpy as np
19 
20 from .cache import PersistentKeyValueCache, DelayedUpdateHook
21 
22 AZURE_ALLOWED_TABLE_NAME_PATTERN = re.compile(""^[A-Za-z][A-Za-z0-9]{2,62}$"")
23 AZURE_ALLOWED_TABLE_BATCH_SIZE = 100
24 
","Before: 18
After: 20",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,133,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 10, 'identifier': 19, '.': 6, 'import': 4, ',': 3, 'import_statement': 1, 'aliased_import': 1, 'as': 1, 'relative_import': 1, 'import_prefix': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'call': 1, 'attribute': 1, 'argument_list': 1, '(': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ')': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6163348020344082,0.612782292113029,"(tensor([0.9679]), tensor([0.9736]), tensor([0.9708]), tensor([0.9731]))"
"88             with self._threadLock:
89                 return self._isEmpty()
90 
91         def _pop(self, minLength=None):
92             length, index = self._getMaxPriorityInfo()
93             if minLength is None or length >= minLength:
94                 q = self.partitionCommandsQueue.pop(index)
95                 del self.partitionKey2Commands[q.partitionKey]
96                 return q
97             else:
98                 return None
99 
100         def _isEmpty(self):
","397             with self._threadLock:
398                 return self._isEmpty()
399 
400         def _pop(self, minLength=None):
401             length, index = self._getMaxPriorityInfo()
402             if index is not None and (minLength is None or length >= minLength):
403                 q = self.partitionCommandsQueue.pop(index)
404                 del self.partitionKey2Commands[q.partitionKey]
405                 return q
406             else:
407                 return None
408 
409         def _isEmpty(self):
","Before: 93
After: 402",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,769,"{'module': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'attribute': 7, 'identifier': 24, '.': 7, ':': 4, 'block': 4, 'return_statement': 3, 'return': 3, 'call': 3, 'argument_list': 3, '(': 4, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'default_parameter': 1, '=': 3, 'none': 3, 'expression_statement': 2, 'assignment': 2, 'pattern_list': 1, 'if_statement': 1, 'if': 1, 'boolean_operator': 1, 'comparison_operator': 2, 'is': 1, 'or': 1, '>=': 1, 'delete_statement': 1, 'del': 1, 'subscript': 1, '[': 1, ']': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.48745922369569755,0.4663800539961269,"(tensor([0.9415]), tensor([0.9503]), tensor([0.9459]), tensor([0.9494]))"
"105             maxLength = max(lengthsList)
106             return maxLength, lengthsList.index(maxLength)
107 
108     def __init__(self, tableName: str, tableService: TableService):
109         """"""
110         :param tableName: name of table
111         :param tableService: instance of :class:`azure.storage.table.TableService` to connect to Azure table storage
112         """"""
113 
114         if not AZURE_ALLOWED_TABLE_NAME_PATTERN.match(tableName):
115             raise ValueError(f""Invalid table name {tableName}, see: https://docs.microsoft.com/en-us/rest/api/storageservices/Understanding-the-Table-Service-Data-Model"")
116 
117         self.tableService = tableService
118         self.tableName = tableName
119         self._partitionQueues = self.PartitionCommandsPriorityQueue()
120         self._contextManager = functools.partial(self.tableService.batch, self.tableName)
121 
122         if not self.exists():
123             self.tableService.create_table(self.tableName)
124 
125     def insertOrReplaceEntity(self, entity: Union[Dict, Entity]):
","416             maxLength = max(lengthsList)
417             return maxLength, lengthsList.index(maxLength)
418 
419     def __init__(self, tableName: str, tableService: TableService, propertyLoaders: Sequence[PropertyLoader] = ()):
420         """"""
421 
422         :param tableName: name of table
423         :param tableService: instance of :class:`azure.storage.table.TableService` to connect to Azure table storage
424         :param propertyLoaders:
425         """"""
426 
427         if not AZURE_ALLOWED_TABLE_NAME_PATTERN.match(tableName):
428             raise ValueError(f""Invalid table name {tableName}, see: https://docs.microsoft.com/en-us/rest/api/storageservices/Understanding-the-Table-Service-Data-Model"")
429 
430         self.tableService = tableService
431         self.tableName = tableName
432         self.propertyLoaders = propertyLoaders
433         self._partitionQueues = self.PartitionCommandsPriorityQueue()
434         self._contextManager = functools.partial(self.tableService.batch, self.tableName)
435 
436         if not self.exists():
437             self.tableService.create_table(self.tableName)
438 
439     def insertOrReplaceEntity(self, entity: Union[Dict, Entity]):
","Before: 108
After: 419, 421, 424",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,905,"{'module': 1, 'expression_statement': 7, 'assignment': 5, 'identifier': 44, '=': 5, 'call': 8, 'argument_list': 8, '(': 9, ')': 9, 'return_statement': 1, 'return': 1, 'expression_list': 1, ',': 4, 'attribute': 15, '.': 15, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 5, 'type': 2, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'if_statement': 2, 'if': 2, 'not_operator': 2, 'not': 2, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6031616419590815,0.580880124209711,"(tensor([0.9257]), tensor([0.9490]), tensor([0.9372]), tensor([0.9466]))"
"140         executionCommand = functools.partial(self._insertEntityViaBatch, entity)
141         self._partitionQueues.addCommand(partitionKey, executionCommand)
142 
143     def getEntity(self, partitionKey: str, rowKey: str):
144         """"""
145         Wraps :func:`azure.storage.table.TableService.get_entity`
146         :param partitionKey:
147         :param rowKey:
148         :return:
149         """"""
150         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
151 
152     def commitBlockingUntilEmpty(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
","458         executionCommand = functools.partial(self._insertEntityViaBatch, entity)
459         self._partitionQueues.addCommand(partitionKey, executionCommand)
460 
461     def getEntity(self, partitionKey: str, rowKey: str) -> Optional[Entity]:
462         """"""
463         Wraps :func:`azure.storage.table.TableService.get_entity`
464         :param partitionKey:
465         :param rowKey:
466         :return:
467         """"""
468         try:
469             entity = self.tableService.get_entity(self.tableName, partitionKey, rowKey)
470             for propertyLoader in self.propertyLoaders:
471                 propertyLoader.loadPropertyValue(entity)
472             return entity
473         except Exception as e:
474             _log.debug(f""Unable to load value for partitionKey {partitionKey} and rowKey {rowKey} from table {self.tableName}: {e}"")
475             return None
476 
477     def commitBlockingUntilEmpty(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
","Before: 143
After: 461",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1198,"{'module': 1, 'expression_statement': 3, 'assignment': 1, 'identifier': 24, '=': 1, 'call': 3, 'attribute': 7, '.': 7, 'argument_list': 3, '(': 4, ',': 6, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 3, 'type': 2, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3436318562331587,0.3043143929000112,"(tensor([0.8401]), tensor([0.9318]), tensor([0.8836]), tensor([0.9217]))"
"140         executionCommand = functools.partial(self._insertEntityViaBatch, entity)
141         self._partitionQueues.addCommand(partitionKey, executionCommand)
142 
143     def getEntity(self, partitionKey: str, rowKey: str):
144         """"""
145         Wraps :func:`azure.storage.table.TableService.get_entity`
146         :param partitionKey:
147         :param rowKey:
148         :return:
149         """"""
150         return self.tableService.get_entity(self.tableName, partitionKey, rowKey)
151 
152     def commitBlockingUntilEmpty(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
","458         executionCommand = functools.partial(self._insertEntityViaBatch, entity)
459         self._partitionQueues.addCommand(partitionKey, executionCommand)
460 
461     def getEntity(self, partitionKey: str, rowKey: str) -> Optional[Entity]:
462         """"""
463         Wraps :func:`azure.storage.table.TableService.get_entity`
464         :param partitionKey:
465         :param rowKey:
466         :return:
467         """"""
468         try:
469             entity = self.tableService.get_entity(self.tableName, partitionKey, rowKey)
470             for propertyLoader in self.propertyLoaders:
471                 propertyLoader.loadPropertyValue(entity)
472             return entity
473         except Exception as e:
474             _log.debug(f""Unable to load value for partitionKey {partitionKey} and rowKey {rowKey} from table {self.tableName}: {e}"")
475             return None
476 
477     def commitBlockingUntilEmpty(self, maxBatchSize=AZURE_ALLOWED_TABLE_BATCH_SIZE):
","Before: 150
After: 468, 469, 470, 471, 472, 473, 474, 475",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1224,"{'module': 1, 'expression_statement': 3, 'assignment': 1, 'identifier': 24, '=': 1, 'call': 3, 'attribute': 7, '.': 7, 'argument_list': 3, '(': 4, ',': 6, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 3, 'type': 2, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3436318562331587,0.3043143929000112,"(tensor([0.8401]), tensor([0.9318]), tensor([0.8836]), tensor([0.9217]))"
"198             maxBatchSize = AZURE_ALLOWED_TABLE_BATCH_SIZE
199         return maxBatchSize
200 
201     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
202         """"""
203         Load all rows of table to :class:`~pandas.DataFrame`
204         :param rowFilterQuery:
205         :param numRecords:
206         :param columns: restrict loading to provided columns
207         :return: :class:`~pandas.DataFrame`
208         """"""
209         if numRecords is None:
210             records = list(self.iterRecords(columns))
211         else:
212             records = []
213             for record in self.iterRecords(columns, rowFilterQuery):
214                 records.append(record)
215                 if len(records) >= numRecords:
216                     break
217         return pd.DataFrame(records, columns=columns)
218 
219     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
","523             maxBatchSize = AZURE_ALLOWED_TABLE_BATCH_SIZE
524         return maxBatchSize
525 
526     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
527         """"""
528         Load all rows of table to :class:`~pandas.DataFrame`
529         :param rowFilterQuery:
530         :param numRecords:
531         :param columns: restrict loading to provided columns
532         :return: :class:`~pandas.DataFrame`
533         """"""
534         if numRecords is None:
535             records = list(self._iterRecords(columns, rowFilterQuery))
536         else:
537             records = []
538             for record in self._iterRecords(columns, rowFilterQuery):
539                 records.append(record)
540                 if len(records) >= numRecords:
541                     break
542         df = pd.DataFrame(records, columns=columns)
543         for propertyLoader in self.propertyLoaders:
544             propertyLoader.loadPropertyValueToDataFrameColumn(df)
545         return df
546 
547     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
","Before: 210
After: 535",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1609,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 35, '=': 7, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 7, ',': 5, 'typed_default_parameter': 3, ':': 8, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'none': 4, ')': 7, 'block': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'call': 6, 'argument_list': 6, 'attribute': 4, '.': 4, 'else_clause': 1, 'else': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '>=': 1, 'break_statement': 1, 'break': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6017231817467176,0.5775721195832942,"(tensor([0.9072]), tensor([0.9508]), tensor([0.9285]), tensor([0.9463]))"
"198             maxBatchSize = AZURE_ALLOWED_TABLE_BATCH_SIZE
199         return maxBatchSize
200 
201     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
202         """"""
203         Load all rows of table to :class:`~pandas.DataFrame`
204         :param rowFilterQuery:
205         :param numRecords:
206         :param columns: restrict loading to provided columns
207         :return: :class:`~pandas.DataFrame`
208         """"""
209         if numRecords is None:
210             records = list(self.iterRecords(columns))
211         else:
212             records = []
213             for record in self.iterRecords(columns, rowFilterQuery):
214                 records.append(record)
215                 if len(records) >= numRecords:
216                     break
217         return pd.DataFrame(records, columns=columns)
218 
219     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
","523             maxBatchSize = AZURE_ALLOWED_TABLE_BATCH_SIZE
524         return maxBatchSize
525 
526     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
527         """"""
528         Load all rows of table to :class:`~pandas.DataFrame`
529         :param rowFilterQuery:
530         :param numRecords:
531         :param columns: restrict loading to provided columns
532         :return: :class:`~pandas.DataFrame`
533         """"""
534         if numRecords is None:
535             records = list(self._iterRecords(columns, rowFilterQuery))
536         else:
537             records = []
538             for record in self._iterRecords(columns, rowFilterQuery):
539                 records.append(record)
540                 if len(records) >= numRecords:
541                     break
542         df = pd.DataFrame(records, columns=columns)
543         for propertyLoader in self.propertyLoaders:
544             propertyLoader.loadPropertyValueToDataFrameColumn(df)
545         return df
546 
547     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
","Before: 213
After: 538",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1637,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 35, '=': 7, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 7, ',': 5, 'typed_default_parameter': 3, ':': 8, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'none': 4, ')': 7, 'block': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'call': 6, 'argument_list': 6, 'attribute': 4, '.': 4, 'else_clause': 1, 'else': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '>=': 1, 'break_statement': 1, 'break': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6017231817467176,0.5775721195832942,"(tensor([0.9072]), tensor([0.9508]), tensor([0.9285]), tensor([0.9463]))"
"198             maxBatchSize = AZURE_ALLOWED_TABLE_BATCH_SIZE
199         return maxBatchSize
200 
201     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
202         """"""
203         Load all rows of table to :class:`~pandas.DataFrame`
204         :param rowFilterQuery:
205         :param numRecords:
206         :param columns: restrict loading to provided columns
207         :return: :class:`~pandas.DataFrame`
208         """"""
209         if numRecords is None:
210             records = list(self.iterRecords(columns))
211         else:
212             records = []
213             for record in self.iterRecords(columns, rowFilterQuery):
214                 records.append(record)
215                 if len(records) >= numRecords:
216                     break
217         return pd.DataFrame(records, columns=columns)
218 
219     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
","523             maxBatchSize = AZURE_ALLOWED_TABLE_BATCH_SIZE
524         return maxBatchSize
525 
526     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
527         """"""
528         Load all rows of table to :class:`~pandas.DataFrame`
529         :param rowFilterQuery:
530         :param numRecords:
531         :param columns: restrict loading to provided columns
532         :return: :class:`~pandas.DataFrame`
533         """"""
534         if numRecords is None:
535             records = list(self._iterRecords(columns, rowFilterQuery))
536         else:
537             records = []
538             for record in self._iterRecords(columns, rowFilterQuery):
539                 records.append(record)
540                 if len(records) >= numRecords:
541                     break
542         df = pd.DataFrame(records, columns=columns)
543         for propertyLoader in self.propertyLoaders:
544             propertyLoader.loadPropertyValueToDataFrameColumn(df)
545         return df
546 
547     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
","Before: 217
After: 542, 543, 544, 545",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1678,"{'module': 1, 'expression_statement': 5, 'assignment': 3, 'identifier': 35, '=': 7, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 7, ',': 5, 'typed_default_parameter': 3, ':': 8, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'none': 4, ')': 7, 'block': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is': 1, 'call': 6, 'argument_list': 6, 'attribute': 4, '.': 4, 'else_clause': 1, 'else': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '>=': 1, 'break_statement': 1, 'break': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6017231817467176,0.5775721195832942,"(tensor([0.9072]), tensor([0.9508]), tensor([0.9285]), tensor([0.9463]))"
"216                     break
217         return pd.DataFrame(records, columns=columns)
218 
219     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
220         """"""
221         Get a generator of dataframe chunks
222         :param rowFilterQuery:
223         :param chunkSize:
224         :param columns:
225         :return:
226         """"""
227         records = []
228         for record in self.iterRecords(columns, rowFilterQuery):
229             records.append(record)
230             if len(records) >= chunkSize:
231                 yield pd.DataFrame(records, columns=columns)
232                 records = []
233 
234     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None) -> Generator[Entity, Any, None]:
","544             propertyLoader.loadPropertyValueToDataFrameColumn(df)
545         return df
546 
547     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
548         """"""
549         Get a generator of dataframe chunks
550         :param rowFilterQuery:
551         :param chunkSize:
552         :param columns:
553         :return:
554         """"""
555         records = []
556         for record in self._iterRecords(columns, rowFilterQuery):
557             records.append(record)
558             if len(records) >= chunkSize:
559                 df = pd.DataFrame(records, columns=columns)
560                 for propertyLoader in self.propertyLoaders:
561                     propertyLoader.loadPropertyValueToDataFrameColumn(df)
562                 yield df
563                 records = []
564 
565     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None):
","Before: 228
After: 556",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1744,"{'module': 1, 'break_statement': 1, 'break': 1, 'return_statement': 1, 'return': 1, 'call': 5, 'attribute': 4, 'identifier': 32, '.': 4, 'argument_list': 5, '(': 6, ',': 6, 'keyword_argument': 2, '=': 6, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 6, 'type': 4, 'typed_default_parameter': 2, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'none': 2, 'block': 3, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 2, 'list': 2, 'for_statement': 1, 'for': 1, 'in': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>=': 1, 'yield': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5550456851223162,0.521987620106827,"(tensor([0.8721]), tensor([0.9117]), tensor([0.8915]), tensor([0.9075]))"
"216                     break
217         return pd.DataFrame(records, columns=columns)
218 
219     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
220         """"""
221         Get a generator of dataframe chunks
222         :param rowFilterQuery:
223         :param chunkSize:
224         :param columns:
225         :return:
226         """"""
227         records = []
228         for record in self.iterRecords(columns, rowFilterQuery):
229             records.append(record)
230             if len(records) >= chunkSize:
231                 yield pd.DataFrame(records, columns=columns)
232                 records = []
233 
234     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None) -> Generator[Entity, Any, None]:
","544             propertyLoader.loadPropertyValueToDataFrameColumn(df)
545         return df
546 
547     def iterDataFrameChunks(self, chunkSize: int, columns: List[str] = None, rowFilterQuery: str = None):
548         """"""
549         Get a generator of dataframe chunks
550         :param rowFilterQuery:
551         :param chunkSize:
552         :param columns:
553         :return:
554         """"""
555         records = []
556         for record in self._iterRecords(columns, rowFilterQuery):
557             records.append(record)
558             if len(records) >= chunkSize:
559                 df = pd.DataFrame(records, columns=columns)
560                 for propertyLoader in self.propertyLoaders:
561                     propertyLoader.loadPropertyValueToDataFrameColumn(df)
562                 yield df
563                 records = []
564 
565     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None):
","Before: 231
After: 559, 560, 561, 562",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1784,"{'module': 1, 'break_statement': 1, 'break': 1, 'return_statement': 1, 'return': 1, 'call': 5, 'attribute': 4, 'identifier': 32, '.': 4, 'argument_list': 5, '(': 6, ',': 6, 'keyword_argument': 2, '=': 6, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 6, 'type': 4, 'typed_default_parameter': 2, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'none': 2, 'block': 3, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 2, 'list': 2, 'for_statement': 1, 'for': 1, 'in': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>=': 1, 'yield': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5550456851223162,0.521987620106827,"(tensor([0.8721]), tensor([0.9117]), tensor([0.8915]), tensor([0.9075]))"
"231                 yield pd.DataFrame(records, columns=columns)
232                 records = []
233 
234     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None) -> Generator[Entity, Any, None]:
235         """"""
236         Get a generator of table entities
237         :param rowFilterQuery:
238         :param columns:
239         :return:
240         """"""
241         columnNamesAsCommaSeparatedString = None
242         if columns is not None:
243             columnNamesAsCommaSeparatedString = "","".join(columns)
244         for record in self.tableService.query_entities(self.tableName, select=columnNamesAsCommaSeparatedString,
245                 filter=rowFilterQuery):
246             yield record
247 
248     def insertDataFrameToTable(self, df: pd.DataFrame, partitionKeyGenerator: Callable[[str], str] = None, numRecords: int = None):
","562                 yield df
563                 records = []
564 
565     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None):
566         """"""
567 
568         Get a generator of table entities
569         :param rowFilterQuery:
570         :param columns:
571         :return:
572         """"""
573         for entity in self._iterRecords(columns, rowFilterQuery):
574             for propertyLoader in self.propertyLoaders:
575                 propertyLoader.loadPropertyValue(entity)
576             yield entity
577 
578     def _iterRecords(self, columns: Optional[List[str]], rowFilterQuery: Optional[str]):
","Before: 234
After: 565, 567, 573, 574, 575, 576, 577, 578, 579",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1837,"{'module': 1, 'expression_statement': 6, 'yield': 4, 'call': 3, 'attribute': 5, 'identifier': 32, '.': 5, 'argument_list': 3, '(': 4, ',': 7, 'keyword_argument': 3, '=': 8, ')': 4, 'assignment': 3, 'list': 1, '[': 3, ']': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 2, ':': 5, 'type': 7, 'generic_type': 2, 'type_parameter': 2, 'none': 5, '->': 1, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'for_statement': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2845336053005618,0.2572317541889818,"(tensor([0.8493]), tensor([0.7750]), tensor([0.8105]), tensor([0.7819]))"
"231                 yield pd.DataFrame(records, columns=columns)
232                 records = []
233 
234     def iterRecords(self, columns: List[str] = None, rowFilterQuery: str = None) -> Generator[Entity, Any, None]:
235         """"""
236         Get a generator of table entities
237         :param rowFilterQuery:
238         :param columns:
239         :return:
240         """"""
241         columnNamesAsCommaSeparatedString = None
242         if columns is not None:
243             columnNamesAsCommaSeparatedString = "","".join(columns)
244         for record in self.tableService.query_entities(self.tableName, select=columnNamesAsCommaSeparatedString,
245                 filter=rowFilterQuery):
246             yield record
247 
248     def insertDataFrameToTable(self, df: pd.DataFrame, partitionKeyGenerator: Callable[[str], str] = None, numRecords: int = None):
","575                 propertyLoader.loadPropertyValue(entity)
576             yield entity
577 
578     def _iterRecords(self, columns: Optional[List[str]], rowFilterQuery: Optional[str]):
579 
580         columnNamesAsCommaSeparatedString = None
581         if columns is not None:
582             columnNamesAsCommaSeparatedString = "","".join(columns)
583         return self.tableService.query_entities(self.tableName, select=columnNamesAsCommaSeparatedString,
584                 filter=rowFilterQuery)
585 
586     def insertDataFrameToTable(self, df: pd.DataFrame, partitionKeyGenerator: Callable[[str], str] = None, numRecords: int = None):
","Before: 244, 245, 246
After: 583, 584",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,1894,"{'module': 1, 'expression_statement': 6, 'yield': 4, 'call': 3, 'attribute': 5, 'identifier': 32, '.': 5, 'argument_list': 3, '(': 4, ',': 7, 'keyword_argument': 3, '=': 8, ')': 4, 'assignment': 3, 'list': 1, '[': 3, ']': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 2, ':': 5, 'type': 7, 'generic_type': 2, 'type_parameter': 2, 'none': 5, '->': 1, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'for_statement': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.35682872360554047,0.3322745255185005,"(tensor([0.9059]), tensor([0.8779]), tensor([0.8917]), tensor([0.8806]))"
"262             entity[""PartitionKey""] = self.tableName if partitionKeyGenerator is None else partitionKeyGenerator(idx)
263             self.insertOrReplaceEntity(entity)
264 
265     def _insertOrReplaceEntityViaBatch(self, entity, batch: TableBatch):
266         return batch.insert_or_replace_entity(entity)
267 
268     def _insertEntityViaBatch(self, entity, batch: TableBatch):
","598             entity = row.to_dict()
599             entity[""RowKey""] = idx
600             entity[""PartitionKey""] = self.tableName if partitionKeyGenerator is None else partitionKeyGenerator(idx)
601             self.insertOrReplaceEntity(entity)
602 
603     @staticmethod
604     def _insertOrReplaceEntityViaBatch(entity, batch: TableBatch):
605         return batch.insert_or_replace_entity(entity)
606 
607     @staticmethod
","Before: 265
After: 603, 604",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,2085,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'subscript': 1, 'identifier': 17, '[': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ']': 1, '=': 1, 'conditional_expression': 1, 'attribute': 3, '.': 3, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'else': 1, 'call': 3, 'argument_list': 3, '(': 4, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, ':': 2, 'type': 1, 'block': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4635551440711968,0.42003068660434584,"(tensor([0.8849]), tensor([0.9266]), tensor([0.9053]), tensor([0.9223]))"
"265     def _insertOrReplaceEntityViaBatch(self, entity, batch: TableBatch):
266         return batch.insert_or_replace_entity(entity)
267 
268     def _insertEntityViaBatch(self, entity, batch: TableBatch):
269         return batch.insert_entity(entity)
270 
271     def exists(self):
","602 
603     @staticmethod
604     def _insertOrReplaceEntityViaBatch(entity, batch: TableBatch):
605         return batch.insert_or_replace_entity(entity)
606 
607     @staticmethod
608     def _insertEntityViaBatch(entity, batch: TableBatch):
609         return batch.insert_entity(entity)
610 
611     def exists(self):
","Before: 268
After: 607, 608",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,2113,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 16, 'parameters': 2, '(': 4, ',': 4, 'typed_parameter': 2, ':': 4, 'type': 2, ')': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 2, 'attribute': 2, '.': 2, 'argument_list': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4996631421474755,0.4252681592153508,"(tensor([0.9112]), tensor([0.9520]), tensor([0.9311]), tensor([0.9477]))"
"500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = PeriodicUpdateHook(deferredCommitDelaySecs, noUpdateFn=self._commit, periodicFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","618     """"""
619     CACHE_VALUE_IDENTIFIER = ""cache_value""
620 
621     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
622             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
623             blobBackend: AzureTableBlobBackend = None, serialiser: Serialiser = None, max_workers: int = None):
624         """"""
625 
626 
627         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
628         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
629         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
630         :param maxBatchSize: maximal batch size for each commit.
631         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
632         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
633                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
634         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
635         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
636         :param max_workers: maximal number of workers to load data from blob backend
637         """"""
638 
639         self._deferredCommitDelaySecs = deferredCommitDelaySecs
640         self._partitionKeyGenerator = partitionKeyGenerator
641 
642         def createPropertyLoaders():
643             if blobBackend is None and serialiser is None:
644                 _propertyLoaders = ()
645             elif blobBackend is None and serialiser is not None:
646                 _propertyLoaders = (SerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser),)
647             elif blobBackend is not None and serialiser is None:
648                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
649                 _propertyLoaders = (BlobBackedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, blobBackend, tableName,
650                     propertyBlobStatusName, max_workers),)
651             else:
652                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
653                 _propertyLoaders = (BlobBackedSerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser, blobBackend,
654                 tableName, propertyBlobStatusName, max_workers),)
655             return _propertyLoaders
656 
657         propertyLoaders = createPropertyLoaders()
658         self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService, propertyLoaders=propertyLoaders)
659         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
660         self._maxBatchSize = maxBatchSize
661         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
662 
663         self._inMemoryCache = None
664 
665         if inMemory:
666             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
667             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
668             self._inMemoryCache = df[self.CACHE_VALUE_IDENTIFIER].to_dict()
669 
670     def set(self, key, value):
","Before: 505
After: 623",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4022,"{'module': 1, 'expression_statement': 9, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 27, ':': 20, 'identifier': 165, 'assignment': 5, 'type': 7, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 12, '.': 13, '/': 10, '-': 9, '=': 2, ',': 8, 'for': 3, 'from': 2, 'if': 2, 'none': 2, 'comparison_operator': 5, 'in': 5, 'for_statement': 1, 'expression_list': 1, 'is': 1, 'block': 1, 'not': 1, 'boolean_operator': 1, 'and': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5812010178145551,0.572805720780374,"(tensor([0.9080]), tensor([0.9437]), tensor([0.9255]), tensor([0.9400]))"
"500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = PeriodicUpdateHook(deferredCommitDelaySecs, noUpdateFn=self._commit, periodicFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","618     """"""
619     CACHE_VALUE_IDENTIFIER = ""cache_value""
620 
621     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
622             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
623             blobBackend: AzureTableBlobBackend = None, serialiser: Serialiser = None, max_workers: int = None):
624         """"""
625 
626 
627         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
628         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
629         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
630         :param maxBatchSize: maximal batch size for each commit.
631         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
632         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
633                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
634         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
635         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
636         :param max_workers: maximal number of workers to load data from blob backend
637         """"""
638 
639         self._deferredCommitDelaySecs = deferredCommitDelaySecs
640         self._partitionKeyGenerator = partitionKeyGenerator
641 
642         def createPropertyLoaders():
643             if blobBackend is None and serialiser is None:
644                 _propertyLoaders = ()
645             elif blobBackend is None and serialiser is not None:
646                 _propertyLoaders = (SerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser),)
647             elif blobBackend is not None and serialiser is None:
648                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
649                 _propertyLoaders = (BlobBackedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, blobBackend, tableName,
650                     propertyBlobStatusName, max_workers),)
651             else:
652                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
653                 _propertyLoaders = (BlobBackedSerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser, blobBackend,
654                 tableName, propertyBlobStatusName, max_workers),)
655             return _propertyLoaders
656 
657         propertyLoaders = createPropertyLoaders()
658         self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService, propertyLoaders=propertyLoaders)
659         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
660         self._maxBatchSize = maxBatchSize
661         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
662 
663         self._inMemoryCache = None
664 
665         if inMemory:
666             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
667             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
668             self._inMemoryCache = df[self.CACHE_VALUE_IDENTIFIER].to_dict()
669 
670     def set(self, key, value):
","Before: 524, 525, 526, 527
After: 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 657, 658",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4051,"{'module': 1, 'expression_statement': 9, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 27, ':': 20, 'identifier': 165, 'assignment': 5, 'type': 7, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 12, '.': 13, '/': 10, '-': 9, '=': 2, ',': 8, 'for': 3, 'from': 2, 'if': 2, 'none': 2, 'comparison_operator': 5, 'in': 5, 'for_statement': 1, 'expression_list': 1, 'is': 1, 'block': 1, 'not': 1, 'boolean_operator': 1, 'and': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5812010178145551,0.572805720780374,"(tensor([0.9080]), tensor([0.9437]), tensor([0.9255]), tensor([0.9400]))"
"500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = PeriodicUpdateHook(deferredCommitDelaySecs, noUpdateFn=self._commit, periodicFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","618     """"""
619     CACHE_VALUE_IDENTIFIER = ""cache_value""
620 
621     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
622             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
623             blobBackend: AzureTableBlobBackend = None, serialiser: Serialiser = None, max_workers: int = None):
624         """"""
625 
626 
627         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
628         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
629         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
630         :param maxBatchSize: maximal batch size for each commit.
631         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
632         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
633                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
634         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
635         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
636         :param max_workers: maximal number of workers to load data from blob backend
637         """"""
638 
639         self._deferredCommitDelaySecs = deferredCommitDelaySecs
640         self._partitionKeyGenerator = partitionKeyGenerator
641 
642         def createPropertyLoaders():
643             if blobBackend is None and serialiser is None:
644                 _propertyLoaders = ()
645             elif blobBackend is None and serialiser is not None:
646                 _propertyLoaders = (SerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser),)
647             elif blobBackend is not None and serialiser is None:
648                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
649                 _propertyLoaders = (BlobBackedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, blobBackend, tableName,
650                     propertyBlobStatusName, max_workers),)
651             else:
652                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
653                 _propertyLoaders = (BlobBackedSerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser, blobBackend,
654                 tableName, propertyBlobStatusName, max_workers),)
655             return _propertyLoaders
656 
657         propertyLoaders = createPropertyLoaders()
658         self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService, propertyLoaders=propertyLoaders)
659         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
660         self._maxBatchSize = maxBatchSize
661         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
662 
663         self._inMemoryCache = None
664 
665         if inMemory:
666             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
667             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
668             self._inMemoryCache = df[self.CACHE_VALUE_IDENTIFIER].to_dict()
669 
670     def set(self, key, value):
","Before: 531
After: 661",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4151,"{'module': 1, 'expression_statement': 9, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 27, ':': 20, 'identifier': 165, 'assignment': 5, 'type': 7, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 12, '.': 13, '/': 10, '-': 9, '=': 2, ',': 8, 'for': 3, 'from': 2, 'if': 2, 'none': 2, 'comparison_operator': 5, 'in': 5, 'for_statement': 1, 'expression_list': 1, 'is': 1, 'block': 1, 'not': 1, 'boolean_operator': 1, 'and': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5812010178145551,0.572805720780374,"(tensor([0.9080]), tensor([0.9437]), tensor([0.9255]), tensor([0.9400]))"
"500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = PeriodicUpdateHook(deferredCommitDelaySecs, noUpdateFn=self._commit, periodicFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","618     """"""
619     CACHE_VALUE_IDENTIFIER = ""cache_value""
620 
621     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
622             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
623             blobBackend: AzureTableBlobBackend = None, serialiser: Serialiser = None, max_workers: int = None):
624         """"""
625 
626 
627         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
628         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
629         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
630         :param maxBatchSize: maximal batch size for each commit.
631         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
632         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
633                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
634         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
635         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
636         :param max_workers: maximal number of workers to load data from blob backend
637         """"""
638 
639         self._deferredCommitDelaySecs = deferredCommitDelaySecs
640         self._partitionKeyGenerator = partitionKeyGenerator
641 
642         def createPropertyLoaders():
643             if blobBackend is None and serialiser is None:
644                 _propertyLoaders = ()
645             elif blobBackend is None and serialiser is not None:
646                 _propertyLoaders = (SerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser),)
647             elif blobBackend is not None and serialiser is None:
648                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
649                 _propertyLoaders = (BlobBackedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, blobBackend, tableName,
650                     propertyBlobStatusName, max_workers),)
651             else:
652                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
653                 _propertyLoaders = (BlobBackedSerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser, blobBackend,
654                 tableName, propertyBlobStatusName, max_workers),)
655             return _propertyLoaders
656 
657         propertyLoaders = createPropertyLoaders()
658         self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService, propertyLoaders=propertyLoaders)
659         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
660         self._maxBatchSize = maxBatchSize
661         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
662 
663         self._inMemoryCache = None
664 
665         if inMemory:
666             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
667             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
668             self._inMemoryCache = df[self.CACHE_VALUE_IDENTIFIER].to_dict()
669 
670     def set(self, key, value):
","Before: 533
After: 663",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4159,"{'module': 1, 'expression_statement': 9, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 27, ':': 20, 'identifier': 165, 'assignment': 5, 'type': 7, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 12, '.': 13, '/': 10, '-': 9, '=': 2, ',': 8, 'for': 3, 'from': 2, 'if': 2, 'none': 2, 'comparison_operator': 5, 'in': 5, 'for_statement': 1, 'expression_list': 1, 'is': 1, 'block': 1, 'not': 1, 'boolean_operator': 1, 'and': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5812010178145551,0.572805720780374,"(tensor([0.9080]), tensor([0.9437]), tensor([0.9255]), tensor([0.9400]))"
"500     """"""
501     CACHE_VALUE_IDENTIFIER = ""cache_value""
502 
503     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
504             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
505             blobBackend: AzureTableBlobBackend = None, max_workers: int = None):
506         """"""
507 
508 
509         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
510         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
511         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
512         :param maxBatchSize: maximal batch size for each commit.
513         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
514         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
515                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
516         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
517         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
518         :param max_workers: maximal number of workers to load data from blob backend
519         """"""
520 
521         self._deferredCommitDelaySecs = deferredCommitDelaySecs
522         self._partitionKeyGenerator = partitionKeyGenerator
523 
524         if blobBackend is None:
525             self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService)
526         else:
527             self._batchCommitTable = BlobBackedAzureLazyCommitTable(tableName, tableService, blobBackedProperties=(BlobBackedProperty(self.CACHE_VALUE_IDENTIFIER, blobBackend, max_workers),))
528 
529         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
530         self._maxBatchSize = maxBatchSize
531         self._updateHook = PeriodicUpdateHook(deferredCommitDelaySecs, noUpdateFn=self._commit, periodicFn=self._periodicallyCommit)
532 
533         self._inMemoryDf = None
534 
535         if inMemory:
536             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
","618     """"""
619     CACHE_VALUE_IDENTIFIER = ""cache_value""
620 
621     def __init__(self, tableService: TableService, tableName=""cache"", partitionKeyGenerator: Callable[[str], str] = None,
622             maxBatchSize=100, minSizeForPeriodicCommit: Optional[int] = 100, deferredCommitDelaySecs=1.0, inMemory=False,
623             blobBackend: AzureTableBlobBackend = None, serialiser: Serialiser = None, max_workers: int = None):
624         """"""
625 
626 
627         :param tableService: https://docs.microsoft.com/en-us/python/api/azure-cosmosdb-table/azure.cosmosdb.table.tableservice.tableservice?view=azure-python
628         :param tableName: name of table, needs to match restrictions for Azure storage resources, see https://docs.microsoft.com/en-gb/azure/azure-resource-manager/management/resource-name-rules
629         :param partitionKeyGenerator: callable to generate a partitionKey from provided string, if None partitionKey in requests defaults to tableName
630         :param maxBatchSize: maximal batch size for each commit.
631         :param deferredCommitDelaySecs: the time frame during which no new data must be added for a pending transaction to be committed
632         :param minSizeForPeriodicCommit: minimal size of a batch to be committed in a periodic thread.
633                                          If None, commits are only executed in a deferred manner, i.e. commit only if there is no update for deferredCommitDelaySecs
634         :param inMemory: boolean flag, to indicate, if table should be loaded in memory at construction
635         :param blobBackend: if not None, blob storage will be used to store actual value and cache_value in table only contains a reference
636         :param max_workers: maximal number of workers to load data from blob backend
637         """"""
638 
639         self._deferredCommitDelaySecs = deferredCommitDelaySecs
640         self._partitionKeyGenerator = partitionKeyGenerator
641 
642         def createPropertyLoaders():
643             if blobBackend is None and serialiser is None:
644                 _propertyLoaders = ()
645             elif blobBackend is None and serialiser is not None:
646                 _propertyLoaders = (SerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser),)
647             elif blobBackend is not None and serialiser is None:
648                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
649                 _propertyLoaders = (BlobBackedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, blobBackend, tableName,
650                     propertyBlobStatusName, max_workers),)
651             else:
652                 propertyBlobStatusName = self.CACHE_VALUE_IDENTIFIER + ""_blob_backed""
653                 _propertyLoaders = (BlobBackedSerialisedPropertyLoader(self.CACHE_VALUE_IDENTIFIER, serialiser, blobBackend,
654                 tableName, propertyBlobStatusName, max_workers),)
655             return _propertyLoaders
656 
657         propertyLoaders = createPropertyLoaders()
658         self._batchCommitTable = AzureLazyBatchCommitTable(tableName, tableService, propertyLoaders=propertyLoaders)
659         self._minSizeForPeriodicCommit = minSizeForPeriodicCommit
660         self._maxBatchSize = maxBatchSize
661         self._updateHook = DelayedUpdateHook(self._commit, deferredCommitDelaySecs, periodicallyExecutedFn=self._periodicallyCommit)
662 
663         self._inMemoryCache = None
664 
665         if inMemory:
666             df = self._batchCommitTable.loadTableToDataFrame(columns=['RowKey', self.CACHE_VALUE_IDENTIFIER]).set_index(""RowKey"")
667             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
668             self._inMemoryCache = df[self.CACHE_VALUE_IDENTIFIER].to_dict()
669 
670     def set(self, key, value):
","Before: 538
After: 668",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4241,"{'module': 1, 'expression_statement': 9, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 27, ':': 20, 'identifier': 165, 'assignment': 5, 'type': 7, 'constrained_type': 1, '//': 2, 'binary_operator': 20, 'attribute': 12, '.': 13, '/': 10, '-': 9, '=': 2, ',': 8, 'for': 3, 'from': 2, 'if': 2, 'none': 2, 'comparison_operator': 5, 'in': 5, 'for_statement': 1, 'expression_list': 1, 'is': 1, 'block': 1, 'not': 1, 'boolean_operator': 1, 'and': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5812010178145551,0.572805720780374,"(tensor([0.9080]), tensor([0.9437]), tensor([0.9255]), tensor([0.9400]))"
"537             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
538             self._inMemoryDf = df
539 
540     def set(self, key, value):
541         keyAsString = str(key)
542         partitionKey = self._getPartitionKeyForRowKey(keyAsString)
543         entity = {'PartitionKey': partitionKey, 'RowKey': keyAsString, self.CACHE_VALUE_IDENTIFIER: value}
544         self._batchCommitTable.insertOrReplaceEntity(entity)
545         self._updateHook.handleUpdate()
546 
547         if self._inMemoryDf is not None:
548             self._inMemoryDf.loc[keyAsString] = [value]
549 
550     def get(self, key):
","667             _log.info(f""Loaded {len(df)} entries of table {tableName} in memory"")
668             self._inMemoryCache = df[self.CACHE_VALUE_IDENTIFIER].to_dict()
669 
670     def set(self, key, value):
671         keyAsString = str(key)
672         partitionKey = self._getPartitionKeyForRowKey(keyAsString)
673         entity = {'PartitionKey': partitionKey, 'RowKey': keyAsString, self.CACHE_VALUE_IDENTIFIER: value}
674         self._batchCommitTable.insertOrReplaceEntity(entity)
675         self._updateHook.handleUpdate()
676 
677         if self._inMemoryCache is not None:
678             self._inMemoryCache[keyAsString] = value
679 
680     def get(self, key):
","Before: 547, 548
After: 677, 678",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4344,"{'module': 1, 'expression_statement': 8, 'call': 6, 'attribute': 11, 'identifier': 39, '.': 11, 'argument_list': 6, '(': 7, 'string': 3, 'string_start': 3, 'string_content': 5, 'interpolation': 2, '{': 3, ')': 7, '}': 3, 'string_end': 3, 'assignment': 5, '=': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, ':': 5, 'block': 2, 'dictionary': 1, 'pair': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'subscript': 1, '[': 2, ']': 2, 'list': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6311794596777196,0.605078439949557,"(tensor([0.9407]), tensor([0.9206]), tensor([0.9306]), tensor([0.9226]))"
"547         if self._inMemoryDf is not None:
548             self._inMemoryDf.loc[keyAsString] = [value]
549 
550     def get(self, key):
551         keyAsString = str(key)
552         value = self._getFromInMemoryDf(keyAsString)
553         if value is None:
554             value = self._getFromTable(keyAsString)
555         return value
556 
557     def _getFromTable(self, key: str):
","677         if self._inMemoryCache is not None:
678             self._inMemoryCache[keyAsString] = value
679 
680     def get(self, key):
681         keyAsString = str(key)
682         value = self._getFromInMemoryCache(keyAsString)
683         if value is None:
684             value = self._getFromTable(keyAsString)
685         return value
686 
687     def _getFromTable(self, key: str):
","Before: 552
After: 682",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4396,"{'module': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'attribute': 5, 'identifier': 23, '.': 5, 'is not': 2, 'none': 2, ':': 3, 'block': 3, 'expression_statement': 4, 'assignment': 4, 'subscript': 1, '[': 2, ']': 2, '=': 4, 'list': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 4, ',': 1, ')': 4, 'call': 3, 'argument_list': 3, 'is': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5149284943954836,0.4635060296291535,"(tensor([0.9364]), tensor([0.9108]), tensor([0.9234]), tensor([0.9133]))"
"554             value = self._getFromTable(keyAsString)
555         return value
556 
557     def _getFromTable(self, key: str):
558         try:
559             partitionKey = self._getPartitionKeyForRowKey(key)
560             value = self._batchCommitTable.getEntity(partitionKey, key)[self.CACHE_VALUE_IDENTIFIER]
561             return value
562         except Exception as e:
563             _log.debug(f""Unable to load value for row_key {key}: {e}"")
564             return None
565 
566     def _getFromInMemoryDf(self, key):
","684             value = self._getFromTable(keyAsString)
685         return value
686 
687     def _getFromTable(self, key: str):
688         partitionKey = self._getPartitionKeyForRowKey(key)
689         entity = self._batchCommitTable.getEntity(partitionKey, key)
690         if entity is not None:
691             return entity[self.CACHE_VALUE_IDENTIFIER]
692         return None
693 
694     def _getFromInMemoryCache(self, key):
","Before: 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572
After: 688, 689, 690, 691, 692, 693, 694, 695, 697",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4438,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'identifier': 28, '=': 3, 'call': 4, 'attribute': 6, '.': 6, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 3, 'return': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 1, ':': 4, 'type': 1, 'block': 3, 'try_statement': 1, 'try': 1, 'subscript': 1, '[': 1, ']': 1, 'except_clause': 1, 'except': 1, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 2, '{': 2, '}': 2, 'string_end': 1, 'none': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.338902251021001,0.2972456482244597,"(tensor([0.9245]), tensor([0.8725]), tensor([0.8978]), tensor([0.8774]))"
"575     def _getPartitionKeyForRowKey(self, key: str):
576         return self._batchCommitTable.tableName if self._partitionKeyGenerator is None else self._partitionKeyGenerator(key)
577 
578     def _commit(self):
579         self._batchCommitTable.commitNonBlockingCurrentQueueState(self._maxBatchSize)
580 
581     def _periodicallyCommit(self):
","699     def _getPartitionKeyForRowKey(self, key: str):
700         return self._batchCommitTable.tableName if self._partitionKeyGenerator is None else self._partitionKeyGenerator(key)
701 
702     def _commit(self):
703         self._batchCommitTable.commitBlockingUntilEmpty(self._maxBatchSize)
704 
705     def _periodicallyCommit(self):
","Before: 579
After: 703",add serialiser for azure table storage data model,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,75be4b4d4ca0e49c02b951fa3bf2dc2a253178d3,275d342e5345c5867854f4849ab4f6d8707e2344,0,4672,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 19, 'parameters': 2, '(': 4, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 1, ')': 4, 'block': 2, 'return_statement': 1, 'return': 1, 'conditional_expression': 1, 'attribute': 7, '.': 7, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'else': 1, 'call': 2, 'argument_list': 2, 'expression_statement': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 21, 'name': '__init__', 'long_name': '__init__( self , partitionKey )', 'start_line': 36, 'end_line': 38, 'full_parameters': ['self', ' partitionKey'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 3, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6576209490114577,0.5946242743126214,"(tensor([0.9574]), tensor([0.9508]), tensor([0.9541]), tensor([0.9515]))"
"146         blobName = self._getBlobNameFromIdentifier(valueIdentifier)
147         return self._getBlobValue(containerName, blobName)
148 
149     def getValueReference(self, partitionKey: str, rowKey: str, valueName: str, blobNamePrefix: str = None) -> str:
150         blobName = self._getBlobNameFromKeys(blobNamePrefix, partitionKey, rowKey, valueName)
151         return self.blockBlobService.make_blob_url(self.containerName, blobName)
152 
153     def setValueForReference(self, valueIdentifier: str, value):
","143         blobName = self._getBlobNameFromIdentifier(valueIdentifier)
144         return self._getBlobValue(containerName, blobName)
145 
146     def getValueReference(self, partitionKey: str, rowKey: str, valueName: str, blobNamePrefix: str = None) -> str:
147         blobName = self._getBlobNameFromKeys(partitionKey, rowKey, valueName, blobPrefix=blobNamePrefix)
148         return self.blockBlobService.make_blob_url(self.containerName, blobName)
149 
150     def setValueForReference(self, valueIdentifier: str, value):
","Before: 150
After: 147",add support for batch size of 100,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,c00d3f09662c8ac68b5927a2ce0d5a449fc14936,7bdcc907f1ad7d4878d239e05e7a0b1672ef0e2e,0,1007,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 32, '=': 3, 'call': 4, 'attribute': 6, '.': 6, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 2, 'return': 2, ',': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 3, ':': 5, 'type': 5, 'typed_default_parameter': 1, 'none': 1, '->': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7563618924596331,0.7333333172398421,"(tensor([0.9773]), tensor([0.9819]), tensor([0.9796]), tensor([0.9814]))"
"416             maxLength = max(lengthsList)
417             return maxLength, lengthsList.index(maxLength)
418 
419     def __init__(self, tableName: str, tableService: TableService, propertyLoaders: Sequence[PropertyLoader] = ()):
420         """"""
421 
422         :param tableName: name of table
423         :param tableService: instance of :class:`azure.storage.table.TableService` to connect to Azure table storage
424         :param propertyLoaders:
425         """"""
426 
427         if not AZURE_ALLOWED_TABLE_NAME_PATTERN.match(tableName):
428             raise ValueError(f""Invalid table name {tableName}, see: https://docs.microsoft.com/en-us/rest/api/storageservices/Understanding-the-Table-Service-Data-Model"")
429 
430         self.tableService = tableService
431         self.tableName = tableName
432         self.propertyLoaders = propertyLoaders
433         self._partitionQueues = self.PartitionCommandsPriorityQueue()
434         self._contextManager = functools.partial(self.tableService.batch, self.tableName)
435 
436         if not self.exists():
437             self.tableService.create_table(self.tableName)
438 
439     def insertOrReplaceEntity(self, entity: Union[Dict, Entity]):
","416             maxLength = max(lengthsList)
417             return maxLength, lengthsList.index(maxLength)
418 
419     def __init__(self, tableName: str, tableService: TableService, propertyLoaders: Sequence[PropertyLoader] = ()):
420         """"""
421 
422         :param tableName: name of table
423         :param tableService: instance of :class:`azure.storage.table.TableService` to connect to Azure table storage
424         :param propertyLoaders:
425         """"""
426 
427         if not self.AZURE_ALLOWED_TABLE_NAME_PATTERN.match(tableName):
428             raise ValueError(f""Invalid table name {tableName}, see: https://docs.microsoft.com/en-us/rest/api/storageservices/Understanding-the-Table-Service-Data-Model"")
429 
430         self.tableService = tableService
431         self.tableName = tableName
432         self.propertyLoaders = propertyLoaders
433         self._partitionQueues = self.PartitionCommandsPriorityQueue()
434         self._contextManager = functools.partial(self.tableService.batch, self.tableName)
435 
436         if not self.exists():
437             self.tableService.create_table(self.tableName)
438 
439     def insertOrReplaceEntity(self, entity: Union[Dict, Entity]):
","Before: 427
After: 427",add support for batch size of 100,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,c00d3f09662c8ac68b5927a2ce0d5a449fc14936,7bdcc907f1ad7d4878d239e05e7a0b1672ef0e2e,0,3433,"{'module': 1, 'expression_statement': 8, 'assignment': 6, 'identifier': 50, '=': 7, 'call': 8, 'argument_list': 8, '(': 10, ')': 10, 'return_statement': 1, 'return': 1, 'expression_list': 1, ',': 5, 'attribute': 16, '.': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 4, 'typed_default_parameter': 1, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'tuple': 1, 'block': 3, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'if_statement': 2, 'if': 2, 'not_operator': 2, 'not': 2, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9850061038070599,0.9838440017192508,"(tensor([0.9968]), tensor([0.9977]), tensor([0.9973]), tensor([0.9976]))"
"514         maxBatchSize = self._validateMaxBatchSize(maxBatchSize)
515         commands = self._partitionQueues.pop(minLength)
516         if commands is not None:
517             commands.execute(self._contextManager, maxBatchSize)
518 
519     @staticmethod
520     def _validateMaxBatchSize(maxBatchSize):
521         if maxBatchSize > AZURE_ALLOWED_TABLE_BATCH_SIZE:
522             _log.warning(f""Provided maxBatchSize is larger than allowed size {AZURE_ALLOWED_TABLE_BATCH_SIZE}. Will use maxBatchSize {AZURE_ALLOWED_TABLE_BATCH_SIZE} instead."")
523             maxBatchSize = AZURE_ALLOWED_TABLE_BATCH_SIZE
","516         if commands is not None:
517             commands.execute(self._contextManager, maxBatchSize)
518 
519     def _validateMaxBatchSize(self, maxBatchSize):
520         if maxBatchSize > self.AZURE_ALLOWED_TABLE_BATCH_SIZE:
521             _log.warning(f""Provided maxBatchSize is larger than allowed size {self.AZURE_ALLOWED_TABLE_BATCH_SIZE}. Will use maxBatchSize {self.AZURE_ALLOWED_TABLE_BATCH_SIZE} instead."")
522             maxBatchSize = self.AZURE_ALLOWED_TABLE_BATCH_SIZE
523         return maxBatchSize
524 
525     def loadTableToDataFrame(self, columns: List[str] = None, rowFilterQuery: str = None, numRecords: int = None):
","Before: 519, 520, 521, 522, 523
After: 519, 520, 521, 522",add support for batch size of 100,Sync faz,https://github.com/opcode81/sensAI,src/sensai/util/cache_azure.py,c00d3f09662c8ac68b5927a2ce0d5a449fc14936,7bdcc907f1ad7d4878d239e05e7a0b1672ef0e2e,0,4132,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'identifier': 24, '=': 2, 'call': 4, 'attribute': 6, '.': 6, 'argument_list': 4, '(': 5, ')': 5, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'is not': 2, 'none': 1, ':': 3, 'block': 3, ',': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '>': 1, 'string': 1, 'string_start': 1, 'string_content': 3, 'interpolation': 2, '{': 2, '}': 2, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': 'serialise', 'long_name': 'serialise( self , value )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache_azure.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.43351104094711723,0.42580476915017734,"(tensor([0.8965]), tensor([0.9234]), tensor([0.9098]), tensor([0.9206]))"
"369             normalisationMode=self.normalisationMode)
370         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
371 
372     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
373         if len(outputs.columns) != 1:
374             raise ValueError(""Expected one output dimension: the class labels"")
375 
376         # transform outputs: for each data point, the new output shall be the index in the list of labels
377         labels: pd.Series = outputs.iloc[:, 0]
378         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
379 
380         self.model = self._createTorchModel()
381 
382         dataSetProvider = self._createDataSetProvider(inputs, outputs)
383         self.model.fit(dataSetProvider, **self.nnOptimiserParams)
384 
385     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","369             normalisationMode=self.normalisationMode)
370         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
371 
372     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
373         if len(outputs.columns) != 1:
374             raise ValueError(""Expected one output dimension: the class labels"")
375 
376         # transform outputs: for each data point, the new output shall be the index in the list of labels
377         labels: pd.Series = outputs.iloc[:, 0]
378         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
379 
380         self.model = self._createTorchModel()
381 
382         dataSetProvider = self._createDataSetProvider(inputs, outputs)
383         self.model.fit(dataSetProvider, self.nnOptimiserParams)
384 
385     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","Before: 383
After: 383",fix typo in src/sensai/torch/torch_base.py,Fixed NNOptimiser parameters not being passed correctly in TorchVectorClassificationModel,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,be101a4ca60816d3d24a64ac6f56df0de508b99a,f41c3fff82c175e72580225dd01b173fd452e2e8,0,3407,"{'module': 1, 'ERROR': 1, 'identifier': 55, '=': 7, 'attribute': 19, '.': 19, ')': 10, 'return_statement': 1, 'return': 1, 'call': 8, 'argument_list': 8, '(': 9, ',': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 3, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'integer': 2, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'comment': 1, 'expression_statement': 5, 'assignment': 4, 'subscript': 1, '[': 2, 'slice': 1, ']': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'keyword_argument': 2, 'dictionary_splat': 1, '**': 1}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 36, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 58, 'name': '__init__', 'long_name': '__init__( self , cuda = True )', 'start_line': 30, 'end_line': 36, 'full_parameters': ['self', ' cuda = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9759004098481702,0.9759004098481702,"(tensor([0.9993]), tensor([0.9971]), tensor([0.9982]), tensor([0.9974]))"
"1 from abc import ABC
2 
3 import torch
4 from torch import nn
5 from torch.nn import functional as F
","1 from abc import ABC, abstractmethod
2 from typing import Sequence
3 
4 import torch
5 from torch import nn
","Before: 1
After: 1, 2",add residualfeedforwardnetwork and lstnetwork,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_modules.py,6366dbb6dc78dd5c63d5743006e156359440b1ea,89de85ccd6c79a4ccf7969dafc7ff4a754bf2178,0,9,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 5, 'identifier': 5, 'import': 3, 'import_statement': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 17, 'end_line': 20, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 18, 'end_line': 21, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.35553046704311847,0.250655313615639,"(tensor([0.9107]), tensor([0.9233]), tensor([0.9169]), tensor([0.9220]))"
"430     def __str__(self):
431         return f""{self.__class__.__name__}[params={self.params}]""
432 
433     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
434             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
435             createTorchModule=True):
436         """"""
437         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
438 
439             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
440               the `trainFraction` parameter of this object)
441             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
442             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
443               the validation set
444 
445         :param model: the model to be fitted
446         :param data: the data to use (see variants above)
447         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
448             If False, (re-)train the existing module.
449         """"""
450         self.log.info(f""Preparing parameter learning of {model} via {self}"")
451         self.cuda = model.cuda
452 
453         useValidation = self.params.trainFraction != 1.0
454 
455         def toDataSetProvider(d) -> TorchDataSetProvider:
456             if isinstance(d, TorchDataSetProvider):
457                 return d
458             elif isinstance(d, DataUtil):
459                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
460             else:
461                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
462 
463         # initialise data to be generated
464         self.trainingLog = []
465         self.bestEpoch = None
466 
467         def trainingLog(s):
468             self.log.info(s)
469             self.trainingLog.append(s)
470 
471         self._init_cuda()
472 
473         # Set the random seed manually for reproducibility.
474         seed = 42
475         torch.manual_seed(seed)
476         if self.cuda:
477             torchcuda.manual_seed_all(seed)
478         torch.backends.cudnn.benchmark = False
479         torch.backends.cudnn.deterministic = True
480 
481         # obtain data, splitting it into training and validation set(s)
482         validationSets = []
483         trainingSets = []
484         outputScalers = []
485         if type(data) != list:
486             data = [data]
487         self.log.info(""Obtaining input/output training instances"")
488         for idxDataItem, dataItem in enumerate(data):
489             if isinstance(dataItem, TorchDataSet):
490                 if useValidation:
491                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
492                 trainingSet = dataItem
493                 validationSet = None
494                 outputScaler = TensorScalerIdentity()
495             elif type(dataItem) == tuple:
496                 trainingSet, validationSet = dataItem
497                 outputScaler = TensorScalerIdentity()
498             else:
499                 dataSetProvider = toDataSetProvider(dataItem)
500                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
501                 outputScaler = dataSetProvider.getOutputTensorScaler()
502             trainingSets.append(trainingSet)
503             if validationSet is not None:
504                 validationSets.append(validationSet)
505             outputScalers.append(outputScaler)
506             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
507         trainingLog(""Number of validation sets: %d"" % len(validationSets))
508 
509         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
510         if self.cuda:
511             torchModel.cuda()
512         model.setTorchModule(torchModel)
513 
514         nParams = sum([p.nelement() for p in torchModel.parameters()])
515         self.log.info(f""Learning parameters of {model} via {self}"")
516         trainingLog('Number of parameters: %d' % nParams)
517 
518         lossEvaluator = self.params.lossEvaluator
519         criterion = lossEvaluator.getTrainingCriterion()
520 
521         if self.cuda:
522             criterion = criterion.cuda()
523 
524         best_val = 1e9
525         best_epoch = 0
526         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
527             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
528 
529         bestModelBytes = model.getModuleBytes()
530         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
531         validationMetricName = lossEvaluator.getValidationMetricName()
532         try:
533             self.log.info('Begin training')
534             self.log.info('Press Ctrl+C to end training early')
535             for epoch in range(1, self.params.epochs + 1):
536                 epoch_start_time = time.time()
537 
538                 # perform training step, processing all the training data once
539                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
540 
541                 # perform validation, computing the mean metrics across all validation sets (if more than one),
542                 # and check for new best result according to validation results
543                 isNewBest = False
544                 if useValidation:
545                     metricsSum = None
546                     metricsKeys = None
547                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
548                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
549                         metricsArray = np.array(list(metrics.values()))
550                         if i == 0:
551                             metricsSum = metricsArray
552                             metricsKeys = metrics.keys()
553                         else:
554                             metricsSum += metricsArray
555                     metricsSum /= len(validationSets)  # mean results
556                     metrics = dict(zip(metricsKeys, metricsSum))
557                     current_val = metrics[lossEvaluator.getValidationMetricName()]
558                     isNewBest = current_val < best_val
559                     if isNewBest:
560                         best_val = current_val
561                         best_epoch = epoch
562                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
563                     else:
564                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
565                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
566                 else:
567                     valStr = """"
568                 trainingLog(
569                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
570                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
571                 if useValidation and isNewBest:
572                     bestModelBytes = model.getModuleBytes()
573             trainingLog(""Training complete"")
574         except KeyboardInterrupt:
575             trainingLog('Exiting from training early')
576 
577         # reload best model according to validation results
578         if useValidation:
579             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
580             self.bestEpoch = best_epoch
581             model.setModuleBytes(bestModelBytes)
582 
583     def getTrainingLog(self):
","430     def __str__(self):
431         return f""{self.__class__.__name__}[params={self.params}]""
432 
433     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
434             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
435             createTorchModule=True):
436         """"""
437         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
438 
439             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
440               the `trainFraction` parameter of this object)
441             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
442             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
443               the validation set
444 
445         :param model: the model to be fitted
446         :param data: the data to use (see variants above)
447         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
448             If False, (re-)train the existing module.
449         """"""
450         self.cuda = model.cuda
451         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
452 
453         useValidation = self.params.trainFraction != 1.0
454 
455         def toDataSetProvider(d) -> TorchDataSetProvider:
456             if isinstance(d, TorchDataSetProvider):
457                 return d
458             elif isinstance(d, DataUtil):
459                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
460             else:
461                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
462 
463         # initialise data to be generated
464         self.trainingLog = []
465         self.bestEpoch = None
466 
467         def trainingLog(s):
468             self.log.info(s)
469             self.trainingLog.append(s)
470 
471         self._init_cuda()
472 
473         # Set the random seed manually for reproducibility.
474         seed = 42
475         torch.manual_seed(seed)
476         if self.cuda:
477             torchcuda.manual_seed_all(seed)
478         torch.backends.cudnn.benchmark = False
479         torch.backends.cudnn.deterministic = True
480 
481         # obtain data, splitting it into training and validation set(s)
482         validationSets = []
483         trainingSets = []
484         outputScalers = []
485         if type(data) != list:
486             data = [data]
487         self.log.info(""Obtaining input/output training instances"")
488         for idxDataItem, dataItem in enumerate(data):
489             if isinstance(dataItem, TorchDataSet):
490                 if useValidation:
491                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
492                 trainingSet = dataItem
493                 validationSet = None
494                 outputScaler = TensorScalerIdentity()
495             elif type(dataItem) == tuple:
496                 trainingSet, validationSet = dataItem
497                 outputScaler = TensorScalerIdentity()
498             else:
499                 dataSetProvider = toDataSetProvider(dataItem)
500                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
501                 outputScaler = dataSetProvider.getOutputTensorScaler()
502             trainingSets.append(trainingSet)
503             if validationSet is not None:
504                 validationSets.append(validationSet)
505             outputScalers.append(outputScaler)
506             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
507         trainingLog(""Number of validation sets: %d"" % len(validationSets))
508 
509         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
510         if self.cuda:
511             torchModel.cuda()
512         model.setTorchModule(torchModel)
513 
514         nParams = sum([p.nelement() for p in torchModel.parameters()])
515         self.log.info(f""Learning parameters of {model} via {self}"")
516         trainingLog('Number of parameters: %d' % nParams)
517 
518         lossEvaluator = self.params.lossEvaluator
519         criterion = lossEvaluator.getTrainingCriterion()
520 
521         if self.cuda:
522             criterion = criterion.cuda()
523 
524         best_val = 1e9
525         best_epoch = 0
526         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
527             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
528 
529         bestModelBytes = model.getModuleBytes()
530         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
531         validationMetricName = lossEvaluator.getValidationMetricName()
532         try:
533             self.log.info(f'Begin training with cuda={self.cuda}')
534             self.log.info('Press Ctrl+C to end training early')
535             for epoch in range(1, self.params.epochs + 1):
536                 epoch_start_time = time.time()
537 
538                 # perform training step, processing all the training data once
539                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
540 
541                 # perform validation, computing the mean metrics across all validation sets (if more than one),
542                 # and check for new best result according to validation results
543                 isNewBest = False
544                 if useValidation:
545                     metricsSum = None
546                     metricsKeys = None
547                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
548                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
549                         metricsArray = np.array(list(metrics.values()))
550                         if i == 0:
551                             metricsSum = metricsArray
552                             metricsKeys = metrics.keys()
553                         else:
554                             metricsSum += metricsArray
555                     metricsSum /= len(validationSets)  # mean results
556                     metrics = dict(zip(metricsKeys, metricsSum))
557                     current_val = metrics[lossEvaluator.getValidationMetricName()]
558                     isNewBest = current_val < best_val
559                     if isNewBest:
560                         best_val = current_val
561                         best_epoch = epoch
562                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
563                     else:
564                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
565                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
566                 else:
567                     valStr = """"
568                 trainingLog(
569                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
570                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
571                 if useValidation and isNewBest:
572                     bestModelBytes = model.getModuleBytes()
573             trainingLog(""Training complete"")
574         except KeyboardInterrupt:
575             trainingLog('Exiting from training early')
576 
577         # reload best model according to validation results
578         if useValidation:
579             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
580             self.bestEpoch = best_epoch
581             model.setModuleBytes(bestModelBytes)
582 
583     def getTrainingLog(self):
","Before: 450
After: 451",add cuda to nnoptimiser log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,6366dbb6dc78dd5c63d5743006e156359440b1ea,89de85ccd6c79a4ccf7969dafc7ff4a754bf2178,0,3772,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 360, 'parameters': 4, '(': 80, ')': 80, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 16, '{': 16, 'attribute': 94, '.': 94, '}': 16, 'string_content': 33, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 15, ']': 15, 'default_parameter': 1, '=': 55, 'true': 2, 'expression_statement': 76, 'call': 74, 'argument_list': 74, 'assignment': 50, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 5, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 29, 'end_line': 54, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 29, 'end_line': 54, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9853637086290169,0.9854302360135274,"(tensor([0.9904]), tensor([0.9909]), tensor([0.9907]), tensor([0.9909]))"
"430     def __str__(self):
431         return f""{self.__class__.__name__}[params={self.params}]""
432 
433     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
434             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
435             createTorchModule=True):
436         """"""
437         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
438 
439             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
440               the `trainFraction` parameter of this object)
441             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
442             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
443               the validation set
444 
445         :param model: the model to be fitted
446         :param data: the data to use (see variants above)
447         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
448             If False, (re-)train the existing module.
449         """"""
450         self.log.info(f""Preparing parameter learning of {model} via {self}"")
451         self.cuda = model.cuda
452 
453         useValidation = self.params.trainFraction != 1.0
454 
455         def toDataSetProvider(d) -> TorchDataSetProvider:
456             if isinstance(d, TorchDataSetProvider):
457                 return d
458             elif isinstance(d, DataUtil):
459                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
460             else:
461                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
462 
463         # initialise data to be generated
464         self.trainingLog = []
465         self.bestEpoch = None
466 
467         def trainingLog(s):
468             self.log.info(s)
469             self.trainingLog.append(s)
470 
471         self._init_cuda()
472 
473         # Set the random seed manually for reproducibility.
474         seed = 42
475         torch.manual_seed(seed)
476         if self.cuda:
477             torchcuda.manual_seed_all(seed)
478         torch.backends.cudnn.benchmark = False
479         torch.backends.cudnn.deterministic = True
480 
481         # obtain data, splitting it into training and validation set(s)
482         validationSets = []
483         trainingSets = []
484         outputScalers = []
485         if type(data) != list:
486             data = [data]
487         self.log.info(""Obtaining input/output training instances"")
488         for idxDataItem, dataItem in enumerate(data):
489             if isinstance(dataItem, TorchDataSet):
490                 if useValidation:
491                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
492                 trainingSet = dataItem
493                 validationSet = None
494                 outputScaler = TensorScalerIdentity()
495             elif type(dataItem) == tuple:
496                 trainingSet, validationSet = dataItem
497                 outputScaler = TensorScalerIdentity()
498             else:
499                 dataSetProvider = toDataSetProvider(dataItem)
500                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
501                 outputScaler = dataSetProvider.getOutputTensorScaler()
502             trainingSets.append(trainingSet)
503             if validationSet is not None:
504                 validationSets.append(validationSet)
505             outputScalers.append(outputScaler)
506             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
507         trainingLog(""Number of validation sets: %d"" % len(validationSets))
508 
509         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
510         if self.cuda:
511             torchModel.cuda()
512         model.setTorchModule(torchModel)
513 
514         nParams = sum([p.nelement() for p in torchModel.parameters()])
515         self.log.info(f""Learning parameters of {model} via {self}"")
516         trainingLog('Number of parameters: %d' % nParams)
517 
518         lossEvaluator = self.params.lossEvaluator
519         criterion = lossEvaluator.getTrainingCriterion()
520 
521         if self.cuda:
522             criterion = criterion.cuda()
523 
524         best_val = 1e9
525         best_epoch = 0
526         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
527             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
528 
529         bestModelBytes = model.getModuleBytes()
530         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
531         validationMetricName = lossEvaluator.getValidationMetricName()
532         try:
533             self.log.info('Begin training')
534             self.log.info('Press Ctrl+C to end training early')
535             for epoch in range(1, self.params.epochs + 1):
536                 epoch_start_time = time.time()
537 
538                 # perform training step, processing all the training data once
539                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
540 
541                 # perform validation, computing the mean metrics across all validation sets (if more than one),
542                 # and check for new best result according to validation results
543                 isNewBest = False
544                 if useValidation:
545                     metricsSum = None
546                     metricsKeys = None
547                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
548                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
549                         metricsArray = np.array(list(metrics.values()))
550                         if i == 0:
551                             metricsSum = metricsArray
552                             metricsKeys = metrics.keys()
553                         else:
554                             metricsSum += metricsArray
555                     metricsSum /= len(validationSets)  # mean results
556                     metrics = dict(zip(metricsKeys, metricsSum))
557                     current_val = metrics[lossEvaluator.getValidationMetricName()]
558                     isNewBest = current_val < best_val
559                     if isNewBest:
560                         best_val = current_val
561                         best_epoch = epoch
562                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
563                     else:
564                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
565                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
566                 else:
567                     valStr = """"
568                 trainingLog(
569                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
570                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
571                 if useValidation and isNewBest:
572                     bestModelBytes = model.getModuleBytes()
573             trainingLog(""Training complete"")
574         except KeyboardInterrupt:
575             trainingLog('Exiting from training early')
576 
577         # reload best model according to validation results
578         if useValidation:
579             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
580             self.bestEpoch = best_epoch
581             model.setModuleBytes(bestModelBytes)
582 
583     def getTrainingLog(self):
","430     def __str__(self):
431         return f""{self.__class__.__name__}[params={self.params}]""
432 
433     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
434             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
435             createTorchModule=True):
436         """"""
437         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
438 
439             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
440               the `trainFraction` parameter of this object)
441             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
442             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
443               the validation set
444 
445         :param model: the model to be fitted
446         :param data: the data to use (see variants above)
447         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
448             If False, (re-)train the existing module.
449         """"""
450         self.cuda = model.cuda
451         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
452 
453         useValidation = self.params.trainFraction != 1.0
454 
455         def toDataSetProvider(d) -> TorchDataSetProvider:
456             if isinstance(d, TorchDataSetProvider):
457                 return d
458             elif isinstance(d, DataUtil):
459                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
460             else:
461                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
462 
463         # initialise data to be generated
464         self.trainingLog = []
465         self.bestEpoch = None
466 
467         def trainingLog(s):
468             self.log.info(s)
469             self.trainingLog.append(s)
470 
471         self._init_cuda()
472 
473         # Set the random seed manually for reproducibility.
474         seed = 42
475         torch.manual_seed(seed)
476         if self.cuda:
477             torchcuda.manual_seed_all(seed)
478         torch.backends.cudnn.benchmark = False
479         torch.backends.cudnn.deterministic = True
480 
481         # obtain data, splitting it into training and validation set(s)
482         validationSets = []
483         trainingSets = []
484         outputScalers = []
485         if type(data) != list:
486             data = [data]
487         self.log.info(""Obtaining input/output training instances"")
488         for idxDataItem, dataItem in enumerate(data):
489             if isinstance(dataItem, TorchDataSet):
490                 if useValidation:
491                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
492                 trainingSet = dataItem
493                 validationSet = None
494                 outputScaler = TensorScalerIdentity()
495             elif type(dataItem) == tuple:
496                 trainingSet, validationSet = dataItem
497                 outputScaler = TensorScalerIdentity()
498             else:
499                 dataSetProvider = toDataSetProvider(dataItem)
500                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
501                 outputScaler = dataSetProvider.getOutputTensorScaler()
502             trainingSets.append(trainingSet)
503             if validationSet is not None:
504                 validationSets.append(validationSet)
505             outputScalers.append(outputScaler)
506             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
507         trainingLog(""Number of validation sets: %d"" % len(validationSets))
508 
509         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
510         if self.cuda:
511             torchModel.cuda()
512         model.setTorchModule(torchModel)
513 
514         nParams = sum([p.nelement() for p in torchModel.parameters()])
515         self.log.info(f""Learning parameters of {model} via {self}"")
516         trainingLog('Number of parameters: %d' % nParams)
517 
518         lossEvaluator = self.params.lossEvaluator
519         criterion = lossEvaluator.getTrainingCriterion()
520 
521         if self.cuda:
522             criterion = criterion.cuda()
523 
524         best_val = 1e9
525         best_epoch = 0
526         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
527             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
528 
529         bestModelBytes = model.getModuleBytes()
530         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
531         validationMetricName = lossEvaluator.getValidationMetricName()
532         try:
533             self.log.info(f'Begin training with cuda={self.cuda}')
534             self.log.info('Press Ctrl+C to end training early')
535             for epoch in range(1, self.params.epochs + 1):
536                 epoch_start_time = time.time()
537 
538                 # perform training step, processing all the training data once
539                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
540 
541                 # perform validation, computing the mean metrics across all validation sets (if more than one),
542                 # and check for new best result according to validation results
543                 isNewBest = False
544                 if useValidation:
545                     metricsSum = None
546                     metricsKeys = None
547                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
548                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
549                         metricsArray = np.array(list(metrics.values()))
550                         if i == 0:
551                             metricsSum = metricsArray
552                             metricsKeys = metrics.keys()
553                         else:
554                             metricsSum += metricsArray
555                     metricsSum /= len(validationSets)  # mean results
556                     metrics = dict(zip(metricsKeys, metricsSum))
557                     current_val = metrics[lossEvaluator.getValidationMetricName()]
558                     isNewBest = current_val < best_val
559                     if isNewBest:
560                         best_val = current_val
561                         best_epoch = epoch
562                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
563                     else:
564                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
565                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
566                 else:
567                     valStr = """"
568                 trainingLog(
569                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
570                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
571                 if useValidation and isNewBest:
572                     bestModelBytes = model.getModuleBytes()
573             trainingLog(""Training complete"")
574         except KeyboardInterrupt:
575             trainingLog('Exiting from training early')
576 
577         # reload best model according to validation results
578         if useValidation:
579             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
580             self.bestEpoch = best_epoch
581             model.setModuleBytes(bestModelBytes)
582 
583     def getTrainingLog(self):
","Before: 533
After: 533",add cuda to nnoptimiser log,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,6366dbb6dc78dd5c63d5743006e156359440b1ea,89de85ccd6c79a4ccf7969dafc7ff4a754bf2178,0,4622,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 360, 'parameters': 4, '(': 80, ')': 80, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 16, '{': 16, 'attribute': 94, '.': 94, '}': 16, 'string_content': 33, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 15, ']': 15, 'default_parameter': 1, '=': 55, 'true': 2, 'expression_statement': 76, 'call': 74, 'argument_list': 74, 'assignment': 50, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 5, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 29, 'end_line': 54, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 29, 'end_line': 54, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9853637086290169,0.9854302360135274,"(tensor([0.9904]), tensor([0.9909]), tensor([0.9907]), tensor([0.9909]))"
"137             result = result and self.getFeatureGenerator().isFitted()
138         return result
139 
140     def isFitted(self):
141         underlyingModelIsFitted = not self._underlyingModelRequiresFitting() or self._isFitted
142         if not underlyingModelIsFitted:
143             return False
144         if not self._preProcessorsAreFitted():
145             return False
146         return True
147 
148     def _checkModelInputColumns(self, modelInput: pd.DataFrame):
","137             result = result and self.getFeatureGenerator().isFitted()
138         return result
139 
140     def isFitted(self):
141         if not self._isUnderlyingModelFitted():
142             return False
143         if not self._preProcessorsAreFitted():
144             return False
145         return True
146 
147     def _isUnderlyingModelFitted(self):
","Before: 141, 142
After: 141, 147, 148, 149, 150",fix vector_model.py for python3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,6366dbb6dc78dd5c63d5743006e156359440b1ea,89de85ccd6c79a4ccf7969dafc7ff4a754bf2178,1,763,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 16, '=': 2, 'boolean_operator': 2, 'and': 1, 'call': 4, 'attribute': 5, '.': 5, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 4, 'return': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'not_operator': 3, 'not': 3, 'or': 1, 'if_statement': 2, 'if': 2, 'false': 2, 'true': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.502325328913208,0.4893555564916887,"(tensor([0.9532]), tensor([0.9009]), tensor([0.9263]), tensor([0.9059]))"
"173             X = self._inputTransformerChain.apply(X)
174         return X
175 
176     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
177         """"""
178         Performs a prediction for the given input data frame
179 
180         :param x: the input data
181         :return: a DataFrame with the same index as the input
182         """"""
183         if not self.isFitted():
184             raise Exception(f""Calling predict with unfitted model. ""
185                             f""This might lead to errors down the line, especially if input/output checks are enabled"")
186         x = self._computeModelInputs(x)
187         self._checkModelInputColumns(x)
188         y = self._predict(x)
189         y.index = x.index
190         return y
191 
192     @abstractmethod
","176             X = self._inputTransformerChain.apply(X)
177         return X
178 
179     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
180         """"""
181         Performs a prediction for the given input data frame
182 
183         :param x: the input data
184         :return: a DataFrame with the same index as the input
185         """"""
186         if not self.isFitted():
187             raise Exception(f""Calling predict with unfitted model {self} ""
188                             f""(isUnderlyingModelFitted={self._isUnderlyingModelFitted()}, ""
189                             f""preProcessorsAreFitted={self._preProcessorsAreFitted()})"")
190         x = self._computeModelInputs(x)
191         self._checkModelInputColumns(x)
192         y = self._predict(x)
193         y.index = x.index
194         return y
195 
196     @abstractmethod
","Before: 184, 185
After: 187, 188, 189",fix vector_model.py for python3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,6366dbb6dc78dd5c63d5743006e156359440b1ea,89de85ccd6c79a4ccf7969dafc7ff4a754bf2178,1,1101,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 32, '=': 4, 'call': 6, 'attribute': 10, '.': 10, 'argument_list': 6, '(': 7, ')': 7, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5159454412227796,0.5111742122664475,"(tensor([0.9289]), tensor([0.9279]), tensor([0.9284]), tensor([0.9280]))"
"146     def _evalModel(self, model: PredictorModel, data: InputOutputData) -> TEvalData:
147         pass
148 
149     def _computeMetrics(self, model: PredictorModel, onTrainingData=False) -> Dict[str, float]:
150         self.fitModel(model)
151         evalData = self.evalModel(model, onTrainingData=onTrainingData)
152         return evalData.getEvalStats().getAll()
153 
154     def fitModel(self, model: FittableModel):
","146     def _evalModel(self, model: PredictorModel, data: InputOutputData) -> TEvalData:
147         pass
148 
149     def _computeMetrics(self, model: FittableModel, onTrainingData=False) -> Dict[str, float]:
150         self.fitModel(model)
151         evalData = self.evalModel(model, onTrainingData=onTrainingData)
152         return evalData.getEvalStats().getAll()
153 
154     def fitModel(self, model: FittableModel):
","Before: 149
After: 149",fix typo in predictormodelevaluator.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/evaluator.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1283,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 27, 'parameters': 2, '(': 6, ',': 6, 'typed_parameter': 3, ':': 5, 'type': 7, ')': 6, '->': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'default_parameter': 1, '=': 3, 'false': 1, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'expression_statement': 2, 'call': 4, 'attribute': 4, '.': 4, 'argument_list': 4, 'assignment': 1, 'keyword_argument': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 9, 'token_count': 19, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 9, 'token_count': 19, 'name': '_computeMetrics', 'long_name': '_computeMetrics( self , model , ** kwargs )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' model', ' ** kwargs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/evaluator.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9660854289024723,0.9617726407823857,"(tensor([0.9930]), tensor([0.9953]), tensor([0.9942]), tensor([0.9951]))"
"175     """"""
176     log = log.getChild(__qualname__)
177 
178     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]], List[Dict[str, Sequence[Any]]]],
179             numProcesses=1, csvResultsPath: str = None, parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None):
180         """"""
181         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
182         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
183             where each dictionary in the list has the same keys
184         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
185         :param csvResultsPath: the path of a CSV file to which the results shall be written
186         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
187             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
188         """"""
189         self.modelFactory = modelFactory
190         if type(parameterOptions) == list:
191             self.parameterOptionsList = parameterOptions
192             paramNames = set(parameterOptions[0].keys())
193             for d in parameterOptions[1:]:
194                 if set(d.keys()) != paramNames:
195                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
196         else:
197             self.parameterOptionsList = [parameterOptions]
198         self.numProcesses = numProcesses
199         self.csvResultsPath = csvResultsPath
200         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
201 
202         self.numCombinations = 0
203         for parameterOptions in self.parameterOptionsList:
204             n = 1
205             for options in parameterOptions.values():
206                 n *= len(options)
207             self.numCombinations += n
208         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
209 
210         self._executor = None
211 
212     @classmethod
","177     """"""
178     log = log.getChild(__qualname__)
179 
180     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]],
181             List[Dict[str, Sequence[Any]]]], numProcesses=1, csvResultsPath: str = None,
182             parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None, modelSaveDirectory: str = None,
183             name: str = None):
184         """"""
185         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
186         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
187             where each dictionary in the list has the same keys
188         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
189         :param csvResultsPath: the path to a directory or concrete CSV file to which the results shall be written;
190             if it is None, no CSV data will be written; if it is a directory, a file name starting with this grid search's name (see below)
191             will be created.
192             The resulting CSV data will contain one line per evaluated parameter combination.
193         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
194             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
195         :param modelSaveDirectory the directory where the serialized models shall be saved; if None, models are not saved
196         :param name: the name of this grid search, which will, in particular, be prepended to all saved model files;
197             if None, a default name will be generated of the form ""gridSearch_<timestamp>""
198         """"""
199         self.modelFactory = modelFactory
200         if type(parameterOptions) == list:
201             self.parameterOptionsList = parameterOptions
202             paramNames = set(parameterOptions[0].keys())
203             for d in parameterOptions[1:]:
204                 if set(d.keys()) != paramNames:
205                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
206         else:
207             self.parameterOptionsList = [parameterOptions]
208         self.numProcesses = numProcesses
209         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
210         self.modelSaveDirectory = modelSaveDirectory
211         self.name = name if name is not None else ""gridSearch_"" + datetime.now().strftime('%Y%m%d-%H%M%S')
212         self.csvResultsPath = csvResultsPath
213         if self.csvResultsPath is not None and os.path.isdir(csvResultsPath):
214             self.csvResultsPath = os.path.join(self.csvResultsPath, f""{self.name}_results.csv"")
215 
216         self.numCombinations = 0
217         for parameterOptions in self.parameterOptionsList:
218             n = 1
219             for options in parameterOptions.values():
220                 n *= len(options)
221             self.numCombinations += n
222         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
223 
224         self._executor = None
225 
226     @classmethod
","Before: 178, 179
After: 180, 181, 182, 183",update gridsearch hyperopt,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1209,"{'module': 1, 'expression_statement': 8, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 14, ':': 10, 'identifier': 111, 'assignment': 3, 'type': 3, 'try': 1, 'binary_operator': 3, '-': 2, ',': 1, 'comparison_operator': 2, 'in': 2, 'for': 1, 'call': 2, 'argument_list': 2, '(': 2, 'integer': 1, ')': 2, 'boolean_operator': 1, 'attribute': 2, '.': 2, 'is': 1, '/': 1, 'or': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4917772967607936,0.48349525779069513,"(tensor([0.8863]), tensor([0.9459]), tensor([0.9151]), tensor([0.9396]))"
"175     """"""
176     log = log.getChild(__qualname__)
177 
178     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]], List[Dict[str, Sequence[Any]]]],
179             numProcesses=1, csvResultsPath: str = None, parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None):
180         """"""
181         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
182         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
183             where each dictionary in the list has the same keys
184         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
185         :param csvResultsPath: the path of a CSV file to which the results shall be written
186         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
187             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
188         """"""
189         self.modelFactory = modelFactory
190         if type(parameterOptions) == list:
191             self.parameterOptionsList = parameterOptions
192             paramNames = set(parameterOptions[0].keys())
193             for d in parameterOptions[1:]:
194                 if set(d.keys()) != paramNames:
195                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
196         else:
197             self.parameterOptionsList = [parameterOptions]
198         self.numProcesses = numProcesses
199         self.csvResultsPath = csvResultsPath
200         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
201 
202         self.numCombinations = 0
203         for parameterOptions in self.parameterOptionsList:
204             n = 1
205             for options in parameterOptions.values():
206                 n *= len(options)
207             self.numCombinations += n
208         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
209 
210         self._executor = None
211 
212     @classmethod
","177     """"""
178     log = log.getChild(__qualname__)
179 
180     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]],
181             List[Dict[str, Sequence[Any]]]], numProcesses=1, csvResultsPath: str = None,
182             parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None, modelSaveDirectory: str = None,
183             name: str = None):
184         """"""
185         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
186         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
187             where each dictionary in the list has the same keys
188         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
189         :param csvResultsPath: the path to a directory or concrete CSV file to which the results shall be written;
190             if it is None, no CSV data will be written; if it is a directory, a file name starting with this grid search's name (see below)
191             will be created.
192             The resulting CSV data will contain one line per evaluated parameter combination.
193         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
194             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
195         :param modelSaveDirectory the directory where the serialized models shall be saved; if None, models are not saved
196         :param name: the name of this grid search, which will, in particular, be prepended to all saved model files;
197             if None, a default name will be generated of the form ""gridSearch_<timestamp>""
198         """"""
199         self.modelFactory = modelFactory
200         if type(parameterOptions) == list:
201             self.parameterOptionsList = parameterOptions
202             paramNames = set(parameterOptions[0].keys())
203             for d in parameterOptions[1:]:
204                 if set(d.keys()) != paramNames:
205                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
206         else:
207             self.parameterOptionsList = [parameterOptions]
208         self.numProcesses = numProcesses
209         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
210         self.modelSaveDirectory = modelSaveDirectory
211         self.name = name if name is not None else ""gridSearch_"" + datetime.now().strftime('%Y%m%d-%H%M%S')
212         self.csvResultsPath = csvResultsPath
213         if self.csvResultsPath is not None and os.path.isdir(csvResultsPath):
214             self.csvResultsPath = os.path.join(self.csvResultsPath, f""{self.name}_results.csv"")
215 
216         self.numCombinations = 0
217         for parameterOptions in self.parameterOptionsList:
218             n = 1
219             for options in parameterOptions.values():
220                 n *= len(options)
221             self.numCombinations += n
222         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
223 
224         self._executor = None
225 
226     @classmethod
","Before: 185
After: 189, 190, 191, 192, 195, 196, 197",update gridsearch hyperopt,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1361,"{'module': 1, 'expression_statement': 8, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 14, ':': 10, 'identifier': 111, 'assignment': 3, 'type': 3, 'try': 1, 'binary_operator': 3, '-': 2, ',': 1, 'comparison_operator': 2, 'in': 2, 'for': 1, 'call': 2, 'argument_list': 2, '(': 2, 'integer': 1, ')': 2, 'boolean_operator': 1, 'attribute': 2, '.': 2, 'is': 1, '/': 1, 'or': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4917772967607936,0.48349525779069513,"(tensor([0.8863]), tensor([0.9459]), tensor([0.9151]), tensor([0.9396]))"
"175     """"""
176     log = log.getChild(__qualname__)
177 
178     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]], List[Dict[str, Sequence[Any]]]],
179             numProcesses=1, csvResultsPath: str = None, parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None):
180         """"""
181         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
182         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
183             where each dictionary in the list has the same keys
184         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
185         :param csvResultsPath: the path of a CSV file to which the results shall be written
186         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
187             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
188         """"""
189         self.modelFactory = modelFactory
190         if type(parameterOptions) == list:
191             self.parameterOptionsList = parameterOptions
192             paramNames = set(parameterOptions[0].keys())
193             for d in parameterOptions[1:]:
194                 if set(d.keys()) != paramNames:
195                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
196         else:
197             self.parameterOptionsList = [parameterOptions]
198         self.numProcesses = numProcesses
199         self.csvResultsPath = csvResultsPath
200         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
201 
202         self.numCombinations = 0
203         for parameterOptions in self.parameterOptionsList:
204             n = 1
205             for options in parameterOptions.values():
206                 n *= len(options)
207             self.numCombinations += n
208         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
209 
210         self._executor = None
211 
212     @classmethod
","177     """"""
178     log = log.getChild(__qualname__)
179 
180     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]],
181             List[Dict[str, Sequence[Any]]]], numProcesses=1, csvResultsPath: str = None,
182             parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None, modelSaveDirectory: str = None,
183             name: str = None):
184         """"""
185         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
186         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
187             where each dictionary in the list has the same keys
188         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
189         :param csvResultsPath: the path to a directory or concrete CSV file to which the results shall be written;
190             if it is None, no CSV data will be written; if it is a directory, a file name starting with this grid search's name (see below)
191             will be created.
192             The resulting CSV data will contain one line per evaluated parameter combination.
193         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
194             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
195         :param modelSaveDirectory the directory where the serialized models shall be saved; if None, models are not saved
196         :param name: the name of this grid search, which will, in particular, be prepended to all saved model files;
197             if None, a default name will be generated of the form ""gridSearch_<timestamp>""
198         """"""
199         self.modelFactory = modelFactory
200         if type(parameterOptions) == list:
201             self.parameterOptionsList = parameterOptions
202             paramNames = set(parameterOptions[0].keys())
203             for d in parameterOptions[1:]:
204                 if set(d.keys()) != paramNames:
205                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
206         else:
207             self.parameterOptionsList = [parameterOptions]
208         self.numProcesses = numProcesses
209         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
210         self.modelSaveDirectory = modelSaveDirectory
211         self.name = name if name is not None else ""gridSearch_"" + datetime.now().strftime('%Y%m%d-%H%M%S')
212         self.csvResultsPath = csvResultsPath
213         if self.csvResultsPath is not None and os.path.isdir(csvResultsPath):
214             self.csvResultsPath = os.path.join(self.csvResultsPath, f""{self.name}_results.csv"")
215 
216         self.numCombinations = 0
217         for parameterOptions in self.parameterOptionsList:
218             n = 1
219             for options in parameterOptions.values():
220                 n *= len(options)
221             self.numCombinations += n
222         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
223 
224         self._executor = None
225 
226     @classmethod
","Before: 199
After: 210, 211, 212, 213, 214",update gridsearch hyperopt,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1363,"{'module': 1, 'expression_statement': 8, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 14, ':': 10, 'identifier': 111, 'assignment': 3, 'type': 3, 'try': 1, 'binary_operator': 3, '-': 2, ',': 1, 'comparison_operator': 2, 'in': 2, 'for': 1, 'call': 2, 'argument_list': 2, '(': 2, 'integer': 1, ')': 2, 'boolean_operator': 1, 'attribute': 2, '.': 2, 'is': 1, '/': 1, 'or': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4917772967607936,0.48349525779069513,"(tensor([0.8863]), tensor([0.9459]), tensor([0.9151]), tensor([0.9396]))"
"210         self._executor = None
211 
212     @classmethod
213     def _evalParams(cls, modelFactory, metricsEvaluator: MetricsDictProvider, skipDecider: ParameterCombinationSkipDecider, **params) -> Optional[Dict[str, Any]]:
214         if skipDecider is not None:
215             if skipDecider.isSkipped(params):
216                 cls.log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
217                 return None
218         cls.log.info(f""Evaluating {params}"")
219         model = modelFactory(**params)
220         # TODO or not TODO: for some evaluators additional kwargs can be passed, e.g. onTrainingData
221         values = metricsEvaluator.computeMetrics(model)
222         values[""str(model)""] = str(model)
223         values.update(**params)
224         if skipDecider is not None:
225             skipDecider.tell(params, values)
226         return values
227 
228     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
","224         self._executor = None
225 
226     @classmethod
227     def _evalParams(cls, modelFactory: Callable[..., VectorModel], metricsEvaluator: MetricsDictProvider,
228             skipDecider: ParameterCombinationSkipDecider, gridSearchName, combinationIdx,
229             modelSaveDirectory: Optional[str], **params) -> Optional[Dict[str, Any]]:
230         if skipDecider is not None:
231             if skipDecider.isSkipped(params):
232                 cls.log.info(f""Parameter combination is skipped according to {skipDecider}: {params}"")
233                 return None
234         cls.log.info(f""Evaluating {params}"")
235         model = modelFactory(**params)
236         # TODO or not TODO: for some evaluators additional kwargs can be passed, e.g. onTrainingData
237         values = metricsEvaluator.computeMetrics(model)
238         if modelSaveDirectory is not None:
239             filename = f""{gridSearchName}_{model.__class__.__name__}_{combinationIdx}_{uuid.uuid4()}.pickle""
240             log.info(f""Saving trained model to {filename} ..."")
241             model.save(os.path.join(modelSaveDirectory, filename))
242             values[""filename""] = filename
243         values[""str(model)""] = str(model)
244         values.update(**params)
245         if skipDecider is not None:
246             skipDecider.tell(params, values)
247         return values
248 
249     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
","Before: 213
After: 227, 228, 229",update gridsearch hyperopt,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1504,"{'module': 1, 'expression_statement': 8, 'assignment': 4, 'attribute': 9, 'identifier': 47, '.': 9, '=': 4, 'none': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 6, 'typed_parameter': 2, ':': 6, 'type': 6, 'dictionary_splat_pattern': 1, '**': 3, ')': 9, '->': 1, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 4, 'if_statement': 3, 'if': 3, 'comparison_operator': 2, 'is not': 4, 'call': 8, 'argument_list': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'interpolation': 3, '{': 3, '}': 3, 'string_end': 3, 'return_statement': 2, 'return': 2, 'dictionary_splat': 2, 'comment': 1, 'subscript': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5022200822625962,0.4955430604467175,"(tensor([0.8730]), tensor([0.9303]), tensor([0.9007]), tensor([0.9242]))"
"225             skipDecider.tell(params, values)
226         return values
227 
228     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
229         """"""
230         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
231         to that file directly after being computed
232 
233         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
234         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
235             Note that the column names that are generated depend on the evaluator/validator being applied.
236         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
237         :return: the data frame with all evaluation results
238         """"""
239         if self.trackedExperiment is not None:
240             loggingCallback = self.trackedExperiment.trackValues
241         elif metricsEvaluator.trackedExperiment is not None:
242             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
243         else:
244             loggingCallback = None
245         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath,
246                                                   sortColumnName=sortColumnName, ascending=ascending)
247 
248         def collectResult(values):
249             if values is None:
250                 return
251             if loggingCallback is not None:
252                 loggingCallback(values)
253             paramsMetricsCollection.addValues(values)
254             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
255 
256         if self.numProcesses == 1:
257             for parameterOptions in self.parameterOptionsList:
258                 for paramsDict in iterParamCombinations(parameterOptions):
259                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, **paramsDict))
260         else:
261             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
262             futures = []
263             for parameterOptions in self.parameterOptionsList:
264                 for paramsDict in iterParamCombinations(parameterOptions):
265                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
266                                                    **paramsDict))
267             for future in futures:
268                 collectResult(future.result())
269 
270         return paramsMetricsCollection.getDataFrame()
271 
272 
","246             skipDecider.tell(params, values)
247         return values
248 
249     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
250         """"""
251         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
252         to that file directly after being computed
253 
254         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
255         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
256             Note that the column names that are generated depend on the evaluator/validator being applied.
257         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
258         :return: the data frame with all evaluation results
259         """"""
260         if self.trackedExperiment is not None:
261             loggingCallback = self.trackedExperiment.trackValues
262         elif metricsEvaluator.trackedExperiment is not None:
263             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
264         else:
265             loggingCallback = None
266         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
267                 ascending=ascending)
268 
269         def collectResult(values):
270             if values is None:
271                 return
272             if loggingCallback is not None:
273                 loggingCallback(values)
274             paramsMetricsCollection.addValues(values)
275             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
276 
277         if self.numProcesses == 1:
278             combinationIdx = 0
279             for parameterOptions in self.parameterOptionsList:
280                 for paramsDict in iterParamCombinations(parameterOptions):
281                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
282                             combinationIdx, self.modelSaveDirectory, **paramsDict))
283                     combinationIdx += 1
284         else:
285             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
286             futures = []
287             combinationIdx = 0
288             for parameterOptions in self.parameterOptionsList:
289                 for paramsDict in iterParamCombinations(parameterOptions):
290                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
291                             self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
292                     combinationIdx += 1
293             for future in futures:
294                 collectResult(future.result())
295 
296         return paramsMetricsCollection.getDataFrame()
297 
298 
","Before: 245, 246
After: 266, 267",update gridsearch hyperopt,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1757,"{'module': 1, 'expression_statement': 14, 'call': 17, 'attribute': 27, 'identifier': 97, '.': 27, 'argument_list': 17, '(': 19, ',': 13, ')': 19, 'return_statement': 3, 'return': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 15, 'type': 2, 'default_parameter': 2, '=': 12, 'none': 6, 'true': 1, '->': 1, 'block': 14, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is not': 6, 'assignment': 6, 'elif_clause': 1, 'elif': 1, 'else_clause': 2, 'else': 2, 'keyword_argument': 4, 'is': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'dictionary_splat': 2, '**': 2, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.648887945339349,0.6428962226713333,"(tensor([0.9461]), tensor([0.9666]), tensor([0.9562]), tensor([0.9645]))"
"225             skipDecider.tell(params, values)
226         return values
227 
228     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
229         """"""
230         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
231         to that file directly after being computed
232 
233         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
234         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
235             Note that the column names that are generated depend on the evaluator/validator being applied.
236         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
237         :return: the data frame with all evaluation results
238         """"""
239         if self.trackedExperiment is not None:
240             loggingCallback = self.trackedExperiment.trackValues
241         elif metricsEvaluator.trackedExperiment is not None:
242             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
243         else:
244             loggingCallback = None
245         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath,
246                                                   sortColumnName=sortColumnName, ascending=ascending)
247 
248         def collectResult(values):
249             if values is None:
250                 return
251             if loggingCallback is not None:
252                 loggingCallback(values)
253             paramsMetricsCollection.addValues(values)
254             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
255 
256         if self.numProcesses == 1:
257             for parameterOptions in self.parameterOptionsList:
258                 for paramsDict in iterParamCombinations(parameterOptions):
259                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, **paramsDict))
260         else:
261             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
262             futures = []
263             for parameterOptions in self.parameterOptionsList:
264                 for paramsDict in iterParamCombinations(parameterOptions):
265                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
266                                                    **paramsDict))
267             for future in futures:
268                 collectResult(future.result())
269 
270         return paramsMetricsCollection.getDataFrame()
271 
272 
","246             skipDecider.tell(params, values)
247         return values
248 
249     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
250         """"""
251         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
252         to that file directly after being computed
253 
254         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
255         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
256             Note that the column names that are generated depend on the evaluator/validator being applied.
257         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
258         :return: the data frame with all evaluation results
259         """"""
260         if self.trackedExperiment is not None:
261             loggingCallback = self.trackedExperiment.trackValues
262         elif metricsEvaluator.trackedExperiment is not None:
263             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
264         else:
265             loggingCallback = None
266         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
267                 ascending=ascending)
268 
269         def collectResult(values):
270             if values is None:
271                 return
272             if loggingCallback is not None:
273                 loggingCallback(values)
274             paramsMetricsCollection.addValues(values)
275             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
276 
277         if self.numProcesses == 1:
278             combinationIdx = 0
279             for parameterOptions in self.parameterOptionsList:
280                 for paramsDict in iterParamCombinations(parameterOptions):
281                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
282                             combinationIdx, self.modelSaveDirectory, **paramsDict))
283                     combinationIdx += 1
284         else:
285             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
286             futures = []
287             combinationIdx = 0
288             for parameterOptions in self.parameterOptionsList:
289                 for paramsDict in iterParamCombinations(parameterOptions):
290                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
291                             self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
292                     combinationIdx += 1
293             for future in futures:
294                 collectResult(future.result())
295 
296         return paramsMetricsCollection.getDataFrame()
297 
298 
","Before: 259
After: 281, 282, 283, 287",update gridsearch hyperopt,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1909,"{'module': 1, 'expression_statement': 14, 'call': 17, 'attribute': 27, 'identifier': 97, '.': 27, 'argument_list': 17, '(': 19, ',': 13, ')': 19, 'return_statement': 3, 'return': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 15, 'type': 2, 'default_parameter': 2, '=': 12, 'none': 6, 'true': 1, '->': 1, 'block': 14, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is not': 6, 'assignment': 6, 'elif_clause': 1, 'elif': 1, 'else_clause': 2, 'else': 2, 'keyword_argument': 4, 'is': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'dictionary_splat': 2, '**': 2, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.648887945339349,0.6428962226713333,"(tensor([0.9461]), tensor([0.9666]), tensor([0.9562]), tensor([0.9645]))"
"225             skipDecider.tell(params, values)
226         return values
227 
228     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
229         """"""
230         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
231         to that file directly after being computed
232 
233         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
234         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
235             Note that the column names that are generated depend on the evaluator/validator being applied.
236         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
237         :return: the data frame with all evaluation results
238         """"""
239         if self.trackedExperiment is not None:
240             loggingCallback = self.trackedExperiment.trackValues
241         elif metricsEvaluator.trackedExperiment is not None:
242             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
243         else:
244             loggingCallback = None
245         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath,
246                                                   sortColumnName=sortColumnName, ascending=ascending)
247 
248         def collectResult(values):
249             if values is None:
250                 return
251             if loggingCallback is not None:
252                 loggingCallback(values)
253             paramsMetricsCollection.addValues(values)
254             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
255 
256         if self.numProcesses == 1:
257             for parameterOptions in self.parameterOptionsList:
258                 for paramsDict in iterParamCombinations(parameterOptions):
259                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, **paramsDict))
260         else:
261             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
262             futures = []
263             for parameterOptions in self.parameterOptionsList:
264                 for paramsDict in iterParamCombinations(parameterOptions):
265                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
266                                                    **paramsDict))
267             for future in futures:
268                 collectResult(future.result())
269 
270         return paramsMetricsCollection.getDataFrame()
271 
272 
","246             skipDecider.tell(params, values)
247         return values
248 
249     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
250         """"""
251         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
252         to that file directly after being computed
253 
254         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
255         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
256             Note that the column names that are generated depend on the evaluator/validator being applied.
257         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
258         :return: the data frame with all evaluation results
259         """"""
260         if self.trackedExperiment is not None:
261             loggingCallback = self.trackedExperiment.trackValues
262         elif metricsEvaluator.trackedExperiment is not None:
263             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
264         else:
265             loggingCallback = None
266         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
267                 ascending=ascending)
268 
269         def collectResult(values):
270             if values is None:
271                 return
272             if loggingCallback is not None:
273                 loggingCallback(values)
274             paramsMetricsCollection.addValues(values)
275             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
276 
277         if self.numProcesses == 1:
278             combinationIdx = 0
279             for parameterOptions in self.parameterOptionsList:
280                 for paramsDict in iterParamCombinations(parameterOptions):
281                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
282                             combinationIdx, self.modelSaveDirectory, **paramsDict))
283                     combinationIdx += 1
284         else:
285             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
286             futures = []
287             combinationIdx = 0
288             for parameterOptions in self.parameterOptionsList:
289                 for paramsDict in iterParamCombinations(parameterOptions):
290                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
291                             self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
292                     combinationIdx += 1
293             for future in futures:
294                 collectResult(future.result())
295 
296         return paramsMetricsCollection.getDataFrame()
297 
298 
","Before: 266
After: 291, 292",update gridsearch hyperopt,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1995,"{'module': 1, 'expression_statement': 14, 'call': 17, 'attribute': 27, 'identifier': 97, '.': 27, 'argument_list': 17, '(': 19, ',': 13, ')': 19, 'return_statement': 3, 'return': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 15, 'type': 2, 'default_parameter': 2, '=': 12, 'none': 6, 'true': 1, '->': 1, 'block': 14, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is not': 6, 'assignment': 6, 'elif_clause': 1, 'elif': 1, 'else_clause': 2, 'else': 2, 'keyword_argument': 4, 'is': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 1, 'for_statement': 5, 'for': 5, 'in': 5, 'dictionary_splat': 2, '**': 2, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 29, 'end_line': 42, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.648887945339349,0.6428962226713333,"(tensor([0.9461]), tensor([0.9666]), tensor([0.9562]), tensor([0.9645]))"
"1 import io
2 import logging
3 from abc import ABC, abstractmethod
4 from typing import Union, Tuple, Callable, Optional
5 
6 import numpy as np
7 import pandas as pd
8 import torch
","1 import io
2 import logging
3 from abc import ABC, abstractmethod
4 from typing import Union, Tuple, Callable, Optional, List
5 
6 import numpy as np
7 import pandas as pd
8 import torch
","Before: 4
After: 4, 10",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,36,"{'module': 1, 'import_statement': 4, 'import': 6, 'dotted_name': 12, 'identifier': 14, 'import_from_statement': 2, 'from': 2, ',': 4, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9076141716697395,0.9062843304842927,"(tensor([0.9925]), tensor([0.9974]), tensor([0.9949]), tensor([0.9969]))"
"8 import torch
9 from torch import nn
10 
11 from .torch_data import TensorScaler, VectorDataUtil, ClassificationVectorDataUtil, TorchDataSet, \
12     TorchDataSetProviderFromDataUtil, TorchDataSetProvider
13 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams
14 from ..normalisation import NormalisationMode
15 from ..util.dtype import toFloatArray
16 from ..util.string import objectRepr
17 from ..vector_model import VectorRegressionModel, VectorClassificationModel
","9 from torch import nn
10 from torch.nn import functional as F
11 
12 from .torch_data import TensorScaler, VectorDataUtil, ClassificationVectorDataUtil, TorchDataSet, \
13     TorchDataSetProviderFromDataUtil, TorchDataSetProvider
14 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams, TrainingInfo
15 from ..normalisation import NormalisationMode
16 from ..util.dtype import toFloatArray
17 from ..util.string import objectRepr, ToStringMixin
18 from ..vector_model import VectorRegressionModel, VectorClassificationModel
","Before: 13
After: 14",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,106,"{'module': 1, 'import_statement': 1, 'import': 7, 'dotted_name': 21, 'identifier': 23, 'import_from_statement': 6, 'from': 6, 'relative_import': 5, 'import_prefix': 5, '.': 10, ',': 8, 'line_continuation': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6209653655780962,0.6059888561272797,"(tensor([0.9591]), tensor([0.9788]), tensor([0.9689]), tensor([0.9768]))"
"11 from .torch_data import TensorScaler, VectorDataUtil, ClassificationVectorDataUtil, TorchDataSet, \
12     TorchDataSetProviderFromDataUtil, TorchDataSetProvider
13 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams
14 from ..normalisation import NormalisationMode
15 from ..util.dtype import toFloatArray
16 from ..util.string import objectRepr
17 from ..vector_model import VectorRegressionModel, VectorClassificationModel
18 
19 log = logging.getLogger(__name__)
20 
","12 from .torch_data import TensorScaler, VectorDataUtil, ClassificationVectorDataUtil, TorchDataSet, \
13     TorchDataSetProviderFromDataUtil, TorchDataSetProvider
14 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams, TrainingInfo
15 from ..normalisation import NormalisationMode
16 from ..util.dtype import toFloatArray
17 from ..util.string import objectRepr, ToStringMixin
18 from ..vector_model import VectorRegressionModel, VectorClassificationModel
19 
20 log = logging.getLogger(__name__)
21 
","Before: 16
After: 17",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,143,"{'module': 1, 'import_from_statement': 6, 'from': 6, 'relative_import': 6, 'import_prefix': 6, '.': 13, 'dotted_name': 21, 'identifier': 27, 'import': 6, ',': 9, 'line_continuation': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.66158160672937,0.6558686224306794,"(tensor([0.9766]), tensor([0.9888]), tensor([0.9827]), tensor([0.9876]))"
"81         mean = torch.mean(results, 0)
82         stddev = torch.std(results, 0, unbiased=False)
83         return mean, stddev
84 
85 
86 class TorchModel(ABC):
87     """"""
88     sensAI abstraction for torch models, which supports one-line training, allows for convenient model application,
89     has basic mechanisms for data scaling, and soundly handles persistence (via pickle).
90     An instance wraps a torch.nn.Module, which is constructed on demand during training via the factory method
","82         mean = torch.mean(results, 0)
83         stddev = torch.std(results, 0, unbiased=False)
84         return mean, stddev
85 
86 
87 class TorchModel(ABC, ToStringMixin):
88     """"""
89     sensAI abstraction for torch models, which supports one-line training, allows for convenient model application,
90     has basic mechanisms for data scaling, and soundly handles persistence (via pickle).
91     An instance wraps a torch.nn.Module, which is constructed on demand during training via the factory method
","Before: 86
After: 87",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,604,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 38, '=': 3, 'call': 3, 'attribute': 2, '.': 3, 'argument_list': 4, '(': 4, ',': 8, 'integer': 2, ')': 4, 'keyword_argument': 1, 'false': 1, 'return_statement': 1, 'return': 1, 'expression_list': 1, 'class_definition': 1, 'class': 1, ':': 1, 'ERROR': 7, 'string_start': 1, 'for': 2, 'binary_operator': 1, '-': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7575402232769872,0.7573297077020494,"(tensor([0.9659]), tensor([0.9836]), tensor([0.9747]), tensor([0.9818]))"
"88     sensAI abstraction for torch models, which supports one-line training, allows for convenient model application,
89     has basic mechanisms for data scaling, and soundly handles persistence (via pickle).
90     An instance wraps a torch.nn.Module, which is constructed on demand during training via the factory method
91     createTorchModule.
92     """"""
93     log = log.getChild(__qualname__)
94 
95     def __init__(self, cuda=True):
96         self.cuda = cuda
97         self.module: Optional[torch.nn.Module] = None
","89     sensAI abstraction for torch models, which supports one-line training, allows for convenient model application,
90     has basic mechanisms for data scaling, and soundly handles persistence (via pickle).
91     An instance wraps a torch.nn.Module, which is constructed on demand during training via the factory method
92     createTorchModule.
93     """"""
94     log: logging.Logger = log.getChild(__qualname__)
95 
96     def __init__(self, cuda=True):
97         self.cuda: bool = cuda
98         self.module: Optional[torch.nn.Module] = None
","Before: 93
After: 94",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,622,"{'module': 1, 'ERROR': 8, 'identifier': 44, 'for': 2, ',': 5, 'binary_operator': 1, '-': 1, 'attribute': 3, 'call': 1, 'argument_list': 1, '(': 1, ')': 1, '.': 4, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7187147351442571,0.719910446993468,"(tensor([0.9759]), tensor([0.9858]), tensor([0.9808]), tensor([0.9848]))"
"92     """"""
93     log = log.getChild(__qualname__)
94 
95     def __init__(self, cuda=True):
96         self.cuda = cuda
97         self.module: Optional[torch.nn.Module] = None
98         self.outputScaler: Optional[TensorScaler] = None
99         self.inputScaler: Optional[TensorScaler] = None
100         self.bestEpoch = None
101         self._gpu = None
102 
103     def setTorchModule(self, module: torch.nn.Module):
","93     """"""
94     log: logging.Logger = log.getChild(__qualname__)
95 
96     def __init__(self, cuda=True):
97         self.cuda: bool = cuda
98         self.module: Optional[torch.nn.Module] = None
99         self.outputScaler: Optional[TensorScaler] = None
100         self.inputScaler: Optional[TensorScaler] = None
101         self.trainingInfo: Optional[TrainingInfo] = None
102         self._gpu: Optional[int] = None
103 
104     def setTorchModule(self, module: torch.nn.Module) -> None:
","Before: 96
After: 97",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,644,"{'module': 1, 'ERROR': 1, 'string_start': 1, 'expression_statement': 7, 'assignment': 7, 'identifier': 28, '=': 8, 'call': 1, 'attribute': 9, '.': 9, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'default_parameter': 1, 'true': 1, ':': 4, 'block': 1, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 5}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.511560967169221,0.5103949776851985,"(tensor([0.9359]), tensor([0.9614]), tensor([0.9485]), tensor([0.9588]))"
"92     """"""
93     log = log.getChild(__qualname__)
94 
95     def __init__(self, cuda=True):
96         self.cuda = cuda
97         self.module: Optional[torch.nn.Module] = None
98         self.outputScaler: Optional[TensorScaler] = None
99         self.inputScaler: Optional[TensorScaler] = None
100         self.bestEpoch = None
101         self._gpu = None
102 
103     def setTorchModule(self, module: torch.nn.Module):
","93     """"""
94     log: logging.Logger = log.getChild(__qualname__)
95 
96     def __init__(self, cuda=True):
97         self.cuda: bool = cuda
98         self.module: Optional[torch.nn.Module] = None
99         self.outputScaler: Optional[TensorScaler] = None
100         self.inputScaler: Optional[TensorScaler] = None
101         self.trainingInfo: Optional[TrainingInfo] = None
102         self._gpu: Optional[int] = None
103 
104     def setTorchModule(self, module: torch.nn.Module) -> None:
","Before: 100, 101
After: 101, 102",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,709,"{'module': 1, 'ERROR': 1, 'string_start': 1, 'expression_statement': 7, 'assignment': 7, 'identifier': 28, '=': 8, 'call': 1, 'attribute': 9, '.': 9, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'default_parameter': 1, 'true': 1, ':': 4, 'block': 1, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 5}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.511560967169221,0.5103949776851985,"(tensor([0.9359]), tensor([0.9614]), tensor([0.9485]), tensor([0.9588]))"
"100         self.bestEpoch = None
101         self._gpu = None
102 
103     def setTorchModule(self, module: torch.nn.Module):
104         self.module = module
105 
106     def getModuleBytes(self):
","101         self.trainingInfo: Optional[TrainingInfo] = None
102         self._gpu: Optional[int] = None
103 
104     def setTorchModule(self, module: torch.nn.Module) -> None:
105         self.module = module
106 
107     def getModuleBytes(self) -> bytes:
","Before: 103
After: 104",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,738,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'attribute': 5, 'identifier': 13, '.': 5, '=': 3, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 1, ')': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3199874843297437,0.2809624728031707,"(tensor([0.8914]), tensor([0.9384]), tensor([0.9143]), tensor([0.9335]))"
"103     def setTorchModule(self, module: torch.nn.Module):
104         self.module = module
105 
106     def getModuleBytes(self):
107         bytesIO = io.BytesIO()
108         torch.save(self.module, bytesIO)
109         return bytesIO.getvalue()
110 
111     def setModuleBytes(self, modelBytes):
","104     def setTorchModule(self, module: torch.nn.Module) -> None:
105         self.module = module
106 
107     def getModuleBytes(self) -> bytes:
108         bytesIO = io.BytesIO()
109         torch.save(self.module, bytesIO)
110         return bytesIO.getvalue()
111 
112     def setModuleBytes(self, modelBytes: bytes) -> None:
","Before: 106
After: 107",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,755,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 21, 'parameters': 2, '(': 5, ',': 2, 'typed_parameter': 1, ':': 3, 'type': 1, 'attribute': 7, '.': 7, ')': 5, 'block': 2, 'expression_statement': 3, 'assignment': 2, '=': 2, 'call': 3, 'argument_list': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4735379941434666,0.4408041018250886,"(tensor([0.9321]), tensor([0.9774]), tensor([0.9542]), tensor([0.9727]))"
"108         torch.save(self.module, bytesIO)
109         return bytesIO.getvalue()
110 
111     def setModuleBytes(self, modelBytes):
112         modelFile = io.BytesIO(modelBytes)
113         self._loadModel(modelFile)
114 
115     def getTorchModule(self):
","109         torch.save(self.module, bytesIO)
110         return bytesIO.getvalue()
111 
112     def setModuleBytes(self, modelBytes: bytes) -> None:
113         modelFile = io.BytesIO(modelBytes)
114         self._loadModel(modelFile)
115 
116     def getTorchModule(self) -> torch.nn.Module:
","Before: 111
After: 112",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,803,"{'module': 1, 'expression_statement': 3, 'call': 4, 'attribute': 5, 'identifier': 17, '.': 5, 'argument_list': 4, '(': 5, ',': 2, ')': 5, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.47161200171902534,0.44394589559823766,"(tensor([0.9357]), tensor([0.9764]), tensor([0.9556]), tensor([0.9721]))"
"112         modelFile = io.BytesIO(modelBytes)
113         self._loadModel(modelFile)
114 
115     def getTorchModule(self):
116         return self.module
117 
118     def _setCudaEnabled(self, isCudaEnabled):
","113         modelFile = io.BytesIO(modelBytes)
114         self._loadModel(modelFile)
115 
116     def getTorchModule(self) -> torch.nn.Module:
117         return self.module
118 
119     def _setCudaEnabled(self, isCudaEnabled: bool) -> None:
","Before: 115
After: 116",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,835,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 11, '=': 1, 'call': 2, 'attribute': 3, '.': 3, 'argument_list': 2, '(': 3, ')': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.41361792097739225,0.38453135659558585,"(tensor([0.9271]), tensor([0.9749]), tensor([0.9504]), tensor([0.9699]))"
"115     def getTorchModule(self):
116         return self.module
117 
118     def _setCudaEnabled(self, isCudaEnabled):
119         self.cuda = isCudaEnabled
120 
121     def _isCudaEnabled(self):
","116     def getTorchModule(self) -> torch.nn.Module:
117         return self.module
118 
119     def _setCudaEnabled(self, isCudaEnabled: bool) -> None:
120         self.cuda = isCudaEnabled
121 
122     def _isCudaEnabled(self) -> bool:
","Before: 118
After: 119",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,852,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 10, 'parameters': 2, '(': 2, ')': 2, ':': 2, 'block': 2, 'return_statement': 1, 'return': 1, 'attribute': 2, '.': 2, ',': 1, 'expression_statement': 1, 'assignment': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3369788950503753,0.29565012486929926,"(tensor([0.9064]), tensor([0.9646]), tensor([0.9346]), tensor([0.9584]))"
"118     def _setCudaEnabled(self, isCudaEnabled):
119         self.cuda = isCudaEnabled
120 
121     def _isCudaEnabled(self):
122         return self.cuda
123 
124     def _loadModel(self, modelFile):
","119     def _setCudaEnabled(self, isCudaEnabled: bool) -> None:
120         self.cuda = isCudaEnabled
121 
122     def _isCudaEnabled(self) -> bool:
123         return self.cuda
124 
125     def _loadModel(self, modelFile) -> None:  # TODO: complete type hints: what types are allowed for modelFile?
","Before: 121
After: 122",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,869,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 10, 'parameters': 2, '(': 2, ',': 1, ')': 2, ':': 2, 'block': 2, 'expression_statement': 1, 'assignment': 1, 'attribute': 2, '.': 2, '=': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.28242774254471126,0.23562746768736337,"(tensor([0.8615]), tensor([0.9702]), tensor([0.9126]), tensor([0.9581]))"
"121     def _isCudaEnabled(self):
122         return self.cuda
123 
124     def _loadModel(self, modelFile):
125         try:
126             self.module = torch.load(modelFile)
127             self._gpu = self._getGPUFromModelParameterDevice()
128         except:
129             if self._isCudaEnabled():
130                 if torch.cuda.device_count() > 0:
131                     newDevice = ""cuda:0""
132                 else:
133                     newDevice = ""cpu""
134                 self.log.warning(f""Loading of CUDA model failed, trying to map model to device {newDevice}..."")
135                 if type(modelFile) != str:
136                     modelFile.seek(0)
137                 try:
138                     self.module = torch.load(modelFile, map_location=newDevice)
139                 except:
140                     self.log.warning(f""Failure to map model to device {newDevice}, trying CPU..."")
141                     if newDevice != ""cpu"":
142                         newDevice = ""cpu""
143                         self.module = torch.load(modelFile, map_location=newDevice)
144                 if newDevice == ""cpu"":
145                     self._setCudaEnabled(False)
146                     self._gpu = None
147                 else:
148                     self._gpu = 0
149                 self.log.info(f""Model successfully loaded to {newDevice}"")
150             else:
151                 raise
152 
153     @abstractmethod
","122     def _isCudaEnabled(self) -> bool:
123         return self.cuda
124 
125     def _loadModel(self, modelFile) -> None:  # TODO: complete type hints: what types are allowed for modelFile?
126         try:
127             self.module = torch.load(modelFile)
128             self._gpu = self._getGPUFromModelParameterDevice()
129         except:
130             if self._isCudaEnabled():
131                 if torch.cuda.device_count() > 0:
132                     newDevice = ""cuda:0""
133                 else:
134                     newDevice = ""cpu""
135                 self.log.warning(f""Loading of CUDA model failed, trying to map model to device {newDevice}..."")
136                 if type(modelFile) != str:
137                     modelFile.seek(0)
138                 try:
139                     self.module = torch.load(modelFile, map_location=newDevice)
140                 except:
141                     self.log.warning(f""Failure to map model to device {newDevice}, trying CPU..."")
142                     if newDevice != ""cpu"":
143                         newDevice = ""cpu""
144                         self.module = torch.load(modelFile, map_location=newDevice)
145                 if newDevice == ""cpu"":
146                     self._setCudaEnabled(False)
147                     self._gpu = None
148                 else:
149                     self._gpu = 0
150                 self.log.info(f""Model successfully loaded to {newDevice}"")
151             else:
152                 raise
153 
154     @abstractmethod
","Before: 124
After: 125",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,886,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 63, 'parameters': 2, '(': 14, ')': 14, ':': 14, 'block': 14, 'return_statement': 1, 'return': 1, 'attribute': 22, '.': 22, ',': 3, 'try_statement': 2, 'try': 2, 'expression_statement': 14, 'assignment': 9, '=': 11, 'call': 12, 'argument_list': 12, 'except_clause': 2, 'except': 2, 'if_statement': 5, 'if': 5, 'comparison_operator': 4, '>': 1, 'integer': 3, 'string': 8, 'string_start': 8, 'string_content': 10, 'string_end': 8, 'else_clause': 3, 'else': 3, 'interpolation': 3, '{': 3, '}': 3, '!=': 2, 'keyword_argument': 2, '==': 1, 'false': 1, 'none': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5941621392456582,0.5873716154104129,"(tensor([0.9532]), tensor([0.9805]), tensor([0.9667]), tensor([0.9777]))"
"154     def createTorchModule(self) -> torch.nn.Module:
155         pass
156 
157     def __getstate__(self):
158         state = dict(self.__dict__)
159         del state[""module""]
160         state[""modelBytes""] = self.getModuleBytes()
161         return state
162 
163     def __setstate__(self, d):
","155     def createTorchModule(self) -> torch.nn.Module:
156         pass
157 
158     def __getstate__(self) -> dict:
159         state = dict(self.__dict__)
160         del state[""module""]
161         state[""modelBytes""] = self.getModuleBytes()
162         return state
163 
164     def __setstate__(self, d: dict) -> None:
","Before: 157
After: 158",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1209,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 16, 'parameters': 2, '(': 4, ')': 4, '->': 1, 'type': 1, 'attribute': 4, '.': 4, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'call': 2, 'argument_list': 2, 'delete_statement': 1, 'del': 1, 'subscript': 2, '[': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5520142715612466,0.5502466143901099,"(tensor([0.9609]), tensor([0.9788]), tensor([0.9698]), tensor([0.9770]))"
"160         state[""modelBytes""] = self.getModuleBytes()
161         return state
162 
163     def __setstate__(self, d):
164         modelBytes = None
165         if ""modelBytes"" in d:
166             modelBytes = d[""modelBytes""]
167             del d[""modelBytes""]
168         self.__dict__ = d
169         if modelBytes is not None:
170             self.setModuleBytes(modelBytes)
171 
172     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy=True, createBatch=False, mcDropoutSamples=None, mcDropoutProbability=None, scaleOutput=False,
","161         state[""modelBytes""] = self.getModuleBytes()
162         return state
163 
164     def __setstate__(self, d: dict) -> None:
165         # backward compatibility
166         if ""bestEpoch"" in d:
167             d[""trainingInfo""] = TrainingInfo(bestEpoch=d[""bestEpoch""])
168             del d[""bestEpoch""]
169 
170         modelBytes = None
171         if ""modelBytes"" in d:
172             modelBytes = d[""modelBytes""]
173             del d[""modelBytes""]
174         self.__dict__ = d
175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
","Before: 163
After: 164, 165, 166, 167, 168, 169",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1265,"{'module': 1, 'expression_statement': 5, 'assignment': 4, 'subscript': 3, 'identifier': 19, '[': 3, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ']': 3, '=': 4, 'call': 2, 'attribute': 3, '.': 3, 'argument_list': 2, '(': 3, ')': 3, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, ':': 3, 'block': 3, 'none': 2, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'in': 1, 'delete_statement': 1, 'del': 1, 'is not': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.40742535484232517,0.40068417992239413,"(tensor([0.8929]), tensor([0.9119]), tensor([0.9023]), tensor([0.9100]))"
"169         if modelBytes is not None:
170             self.setModuleBytes(modelBytes)
171 
172     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy=True, createBatch=False, mcDropoutSamples=None, mcDropoutProbability=None, scaleOutput=False,
173             scaleInput=False) -> Union[torch.Tensor, np.ndarray, Tuple]:
174         """"""
175         Applies the model to the given input tensor and returns the result (normalized)
176 
177         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
178             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
179         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
180         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
181         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
182         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
183         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
184         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
185 
186         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
187             containing standard deviations
188         """"""
189         def extract(z):
190             if scaleOutput:
191                 z = self.scaledOutput(z)
192             if self._isCudaEnabled():
193                 z = z.cpu()
194             z = z.detach()
195             if asNumpy:
196                 z = z.numpy()
197             return z
198 
199         model = self.getTorchModule()
200         model.eval()
201 
202         if isinstance(X, TorchDataSet):
203             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
204         elif isinstance(X, np.ndarray):
205             X = toFloatArray(X)
206             X = torch.from_numpy(X).float()
207 
208         if self._isCudaEnabled():
209             torch.cuda.set_device(self._gpu)
210             X = X.cuda()
211         if scaleInput:
212             X = self.inputScaler.normalise(X)
213         if createBatch:
214             X = X.view(1, *X.size())
215 
216         maxValue = X.max().item()
217         if maxValue > 2:
218             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
219 
220         if mcDropoutSamples is None:
221             y = model(X)
222             return extract(y)
223         else:
224             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
225             return extract(y), extract(stddev)
226 
227     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 172, 173
After: 178, 179, 180",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1385,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 121, 'is not': 2, 'none': 4, ':': 15, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 8, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7458133431161562,0.7428434088139542,"(tensor([0.9562]), tensor([0.9585]), tensor([0.9574]), tensor([0.9583]))"
"236         """"""
237         return self.apply(X, scaleOutput=True, scaleInput=True, **kwargs)
238 
239     def scaledOutput(self, output):
240         return self.outputScaler.denormalise(output)
241 
242     def _extractParamsFromData(self, data: TorchDataSetProvider):
","243         """"""
244         return self.apply(X, scaleOutput=True, scaleInput=True, **kwargs)
245 
246     def scaledOutput(self, output: torch.Tensor) -> torch.Tensor:
247         return self.outputScaler.denormalise(output)
248 
249     def _extractParamsFromData(self, data: TorchDataSetProvider) -> None:
","Before: 239
After: 246",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1913,"{'module': 1, 'ERROR': 1, 'string_start': 1, 'return_statement': 2, 'return': 2, 'call': 2, 'attribute': 3, 'identifier': 13, '.': 3, 'argument_list': 2, '(': 3, ',': 4, 'keyword_argument': 2, '=': 2, 'true': 2, 'dictionary_splat': 1, '**': 1, ')': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5430551649679416,0.47977697946141573,"(tensor([0.9172]), tensor([0.9728]), tensor([0.9442]), tensor([0.9670]))"
"239     def scaledOutput(self, output):
240         return self.outputScaler.denormalise(output)
241 
242     def _extractParamsFromData(self, data: TorchDataSetProvider):
243         self.outputScaler = data.getOutputTensorScaler()
244         self.inputScaler = data.getInputTensorScaler()
245 
246     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None):
","246     def scaledOutput(self, output: torch.Tensor) -> torch.Tensor:
247         return self.outputScaler.denormalise(output)
248 
249     def _extractParamsFromData(self, data: TorchDataSetProvider) -> None:
250         self.outputScaler = data.getOutputTensorScaler()
251         self.inputScaler = data.getInputTensorScaler()
252 
253     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None) -> None:
","Before: 242
After: 249",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,1942,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 19, 'parameters': 2, '(': 5, ',': 2, ')': 5, ':': 3, 'block': 2, 'return_statement': 1, 'return': 1, 'call': 3, 'attribute': 6, '.': 6, 'argument_list': 3, 'typed_parameter': 1, 'type': 1, 'expression_statement': 2, 'assignment': 2, '=': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5886331887260239,0.5371437275078573,"(tensor([0.9471]), tensor([0.9726]), tensor([0.9597]), tensor([0.9700]))"
"243         self.outputScaler = data.getOutputTensorScaler()
244         self.inputScaler = data.getInputTensorScaler()
245 
246     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None):
247         """"""
248         Fits this model using the given model and strategy
249 
250         :param data: a provider for the data with which to fit the model
251         :param strategy: the fitting strategy; if None, use TorchModelFittingStrategyDefault.
252             Pass your own strategy to perform custom fitting processes, e.g. process which involve multi-stage learning
253         :param nnOptimiserParams: the parameters with which to create an optimiser which can be applied in the fitting strategy
254         """"""
255         self._extractParamsFromData(data)
256         optimiser = NNOptimiser(nnOptimiserParams)
257         if strategy is None:
258             strategy = TorchModelFittingStrategyDefault()
259         strategy.fit(self, data, optimiser)
260         self.bestEpoch = optimiser.getBestEpoch()
261         self._gpu = self._getGPUFromModelParameterDevice()
262 
263     def _getGPUFromModelParameterDevice(self) -> Optional[int]:
","250         self.outputScaler = data.getOutputTensorScaler()
251         self.inputScaler = data.getInputTensorScaler()
252 
253     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None) -> None:
254         """"""
255         Fits this model using the given model and strategy
256 
257         :param data: a provider for the data with which to fit the model
258         :param strategy: the fitting strategy; if None, use TorchModelFittingStrategyDefault.
259             Pass your own strategy to perform custom fitting processes, e.g. process which involve multi-stage learning
260         :param nnOptimiserParams: the parameters with which to create an optimiser which can be applied in the fitting strategy
261         """"""
262         self._extractParamsFromData(data)
263         optimiser = NNOptimiser(nnOptimiserParams)
264         if strategy is None:
265             strategy = TorchModelFittingStrategyDefault()
266         strategy.fit(self, data, optimiser)
267         self.trainingInfo = optimiser.getTrainingInfo()
268         self._gpu = self._getGPUFromModelParameterDevice()
269 
270     def _getGPUFromModelParameterDevice(self) -> Optional[int]:
","Before: 246
After: 253",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,2004,"{'module': 1, 'expression_statement': 9, 'assignment': 6, 'attribute': 10, 'identifier': 37, '.': 10, '=': 7, 'call': 8, 'argument_list': 8, '(': 9, ')': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 3, 'typed_default_parameter': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'none': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.730871350256907,0.7217504136976061,"(tensor([0.9739]), tensor([0.9740]), tensor([0.9740]), tensor([0.9740]))"
"243         self.outputScaler = data.getOutputTensorScaler()
244         self.inputScaler = data.getInputTensorScaler()
245 
246     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None):
247         """"""
248         Fits this model using the given model and strategy
249 
250         :param data: a provider for the data with which to fit the model
251         :param strategy: the fitting strategy; if None, use TorchModelFittingStrategyDefault.
252             Pass your own strategy to perform custom fitting processes, e.g. process which involve multi-stage learning
253         :param nnOptimiserParams: the parameters with which to create an optimiser which can be applied in the fitting strategy
254         """"""
255         self._extractParamsFromData(data)
256         optimiser = NNOptimiser(nnOptimiserParams)
257         if strategy is None:
258             strategy = TorchModelFittingStrategyDefault()
259         strategy.fit(self, data, optimiser)
260         self.bestEpoch = optimiser.getBestEpoch()
261         self._gpu = self._getGPUFromModelParameterDevice()
262 
263     def _getGPUFromModelParameterDevice(self) -> Optional[int]:
","250         self.outputScaler = data.getOutputTensorScaler()
251         self.inputScaler = data.getInputTensorScaler()
252 
253     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None) -> None:
254         """"""
255         Fits this model using the given model and strategy
256 
257         :param data: a provider for the data with which to fit the model
258         :param strategy: the fitting strategy; if None, use TorchModelFittingStrategyDefault.
259             Pass your own strategy to perform custom fitting processes, e.g. process which involve multi-stage learning
260         :param nnOptimiserParams: the parameters with which to create an optimiser which can be applied in the fitting strategy
261         """"""
262         self._extractParamsFromData(data)
263         optimiser = NNOptimiser(nnOptimiserParams)
264         if strategy is None:
265             strategy = TorchModelFittingStrategyDefault()
266         strategy.fit(self, data, optimiser)
267         self.trainingInfo = optimiser.getTrainingInfo()
268         self._gpu = self._getGPUFromModelParameterDevice()
269 
270     def _getGPUFromModelParameterDevice(self) -> Optional[int]:
","Before: 260
After: 267",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,2075,"{'module': 1, 'expression_statement': 9, 'assignment': 6, 'attribute': 10, 'identifier': 37, '.': 10, '=': 7, 'call': 8, 'argument_list': 8, '(': 9, ')': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 3, 'typed_default_parameter': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'none': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.730871350256907,0.7217504136976061,"(tensor([0.9739]), tensor([0.9740]), tensor([0.9740]), tensor([0.9740]))"
"434             normalisationMode=self.normalisationMode)
435         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
436 
437     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
438         if len(outputs.columns) != 1:
439             raise ValueError(""Expected one output dimension: the class labels"")
440 
441         # transform outputs: for each data point, the new output shall be the index in the list of labels
442         labels: pd.Series = outputs.iloc[:, 0]
443         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
444 
445         self.model = self._createTorchModel()
446 
447         dataSetProvider = self._createDataSetProvider(inputs, outputs)
448         self.model.fit(dataSetProvider, **self.nnOptimiserParams)
449 
450     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","444             normalisationMode=self.normalisationMode)
445         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
446 
447     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
448         if len(outputs.columns) != 1:
449             raise ValueError(""Expected one output dimension: the class labels"")
450 
451         # transform outputs: for each data point, the new output shall be the index in the list of labels
452         labels: pd.Series = outputs.iloc[:, 0]
453         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
454 
455         self.model = self._createTorchModel()
456 
457         dataSetProvider = self._createDataSetProvider(inputs, outputs)
458         self.model.fit(dataSetProvider, self.nnOptimiserParams)
459 
460     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","Before: 448
After: 458",remove unused imports from src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,3839,"{'module': 1, 'ERROR': 1, 'identifier': 55, '=': 7, 'attribute': 19, '.': 19, ')': 10, 'return_statement': 1, 'return': 1, 'call': 8, 'argument_list': 8, '(': 9, ',': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 3, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'integer': 2, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'comment': 1, 'expression_statement': 5, 'assignment': 4, 'subscript': 1, '[': 2, 'slice': 1, ']': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'keyword_argument': 2, 'dictionary_splat': 1, '**': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 29, 'end_line': 32, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7581769449919338,0.7465022924492218,"(tensor([0.9850]), tensor([0.9828]), tensor([0.9839]), tensor([0.9830]))"
"1 import logging
2 
3 import torch
4 
5 from .mlp_modules import MultiLayerPerceptron
6 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel, TorchVectorClassificationModel
7 from ...torch_opt import NNOptimiserParams
","1 import logging
2 
3 import torch.nn.functional
4 
5 from .mlp_modules import MultiLayerPerceptron
6 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel, TorchVectorClassificationModel
7 from ...torch_opt import NNOptimiserParams
","Before: 3
After: 3",fix multilayerperceptrontorchmodels.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,10,"{'module': 1, 'import_statement': 2, 'import': 4, 'dotted_name': 8, 'identifier': 8, 'import_from_statement': 2, 'from': 2, 'relative_import': 2, 'import_prefix': 2, '.': 4, ',': 2}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9061874434879645,0.8931749530317588,"(tensor([0.9801]), tensor([0.9892]), tensor([0.9846]), tensor([0.9883]))"
"18         self.hiddenDims = hiddenDims
19         self.pDropout = pDropout
20 
21     def __str__(self):
22         return f""_MLP[hiddenDims={self.hiddenDims}, hidAct={self.hidActivationFunction.__name__}, outAct={self.outputActivationFunction.__name__ if self.outputActivationFunction is not None else None}, pDropout={self.pDropout}]""
23 
24     def createTorchModuleForDims(self, inputDim, outputDim):
","18         self.hiddenDims = hiddenDims
19         self.pDropout = pDropout
20 
21     def __str__(self):
22         def name(x):
23             if hasattr(x, ""__name__""):
24                 return x.__name__
25             elif hasattr(x, ""__class__""):
26                 return x.__class__.__name__
27             else:
28                 return str(x)
29 
30         return f""_MLP[hiddenDims={self.hiddenDims}, hidAct={name(self.hidActivationFunction)}, outAct={name(self.outputActivationFunction)}, pDropout={self.pDropout}]""
31 
32     def createTorchModuleForDims(self, inputDim, outputDim):
","Before: 22
After: 22, 23, 24, 25, 26, 27, 28, 29, 30",fix multilayerperceptrontorchmodels.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,219,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 9, 'identifier': 20, '.': 9, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ')': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'string': 1, 'string_start': 1, 'string_content': 5, 'interpolation': 4, '{': 4, '}': 4, 'conditional_expression': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 2, 'else': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4225874760061482,0.4196019922919334,"(tensor([0.8599]), tensor([0.9035]), tensor([0.8812]), tensor([0.8990]))"
"28 
29 
30 class MultiLayerPerceptronVectorRegressionModel(TorchVectorRegressionModel):
31     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=None,
32             normalisationMode=NormalisationMode .MAX_BY_COLUMN,
33             cuda=True, pDropout: float = None, nnOptimiserParams: NNOptimiserParams = None, **nnOptimiserDictParams):
34         """"""
35         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
36         :param hidActivationFunction: the activation function (torch.*) to use for all hidden layers
37         :param outputActivationFunction: the output activation function (torch.* or None)
38         :param normalisationMode: the normalisation mode to apply to input and output data
39         :param cuda: whether to use CUDA (GPU acceleration)
40         :param pDropout: the probability with which to apply dropouts after each hidden layer
41         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
42         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
43         """"""
44         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
45         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
46                 dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
47 
48 
","36 
37 
38 class MultiLayerPerceptronVectorRegressionModel(TorchVectorRegressionModel):
39     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=None,
40             normalisationMode=NormalisationMode .MAX_BY_COLUMN,
41             cuda=True, pDropout: float = None, nnOptimiserParams: NNOptimiserParams = None, **nnOptimiserDictParams):
42         """"""
43         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
44         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
45         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
46         :param normalisationMode: the normalisation mode to apply to input and output data
47         :param cuda: whether to use CUDA (GPU acceleration)
48         :param pDropout: the probability with which to apply dropouts after each hidden layer
49         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
50         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
51         """"""
52         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
53         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
54                 dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
55 
56 
","Before: 36, 37
After: 44, 45",fix multilayerperceptrontorchmodels.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,389,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 35, 'argument_list': 5, '(': 7, ')': 7, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 17, 'default_parameter': 5, '=': 9, 'tuple': 1, 'integer': 2, 'attribute': 4, '.': 4, 'none': 3, 'true': 1, 'typed_default_parameter': 2, 'type': 2, 'dictionary_splat_pattern': 1, '**': 1, 'expression_statement': 3, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 4, 'list': 1, '[': 1, ']': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7714280145835611,0.763690010567905,"(tensor([0.9733]), tensor([0.9777]), tensor([0.9755]), tensor([0.9773]))"
"47 
48 
49 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
50     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=torch.sigmoid,
51             normalisationMode=NormalisationMode.MAX_BY_COLUMN, cuda=True, pDropout=None, nnOptimiserParams: NNOptimiserParams = None,
52             **nnOptimiserDictParams):
53         """"""
54         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
55         :param hidActivationFunction: the activation function (torch.*) to use for all hidden layers
56         :param outputActivationFunction: the output activation function (torch.*)
57         :param normalisationMode: the normalisation mode to apply to input and output data
58         :param cuda: whether to use CUDA (GPU acceleration)
59         :param pDropout: the probability with which to apply dropouts after each hidden layer
60         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
61         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
62         """"""
63         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
64         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
65             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","55 
56 
57 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
58     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=torch.nn.functional.softmax,
59             normalisationMode=NormalisationMode.MAX_BY_COLUMN, cuda=True, pDropout=None, nnOptimiserParams: NNOptimiserParams = None,
60             **nnOptimiserDictParams):
61         """"""
62         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
63         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
64         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
65         :param normalisationMode: the normalisation mode to apply to input and output data
66         :param cuda: whether to use CUDA (GPU acceleration)
67         :param pDropout: the probability with which to apply dropouts after each hidden layer
68         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
69         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
70         """"""
71         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
72         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
73             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","Before: 50
After: 58",fix multilayerperceptrontorchmodels.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,445,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 31, 'argument_list': 3, '(': 6, ')': 5, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 15, 'default_parameter': 6, '=': 8, 'tuple': 1, 'integer': 2, 'attribute': 5, '.': 5, 'true': 1, 'none': 2, 'typed_default_parameter': 1, 'type': 1, 'dictionary_splat_pattern': 1, '**': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 2, 'ERROR': 1, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7511261535629981,0.7410067201232701,"(tensor([0.9677]), tensor([0.9769]), tensor([0.9723]), tensor([0.9759]))"
"47 
48 
49 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
50     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=torch.sigmoid,
51             normalisationMode=NormalisationMode.MAX_BY_COLUMN, cuda=True, pDropout=None, nnOptimiserParams: NNOptimiserParams = None,
52             **nnOptimiserDictParams):
53         """"""
54         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
55         :param hidActivationFunction: the activation function (torch.*) to use for all hidden layers
56         :param outputActivationFunction: the output activation function (torch.*)
57         :param normalisationMode: the normalisation mode to apply to input and output data
58         :param cuda: whether to use CUDA (GPU acceleration)
59         :param pDropout: the probability with which to apply dropouts after each hidden layer
60         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
61         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
62         """"""
63         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
64         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
65             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","55 
56 
57 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
58     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=torch.nn.functional.softmax,
59             normalisationMode=NormalisationMode.MAX_BY_COLUMN, cuda=True, pDropout=None, nnOptimiserParams: NNOptimiserParams = None,
60             **nnOptimiserDictParams):
61         """"""
62         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
63         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
64         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
65         :param normalisationMode: the normalisation mode to apply to input and output data
66         :param cuda: whether to use CUDA (GPU acceleration)
67         :param pDropout: the probability with which to apply dropouts after each hidden layer
68         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
69         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
70         """"""
71         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
72         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
73             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","Before: 55, 56
After: 63, 64",fix multilayerperceptrontorchmodels.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,522,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 31, 'argument_list': 3, '(': 6, ')': 5, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 15, 'default_parameter': 6, '=': 8, 'tuple': 1, 'integer': 2, 'attribute': 5, '.': 5, 'true': 1, 'none': 2, 'typed_default_parameter': 1, 'type': 1, 'dictionary_splat_pattern': 1, '**': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 2, 'ERROR': 1, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7511261535629981,0.7410067201232701,"(tensor([0.9677]), tensor([0.9769]), tensor([0.9723]), tensor([0.9759]))"
"6 
7 
8 class ResidualFeedForwardNetwork(nn.Module):
9     """"""
10     A feed-forward network consisting of a fully connected input layer, a configurable number of residual blocks and a
11     fully connected output layer. Each residual block consists of two fully connected layers with (optionally) batch
12     normalisation and dropout, which can all be bypassed by a so called skip connection. The skip path and the non-skip
13     path are added as the last step within each block.
14 
15     More precisely, the non-skip path consists of the following layers:
","6 
7 
8 class ResidualFeedForwardNetwork(nn.Module):
9     """"""
10     A feed-forward network consisting of a fully connected input layer, a configurable number of residual blocks and a
11     fully connected output layer. Similar architecture are described in e.g. [1] and [2] and are all inspired by
12     ResNet [3]. Each residual block consists of two fully connected layers with (optionally) batch normalisation and
13     dropout, which can all be bypassed by a so called skip connection. The skip path and the non-skip path are added as
14     the last step within each block.
15 
","Before: 11, 12, 13
After: 11, 12, 13, 14",fix typos in src/sensai/torch_models/residualffn/residualffn_modules.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,92,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 64, 'argument_list': 1, '(': 2, 'attribute': 3, '.': 4, ')': 2, ':': 1, 'ERROR': 9, 'string_start': 1, 'block': 1, 'expression_statement': 1, 'binary_operator': 2, '-': 2, ',': 2, 'boolean_operator': 3, 'and': 3, 'with': 1, 'parenthesized_expression': 1, 'with_item': 1, 'as': 1}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 37, 'end_line': 72, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.703656857520314,0.6962654849063534,"(tensor([0.9318]), tensor([0.9585]), tensor([0.9450]), tensor([0.9558]))"
"12     normalisation and dropout, which can all be bypassed by a so called skip connection. The skip path and the non-skip
13     path are added as the last step within each block.
14 
15     More precisely, the non-skip path consists of the following layers:
16       batch normalization -> ReLU, dropout -> fully-connected -> batch normalization -> ReLU, dropout -> fully-connected
17     The use of the activation function before the connected layers is called ""pre-activation"".
18 
19     The skip path does nothing for the case where the input dimension of the block equals the output dimension. If these
20     dimensions are different, the skip-path consists of a fully-connected layer, but with no activation, normalization,
21     or dropout.
","13     dropout, which can all be bypassed by a so called skip connection. The skip path and the non-skip path are added as
14     the last step within each block.
15 
16     More precisely, the non-skip path consists of the following layers:
17       batch normalization -> ReLU, dropout -> fully-connected -> batch normalization -> ReLU, dropout -> fully-connected
18     The use of the activation function before the connected layers is called ""pre-activation"" [4].
19 
20     The skip path does nothing for the case where the input dimension of the block equals the output dimension. If these
21     dimensions are different, the skip-path consists of a fully-connected layer, but with no activation, normalization,
22     or dropout.
","Before: 17
After: 18",fix typos in src/sensai/torch_models/residualffn/residualffn_modules.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,45,"{'module': 1, 'ERROR': 18, 'expression_statement': 5, 'boolean_operator': 2, 'identifier': 101, 'and': 2, ',': 8, 'attribute': 4, '.': 4, 'binary_operator': 10, '-': 10, 'as_pattern': 1, 'as': 1, 'as_pattern_target': 1, ':': 1, '->': 1, '>': 4, 'comparison_operator': 2, 'is': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 37, 'end_line': 72, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8219624480906098,0.8170773071543382,"(tensor([0.9832]), tensor([0.9824]), tensor([0.9828]), tensor([0.9824]))"
"23     Within each block, the dimension can be reduced by a certain factor. This is known as ""bottleneck"" design. It has
24     been shown for the original ResNet, that such a bottleneck design can reduce the number of parameters of the models
25     and improve the training behaviour without compromising the results.
26 
27     Batch normalisation can be deactivated, but normally it improves the results, since it not only provides some
28     regularisation, but also normalises the distribution of the inputs of each layer. Why exactly this is beneficial
29     for the training process and the overall results is not yet fully understood (see e.g. the Wikipedia article on
30     batch normalisation for further references).
31 
32     The overall architecture is inspired by ResNet and adopted to regression as described e.g. in:
","24     Within each block, the dimension can be reduced by a certain factor. This is known as ""bottleneck"" design. It has
25     been shown for the original ResNet, that such a bottleneck design can reduce the number of parameters of the models
26     and improve the training behaviour without compromising the results.
27 
28     Batch normalisation can be deactivated, but normally it improves the results, since it not only provides some
29     regularisation, but also normalises the distribution of the inputs of each layer and therefore addresses the problem
30     of ""internal covariate shift""[5]. The mechanism behind this is not yet fully understood (see e.g. the Wikipedia
31     article on batch normalisation for further references).
32 
33     References:
","Before: 28, 29, 30, 31, 32, 33, 34
After: 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43",fix typos in src/sensai/torch_models/residualffn/residualffn_modules.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,132,"{'module': 1, 'expression_statement': 4, 'identifier': 99, 'ERROR': 14, ',': 5, 'as_pattern': 1, 'comparison_operator': 2, 'attribute': 6, '.': 7, 'is': 2, 'as': 1, 'as_pattern_target': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'for': 3, 'pattern_list': 1, 'not_operator': 1, 'not': 2, '(': 1, ')': 1}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 37, 'end_line': 72, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6719326766273676,0.6649120548277622,"(tensor([0.9565]), tensor([0.9403]), tensor([0.9483]), tensor([0.9419]))"
"43         d.update(additionalEntries)
44         return dictString(d)
45 
46     def _toStringObjectInfo(self) -> str:
47         return self._toStringProperties()
48 
49     def __str__(self):
","55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
59         """"""
60         Creates a string containing information on the objects state, which is the name and value of all attributes
61         without the attributes that are in the list provided by _toStringExclusions. It is used by the methods __str__
62         and __repr__. This method can be overwritten by sub-classes to provide a custom string.
63 
64         :return: a string containing all attribute names and values
65         """"""
66         return self._toStringProperties(exclude=self._toStringExcludes())
67 
68     def _toStringExcludes(self) -> List[str]:
","Before: 47
After: 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76",add some docstring to the string module,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,066562bee005ab7366944b88621393c278927d49,dd5f24b6a4a32e7d03624e3858a66fefabbcefee,0,490,"{'module': 1, 'expression_statement': 1, 'call': 3, 'attribute': 2, 'identifier': 10, '.': 2, 'argument_list': 3, '(': 4, ')': 4, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.13526059419167044,0.10760525655467727,"(tensor([0.6943]), tensor([0.9005]), tensor([0.7841]), tensor([0.8745]))"
"169         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression,
170                                          **self.crossValidatorParams)
171 
172     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
173             additionalEvaluationOnTrainingData=False) -> TEvalData:
174         evaluator = self.createEvaluator(model)
175         evaluator.fitModel(model)
176 
177         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
178             strEvalResults = """"
179             for predictedVarName in model.getPredictedVariableNames():
180                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
181                 if logResults:
182                     log.info(f""Evaluation results for {predictedVarName}: {strEvalResult}"")
183                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
184             if resultWriter is not None:
185                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
186             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
187 
188         evalResultData = evaluator.evalModel(model)
189         gatherResults(evalResultData, resultWriter)
190         if additionalEvaluationOnTrainingData:
191             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
192             gatherResults(evalResultDataTrain, resultWriter.childWithAddedPrefix(""-onTrain-""), subtitlePrefix=""[onTrain] "")
193 
194         return evalResultData
195 
196     @staticmethod
","169         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression,
170                                          **self.crossValidatorParams)
171 
172     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
173             additionalEvaluationOnTrainingData=False) -> TEvalData:
174         evaluator = self.createEvaluator(model)
175         evaluator.fitModel(model)
176 
177         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
178             strEvalResults = f""{model}\n\n""
179             for predictedVarName in model.getPredictedVariableNames():
180                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
181                 if logResults:
182                     log.info(f""Evaluation results for {predictedVarName}: {strEvalResult}"")
183                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
184             if resultWriter is not None:
185                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
186             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
187 
188         evalResultData = evaluator.evalModel(model)
189         gatherResults(evalResultData, resultWriter)
190         if additionalEvaluationOnTrainingData:
191             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
192             gatherResults(evalResultDataTrain, resultWriter.childWithAddedPrefix(""-onTrain-""), subtitlePrefix=""[onTrain] "")
193 
194         return evalResultData
195 
196     @staticmethod
","Before: 178
After: 178",fix eval_util and crossval_util,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,1636,"{'module': 1, 'return_statement': 2, 'return': 2, 'call': 14, 'identifier': 79, 'argument_list': 14, '(': 16, 'attribute': 12, '.': 12, ',': 18, 'keyword_argument': 7, '=': 17, 'dictionary_splat': 1, '**': 1, ')': 16, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 8, 'type': 3, 'default_parameter': 4, 'false': 2, 'true': 2, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'block': 6, 'expression_statement': 12, 'assignment': 5, 'string': 8, 'string_start': 8, 'string_end': 8, 'for_statement': 1, 'for': 1, 'in': 1, 'if_statement': 3, 'if': 3, 'string_content': 7, 'interpolation': 2, '{': 2, '}': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 3, '+': 3, 'escape_sequence': 1, 'comparison_operator': 1, 'is not': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9646618415830112,0.9631320938566422,"(tensor([0.9917]), tensor([0.9925]), tensor([0.9921]), tensor([0.9925]))"
"199             return None
200         return resultWriter.childWithAddedPrefix(model.getName() + ""-"")
201 
202     def performCrossValidation(self, model: TModel, showPlots=False, logResults=True, resultWriter: Optional[ResultWriter] = None) -> TCrossValData:
203         """"""
204         Evaluates the given model via cross-validation
205 
206         :param model: the model to evaluate
207         :param showPlots: whether to show plots that visualise evaluation results (combining all folds)
208         :param logResults: whether to log evaluation results
209         :param resultWriter: a writer with which to store text files and plots. The evaluated model's name is added to each filename
210             automatically
211         :return: cross-validation result data
212         """"""
213         resultWriter = self._resultWriterForModel(resultWriter, model)
214         crossValidator = createVectorModelCrossValidator(self.inputOutputData, model=model, **self.crossValidatorParams)
215         crossValidationData = crossValidator.evalModel(model)
216         strEvalResults = str(crossValidationData.getEvalStatsCollection().aggStats())
217         if logResults:
218             log.info(f""Cross-validation results: {strEvalResults}"")
219         if resultWriter is not None:
220             resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","199             return None
200         return resultWriter.childWithAddedPrefix(model.getName() + ""-"")
201 
202     def performCrossValidation(self, model: TModel, showPlots=False, logResults=True, resultWriter: Optional[ResultWriter] = None) -> TCrossValData:
203         """"""
204         Evaluates the given model via cross-validation
205 
206         :param model: the model to evaluate
207         :param showPlots: whether to show plots that visualise evaluation results (combining all folds)
208         :param logResults: whether to log evaluation results
209         :param resultWriter: a writer with which to store text files and plots. The evaluated model's name is added to each filename
210             automatically
211         :return: cross-validation result data
212         """"""
213         resultWriter = self._resultWriterForModel(resultWriter, model)
214         crossValidator = createVectorModelCrossValidator(self.inputOutputData, model=model, **self.crossValidatorParams)
215         crossValidationData = crossValidator.evalModel(model)
216         strEvalResults = str(crossValidationData.getEvalStatsCollection().aggStats())
217         if logResults:
218             log.info(f""Cross-validation results: {strEvalResults}"")
219         if resultWriter is not None:
220             resultWriter.writeTextFile(""crossval-results"", {strEvalResults})
221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 220
After: 220",fix eval_util and crossval_util,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2084,"{'module': 1, 'return_statement': 3, 'return': 3, 'none': 3, 'call': 11, 'attribute': 11, 'identifier': 52, '.': 11, 'argument_list': 11, '(': 12, 'binary_operator': 1, ')': 12, '+': 1, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 10, 'typed_parameter': 1, ':': 5, 'type': 4, 'default_parameter': 2, '=': 10, 'false': 1, 'true': 1, 'typed_default_parameter': 1, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 3, 'expression_statement': 8, 'assignment': 4, 'keyword_argument': 3, 'dictionary_splat': 1, '**': 1, 'if_statement': 2, 'if': 2, 'interpolation': 1, '{': 1, '}': 1, 'comparison_operator': 1, 'is not': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9733268096119397,0.9714312859871537,"(tensor([0.9981]), tensor([0.9970]), tensor([0.9976]), tensor([0.9971]))"
"114     """"""
115     Utility class for holding and persisting evaluation results
116     """"""
117     def __init__(self, csvPath=None, sortColumnName=None, ascending=True):
118         """"""
119         :param csvPath: path to save the data frame to upon every update
120         :param sortColumnName: the column name by which to sort the data frame that is collected; if None, do not sort
121         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
122         """"""
123         self.sortColumnName = sortColumnName
124         self.csvPath = csvPath
125         self.ascending = ascending
126         self.df = None
127         self.cols = None
128         self._currentRow = 0
129 
130     def addValues(self, values: Dict[str, Any]):
","114     """"""
115     Utility class for holding and persisting evaluation results
116     """"""
117     def __init__(self, csvPath=None, sortColumnName=None, ascending=True, incremental=False):
118         """"""
119         :param csvPath: path to save the data frame to upon every update
120         :param sortColumnName: the column name by which to sort the data frame that is collected; if None, do not sort
121         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
122         :param incremental: whether to add to an existing CSV file instead of overwriting it
123         """"""
124         self.sortColumnName = sortColumnName
125         self.csvPath = csvPath
126         self.ascending = ascending
127         if os.path.exists(csvPath) and incremental:
128             self.df = pd.read_csv(csvPath)
129             log.info(f""Found existing CSV file with {len(self.df)} entries; {csvPath} will be extended (incremental mode)"")
130             self._currentRow = len(self.df)
131             self.valueDicts = [nt._asdict() for nt in self.df.itertuples()]
132         else:
133             if os.path.exists(csvPath):
134                 log.info(f""Results will be written to new file {csvPath}"")
135             else:
136                 log.warning(f""Results in existing file ({csvPath}) will be overwritten (non-incremental mode)"")
137             self.df = None
138             self._currentRow = 0
139             self.valueDicts = []
140 
141     def addValues(self, values: Dict[str, Any]):
","Before: 117
After: 117, 122",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,650,"{'module': 1, 'expression_statement': 8, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'function_definition': 1, 'def': 1, 'identifier': 20, 'parameters': 1, '(': 1, ',': 3, 'default_parameter': 3, '=': 9, 'none': 4, 'true': 1, ')': 1, ':': 1, 'block': 1, 'assignment': 6, 'attribute': 6, '.': 6, 'integer': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4201691584468791,0.42187484975632605,"(tensor([0.8241]), tensor([0.9601]), tensor([0.8869]), tensor([0.9445]))"
"114     """"""
115     Utility class for holding and persisting evaluation results
116     """"""
117     def __init__(self, csvPath=None, sortColumnName=None, ascending=True):
118         """"""
119         :param csvPath: path to save the data frame to upon every update
120         :param sortColumnName: the column name by which to sort the data frame that is collected; if None, do not sort
121         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
122         """"""
123         self.sortColumnName = sortColumnName
124         self.csvPath = csvPath
125         self.ascending = ascending
126         self.df = None
127         self.cols = None
128         self._currentRow = 0
129 
130     def addValues(self, values: Dict[str, Any]):
","114     """"""
115     Utility class for holding and persisting evaluation results
116     """"""
117     def __init__(self, csvPath=None, sortColumnName=None, ascending=True, incremental=False):
118         """"""
119         :param csvPath: path to save the data frame to upon every update
120         :param sortColumnName: the column name by which to sort the data frame that is collected; if None, do not sort
121         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
122         :param incremental: whether to add to an existing CSV file instead of overwriting it
123         """"""
124         self.sortColumnName = sortColumnName
125         self.csvPath = csvPath
126         self.ascending = ascending
127         if os.path.exists(csvPath) and incremental:
128             self.df = pd.read_csv(csvPath)
129             log.info(f""Found existing CSV file with {len(self.df)} entries; {csvPath} will be extended (incremental mode)"")
130             self._currentRow = len(self.df)
131             self.valueDicts = [nt._asdict() for nt in self.df.itertuples()]
132         else:
133             if os.path.exists(csvPath):
134                 log.info(f""Results will be written to new file {csvPath}"")
135             else:
136                 log.warning(f""Results in existing file ({csvPath}) will be overwritten (non-incremental mode)"")
137             self.df = None
138             self._currentRow = 0
139             self.valueDicts = []
140 
141     def addValues(self, values: Dict[str, Any]):
","Before: 126, 127, 128
After: 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,687,"{'module': 1, 'expression_statement': 8, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'function_definition': 1, 'def': 1, 'identifier': 20, 'parameters': 1, '(': 1, ',': 3, 'default_parameter': 3, '=': 9, 'none': 4, 'true': 1, ')': 1, ':': 1, 'block': 1, 'assignment': 6, 'attribute': 6, '.': 6, 'integer': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4201691584468791,0.42187484975632605,"(tensor([0.8241]), tensor([0.9601]), tensor([0.8869]), tensor([0.9445]))"
"127         self.cols = None
128         self._currentRow = 0
129 
130     def addValues(self, values: Dict[str, Any]):
131         """"""
132         Adds the provided values as a new row to the collection.
133         If csvPath was provided in the constructor, saves the updated collection to that file.
134 
135         :param values: Dict holding the evaluation results and parameters
136         :return:
137         """"""
138         if self.df is None:
139             self.cols = list(values.keys())
140 
141             # check sort column and move it to the front
142             if self.sortColumnName is not None:
143                 if self.sortColumnName not in self.cols:
144                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {self.cols}; sorting will not take place!"")
145                 else:
146                     self.cols.remove(self.sortColumnName)
147                     self.cols.insert(0, self.sortColumnName)
148 
149             self.df = pd.DataFrame(columns=self.cols)
150 
151         # append data to data frame
152         self.df.loc[self._currentRow] = [values[c] for c in self.cols]
153         self._currentRow += 1
154 
155         # sort where applicable
156         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
157             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
158             self.df.reset_index(drop=True, inplace=True)
159 
160         self._saveCSV()
161 
162     def _saveCSV(self):
","138             self._currentRow = 0
139             self.valueDicts = []
140 
141     def addValues(self, values: Dict[str, Any]):
142         """"""
143         Adds the provided values as a new row to the collection.
144         If csvPath was provided in the constructor, saves the updated collection to that file.
145 
146         :param values: Dict holding the evaluation results and parameters
147         :return:
148         """"""
149         if self.df is None:
150             cols = list(values.keys())
151 
152             # check sort column and move it to the front
153             if self.sortColumnName is not None:
154                 if self.sortColumnName not in cols:
155                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {cols}; sorting will not take place!"")
156                 else:
157                     cols.remove(self.sortColumnName)
158                     cols.insert(0, self.sortColumnName)
159 
160             self.df = pd.DataFrame(columns=cols)
161         else:
162             # check for new columns
163             for col in values.keys():
164                 if col not in self.df.columns:
165                     self.df[col] = None
166 
167         # append data to data frame
168         self.df.loc[self._currentRow] = [values.get(c) for c in self.df.columns]
169         self._currentRow += 1
170 
171         # sort where applicable
172         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
173             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
174             self.df.reset_index(drop=True, inplace=True)
175 
176         self._saveCSV()
177 
178     def _saveCSV(self):
","Before: 139
After: 150",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,763,"{'module': 1, 'expression_statement': 13, 'assignment': 5, 'attribute': 36, 'identifier': 82, '.': 36, '=': 11, 'none': 4, 'integer': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 10, ',': 7, 'typed_parameter': 1, ':': 7, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 4, ']': 4, ')': 10, 'block': 6, 'string': 2, 'string_start': 2, 'string_content': 4, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is': 1, 'call': 9, 'argument_list': 9, 'comment': 3, 'is not': 4, 'not in': 2, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1, 'keyword_argument': 6, 'subscript': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 2, 'augmented_assignment': 1, '+=': 1, 'boolean_operator': 1, 'and': 1, 'true': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5654776857894395,0.5606366835853867,"(tensor([0.9458]), tensor([0.9545]), tensor([0.9501]), tensor([0.9536]))"
"127         self.cols = None
128         self._currentRow = 0
129 
130     def addValues(self, values: Dict[str, Any]):
131         """"""
132         Adds the provided values as a new row to the collection.
133         If csvPath was provided in the constructor, saves the updated collection to that file.
134 
135         :param values: Dict holding the evaluation results and parameters
136         :return:
137         """"""
138         if self.df is None:
139             self.cols = list(values.keys())
140 
141             # check sort column and move it to the front
142             if self.sortColumnName is not None:
143                 if self.sortColumnName not in self.cols:
144                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {self.cols}; sorting will not take place!"")
145                 else:
146                     self.cols.remove(self.sortColumnName)
147                     self.cols.insert(0, self.sortColumnName)
148 
149             self.df = pd.DataFrame(columns=self.cols)
150 
151         # append data to data frame
152         self.df.loc[self._currentRow] = [values[c] for c in self.cols]
153         self._currentRow += 1
154 
155         # sort where applicable
156         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
157             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
158             self.df.reset_index(drop=True, inplace=True)
159 
160         self._saveCSV()
161 
162     def _saveCSV(self):
","138             self._currentRow = 0
139             self.valueDicts = []
140 
141     def addValues(self, values: Dict[str, Any]):
142         """"""
143         Adds the provided values as a new row to the collection.
144         If csvPath was provided in the constructor, saves the updated collection to that file.
145 
146         :param values: Dict holding the evaluation results and parameters
147         :return:
148         """"""
149         if self.df is None:
150             cols = list(values.keys())
151 
152             # check sort column and move it to the front
153             if self.sortColumnName is not None:
154                 if self.sortColumnName not in cols:
155                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {cols}; sorting will not take place!"")
156                 else:
157                     cols.remove(self.sortColumnName)
158                     cols.insert(0, self.sortColumnName)
159 
160             self.df = pd.DataFrame(columns=cols)
161         else:
162             # check for new columns
163             for col in values.keys():
164                 if col not in self.df.columns:
165                     self.df[col] = None
166 
167         # append data to data frame
168         self.df.loc[self._currentRow] = [values.get(c) for c in self.df.columns]
169         self._currentRow += 1
170 
171         # sort where applicable
172         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
173             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
174             self.df.reset_index(drop=True, inplace=True)
175 
176         self._saveCSV()
177 
178     def _saveCSV(self):
","Before: 143, 144
After: 154, 155",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,791,"{'module': 1, 'expression_statement': 13, 'assignment': 5, 'attribute': 36, 'identifier': 82, '.': 36, '=': 11, 'none': 4, 'integer': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 10, ',': 7, 'typed_parameter': 1, ':': 7, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 4, ']': 4, ')': 10, 'block': 6, 'string': 2, 'string_start': 2, 'string_content': 4, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is': 1, 'call': 9, 'argument_list': 9, 'comment': 3, 'is not': 4, 'not in': 2, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1, 'keyword_argument': 6, 'subscript': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 2, 'augmented_assignment': 1, '+=': 1, 'boolean_operator': 1, 'and': 1, 'true': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5654776857894395,0.5606366835853867,"(tensor([0.9458]), tensor([0.9545]), tensor([0.9501]), tensor([0.9536]))"
"127         self.cols = None
128         self._currentRow = 0
129 
130     def addValues(self, values: Dict[str, Any]):
131         """"""
132         Adds the provided values as a new row to the collection.
133         If csvPath was provided in the constructor, saves the updated collection to that file.
134 
135         :param values: Dict holding the evaluation results and parameters
136         :return:
137         """"""
138         if self.df is None:
139             self.cols = list(values.keys())
140 
141             # check sort column and move it to the front
142             if self.sortColumnName is not None:
143                 if self.sortColumnName not in self.cols:
144                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {self.cols}; sorting will not take place!"")
145                 else:
146                     self.cols.remove(self.sortColumnName)
147                     self.cols.insert(0, self.sortColumnName)
148 
149             self.df = pd.DataFrame(columns=self.cols)
150 
151         # append data to data frame
152         self.df.loc[self._currentRow] = [values[c] for c in self.cols]
153         self._currentRow += 1
154 
155         # sort where applicable
156         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
157             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
158             self.df.reset_index(drop=True, inplace=True)
159 
160         self._saveCSV()
161 
162     def _saveCSV(self):
","138             self._currentRow = 0
139             self.valueDicts = []
140 
141     def addValues(self, values: Dict[str, Any]):
142         """"""
143         Adds the provided values as a new row to the collection.
144         If csvPath was provided in the constructor, saves the updated collection to that file.
145 
146         :param values: Dict holding the evaluation results and parameters
147         :return:
148         """"""
149         if self.df is None:
150             cols = list(values.keys())
151 
152             # check sort column and move it to the front
153             if self.sortColumnName is not None:
154                 if self.sortColumnName not in cols:
155                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {cols}; sorting will not take place!"")
156                 else:
157                     cols.remove(self.sortColumnName)
158                     cols.insert(0, self.sortColumnName)
159 
160             self.df = pd.DataFrame(columns=cols)
161         else:
162             # check for new columns
163             for col in values.keys():
164                 if col not in self.df.columns:
165                     self.df[col] = None
166 
167         # append data to data frame
168         self.df.loc[self._currentRow] = [values.get(c) for c in self.df.columns]
169         self._currentRow += 1
170 
171         # sort where applicable
172         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
173             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
174             self.df.reset_index(drop=True, inplace=True)
175 
176         self._saveCSV()
177 
178     def _saveCSV(self):
","Before: 146, 147
After: 157, 158",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,840,"{'module': 1, 'expression_statement': 13, 'assignment': 5, 'attribute': 36, 'identifier': 82, '.': 36, '=': 11, 'none': 4, 'integer': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 10, ',': 7, 'typed_parameter': 1, ':': 7, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 4, ']': 4, ')': 10, 'block': 6, 'string': 2, 'string_start': 2, 'string_content': 4, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is': 1, 'call': 9, 'argument_list': 9, 'comment': 3, 'is not': 4, 'not in': 2, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1, 'keyword_argument': 6, 'subscript': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 2, 'augmented_assignment': 1, '+=': 1, 'boolean_operator': 1, 'and': 1, 'true': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5654776857894395,0.5606366835853867,"(tensor([0.9458]), tensor([0.9545]), tensor([0.9501]), tensor([0.9536]))"
"127         self.cols = None
128         self._currentRow = 0
129 
130     def addValues(self, values: Dict[str, Any]):
131         """"""
132         Adds the provided values as a new row to the collection.
133         If csvPath was provided in the constructor, saves the updated collection to that file.
134 
135         :param values: Dict holding the evaluation results and parameters
136         :return:
137         """"""
138         if self.df is None:
139             self.cols = list(values.keys())
140 
141             # check sort column and move it to the front
142             if self.sortColumnName is not None:
143                 if self.sortColumnName not in self.cols:
144                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {self.cols}; sorting will not take place!"")
145                 else:
146                     self.cols.remove(self.sortColumnName)
147                     self.cols.insert(0, self.sortColumnName)
148 
149             self.df = pd.DataFrame(columns=self.cols)
150 
151         # append data to data frame
152         self.df.loc[self._currentRow] = [values[c] for c in self.cols]
153         self._currentRow += 1
154 
155         # sort where applicable
156         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
157             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
158             self.df.reset_index(drop=True, inplace=True)
159 
160         self._saveCSV()
161 
162     def _saveCSV(self):
","138             self._currentRow = 0
139             self.valueDicts = []
140 
141     def addValues(self, values: Dict[str, Any]):
142         """"""
143         Adds the provided values as a new row to the collection.
144         If csvPath was provided in the constructor, saves the updated collection to that file.
145 
146         :param values: Dict holding the evaluation results and parameters
147         :return:
148         """"""
149         if self.df is None:
150             cols = list(values.keys())
151 
152             # check sort column and move it to the front
153             if self.sortColumnName is not None:
154                 if self.sortColumnName not in cols:
155                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {cols}; sorting will not take place!"")
156                 else:
157                     cols.remove(self.sortColumnName)
158                     cols.insert(0, self.sortColumnName)
159 
160             self.df = pd.DataFrame(columns=cols)
161         else:
162             # check for new columns
163             for col in values.keys():
164                 if col not in self.df.columns:
165                     self.df[col] = None
166 
167         # append data to data frame
168         self.df.loc[self._currentRow] = [values.get(c) for c in self.df.columns]
169         self._currentRow += 1
170 
171         # sort where applicable
172         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
173             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
174             self.df.reset_index(drop=True, inplace=True)
175 
176         self._saveCSV()
177 
178     def _saveCSV(self):
","Before: 149
After: 160, 161, 162, 163, 164, 165",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,880,"{'module': 1, 'expression_statement': 13, 'assignment': 5, 'attribute': 36, 'identifier': 82, '.': 36, '=': 11, 'none': 4, 'integer': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 10, ',': 7, 'typed_parameter': 1, ':': 7, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 4, ']': 4, ')': 10, 'block': 6, 'string': 2, 'string_start': 2, 'string_content': 4, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is': 1, 'call': 9, 'argument_list': 9, 'comment': 3, 'is not': 4, 'not in': 2, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1, 'keyword_argument': 6, 'subscript': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 2, 'augmented_assignment': 1, '+=': 1, 'boolean_operator': 1, 'and': 1, 'true': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5654776857894395,0.5606366835853867,"(tensor([0.9458]), tensor([0.9545]), tensor([0.9501]), tensor([0.9536]))"
"127         self.cols = None
128         self._currentRow = 0
129 
130     def addValues(self, values: Dict[str, Any]):
131         """"""
132         Adds the provided values as a new row to the collection.
133         If csvPath was provided in the constructor, saves the updated collection to that file.
134 
135         :param values: Dict holding the evaluation results and parameters
136         :return:
137         """"""
138         if self.df is None:
139             self.cols = list(values.keys())
140 
141             # check sort column and move it to the front
142             if self.sortColumnName is not None:
143                 if self.sortColumnName not in self.cols:
144                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {self.cols}; sorting will not take place!"")
145                 else:
146                     self.cols.remove(self.sortColumnName)
147                     self.cols.insert(0, self.sortColumnName)
148 
149             self.df = pd.DataFrame(columns=self.cols)
150 
151         # append data to data frame
152         self.df.loc[self._currentRow] = [values[c] for c in self.cols]
153         self._currentRow += 1
154 
155         # sort where applicable
156         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
157             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
158             self.df.reset_index(drop=True, inplace=True)
159 
160         self._saveCSV()
161 
162     def _saveCSV(self):
","138             self._currentRow = 0
139             self.valueDicts = []
140 
141     def addValues(self, values: Dict[str, Any]):
142         """"""
143         Adds the provided values as a new row to the collection.
144         If csvPath was provided in the constructor, saves the updated collection to that file.
145 
146         :param values: Dict holding the evaluation results and parameters
147         :return:
148         """"""
149         if self.df is None:
150             cols = list(values.keys())
151 
152             # check sort column and move it to the front
153             if self.sortColumnName is not None:
154                 if self.sortColumnName not in cols:
155                     log.warning(f""Specified sort column '{self.sortColumnName}' not in list of columns: {cols}; sorting will not take place!"")
156                 else:
157                     cols.remove(self.sortColumnName)
158                     cols.insert(0, self.sortColumnName)
159 
160             self.df = pd.DataFrame(columns=cols)
161         else:
162             # check for new columns
163             for col in values.keys():
164                 if col not in self.df.columns:
165                     self.df[col] = None
166 
167         # append data to data frame
168         self.df.loc[self._currentRow] = [values.get(c) for c in self.df.columns]
169         self._currentRow += 1
170 
171         # sort where applicable
172         if self.sortColumnName is not None and self.sortColumnName in self.df.columns:
173             self.df.sort_values(self.sortColumnName, axis=0, inplace=True, ascending=self.ascending)
174             self.df.reset_index(drop=True, inplace=True)
175 
176         self._saveCSV()
177 
178     def _saveCSV(self):
","Before: 152
After: 168",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,914,"{'module': 1, 'expression_statement': 13, 'assignment': 5, 'attribute': 36, 'identifier': 82, '.': 36, '=': 11, 'none': 4, 'integer': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 10, ',': 7, 'typed_parameter': 1, ':': 7, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 4, ']': 4, ')': 10, 'block': 6, 'string': 2, 'string_start': 2, 'string_content': 4, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is': 1, 'call': 9, 'argument_list': 9, 'comment': 3, 'is not': 4, 'not in': 2, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1, 'keyword_argument': 6, 'subscript': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 2, 'augmented_assignment': 1, '+=': 1, 'boolean_operator': 1, 'and': 1, 'true': 3}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5654776857894395,0.5606366835853867,"(tensor([0.9458]), tensor([0.9545]), tensor([0.9501]), tensor([0.9536]))"
"177     """"""
178     log = log.getChild(__qualname__)
179 
180     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]],
181             List[Dict[str, Sequence[Any]]]], numProcesses=1, csvResultsPath: str = None,
182             parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None, modelSaveDirectory: str = None,
183             name: str = None):
184         """"""
185         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
186         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
187             where each dictionary in the list has the same keys
188         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
189         :param csvResultsPath: the path to a directory or concrete CSV file to which the results shall be written;
190             if it is None, no CSV data will be written; if it is a directory, a file name starting with this grid search's name (see below)
191             will be created.
192             The resulting CSV data will contain one line per evaluated parameter combination.
193         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
194             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
195         :param modelSaveDirectory the directory where the serialized models shall be saved; if None, models are not saved
196         :param name: the name of this grid search, which will, in particular, be prepended to all saved model files;
197             if None, a default name will be generated of the form ""gridSearch_<timestamp>""
198         """"""
199         self.modelFactory = modelFactory
200         if type(parameterOptions) == list:
201             self.parameterOptionsList = parameterOptions
202             paramNames = set(parameterOptions[0].keys())
203             for d in parameterOptions[1:]:
204                 if set(d.keys()) != paramNames:
205                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
206         else:
207             self.parameterOptionsList = [parameterOptions]
208         self.numProcesses = numProcesses
209         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
210         self.modelSaveDirectory = modelSaveDirectory
211         self.name = name if name is not None else ""gridSearch_"" + datetime.now().strftime('%Y%m%d-%H%M%S')
212         self.csvResultsPath = csvResultsPath
213         if self.csvResultsPath is not None and os.path.isdir(csvResultsPath):
214             self.csvResultsPath = os.path.join(self.csvResultsPath, f""{self.name}_results.csv"")
215 
216         self.numCombinations = 0
217         for parameterOptions in self.parameterOptionsList:
218             n = 1
219             for options in parameterOptions.values():
220                 n *= len(options)
221             self.numCombinations += n
222         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
223 
224         self._executor = None
225 
226     @classmethod
","204     """"""
205     log = log.getChild(__qualname__)
206 
207     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]], List[Dict[str, Sequence[Any]]]],
208             numProcesses=1, csvResultsPath: str = None, incremental=False, incrementalSkipExisting=False,
209             parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None, modelSaveDirectory: str = None,
210             name: str = None):
211         """"""
212         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
213         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
214             where each dictionary in the list has the same keys
215         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
216         :param csvResultsPath: the path to a directory or concrete CSV file to which the results shall be written;
217             if it is None, no CSV data will be written; if it is a directory, a file name starting with this grid search's name (see below)
218             will be created.
219             The resulting CSV data will contain one line per evaluated parameter combination.
220         :param incremental: whether to add to an existing CSV file instead of overwriting it
221         :param incrementalSkipExisting: if incremental mode is on, whether to skip any parameter combinations that are already present
222             in the CSV file
223         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
224             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
225         :param modelSaveDirectory: the directory where the serialized models shall be saved; if None, models are not saved
226         :param name: the name of this grid search, which will, in particular, be prepended to all saved model files;
227             if None, a default name will be generated of the form ""gridSearch_<timestamp>""
228         """"""
229         self.modelFactory = modelFactory
230         if type(parameterOptions) == list:
231             self.parameterOptionsList = parameterOptions
232             paramNames = set(parameterOptions[0].keys())
233             for d in parameterOptions[1:]:
234                 if set(d.keys()) != paramNames:
235                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
236         else:
237             self.parameterOptionsList = [parameterOptions]
238         self.numProcesses = numProcesses
239         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
240         self.modelSaveDirectory = modelSaveDirectory
241         self.name = name if name is not None else ""gridSearch_"" + datetime.now().strftime('%Y%m%d-%H%M%S')
242         self.csvResultsPath = csvResultsPath
243         self.incremental = incremental
244         self.incrementalSkipExisting = incrementalSkipExisting
245         if self.csvResultsPath is not None and os.path.isdir(csvResultsPath):
246             self.csvResultsPath = os.path.join(self.csvResultsPath, f""{self.name}_results.csv"")
247 
248         self.numCombinations = 0
249         for parameterOptions in self.parameterOptionsList:
250             n = 1
251             for options in parameterOptions.values():
252                 n *= len(options)
253             self.numCombinations += n
254         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
255 
256         self._executor = None
257 
258     @classmethod
","Before: 180, 181
After: 207, 208",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,1191,"{'module': 1, 'expression_statement': 8, 'string': 2, 'string_start': 4, 'string_content': 2, 'string_end': 2, 'ERROR': 23, ':': 13, 'identifier': 192, 'assignment': 3, 'type': 3, 'try': 1, 'binary_operator': 3, '-': 2, ',': 8, 'comparison_operator': 5, 'in': 3, 'for': 1, 'call': 3, 'argument_list': 3, '(': 3, 'integer': 1, ')': 3, ';': 4, 'if': 4, 'is': 3, 'none': 3, 'attribute': 4, '.': 4, 'boolean_operator': 1, '/': 1, 'or': 1, 'not': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7208086522898888,0.7163585897008807,"(tensor([0.9273]), tensor([0.9469]), tensor([0.9370]), tensor([0.9449]))"
"177     """"""
178     log = log.getChild(__qualname__)
179 
180     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]],
181             List[Dict[str, Sequence[Any]]]], numProcesses=1, csvResultsPath: str = None,
182             parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None, modelSaveDirectory: str = None,
183             name: str = None):
184         """"""
185         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
186         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
187             where each dictionary in the list has the same keys
188         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
189         :param csvResultsPath: the path to a directory or concrete CSV file to which the results shall be written;
190             if it is None, no CSV data will be written; if it is a directory, a file name starting with this grid search's name (see below)
191             will be created.
192             The resulting CSV data will contain one line per evaluated parameter combination.
193         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
194             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
195         :param modelSaveDirectory the directory where the serialized models shall be saved; if None, models are not saved
196         :param name: the name of this grid search, which will, in particular, be prepended to all saved model files;
197             if None, a default name will be generated of the form ""gridSearch_<timestamp>""
198         """"""
199         self.modelFactory = modelFactory
200         if type(parameterOptions) == list:
201             self.parameterOptionsList = parameterOptions
202             paramNames = set(parameterOptions[0].keys())
203             for d in parameterOptions[1:]:
204                 if set(d.keys()) != paramNames:
205                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
206         else:
207             self.parameterOptionsList = [parameterOptions]
208         self.numProcesses = numProcesses
209         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
210         self.modelSaveDirectory = modelSaveDirectory
211         self.name = name if name is not None else ""gridSearch_"" + datetime.now().strftime('%Y%m%d-%H%M%S')
212         self.csvResultsPath = csvResultsPath
213         if self.csvResultsPath is not None and os.path.isdir(csvResultsPath):
214             self.csvResultsPath = os.path.join(self.csvResultsPath, f""{self.name}_results.csv"")
215 
216         self.numCombinations = 0
217         for parameterOptions in self.parameterOptionsList:
218             n = 1
219             for options in parameterOptions.values():
220                 n *= len(options)
221             self.numCombinations += n
222         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
223 
224         self._executor = None
225 
226     @classmethod
","204     """"""
205     log = log.getChild(__qualname__)
206 
207     def __init__(self, modelFactory: Callable[..., VectorModel], parameterOptions: Union[Dict[str, Sequence[Any]], List[Dict[str, Sequence[Any]]]],
208             numProcesses=1, csvResultsPath: str = None, incremental=False, incrementalSkipExisting=False,
209             parameterCombinationSkipDecider: ParameterCombinationSkipDecider = None, modelSaveDirectory: str = None,
210             name: str = None):
211         """"""
212         :param modelFactory: the function to call with keyword arguments reflecting the parameters to try in order to obtain a model instance
213         :param parameterOptions: a dictionary which maps from parameter names to lists of possible values - or a list of such dictionaries,
214             where each dictionary in the list has the same keys
215         :param numProcesses: the number of parallel processes to use for the search (use 1 to run without multi-processing)
216         :param csvResultsPath: the path to a directory or concrete CSV file to which the results shall be written;
217             if it is None, no CSV data will be written; if it is a directory, a file name starting with this grid search's name (see below)
218             will be created.
219             The resulting CSV data will contain one line per evaluated parameter combination.
220         :param incremental: whether to add to an existing CSV file instead of overwriting it
221         :param incrementalSkipExisting: if incremental mode is on, whether to skip any parameter combinations that are already present
222             in the CSV file
223         :param parameterCombinationSkipDecider: an instance to which parameters combinations can be passed in order to decide whether the
224             combination shall be skipped (e.g. because it is redundant/equivalent to another combination or inadmissible)
225         :param modelSaveDirectory: the directory where the serialized models shall be saved; if None, models are not saved
226         :param name: the name of this grid search, which will, in particular, be prepended to all saved model files;
227             if None, a default name will be generated of the form ""gridSearch_<timestamp>""
228         """"""
229         self.modelFactory = modelFactory
230         if type(parameterOptions) == list:
231             self.parameterOptionsList = parameterOptions
232             paramNames = set(parameterOptions[0].keys())
233             for d in parameterOptions[1:]:
234                 if set(d.keys()) != paramNames:
235                     raise ValueError(""Keys must be the same for all parameter options dictionaries"")
236         else:
237             self.parameterOptionsList = [parameterOptions]
238         self.numProcesses = numProcesses
239         self.parameterCombinationSkipDecider = parameterCombinationSkipDecider
240         self.modelSaveDirectory = modelSaveDirectory
241         self.name = name if name is not None else ""gridSearch_"" + datetime.now().strftime('%Y%m%d-%H%M%S')
242         self.csvResultsPath = csvResultsPath
243         self.incremental = incremental
244         self.incrementalSkipExisting = incrementalSkipExisting
245         if self.csvResultsPath is not None and os.path.isdir(csvResultsPath):
246             self.csvResultsPath = os.path.join(self.csvResultsPath, f""{self.name}_results.csv"")
247 
248         self.numCombinations = 0
249         for parameterOptions in self.parameterOptionsList:
250             n = 1
251             for options in parameterOptions.values():
252                 n *= len(options)
253             self.numCombinations += n
254         log.info(f""Created GridSearch object for {self.numCombinations} parameter combinations"")
255 
256         self._executor = None
257 
258     @classmethod
","Before: 195
After: 225",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,1527,"{'module': 1, 'expression_statement': 8, 'string': 2, 'string_start': 4, 'string_content': 2, 'string_end': 2, 'ERROR': 23, ':': 13, 'identifier': 192, 'assignment': 3, 'type': 3, 'try': 1, 'binary_operator': 3, '-': 2, ',': 8, 'comparison_operator': 5, 'in': 3, 'for': 1, 'call': 3, 'argument_list': 3, '(': 3, 'integer': 1, ')': 3, ';': 4, 'if': 4, 'is': 3, 'none': 3, 'attribute': 4, '.': 4, 'boolean_operator': 1, '/': 1, 'or': 1, 'not': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7208086522898888,0.7163585897008807,"(tensor([0.9273]), tensor([0.9469]), tensor([0.9370]), tensor([0.9449]))"
"246             skipDecider.tell(params, values)
247         return values
248 
249     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
250         """"""
251         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
252         to that file directly after being computed
253 
254         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
255         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
256             Note that the column names that are generated depend on the evaluator/validator being applied.
257         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
258         :return: the data frame with all evaluation results
259         """"""
260         if self.trackedExperiment is not None:
261             loggingCallback = self.trackedExperiment.trackValues
262         elif metricsEvaluator.trackedExperiment is not None:
263             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
264         else:
265             loggingCallback = None
266         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
267                 ascending=ascending)
268 
269         def collectResult(values):
270             if values is None:
271                 return
272             if loggingCallback is not None:
273                 loggingCallback(values)
274             paramsMetricsCollection.addValues(values)
275             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
276 
277         if self.numProcesses == 1:
278             combinationIdx = 0
279             for parameterOptions in self.parameterOptionsList:
280                 for paramsDict in iterParamCombinations(parameterOptions):
281                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
282                             combinationIdx, self.modelSaveDirectory, **paramsDict))
283                     combinationIdx += 1
284         else:
285             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
286             futures = []
287             combinationIdx = 0
288             for parameterOptions in self.parameterOptionsList:
289                 for paramsDict in iterParamCombinations(parameterOptions):
290                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
291                             self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
292                     combinationIdx += 1
293             for future in futures:
294                 collectResult(future.result())
295 
296         return paramsMetricsCollection.getDataFrame()
297 
298 
","278             skipDecider.tell(params, values)
279         return values
280 
281     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
282         """"""
283         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
284         to that file directly after being computed
285 
286         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
287         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
288             Note that the column names that are generated depend on the evaluator/validator being applied.
289         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
290         :return: the data frame with all evaluation results
291         """"""
292         if self.trackedExperiment is not None:
293             loggingCallback = self.trackedExperiment.trackValues
294         elif metricsEvaluator.trackedExperiment is not None:
295             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
296         else:
297             loggingCallback = None
298         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
299             ascending=ascending, incremental=self.incremental)
300 
301         def collectResult(values):
302             if values is None:
303                 return
304             if loggingCallback is not None:
305                 loggingCallback(values)
306             paramsMetricsCollection.addValues(values)
307             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
308 
309         if self.numProcesses == 1:
310             combinationIdx = 0
311             for parameterOptions in self.parameterOptionsList:
312                 for paramsDict in iterParamCombinations(parameterOptions):
313                     if self.incrementalSkipExisting and self.incremental:
314                         if paramsMetricsCollection.contains(paramsDict):
315                             log.info(f""Skipped because parameters are already present in collection (incremental mode): {paramsDict}"")
316                             continue
317                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
318                         combinationIdx, self.modelSaveDirectory, **paramsDict))
319                     combinationIdx += 1
320         else:
321             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
322             futures = []
323             combinationIdx = 0
324             for parameterOptions in self.parameterOptionsList:
325                 for paramsDict in iterParamCombinations(parameterOptions):
326                     if self.incrementalSkipExisting and self.incremental:
327                         if paramsMetricsCollection.contains(paramsDict):
328                             log.info(f""Skipped because parameters are already present in collection (incremental mode): {paramsDict}"")
329                             continue
330                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
331                         self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
332                     combinationIdx += 1
333             for future in futures:
334                 collectResult(future.result())
335 
336         return paramsMetricsCollection.getDataFrame()
337 
338 
","Before: 267
After: 299",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2037,"{'module': 1, 'expression_statement': 18, 'call': 17, 'attribute': 31, 'identifier': 111, '.': 31, 'argument_list': 17, '(': 19, ',': 19, ')': 19, 'return_statement': 3, 'return': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 15, 'type': 2, 'default_parameter': 2, '=': 14, 'none': 6, 'true': 1, '->': 1, 'block': 14, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is not': 6, 'assignment': 8, 'elif_clause': 1, 'elif': 1, 'else_clause': 2, 'else': 2, 'keyword_argument': 4, 'is': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 5, 'for_statement': 5, 'for': 5, 'in': 5, 'dictionary_splat': 2, '**': 2, 'augmented_assignment': 2, '+=': 2, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5737444509211946,0.5690403925537598,"(tensor([0.9316]), tensor([0.9611]), tensor([0.9461]), tensor([0.9581]))"
"246             skipDecider.tell(params, values)
247         return values
248 
249     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
250         """"""
251         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
252         to that file directly after being computed
253 
254         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
255         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
256             Note that the column names that are generated depend on the evaluator/validator being applied.
257         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
258         :return: the data frame with all evaluation results
259         """"""
260         if self.trackedExperiment is not None:
261             loggingCallback = self.trackedExperiment.trackValues
262         elif metricsEvaluator.trackedExperiment is not None:
263             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
264         else:
265             loggingCallback = None
266         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
267                 ascending=ascending)
268 
269         def collectResult(values):
270             if values is None:
271                 return
272             if loggingCallback is not None:
273                 loggingCallback(values)
274             paramsMetricsCollection.addValues(values)
275             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
276 
277         if self.numProcesses == 1:
278             combinationIdx = 0
279             for parameterOptions in self.parameterOptionsList:
280                 for paramsDict in iterParamCombinations(parameterOptions):
281                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
282                             combinationIdx, self.modelSaveDirectory, **paramsDict))
283                     combinationIdx += 1
284         else:
285             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
286             futures = []
287             combinationIdx = 0
288             for parameterOptions in self.parameterOptionsList:
289                 for paramsDict in iterParamCombinations(parameterOptions):
290                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
291                             self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
292                     combinationIdx += 1
293             for future in futures:
294                 collectResult(future.result())
295 
296         return paramsMetricsCollection.getDataFrame()
297 
298 
","278             skipDecider.tell(params, values)
279         return values
280 
281     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
282         """"""
283         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
284         to that file directly after being computed
285 
286         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
287         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
288             Note that the column names that are generated depend on the evaluator/validator being applied.
289         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
290         :return: the data frame with all evaluation results
291         """"""
292         if self.trackedExperiment is not None:
293             loggingCallback = self.trackedExperiment.trackValues
294         elif metricsEvaluator.trackedExperiment is not None:
295             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
296         else:
297             loggingCallback = None
298         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
299             ascending=ascending, incremental=self.incremental)
300 
301         def collectResult(values):
302             if values is None:
303                 return
304             if loggingCallback is not None:
305                 loggingCallback(values)
306             paramsMetricsCollection.addValues(values)
307             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
308 
309         if self.numProcesses == 1:
310             combinationIdx = 0
311             for parameterOptions in self.parameterOptionsList:
312                 for paramsDict in iterParamCombinations(parameterOptions):
313                     if self.incrementalSkipExisting and self.incremental:
314                         if paramsMetricsCollection.contains(paramsDict):
315                             log.info(f""Skipped because parameters are already present in collection (incremental mode): {paramsDict}"")
316                             continue
317                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
318                         combinationIdx, self.modelSaveDirectory, **paramsDict))
319                     combinationIdx += 1
320         else:
321             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
322             futures = []
323             combinationIdx = 0
324             for parameterOptions in self.parameterOptionsList:
325                 for paramsDict in iterParamCombinations(parameterOptions):
326                     if self.incrementalSkipExisting and self.incremental:
327                         if paramsMetricsCollection.contains(paramsDict):
328                             log.info(f""Skipped because parameters are already present in collection (incremental mode): {paramsDict}"")
329                             continue
330                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
331                         self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
332                     combinationIdx += 1
333             for future in futures:
334                 collectResult(future.result())
335 
336         return paramsMetricsCollection.getDataFrame()
337 
338 
","Before: 282
After: 318",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2193,"{'module': 1, 'expression_statement': 18, 'call': 17, 'attribute': 31, 'identifier': 111, '.': 31, 'argument_list': 17, '(': 19, ',': 19, ')': 19, 'return_statement': 3, 'return': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 15, 'type': 2, 'default_parameter': 2, '=': 14, 'none': 6, 'true': 1, '->': 1, 'block': 14, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is not': 6, 'assignment': 8, 'elif_clause': 1, 'elif': 1, 'else_clause': 2, 'else': 2, 'keyword_argument': 4, 'is': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 5, 'for_statement': 5, 'for': 5, 'in': 5, 'dictionary_splat': 2, '**': 2, 'augmented_assignment': 2, '+=': 2, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5737444509211946,0.5690403925537598,"(tensor([0.9316]), tensor([0.9611]), tensor([0.9461]), tensor([0.9581]))"
"246             skipDecider.tell(params, values)
247         return values
248 
249     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
250         """"""
251         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
252         to that file directly after being computed
253 
254         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
255         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
256             Note that the column names that are generated depend on the evaluator/validator being applied.
257         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
258         :return: the data frame with all evaluation results
259         """"""
260         if self.trackedExperiment is not None:
261             loggingCallback = self.trackedExperiment.trackValues
262         elif metricsEvaluator.trackedExperiment is not None:
263             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
264         else:
265             loggingCallback = None
266         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
267                 ascending=ascending)
268 
269         def collectResult(values):
270             if values is None:
271                 return
272             if loggingCallback is not None:
273                 loggingCallback(values)
274             paramsMetricsCollection.addValues(values)
275             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
276 
277         if self.numProcesses == 1:
278             combinationIdx = 0
279             for parameterOptions in self.parameterOptionsList:
280                 for paramsDict in iterParamCombinations(parameterOptions):
281                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
282                             combinationIdx, self.modelSaveDirectory, **paramsDict))
283                     combinationIdx += 1
284         else:
285             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
286             futures = []
287             combinationIdx = 0
288             for parameterOptions in self.parameterOptionsList:
289                 for paramsDict in iterParamCombinations(parameterOptions):
290                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
291                             self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
292                     combinationIdx += 1
293             for future in futures:
294                 collectResult(future.result())
295 
296         return paramsMetricsCollection.getDataFrame()
297 
298 
","278             skipDecider.tell(params, values)
279         return values
280 
281     def run(self, metricsEvaluator: MetricsDictProvider, sortColumnName=None, ascending=True) -> pd.DataFrame:
282         """"""
283         Run the grid search. If csvResultsPath was provided in the constructor, each evaluation result will be saved
284         to that file directly after being computed
285 
286         :param metricsEvaluator: the evaluator or cross-validator with which to evaluate models
287         :param sortColumnName: the name of the column by which to sort the data frame of results; if None, do not sort.
288             Note that the column names that are generated depend on the evaluator/validator being applied.
289         :param ascending: whether to sort in ascending order; has an effect only if sortColumnName is not None
290         :return: the data frame with all evaluation results
291         """"""
292         if self.trackedExperiment is not None:
293             loggingCallback = self.trackedExperiment.trackValues
294         elif metricsEvaluator.trackedExperiment is not None:
295             loggingCallback = metricsEvaluator.trackedExperiment.trackValues
296         else:
297             loggingCallback = None
298         paramsMetricsCollection = ParametersMetricsCollection(csvPath=self.csvResultsPath, sortColumnName=sortColumnName,
299             ascending=ascending, incremental=self.incremental)
300 
301         def collectResult(values):
302             if values is None:
303                 return
304             if loggingCallback is not None:
305                 loggingCallback(values)
306             paramsMetricsCollection.addValues(values)
307             log.info(f""Updated grid search result:\n{paramsMetricsCollection.getDataFrame().to_string()}"")
308 
309         if self.numProcesses == 1:
310             combinationIdx = 0
311             for parameterOptions in self.parameterOptionsList:
312                 for paramsDict in iterParamCombinations(parameterOptions):
313                     if self.incrementalSkipExisting and self.incremental:
314                         if paramsMetricsCollection.contains(paramsDict):
315                             log.info(f""Skipped because parameters are already present in collection (incremental mode): {paramsDict}"")
316                             continue
317                     collectResult(self._evalParams(self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider, self.name,
318                         combinationIdx, self.modelSaveDirectory, **paramsDict))
319                     combinationIdx += 1
320         else:
321             executor = ProcessPoolExecutor(max_workers=self.numProcesses)
322             futures = []
323             combinationIdx = 0
324             for parameterOptions in self.parameterOptionsList:
325                 for paramsDict in iterParamCombinations(parameterOptions):
326                     if self.incrementalSkipExisting and self.incremental:
327                         if paramsMetricsCollection.contains(paramsDict):
328                             log.info(f""Skipped because parameters are already present in collection (incremental mode): {paramsDict}"")
329                             continue
330                     futures.append(executor.submit(self._evalParams, self.modelFactory, metricsEvaluator, self.parameterCombinationSkipDecider,
331                         self.name, combinationIdx, self.modelSaveDirectory, **paramsDict))
332                     combinationIdx += 1
333             for future in futures:
334                 collectResult(future.result())
335 
336         return paramsMetricsCollection.getDataFrame()
337 
338 
","Before: 291
After: 331",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2301,"{'module': 1, 'expression_statement': 18, 'call': 17, 'attribute': 31, 'identifier': 111, '.': 31, 'argument_list': 17, '(': 19, ',': 19, ')': 19, 'return_statement': 3, 'return': 3, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 15, 'type': 2, 'default_parameter': 2, '=': 14, 'none': 6, 'true': 1, '->': 1, 'block': 14, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 4, 'if': 4, 'comparison_operator': 5, 'is not': 6, 'assignment': 8, 'elif_clause': 1, 'elif': 1, 'else_clause': 2, 'else': 2, 'keyword_argument': 4, 'is': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, '==': 1, 'integer': 5, 'for_statement': 5, 'for': 5, 'in': 5, 'dictionary_splat': 2, '**': 2, 'augmented_assignment': 2, '+=': 2, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5737444509211946,0.5690403925537598,"(tensor([0.9316]), tensor([0.9611]), tensor([0.9461]), tensor([0.9581]))"
"337         def _chooseChangedModelParameters(self) -> Dict[str, Any]:
338             pass
339 
340     def __init__(self, modelFactory: Callable[..., VectorModel],
341                  opsAndWeights: List[Tuple[Callable[['SAHyperOpt.State'], 'SAHyperOpt.ParameterChangeOperator'], float]],
342                  initialParameters: Dict[str, Any], metricsEvaluator: MetricsDictProvider,
343                  metricToOptimise, minimiseMetric=False,
344                  collectDataFrame=True, csvResultsPath: Optional[str] = None,
345                  parameterCombinationEquivalenceClassValueCache: ParameterCombinationEquivalenceClassValueCache = None,
346                  p0=0.5, p1=0.0):
347         """"""
348         :param modelFactory: a factory for the generation of models which is called with the current parameter combination
349             (all keyword arguments), initially initialParameters
350         :param opsAndWeights: a sequence of tuples (operator factory, operator weight) for simulated annealing
351         :param initialParameters: the initial parameter combination
352         :param metricsEvaluator: the evaluator/validator to use in order to evaluate models
353         :param metricToOptimise: the name of the metric (as generated by the evaluator/validator) to optimise
354         :param minimiseMetric: whether the metric is to be minimised; if False, maximise the metric
355         :param collectDataFrame: whether to collect (and regularly log) the data frame of all parameter combinations and
356             evaluation results
357         :param csvResultsPath: the (optional) path of a CSV file in which to store a table of all computed results;
358             if this is not None, then collectDataFrame is automatically set to True
359         :param parameterCombinationEquivalenceClassValueCache: a cache in which to store computed results and whose notion
360             of equivalence can be used to avoid duplicate computations
361         :param p0: the initial probability (at the start of the optimisation) of accepting a state with an inferior evaluation
362             to the current state's (for the mean observed evaluation delta)
363         :param p1: the final probability (at the end of the optimisation) of accepting a state with an inferior evaluation
364             to the current state's (for the mean observed evaluation delta)
365         """"""
366         self.minimiseMetric = minimiseMetric
367         self.evaluatorOrValidator = metricsEvaluator
368         self.metricToOptimise = metricToOptimise
369         self.initialParameters = initialParameters
370         self.opsAndWeights = opsAndWeights
371         self.modelFactory = modelFactory
372         self.csvResultsPath = csvResultsPath
373         if csvResultsPath is not None:
374             collectDataFrame = True
375         self.parametersMetricsCollection = ParametersMetricsCollection(csvPath=csvResultsPath) if collectDataFrame else None
376         self.parameterCombinationEquivalenceClassValueCache = parameterCombinationEquivalenceClassValueCache
377         self.p0 = p0
378         self.p1 = p1
379         self._sa = None
380 
381     @classmethod
","377         def _chooseChangedModelParameters(self) -> Dict[str, Any]:
378             pass
379 
380     def __init__(self, modelFactory: Callable[..., VectorModel],
381             opsAndWeights: List[Tuple[Callable[['SAHyperOpt.State'], 'SAHyperOpt.ParameterChangeOperator'], float]],
382             initialParameters: Dict[str, Any], metricsEvaluator: MetricsDictProvider,
383             metricToOptimise, minimiseMetric=False,
384             collectDataFrame=True, csvResultsPath: Optional[str] = None,
385             parameterCombinationEquivalenceClassValueCache: ParameterCombinationEquivalenceClassValueCache = None,
386             p0=0.5, p1=0.0):
387         """"""
388         :param modelFactory: a factory for the generation of models which is called with the current parameter combination
389             (all keyword arguments), initially initialParameters
390         :param opsAndWeights: a sequence of tuples (operator factory, operator weight) for simulated annealing
391         :param initialParameters: the initial parameter combination
392         :param metricsEvaluator: the evaluator/validator to use in order to evaluate models
393         :param metricToOptimise: the name of the metric (as generated by the evaluator/validator) to optimise
394         :param minimiseMetric: whether the metric is to be minimised; if False, maximise the metric
395         :param collectDataFrame: whether to collect (and regularly log) the data frame of all parameter combinations and
396             evaluation results
397         :param csvResultsPath: the (optional) path of a CSV file in which to store a table of all computed results;
398             if this is not None, then collectDataFrame is automatically set to True
399         :param parameterCombinationEquivalenceClassValueCache: a cache in which to store computed results and whose notion
400             of equivalence can be used to avoid duplicate computations
401         :param p0: the initial probability (at the start of the optimisation) of accepting a state with an inferior evaluation
402             to the current state's (for the mean observed evaluation delta)
403         :param p1: the final probability (at the end of the optimisation) of accepting a state with an inferior evaluation
404             to the current state's (for the mean observed evaluation delta)
405         """"""
406         self.minimiseMetric = minimiseMetric
407         self.evaluatorOrValidator = metricsEvaluator
408         self.metricToOptimise = metricToOptimise
409         self.initialParameters = initialParameters
410         self.opsAndWeights = opsAndWeights
411         self.modelFactory = modelFactory
412         self.csvResultsPath = csvResultsPath
413         if csvResultsPath is not None:
414             collectDataFrame = True
415         self.parametersMetricsCollection = ParametersMetricsCollection(csvPath=csvResultsPath) if collectDataFrame else None
416         self.parameterCombinationEquivalenceClassValueCache = parameterCombinationEquivalenceClassValueCache
417         self.p0 = p0
418         self.p1 = p1
419         self._sa = None
420 
421     @classmethod
","Before: 341, 342, 343, 344, 345, 346
After: 381, 382, 383, 384, 385, 386",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2831,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 71, 'parameters': 2, '(': 3, ')': 3, '->': 1, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 8, ',': 16, ']': 8, ':': 9, 'block': 3, 'pass_statement': 1, 'pass': 1, 'typed_parameter': 4, 'ellipsis': 1, 'list': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'default_parameter': 4, '=': 20, 'false': 1, 'true': 2, 'typed_default_parameter': 2, 'none': 5, 'float': 2, 'expression_statement': 14, 'assignment': 13, 'attribute': 12, '.': 12, 'if_statement': 1, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'conditional_expression': 1, 'call': 1, 'argument_list': 1, 'keyword_argument': 1, 'else': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7577249664330654,0.7477983569728471,"(tensor([0.9561]), tensor([0.9562]), tensor([0.9562]), tensor([0.9562]))"
"379         self._sa = None
380 
381     @classmethod
382     def _evalParams(cls, modelFactory, metricsEvaluator: MetricsDictProvider, parametersMetricsCollection: Optional[ParametersMetricsCollection],
383                     parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
384         if trackedExperiment is not None and metricsEvaluator.trackedExperiment is not None:
385             log.warning(f""Tracked experiment already set in evaluator, results will be tracked twice and""
386                         f""might get overwritten!"")
387 
388         metrics = None
389         if parameterCombinationEquivalenceClassValueCache is not None:
390             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
391         if metrics is not None:
392             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
393         else:
394             cls.log.info(f""Evaluating parameter combination {params}"")
395             model = modelFactory(**params)
396             metrics = metricsEvaluator.computeMetrics(model)
397             cls.log.info(f""Got metrics {metrics} for {params}"")
398 
399             values = dict(metrics)
400             values[""str(model)""] = str(model)
401             values.update(**params)
402             if trackedExperiment is not None:
403                 trackedExperiment.trackValues(values)
404             if parametersMetricsCollection is not None:
405                 parametersMetricsCollection.addValues(values)
406                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
407             if parameterCombinationEquivalenceClassValueCache is not None:
408                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
409         return metrics
410 
411     def _computeMetric(self, params):
","419         self._sa = None
420 
421     @classmethod
422     def _evalParams(cls, modelFactory, metricsEvaluator: MetricsDictProvider, parametersMetricsCollection: Optional[ParametersMetricsCollection],
423             parameterCombinationEquivalenceClassValueCache, trackedExperiment, **params):
424         if trackedExperiment is not None and metricsEvaluator.trackedExperiment is not None:
425             log.warning(f""Tracked experiment already set in evaluator, results will be tracked twice and""
426                         f""might get overwritten!"")
427 
428         metrics = None
429         if parameterCombinationEquivalenceClassValueCache is not None:
430             metrics = parameterCombinationEquivalenceClassValueCache.get(params)
431         if metrics is not None:
432             cls.log.info(f""Result for parameter combination {params} could be retrieved from cache, not adding new result"")
433         else:
434             cls.log.info(f""Evaluating parameter combination {params}"")
435             model = modelFactory(**params)
436             metrics = metricsEvaluator.computeMetrics(model)
437             cls.log.info(f""Got metrics {metrics} for {params}"")
438 
439             values = dict(metrics)
440             values[""str(model)""] = str(model)
441             values.update(**params)
442             if trackedExperiment is not None:
443                 trackedExperiment.trackValues(values)
444             if parametersMetricsCollection is not None:
445                 parametersMetricsCollection.addValues(values)
446                 cls.log.info(f""Data frame with all results:\n\n{parametersMetricsCollection.getDataFrame().to_string()}\n"")
447             if parameterCombinationEquivalenceClassValueCache is not None:
448                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
449         return metrics
450 
451     def _computeMetric(self, params):
","Before: 383
After: 423",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3068,"{'module': 1, 'expression_statement': 16, 'assignment': 7, 'attribute': 19, 'identifier': 75, '.': 19, '=': 7, 'none': 9, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 17, ',': 7, 'typed_parameter': 2, ':': 10, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 3, ')': 17, 'block': 8, 'if_statement': 6, 'if': 6, 'boolean_operator': 1, 'comparison_operator': 7, 'is not': 14, 'and': 1, 'call': 16, 'argument_list': 16, 'concatenated_string': 1, 'string': 7, 'string_start': 7, 'string_content': 10, 'string_end': 7, 'interpolation': 5, '{': 5, '}': 5, 'else_clause': 1, 'else': 1, 'dictionary_splat': 2, 'subscript': 1, 'escape_sequence': 3, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7236148379251728,0.7059624466243457,"(tensor([0.9732]), tensor([0.9730]), tensor([0.9731]), tensor([0.9731]))"
"408                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
409         return metrics
410 
411     def _computeMetric(self, params):
412         metrics = self._evalParams(self.modelFactory, self.evaluatorOrValidator, self.parametersMetricsCollection,
413                                    self.parameterCombinationEquivalenceClassValueCache, self.trackedExperiment, **params)
414         metricValue = metrics[self.metricToOptimise]
415         if not self.minimiseMetric:
416             return -metricValue
417         return metricValue
418 
419     def run(self, maxSteps=None, duration=None, randomSeed=42, collectStats=True):
","448                 parameterCombinationEquivalenceClassValueCache.set(params, metrics)
449         return metrics
450 
451     def _computeMetric(self, params):
452         metrics = self._evalParams(self.modelFactory, self.evaluatorOrValidator, self.parametersMetricsCollection,
453             self.parameterCombinationEquivalenceClassValueCache, self.trackedExperiment, **params)
454         metricValue = metrics[self.metricToOptimise]
455         if not self.minimiseMetric:
456             return -metricValue
457         return metricValue
458 
459     def run(self, maxSteps=None, duration=None, randomSeed=42, collectStats=True):
","Before: 413
After: 453",add incremental mode to parameter metrics collection,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/hyperopt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3426,"{'module': 1, 'expression_statement': 3, 'call': 2, 'attribute': 9, 'identifier': 30, '.': 9, 'argument_list': 2, '(': 3, ',': 7, ')': 3, 'return_statement': 3, 'return': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 2, 'block': 2, 'assignment': 2, '=': 2, 'dictionary_splat': 1, '**': 1, 'subscript': 1, '[': 1, ']': 1, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'unary_operator': 1, '-': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 56, 'name': 'iterParamCombinations._iterRecursiveParamCombinations', 'long_name': 'iterParamCombinations._iterRecursiveParamCombinations( pairs , i , params )', 'start_line': 31, 'end_line': 44, 'full_parameters': ['pairs', ' i', ' params'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/hyperopt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6436380875157448,0.6078955000293257,"(tensor([0.9727]), tensor([0.9728]), tensor([0.9728]), tensor([0.9728]))"
"1 import io
2 import logging
3 from abc import ABC, abstractmethod
4 from typing import Union, Tuple, Callable, Optional, List
5 
6 import numpy as np
7 import pandas as pd
8 import torch
","1 import io
2 import logging
3 from abc import ABC, abstractmethod
4 from typing import Union, Tuple, Callable, Optional, List, Sequence
5 
6 import numpy as np
7 import pandas as pd
8 import torch
","Before: 4
After: 4",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,39,"{'module': 1, 'import_statement': 4, 'import': 6, 'dotted_name': 13, 'identifier': 15, 'import_from_statement': 2, 'from': 2, ',': 5, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9122561819614461,0.911053468493639,"(tensor([0.9942]), tensor([0.9976]), tensor([0.9959]), tensor([0.9972]))"
"15 from ..normalisation import NormalisationMode
16 from ..util.dtype import toFloatArray
17 from ..util.string import objectRepr, ToStringMixin
18 from ..vector_model import VectorRegressionModel, VectorClassificationModel
19 
20 log = logging.getLogger(__name__)
21 
22 
23 class MCDropoutCapableNNModule(nn.Module, ABC):
24     """"""
","15 from ..normalisation import NormalisationMode
16 from ..util.dtype import toFloatArray
17 from ..util.string import objectRepr, ToStringMixin
18 from ..vector_model import VectorRegressionModel, VectorClassificationModel
19 
20 log: logging.Logger = logging.getLogger(__name__)
21 
22 
23 class MCDropoutCapableNNModule(nn.Module, ABC):
24     """"""
","Before: 20
After: 20",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,191,"{'module': 1, 'import_from_statement': 4, 'from': 4, 'relative_import': 4, 'import_prefix': 4, '.': 12, 'dotted_name': 10, 'identifier': 20, 'import': 4, ',': 3, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 2, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9312457603037672,0.9308407239947162,"(tensor([0.9907]), tensor([0.9939]), tensor([0.9923]), tensor([0.9935]))"
"27     Then, to apply inference that samples results, call inferMCDropout rather than just using __call__.
28     """"""
29 
30     def __init__(self):
31         super().__init__()
32         self._applyMCDropout = False
33         self._pMCDropoutOverride = None
34 
35     def __setstate__(self, d):
","27     Then, to apply inference that samples results, call inferMCDropout rather than just using __call__.
28     """"""
29 
30     def __init__(self) -> None:
31         super().__init__()
32         self._applyMCDropout = False
33         self._pMCDropoutOverride = None
34 
35     def __setstate__(self, d: dict) -> None:
","Before: 30
After: 30",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,219,"{'module': 1, 'expression_statement': 1, 'identifier': 14, ',': 2, 'ERROR': 3, '.': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8198858095939854,0.8173291840348255,"(tensor([0.9676]), tensor([0.9949]), tensor([0.9810]), tensor([0.9921]))"
"32         self._applyMCDropout = False
33         self._pMCDropoutOverride = None
34 
35     def __setstate__(self, d):
36         if ""_applyMCDropout"" not in d:
37             d[""_applyMCDropout""] = False
38         if ""_pMCDropoutOverride"" not in d:
39             d[""_pMCDropoutOverride""] = None
40         super().__setstate__(d)
41 
42     def _dropout(self, x, pTraining=None, pInference=None):
","32         self._applyMCDropout = False
33         self._pMCDropoutOverride = None
34 
35     def __setstate__(self, d: dict) -> None:
36         if ""_applyMCDropout"" not in d:
37             d[""_applyMCDropout""] = False
38         if ""_pMCDropoutOverride"" not in d:
39             d[""_pMCDropoutOverride""] = None
40         super().__setstate__(d)
41 
42     def _dropout(self, x: torch.Tensor, pTraining=None, pInference=None) -> torch.Tensor:
","Before: 35
After: 35",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,259,"{'module': 1, 'expression_statement': 5, 'assignment': 4, 'attribute': 3, 'identifier': 14, '.': 3, '=': 4, 'false': 2, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 1, ')': 3, ':': 3, 'block': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'not in': 4, 'subscript': 2, '[': 2, ']': 2, 'call': 2, 'argument_list': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.827093987449991,0.8208533324802877,"(tensor([0.9526]), tensor([0.9925]), tensor([0.9721]), tensor([0.9883]))"
"39             d[""_pMCDropoutOverride""] = None
40         super().__setstate__(d)
41 
42     def _dropout(self, x, pTraining=None, pInference=None):
43         """"""
44         This method is to to applied within the module's forward method to apply dropouts during training and/or inference.
45 
46         :param x: the model input tensor
47         :param pTraining: the probability with which to apply dropouts during training; if None, apply no dropout
48         :param pInference:  the probability with which to apply dropouts during MC-Dropout-based inference (via inferMCDropout,
49             which may override the probability via its optional argument);
50             if None, a dropout is not to be applied
51         :return: a potentially modified version of x with some elements dropped out, depending on application context and dropout probabilities
52         """"""
53         if self.training and pTraining is not None:
54             return F.dropout(x, pTraining)
55         elif not self.training and self._applyMCDropout and pInference is not None:
56             return F.dropout(x, pInference if self._pMCDropoutOverride is None else self._pMCDropoutOverride)
57         else:
58             return x
59 
60     def _enableMCDropout(self, enabled=True, pMCDropoutOverride=None):
","39             d[""_pMCDropoutOverride""] = None
40         super().__setstate__(d)
41 
42     def _dropout(self, x: torch.Tensor, pTraining=None, pInference=None) -> torch.Tensor:
43         """"""
44         This method is to to applied within the module's forward method to apply dropouts during training and/or inference.
45 
46         :param x: the model input tensor
47         :param pTraining: the probability with which to apply dropouts during training; if None, apply no dropout
48         :param pInference:  the probability with which to apply dropouts during MC-Dropout-based inference (via inferMCDropout,
49             which may override the probability via its optional argument);
50             if None, a dropout is not to be applied
51         :return: a potentially modified version of x with some elements dropped out, depending on application context and dropout probabilities
52         """"""
53         if self.training and pTraining is not None:
54             return F.dropout(x, pTraining)
55         elif not self.training and self._applyMCDropout and pInference is not None:
56             return F.dropout(x, pInference if self._pMCDropoutOverride is None else self._pMCDropoutOverride)
57         else:
58             return x
59 
60     def _enableMCDropout(self, enabled=True, pMCDropoutOverride=None) -> None:
","Before: 42
After: 42",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,342,"{'module': 1, 'expression_statement': 3, 'assignment': 1, 'subscript': 1, 'identifier': 30, '[': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 1, '=': 3, 'none': 6, 'call': 4, 'attribute': 8, 'argument_list': 4, '(': 5, ')': 5, '.': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'default_parameter': 2, ':': 4, 'block': 4, 'if_statement': 1, 'if': 2, 'boolean_operator': 3, 'and': 3, 'comparison_operator': 3, 'is not': 4, 'return_statement': 3, 'return': 3, 'elif_clause': 1, 'elif': 1, 'not_operator': 1, 'not': 1, 'conditional_expression': 1, 'is': 1, 'else': 2, 'else_clause': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9472947946584583,0.94633121120119,"(tensor([0.9832]), tensor([0.9952]), tensor([0.9891]), tensor([0.9940]))"
"57         else:
58             return x
59 
60     def _enableMCDropout(self, enabled=True, pMCDropoutOverride=None):
61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x, numSamples, p=None):
","57         else:
58             return x
59 
60     def _enableMCDropout(self, enabled=True, pMCDropoutOverride=None) -> None:
61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: torch.Tensor, numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
","Before: 60
After: 60",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,451,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'identifier': 13, ':': 2, 'ERROR': 1, 'type': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 2, 'default_parameter': 2, '=': 4, 'true': 1, 'none': 1, ')': 1, 'block': 1, 'attribute': 2, '.': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6711113337473339,0.6392459927429222,"(tensor([0.9192]), tensor([0.9922]), tensor([0.9543]), tensor([0.9844]))"
"61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x, numSamples, p=None):
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         results = []
74         self._enableMCDropout(True, pMCDropoutOverride=p)
75         try:
76             for i in range(numSamples):
77                 y = self(x)
78                 results.append(y)
79         finally:
80             self._enableMCDropout(False)
81         results = torch.stack(results)
82         mean = torch.mean(results, 0)
83         stddev = torch.std(results, 0, unbiased=False)
84         return mean, stddev
85 
86 
","61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: torch.Tensor, numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         results = []
74         self._enableMCDropout(True, pMCDropoutOverride=p)
75         try:
76             for i in range(numSamples):
77                 y = self(x)
78                 results.append(y)
79         finally:
80             self._enableMCDropout(False)
81         results = torch.stack(results)
82         mean = torch.mean(results, 0)
83         stddev = torch.std(results, 0, unbiased=False)
84         return mean, stddev
85 
86 
","Before: 64
After: 64",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,485,"{'module': 1, 'expression_statement': 11, 'assignment': 7, 'attribute': 8, 'identifier': 42, '.': 8, '=': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 8, 'default_parameter': 1, 'none': 1, ')': 9, ':': 4, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'list': 1, '[': 1, ']': 1, 'call': 8, 'argument_list': 8, 'true': 1, 'keyword_argument': 2, 'try_statement': 1, 'try': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'finally_clause': 1, 'finally': 1, 'false': 2, 'integer': 2, 'return_statement': 1, 'return': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9378241868284993,0.9347801149525763,"(tensor([0.9677]), tensor([0.9827]), tensor([0.9751]), tensor([0.9812]))"
"93     """"""
94     log: logging.Logger = log.getChild(__qualname__)
95 
96     def __init__(self, cuda=True):
97         self.cuda: bool = cuda
98         self.module: Optional[torch.nn.Module] = None
99         self.outputScaler: Optional[TensorScaler] = None
100         self.inputScaler: Optional[TensorScaler] = None
101         self.trainingInfo: Optional[TrainingInfo] = None
102         self._gpu: Optional[int] = None
103 
104     def setTorchModule(self, module: torch.nn.Module) -> None:
","93     """"""
94     log: logging.Logger = log.getChild(__qualname__)
95 
96     def __init__(self, cuda=True) -> None:
97         self.cuda: bool = cuda
98         self.module: Optional[torch.nn.Module] = None
99         self.outputScaler: Optional[TensorScaler] = None
100         self.inputScaler: Optional[TensorScaler] = None
101         self.trainingInfo: Optional[TrainingInfo] = None
102         self._gpu: Optional[int] = None
103 
104     def setTorchModule(self, module: torch.nn.Module) -> None:
","Before: 96
After: 96",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,665,"{'module': 1, 'ERROR': 1, 'string_start': 1, 'expression_statement': 7, 'assignment': 7, 'identifier': 35, ':': 8, 'type': 12, 'attribute': 10, '.': 10, '=': 8, 'call': 1, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'default_parameter': 1, 'true': 1, 'block': 1, 'generic_type': 5, 'type_parameter': 5, '[': 5, ']': 5, 'none': 5}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.950027409461844,0.945679153594656,"(tensor([0.9946]), tensor([0.9966]), tensor([0.9956]), tensor([0.9964]))"
"250         self.outputScaler = data.getOutputTensorScaler()
251         self.inputScaler = data.getInputTensorScaler()
252 
253     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None) -> None:
254         """"""
255         Fits this model using the given model and strategy
256 
257         :param data: a provider for the data with which to fit the model
258         :param strategy: the fitting strategy; if None, use TorchModelFittingStrategyDefault.
259             Pass your own strategy to perform custom fitting processes, e.g. process which involve multi-stage learning
260         :param nnOptimiserParams: the parameters with which to create an optimiser which can be applied in the fitting strategy
261         """"""
262         self._extractParamsFromData(data)
263         optimiser = NNOptimiser(nnOptimiserParams)
264         if strategy is None:
265             strategy = TorchModelFittingStrategyDefault()
266         strategy.fit(self, data, optimiser)
267         self.trainingInfo = optimiser.getTrainingInfo()
268         self._gpu = self._getGPUFromModelParameterDevice()
269 
270     def _getGPUFromModelParameterDevice(self) -> Optional[int]:
","250         self.outputScaler = data.getOutputTensorScaler()
251         self.inputScaler = data.getInputTensorScaler()
252 
253     def fit(self, data: TorchDataSetProvider, nnOptimiserParams: NNOptimiserParams, strategy: ""TorchModelFittingStrategy"" = None) -> None:
254         """"""
255         Fits this model using the given model and strategy
256 
257         :param data: a provider for the data with which to fit the model
258         :param strategy: the fitting strategy; if None, use TorchModelFittingStrategyDefault.
259             Pass your own strategy to perform custom fitting processes, e.g. process which involve multi-stage learning
260         :param nnOptimiserParams: the parameters with which to create an optimiser which can be applied in the fitting strategy
261         """"""
262         self._extractParamsFromData(data)
263         optimiser = NNOptimiser(nnOptimiserParams)
264         if strategy is None:
265             strategy = TorchModelFittingStrategyDefault()
266         self.trainingInfo = strategy.fit(self, data, optimiser)
267         self._gpu = self._getGPUFromModelParameterDevice()
268 
269     def _getGPUFromModelParameterDevice(self) -> Optional[int]:
","Before: 266, 267
After: 266",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2254,"{'module': 1, 'expression_statement': 9, 'assignment': 6, 'attribute': 10, 'identifier': 37, '.': 10, '=': 7, 'call': 8, 'argument_list': 8, '(': 9, ')': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'typed_parameter': 2, ':': 5, 'type': 4, 'typed_default_parameter': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'none': 3, '->': 1, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9414692798556253,0.941319833166178,"(tensor([0.9961]), tensor([0.9930]), tensor([0.9946]), tensor([0.9934]))"
"282     Defines the interface for fitting strategies that can be used in TorchModel.fit
283     """"""
284     @abstractmethod
285     def fit(self, model: TorchModel, data: TorchDataSetProvider, nnOptimiser: NNOptimiser):
286         pass
287 
288 
","292     Defines the interface for fitting strategies that can be used in TorchModel.fit
293     """"""
294     @abstractmethod
295     def fit(self, model: TorchModel, data: TorchDataSetProvider, nnOptimiser: NNOptimiser) -> Optional[TrainingInfo]:
296         pass
297 
298 
","Before: 285
After: 295",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2430,"{'module': 1, 'ERROR': 3, 'identifier': 11, 'for': 1, 'in': 1, 'attribute': 1, '.': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5969775277324589,0.5689320395717082,"(tensor([0.9477]), tensor([0.9738]), tensor([0.9606]), tensor([0.9711]))"
"290     """"""
291     Represents the default fitting strategy, which simply applies the given optimiser to the model and data
292     """"""
293     def fit(self, model: TorchModel, data: TorchDataSetProvider, nnOptimiser: NNOptimiser):
294         nnOptimiser.fit(model, data)
295 
296 
","300     """"""
301     Represents the default fitting strategy, which simply applies the given optimiser to the model and data
302     """"""
303     def fit(self, model: TorchModel, data: TorchDataSetProvider, nnOptimiser: NNOptimiser) -> Optional[TrainingInfo]:
304         return nnOptimiser.fit(model, data)
305 
306 
","Before: 293, 294
After: 303, 304",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2473,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'identifier': 12, 'parameters': 1, '(': 2, ',': 4, 'typed_parameter': 3, ':': 4, 'type': 3, ')': 2, 'block': 1, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6437188172678485,0.5960945017177796,"(tensor([0.9535]), tensor([0.9817]), tensor([0.9674]), tensor([0.9788]))"
"295 
296 
297 class TorchModelFromModuleFactory(TorchModel):
298     def __init__(self, moduleFactory: Callable[[], torch.nn.Module], cuda=True):
299         super().__init__(cuda)
300         self.moduleFactory = moduleFactory
301 
302     def createTorchModule(self) -> torch.nn.Module:
","305 
306 
307 class TorchModelFromModuleFactory(TorchModel):
308     def __init__(self, moduleFactory: Callable[[], torch.nn.Module], cuda: bool = True) -> None:
309         super().__init__(cuda)
310         self.moduleFactory = moduleFactory
311 
312     def createTorchModule(self) -> torch.nn.Module:
","Before: 298
After: 308",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2531,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 16, 'argument_list': 3, '(': 4, ')': 4, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, 'typed_parameter': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, 'attribute': 4, '.': 4, 'default_parameter': 1, '=': 2, 'true': 1, 'expression_statement': 2, 'call': 2, 'assignment': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5736292099960439,0.5133011817677735,"(tensor([0.9595]), tensor([0.9733]), tensor([0.9664]), tensor([0.9719]))"
"308     Base class for TorchModels that can be used within VectorModels, where the input and output dimensions
309     are determined by the data
310     """"""
311     def __init__(self, cuda: bool = True):
312         super().__init__(cuda=cuda)
313         self.inputDim = None
314         self.outputDim = None
315 
316     def _extractParamsFromData(self, data: TorchDataSetProvider):
","318     Base class for TorchModels that can be used within VectorModels, where the input and output dimensions
319     are determined by the data
320     """"""
321     def __init__(self, cuda: bool = True) -> None:
322         super().__init__(cuda=cuda)
323         self.inputDim = None
324         self.outputDim = None
325 
326     def _extractParamsFromData(self, data: TorchDataSetProvider) -> None:
","Before: 311
After: 321",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2614,"{'module': 1, 'ERROR': 3, 'identifier': 19, 'for': 1, ',': 1, 'and': 1, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6190493623953254,0.6010867434471909,"(tensor([0.9644]), tensor([0.9819]), tensor([0.9731]), tensor([0.9801]))"
"313         self.inputDim = None
314         self.outputDim = None
315 
316     def _extractParamsFromData(self, data: TorchDataSetProvider):
317         super()._extractParamsFromData(data)
318         self.inputDim = data.getInputDim()
319         self.outputDim = data.getModelOutputDim()
320 
321     def createTorchModule(self):
","323         self.inputDim = None
324         self.outputDim = None
325 
326     def _extractParamsFromData(self, data: TorchDataSetProvider) -> None:
327         super()._extractParamsFromData(data)
328         self.inputDim = data.getInputDim()
329         self.outputDim = data.getModelOutputDim()
330 
331     def createTorchModule(self) -> torch.nn.Module:
","Before: 316
After: 326",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2662,"{'module': 1, 'expression_statement': 5, 'assignment': 4, 'attribute': 7, 'identifier': 19, '.': 7, '=': 4, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 5, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 1, ')': 5, 'block': 1, 'call': 4, 'argument_list': 4}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4880312509412481,0.42035305129901984,"(tensor([0.9491]), tensor([0.9748]), tensor([0.9618]), tensor([0.9722]))"
"318         self.inputDim = data.getInputDim()
319         self.outputDim = data.getModelOutputDim()
320 
321     def createTorchModule(self):
322         return self.createTorchModuleForDims(self.inputDim, self.outputDim)
323 
324     @abstractmethod
","328         self.inputDim = data.getInputDim()
329         self.outputDim = data.getModelOutputDim()
330 
331     def createTorchModule(self) -> torch.nn.Module:
332         return self.createTorchModuleForDims(self.inputDim, self.outputDim)
333 
334     @abstractmethod
","Before: 321
After: 331",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2715,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 7, 'identifier': 16, '.': 7, '=': 2, 'call': 3, 'argument_list': 3, '(': 4, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.47962597099367127,0.4126701395004649,"(tensor([0.9541]), tensor([0.9745]), tensor([0.9642]), tensor([0.9725]))"
"322         return self.createTorchModuleForDims(self.inputDim, self.outputDim)
323 
324     @abstractmethod
325     def createTorchModuleForDims(self, inputDim, outputDim) -> torch.nn.Module:
326         pass
327 
328 
","332         return self.createTorchModuleForDims(self.inputDim, self.outputDim)
333 
334     @abstractmethod
335     def createTorchModuleForDims(self, inputDim: int, outputDim: int) -> torch.nn.Module:
336         pass
337 
338 
","Before: 325
After: 335",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2760,"{'module': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'attribute': 5, 'identifier': 14, '.': 5, 'argument_list': 1, '(': 2, ',': 3, ')': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.44134071672028896,0.4092175654392008,"(tensor([0.9605]), tensor([0.9731]), tensor([0.9667]), tensor([0.9718]))"
"331     Base class for the implementation of VectorRegressionModels based on TorchModels.
332     An instance of this class will have an instance of TorchModel as the underlying model.
333     """"""
334     def __init__(self, modelClass: Callable[..., TorchModel], modelArgs=(), modelKwArgs=None,
335             normalisationMode=NormalisationMode.NONE, nnOptimiserParams: Union[dict, NNOptimiserParams] = None):
336         """"""
337         :param modelClass: the constructor with which to create the wrapped torch vector model
338         :param modelArgs: the constructor argument list to pass to modelClass
339         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
340         :param normalisationMode: the normalisation mode to apply to input data frames
341         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
342         """"""
343         super().__init__()
344         if modelKwArgs is None:
345             modelKwArgs = {}
346 
347         if nnOptimiserParams is None:
348             nnOptimiserParamsInstance = NNOptimiserParams()
349         else:
350             nnOptimiserParamsInstance = NNOptimiserParams.fromDictOrInstance(nnOptimiserParams)
351         if nnOptimiserParamsInstance.lossEvaluator is None:
352             nnOptimiserParamsInstance.lossEvaluator = NNLossEvaluatorRegression(NNLossEvaluatorRegression.LossFunction.MSELOSS)
353 
354         self.normalisationMode = normalisationMode
355         self.nnOptimiserParams = nnOptimiserParamsInstance
356         self.modelClass = modelClass
357         self.modelArgs = modelArgs
358         self.modelKwArgs = modelKwArgs
359         self.model: Optional[TorchModel] = None
360 
361     def __setstate__(self, state):
","339 class TorchVectorRegressionModel(VectorRegressionModel):
340     """"""
341     Base class for the implementation of VectorRegressionModels based on TorchModels.
342     An instance of this class will have an instance of TorchModel as the underlying model.
343     """"""
344 
345     def __init__(self, modelClass: Callable[..., TorchModel], modelArgs: Sequence = (), modelKwArgs: Optional[dict] = None,
346             normalisationMode: NormalisationMode = NormalisationMode.NONE,
347             nnOptimiserParams: Union[dict, NNOptimiserParams, None] = None) -> None:
348         """"""
","Before: 334, 335
After: 344, 345, 346, 347",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2809,"{'module': 1, 'ERROR': 15, 'identifier': 76, 'for': 1, 'attribute': 1, '.': 2, 'class': 1, 'expression_statement': 8, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 10, 'assignment': 5, 'type': 5, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'block': 1, 'pass_statement': 1, 'pass': 1, 'comparison_operator': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.151995137400232,0.14975380930409402,"(tensor([0.9050]), tensor([0.7875]), tensor([0.8422]), tensor([0.7979]))"
"358         self.modelKwArgs = modelKwArgs
359         self.model: Optional[TorchModel] = None
360 
361     def __setstate__(self, state):
362         state[""nnOptimiserParams""] = NNOptimiserParams.fromDictOrInstance(state[""nnOptimiserParams""])
363         s = super()
364         if hasattr(s, '__setstate__'):
365             s.__setstate__(state)
366         else:
367             self.__dict__ = state
368 
369     def _createTorchModel(self) -> TorchModel:
","370         self.modelKwArgs = modelKwArgs
371         self.model: Optional[TorchModel] = None
372 
373     def __setstate__(self, state) -> None:
374         state[""nnOptimiserParams""] = NNOptimiserParams.fromDictOrInstance(state[""nnOptimiserParams""])
375         s = super()
376         if hasattr(s, '__setstate__'):
377             s.__setstate__(state)
378         else:
379             self.__dict__ = state
380 
381     def _createTorchModel(self) -> TorchModel:
","Before: 361
After: 373",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3002,"{'module': 1, 'expression_statement': 6, 'assignment': 5, 'attribute': 5, 'identifier': 24, '.': 5, '=': 5, ':': 4, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 5, ',': 2, ')': 5, 'block': 3, 'subscript': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 4, 'argument_list': 4, 'if_statement': 1, 'if': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6245275610184033,0.5900771063658014,"(tensor([0.9710]), tensor([0.9748]), tensor([0.9729]), tensor([0.9744]))"
"373         dataUtil = VectorDataUtil(inputs, outputs, self.model.cuda, normalisationMode=self.normalisationMode)
374         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
375 
376     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
377         self.model = self._createTorchModel()
378         dataSetProvider = self._createDataSetProvider(inputs, outputs)
379         self.model.fit(dataSetProvider, self.nnOptimiserParams)
380 
381     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","385         dataUtil = VectorDataUtil(inputs, outputs, self.model.cuda, normalisationMode=self.normalisationMode)
386         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
387 
388     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
389         self.model = self._createTorchModel()
390         dataSetProvider = self._createDataSetProvider(inputs, outputs)
391         self.model.fit(dataSetProvider, self.nnOptimiserParams)
392 
393     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","Before: 376
After: 388",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3211,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'identifier': 38, '=': 4, 'call': 5, 'argument_list': 5, '(': 6, ',': 8, 'attribute': 13, '.': 13, 'keyword_argument': 1, ')': 6, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 3, 'type': 2, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7037320701007498,0.6586149558409637,"(tensor([0.9800]), tensor([0.9839]), tensor([0.9819]), tensor([0.9835]))"
"392         yArray = self._predictOutputsForInputDataFrame(inputs)
393         return pd.DataFrame(yArray, columns=self.getModelOutputVariableNames())
394 
395     def __str__(self):
396         return objectRepr(self, [""model"", ""normalisationMode"", ""nnOptimiserParams""])
397 
398 
","404         yArray = self._predictOutputsForInputDataFrame(inputs)
405         return pd.DataFrame(yArray, columns=self.getModelOutputVariableNames())
406 
407     def __str__(self) -> str:
408         return objectRepr(self, [""model"", ""normalisationMode"", ""nnOptimiserParams""])
409 
410 
","Before: 395
After: 407",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3448,"{'module': 1, 'expression_statement': 1, 'assignment': 1, 'identifier': 14, '=': 2, 'call': 4, 'attribute': 3, '.': 3, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 2, 'return': 2, ',': 4, 'keyword_argument': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'list': 1, '[': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6785581616069449,0.6213883177814905,"(tensor([0.9628]), tensor([0.9743]), tensor([0.9685]), tensor([0.9731]))"
"401     Base class for the implementation of VectorClassificationModels based on TorchModels.
402     An instance of this class will have an instance of TorchModel as the underlying model.
403     """"""
404     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs=(), modelKwArgs=None,
405             normalisationMode=NormalisationMode.NONE, nnOptimiserParams: Union[dict, NNOptimiserParams] = None):
406         """"""
407         :param modelClass: the constructor with which to create the wrapped torch vector model
408         :param modelArgs: the constructor argument list to pass to modelClass
409         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
410         :param normalisationMode: the normalisation mode to apply to input data frames
411         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
412         """"""
413         super().__init__()
414         if modelKwArgs is None:
415             modelKwArgs = {}
416 
417         if nnOptimiserParams is None:
418             nnOptimiserParamsInstance = NNOptimiserParams()
419         else:
420             nnOptimiserParamsInstance = NNOptimiserParams.fromDictOrInstance(nnOptimiserParams)
421         if nnOptimiserParamsInstance.lossEvaluator is None:
422             nnOptimiserParamsInstance.lossEvaluator = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
423 
424         self.normalisationMode = normalisationMode
425         self.nnOptimiserParams = nnOptimiserParams
426         self.modelClass = modelClass
427         self.modelArgs = modelArgs
428         self.modelKwArgs = modelKwArgs
429         self.model: Optional[VectorTorchModel] = None
430 
431     def __setstate__(self, state):
","413     Base class for the implementation of VectorClassificationModels based on TorchModels.
414     An instance of this class will have an instance of TorchModel as the underlying model.
415     """"""
416     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs: Sequence = (), modelKwArgs: Optional[dict] = None,
417             normalisationMode: NormalisationMode = NormalisationMode.NONE,
418             nnOptimiserParams: Union[dict, NNOptimiserParams, None] = None) -> None:
419         """"""
420         :param modelClass: the constructor with which to create the wrapped torch vector model
421         :param modelArgs: the constructor argument list to pass to modelClass
422         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
423         :param normalisationMode: the normalisation mode to apply to input data frames
424         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
425         """"""
426         super().__init__()
427         if modelKwArgs is None:
428             modelKwArgs = {}
429 
430         if nnOptimiserParams is None:
431             nnOptimiserParamsInstance = NNOptimiserParams()
432         else:
433             nnOptimiserParamsInstance = NNOptimiserParams.fromDictOrInstance(nnOptimiserParams)
434         if nnOptimiserParamsInstance.lossEvaluator is None:
435             nnOptimiserParamsInstance.lossEvaluator = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
436 
437         self.normalisationMode = normalisationMode
438         self.nnOptimiserParams = nnOptimiserParams
439         self.modelClass = modelClass
440         self.modelArgs = modelArgs
441         self.modelKwArgs = modelKwArgs
442         self.model: Optional[VectorTorchModel] = None
443 
444     def __setstate__(self, state) -> None:
","Before: 404, 405
After: 416, 417, 418",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3521,"{'module': 1, 'ERROR': 15, 'identifier': 76, 'for': 1, 'attribute': 1, '.': 2, 'class': 1, 'expression_statement': 8, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 10, 'assignment': 5, 'type': 5, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'block': 1, 'pass_statement': 1, 'pass': 1, 'comparison_operator': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6069209784869491,0.6038298957658385,"(tensor([0.9628]), tensor([0.9694]), tensor([0.9661]), tensor([0.9687]))"
"428         self.modelKwArgs = modelKwArgs
429         self.model: Optional[VectorTorchModel] = None
430 
431     def __setstate__(self, state):
432         state[""nnOptimiserParams""] = NNOptimiserParams.fromDictOrInstance(state[""nnOptimiserParams""])
433         s = super()
434         if hasattr(s, '__setstate__'):
435             s.__setstate__(state)
436         else:
437             self.__dict__ = state
438 
439     def _createTorchModel(self) -> VectorTorchModel:
","441         self.modelKwArgs = modelKwArgs
442         self.model: Optional[VectorTorchModel] = None
443 
444     def __setstate__(self, state) -> None:
445         state[""nnOptimiserParams""] = NNOptimiserParams.fromDictOrInstance(state[""nnOptimiserParams""])
446         s = super()
447         if hasattr(s, '__setstate__'):
448             s.__setstate__(state)
449         else:
450             self.__dict__ = state
451 
452     def _createTorchModel(self) -> VectorTorchModel:
","Before: 431
After: 444",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3714,"{'module': 1, 'expression_statement': 6, 'assignment': 5, 'attribute': 5, 'identifier': 24, '.': 5, '=': 5, ':': 4, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'none': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 5, ',': 2, ')': 5, 'block': 3, 'subscript': 2, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'call': 4, 'argument_list': 4, 'if_statement': 1, 'if': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6245275610184033,0.5900771063658014,"(tensor([0.9695]), tensor([0.9735]), tensor([0.9715]), tensor([0.9731]))"
"444             normalisationMode=self.normalisationMode)
445         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
446 
447     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
448         if len(outputs.columns) != 1:
449             raise ValueError(""Expected one output dimension: the class labels"")
450 
451         # transform outputs: for each data point, the new output shall be the index in the list of labels
452         labels: pd.Series = outputs.iloc[:, 0]
453         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
454 
455         self.model = self._createTorchModel()
456 
457         dataSetProvider = self._createDataSetProvider(inputs, outputs)
458         self.model.fit(dataSetProvider, self.nnOptimiserParams)
459 
460     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","457             normalisationMode=self.normalisationMode)
458         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
459 
460     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
461         if len(outputs.columns) != 1:
462             raise ValueError(""Expected one output dimension: the class labels"")
463 
464         # transform outputs: for each data point, the new output shall be the index in the list of labels
465         labels: pd.Series = outputs.iloc[:, 0]
466         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
467 
468         self.model = self._createTorchModel()
469 
470         dataSetProvider = self._createDataSetProvider(inputs, outputs)
471         self.model.fit(dataSetProvider, self.nnOptimiserParams)
472 
473     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","Before: 447
After: 460",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3933,"{'module': 1, 'ERROR': 1, 'identifier': 55, '=': 7, 'attribute': 19, '.': 19, ')': 10, 'return_statement': 1, 'return': 1, 'call': 8, 'argument_list': 8, '(': 9, ',': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 3, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '!=': 1, 'integer': 2, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'comment': 1, 'expression_statement': 5, 'assignment': 4, 'subscript': 1, '[': 2, 'slice': 1, ']': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7596670821959325,0.7409594335562674,"(tensor([0.9793]), tensor([0.9819]), tensor([0.9806]), tensor([0.9816]))"
"467             i += batchSize
468         return np.concatenate(results)
469 
470     def _predictClassProbabilities(self, inputs: pd.DataFrame):
471         y = self._predictOutputsForInputDataFrame(inputs)
472         normalisationConstants = y.sum(axis=1)
473         for i in range(y.shape[0]):
474             y[i,:] /= normalisationConstants[i]
475         return pd.DataFrame(y, columns=self._labels)
476 
477     def __str__(self):
","480             i += batchSize
481         return np.concatenate(results)
482 
483     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
484         y = self._predictOutputsForInputDataFrame(inputs)
485         normalisationConstants = y.sum(axis=1)
486         for i in range(y.shape[0]):
487             y[i,:] /= normalisationConstants[i]
488         return pd.DataFrame(y, columns=self._labels)
489 
490     def __str__(self) -> str:
","Before: 470
After: 483",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,4213,"{'module': 1, 'expression_statement': 4, 'augmented_assignment': 2, 'identifier': 32, '+=': 1, 'return_statement': 2, 'return': 2, 'call': 5, 'attribute': 7, '.': 7, 'argument_list': 5, '(': 6, ')': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, 'typed_parameter': 1, ':': 4, 'type': 1, 'block': 2, 'assignment': 2, '=': 4, 'keyword_argument': 2, 'integer': 2, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 3, '[': 3, ']': 3, 'slice': 1, '/=': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6066686392353728,0.5703056625359635,"(tensor([0.9604]), tensor([0.9730]), tensor([0.9667]), tensor([0.9718]))"
"474             y[i,:] /= normalisationConstants[i]
475         return pd.DataFrame(y, columns=self._labels)
476 
477     def __str__(self):
478         return objectRepr(self, [""model"", ""normalisationMode"", ""nnOptimiserParams""])
","487             y[i,:] /= normalisationConstants[i]
488         return pd.DataFrame(y, columns=self._labels)
489 
490     def __str__(self) -> str:
491         return objectRepr(self, [""model"", ""normalisationMode"", ""nnOptimiserParams""])
","Before: 477
After: 490",fix typos in src/sensai/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,4305,"{'module': 1, 'expression_statement': 1, 'augmented_assignment': 1, 'subscript': 2, 'identifier': 12, '[': 2, ',': 2, 'slice': 1, ':': 2, ']': 2, '/=': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'attribute': 2, '.': 2, 'argument_list': 1, '(': 2, 'keyword_argument': 1, '=': 1, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 22, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7260535973232577,0.6767378236455583,"(tensor([0.9671]), tensor([0.9815]), tensor([0.9742]), tensor([0.9800]))"
"11 from ...torch_base import TorchVectorClassificationModel, VectorTorchModel
12 from ...torch_data import TorchDataSetProviderFromDataUtil, TensorScalerIdentity, TensorScaler, DataUtil
13 from ...torch_opt import NNOptimiserParams
14 from ....util.string import objectRepr
15 
16 log = logging.getLogger(__name__)
17 
18 
19 class LSTNetworkVectorClassificationModel(TorchVectorClassificationModel):
20     """"""
","11 from ...torch_base import TorchVectorClassificationModel, VectorTorchModel
12 from ...torch_data import TorchDataSetProviderFromDataUtil, TensorScalerIdentity, TensorScaler, DataUtil
13 from ...torch_opt import NNOptimiserParams
14 from ....util.string import objectRepr
15 
16 log: logging.Logger = logging.getLogger(__name__)
17 
18 
19 class LSTNetworkVectorClassificationModel(TorchVectorClassificationModel):
20     """"""
","Before: 16
After: 16",fix typo in lstnet_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,128,"{'module': 1, 'import_from_statement': 4, 'from': 4, 'relative_import': 4, 'import_prefix': 4, '.': 15, 'dotted_name': 12, 'identifier': 19, 'import': 4, ',': 4, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 182, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 58, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 182, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 58, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.933847757608669,0.933771538204209,"(tensor([0.9925]), tensor([0.9950]), tensor([0.9938]), tensor([0.9947]))"
"5 from .mlp_modules import MultiLayerPerceptron
6 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel, TorchVectorClassificationModel
7 from ...torch_opt import NNOptimiserParams
8 from .... import NormalisationMode
9 
10 log = logging.getLogger(__name__)
11 
12 
13 class MultiLayerPerceptronTorchModel(VectorTorchModel):
14     def __init__(self, cuda, hiddenDims, hidActivationFunction, outputActivationFunction, pDropout=None):
","6 from .mlp_modules import MultiLayerPerceptron
7 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel, TorchVectorClassificationModel
8 from ...torch_opt import NNOptimiserParams
9 from .... import NormalisationMode
10 
11 log: logging.Logger = logging.getLogger(__name__)
12 
13 
14 class MultiLayerPerceptronTorchModel(VectorTorchModel):
15     def __init__(self, cuda: bool, hiddenDims: Sequence[int], hidActivationFunction: Callable[[torch.Tensor], torch.Tensor],
","Before: 10
After: 11",fix bug in multilayerperceptrontorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,78,"{'module': 1, 'import_from_statement': 4, 'from': 4, 'relative_import': 4, 'import_prefix': 4, '.': 12, 'dotted_name': 9, 'identifier': 15, 'import': 4, ',': 2, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4418770366561978,0.4338682107112055,"(tensor([0.9376]), tensor([0.9588]), tensor([0.9481]), tensor([0.9566]))"
"11 
12 
13 class MultiLayerPerceptronTorchModel(VectorTorchModel):
14     def __init__(self, cuda, hiddenDims, hidActivationFunction, outputActivationFunction, pDropout=None):
15         super().__init__(cuda=cuda)
16         self.hidActivationFunction = hidActivationFunction
17         self.outputActivationFunction = outputActivationFunction
18         self.hiddenDims = hiddenDims
19         self.pDropout = pDropout
20 
21     def __str__(self):
","12 
13 
14 class MultiLayerPerceptronTorchModel(VectorTorchModel):
15     def __init__(self, cuda: bool, hiddenDims: Sequence[int], hidActivationFunction: Callable[[torch.Tensor], torch.Tensor],
16             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]], pDropout: Optional[float] = None) -> None:
17         super().__init__(cuda=cuda)
18         self.hidActivationFunction = hidActivationFunction
19         self.outputActivationFunction = outputActivationFunction
20         self.hiddenDims = hiddenDims
21         self.pDropout = pDropout
22 
23     def __str__(self) -> str:
","Before: 14
After: 15, 16",fix bug in multilayerperceptrontorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,109,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 25, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'default_parameter': 1, '=': 6, 'none': 1, 'expression_statement': 5, 'call': 2, 'attribute': 5, '.': 5, 'keyword_argument': 1, 'assignment': 4}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2631942429494442,0.26861835900225944,"(tensor([0.8835]), tensor([0.9542]), tensor([0.9175]), tensor([0.9466]))"
"18         self.hiddenDims = hiddenDims
19         self.pDropout = pDropout
20 
21     def __str__(self):
22         def name(x):
23             if hasattr(x, ""__name__""):
24                 return x.__name__
25             elif hasattr(x, ""__class__""):
26                 return x.__class__.__name__
27             else:
28                 return str(x)
29 
30         return f""_MLP[hiddenDims={self.hiddenDims}, hidAct={name(self.hidActivationFunction)}, outAct={name(self.outputActivationFunction)}, pDropout={self.pDropout}]""
31 
32     def createTorchModuleForDims(self, inputDim, outputDim):
","20         self.hiddenDims = hiddenDims
21         self.pDropout = pDropout
22 
23     def __str__(self) -> str:
24         def name(x):
25             if hasattr(x, ""__name__""):
26                 return x.__name__
27             elif hasattr(x, ""__class__""):
28                 return x.__class__.__name__
29             else:
30                 return str(x)
31 
32         return f""_MLP[hiddenDims={self.hiddenDims}, hidAct={name(self.hidActivationFunction)}, outAct={name(self.outputActivationFunction)}, pDropout={self.pDropout}]""
33 
34     def createTorchModuleForDims(self, inputDim: int, outputDim: int) -> torch.nn.Module:
","Before: 21
After: 23",fix bug in multilayerperceptrontorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,167,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 9, 'identifier': 31, '.': 9, '=': 2, 'function_definition': 2, 'def': 2, 'parameters': 2, '(': 7, ')': 7, ':': 5, 'block': 5, 'if_statement': 1, 'if': 1, 'call': 5, 'argument_list': 5, ',': 2, 'string': 3, 'string_start': 3, 'string_content': 7, 'string_end': 3, 'return_statement': 4, 'return': 4, 'elif_clause': 1, 'elif': 1, 'else_clause': 1, 'else': 1, 'interpolation': 4, '{': 4, '}': 4}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6490019898483221,0.6379516840287531,"(tensor([0.9587]), tensor([0.9802]), tensor([0.9693]), tensor([0.9780]))"
"29 
30         return f""_MLP[hiddenDims={self.hiddenDims}, hidAct={name(self.hidActivationFunction)}, outAct={name(self.outputActivationFunction)}, pDropout={self.pDropout}]""
31 
32     def createTorchModuleForDims(self, inputDim, outputDim):
33         return MultiLayerPerceptron(inputDim, outputDim, self.hiddenDims,
34             hidActivationFn=self.hidActivationFunction, outputActivationFn=self.outputActivationFunction,
35             pDropout=self.pDropout)
36 
37 
","31 
32         return f""_MLP[hiddenDims={self.hiddenDims}, hidAct={name(self.hidActivationFunction)}, outAct={name(self.outputActivationFunction)}, pDropout={self.pDropout}]""
33 
34     def createTorchModuleForDims(self, inputDim: int, outputDim: int) -> torch.nn.Module:
35         return MultiLayerPerceptron(inputDim, outputDim, self.hiddenDims,
36             hidActivationFn=self.hidActivationFunction, outputActivationFn=self.outputActivationFunction,
37             pDropout=self.pDropout)
38 
39 
","Before: 32
After: 34",fix bug in multilayerperceptrontorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,294,"{'module': 1, 'return_statement': 2, 'return': 2, 'string': 1, 'string_start': 1, 'string_content': 5, 'interpolation': 4, '{': 4, 'attribute': 8, 'identifier': 28, '.': 8, '}': 4, 'call': 3, 'argument_list': 3, '(': 4, ')': 4, 'string_end': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, ':': 1, 'block': 1, 'keyword_argument': 3, '=': 3}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6732367641222303,0.6484410826059971,"(tensor([0.9682]), tensor([0.9841]), tensor([0.9761]), tensor([0.9825]))"
"36 
37 
38 class MultiLayerPerceptronVectorRegressionModel(TorchVectorRegressionModel):
39     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=None,
40             normalisationMode=NormalisationMode .MAX_BY_COLUMN,
41             cuda=True, pDropout: float = None, nnOptimiserParams: NNOptimiserParams = None, **nnOptimiserDictParams):
42         """"""
43         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
44         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
45         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
46         :param normalisationMode: the normalisation mode to apply to input and output data
47         :param cuda: whether to use CUDA (GPU acceleration)
48         :param pDropout: the probability with which to apply dropouts after each hidden layer
49         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
50         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
51         """"""
52         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
53         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
54                 dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
55 
56 
","38 
39 
40 class MultiLayerPerceptronVectorRegressionModel(TorchVectorRegressionModel):
41     def __init__(self, hiddenDims: Sequence[int] = (5, 5), hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
42             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
43             normalisationMode: NormalisationMode = NormalisationMode .MAX_BY_COLUMN,
44             cuda: bool = True, pDropout: Optional[float] = None, nnOptimiserParams: Optional[NNOptimiserParams] = None,
45             **nnOptimiserDictParams) -> None:
46         """"""
47         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
48         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
49         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
50         :param normalisationMode: the normalisation mode to apply to input and output data
51         :param cuda: whether to use CUDA (GPU acceleration)
52         :param pDropout: the probability with which to apply dropouts after each hidden layer
53         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
54         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
55         """"""
56         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
57         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
58                 dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
59 
60 
","Before: 39, 40, 41
After: 41, 42, 43, 44, 45",fix bug in multilayerperceptrontorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,371,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 35, 'argument_list': 5, '(': 7, ')': 7, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 17, 'default_parameter': 5, '=': 9, 'tuple': 1, 'integer': 2, 'attribute': 4, '.': 4, 'none': 3, 'true': 1, 'typed_default_parameter': 2, 'type': 2, 'dictionary_splat_pattern': 1, '**': 1, 'expression_statement': 3, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 4, 'list': 1, '[': 1, ']': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6393281264208588,0.6307069910310328,"(tensor([0.9316]), tensor([0.9499]), tensor([0.9407]), tensor([0.9481]))"
"55 
56 
57 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
58     def __init__(self, hiddenDims=(5, 5), hidActivationFunction=torch.sigmoid, outputActivationFunction=torch.nn.functional.softmax,
59             normalisationMode=NormalisationMode.MAX_BY_COLUMN, cuda=True, pDropout=None, nnOptimiserParams: NNOptimiserParams = None,
60             **nnOptimiserDictParams):
61         """"""
62         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
63         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
64         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
65         :param normalisationMode: the normalisation mode to apply to input and output data
66         :param cuda: whether to use CUDA (GPU acceleration)
67         :param pDropout: the probability with which to apply dropouts after each hidden layer
68         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
69         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
70         """"""
71         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
72         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
73             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","59 
60 
61 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
62     def __init__(self, hiddenDims: Sequence[int] = (5, 5),
63             hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
64             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]] = torch.nn.functional.softmax,
65             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN, cuda: bool = True, pDropout: Optional[float] = None,
66             nnOptimiserParams: Optional[NNOptimiserParams] = None, **nnOptimiserDictParams) -> None:
67         """"""
68         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
69         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
70         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
71         :param normalisationMode: the normalisation mode to apply to input and output data
72         :param cuda: whether to use CUDA (GPU acceleration)
73         :param pDropout: the probability with which to apply dropouts after each hidden layer
74         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
75         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
76         """"""
77         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
78         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
79             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","Before: 58, 59, 60
After: 62, 63, 64, 65, 66",fix bug in multilayerperceptrontorchmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,513,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 33, 'argument_list': 3, '(': 6, ')': 5, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 15, 'default_parameter': 6, '=': 8, 'tuple': 1, 'integer': 2, 'attribute': 7, '.': 7, 'true': 1, 'none': 2, 'typed_default_parameter': 1, 'type': 1, 'dictionary_splat_pattern': 1, '**': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 2, 'ERROR': 1, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims , hidActivationFunction , outputActivationFunction , pDropout = None )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['self', ' cuda', ' hiddenDims', ' hidActivationFunction', ' outputActivationFunction', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6199988381447925,0.6089511025035741,"(tensor([0.9270]), tensor([0.9465]), tensor([0.9366]), tensor([0.9445]))"
"6 
7 
8 class MultiLayerPerceptron(MCDropoutCapableNNModule):
9     def __init__(self, inputDim, outputDim, hiddenDims, hidActivationFn=torch.sigmoid, outputActivationFn=torch.sigmoid,
10             pDropout=None):
11         super().__init__()
12         self.inputDim = inputDim
13         self.outputDim = outputDim
14         self.hiddenDims = hiddenDims
15         self.hidActivationFn = hidActivationFn
16         self.outputActivationFn = outputActivationFn
17         self.pDropout = pDropout
18         self.layers = nn.ModuleList()
19         if pDropout is not None:
20             self.dropout = nn.Dropout(p=pDropout)
21         else:
22             self.dropout = None
23         prevDim = inputDim
24         for dim in [*hiddenDims, outputDim]:
25             self.layers.append(nn.Linear(prevDim, dim))
26             prevDim = dim
27 
28     def __str__(self):
","9 
10 
11 class MultiLayerPerceptron(MCDropoutCapableNNModule):
12     def __init__(self, inputDim: float, outputDim: float, hiddenDims: Sequence[int],
13             hidActivationFn: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
14             outputActivationFn: Optional[Callable[[torch.Tensor], torch.Tensor]] = torch.sigmoid,
15             pDropout: Optional[float] = None):
16         super().__init__()
17         self.inputDim = inputDim
18         self.outputDim = outputDim
19         self.hiddenDims = hiddenDims
20         self.hidActivationFn = hidActivationFn
21         self.outputActivationFn = outputActivationFn
22         self.pDropout = pDropout
23         self.layers = nn.ModuleList()
24         if pDropout is not None:
25             self.dropout = nn.Dropout(p=pDropout)
26         else:
27             self.dropout = None
28         prevDim = inputDim
29         for dim in [*hiddenDims, outputDim]:
30             self.layers.append(nn.Linear(prevDim, dim))
31             prevDim = dim
32 
33     def __str__(self):
","Before: 9, 10
After: 12, 13, 14, 15",fix bug in multilayerperceptron(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_modules.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,77,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 61, 'argument_list': 7, '(': 8, ')': 8, ':': 5, 'block': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'default_parameter': 3, '=': 15, 'attribute': 17, '.': 17, 'none': 3, 'expression_statement': 13, 'call': 6, 'assignment': 11, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'keyword_argument': 1, 'else_clause': 1, 'else': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'list': 1, '[': 1, 'list_splat': 1, '*': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 18, 'token_count': 129, 'name': '__init__', 'long_name': '__init__( self , inputDim , outputDim , hiddenDims , hidActivationFn = torch . sigmoid , outputActivationFn = torch . sigmoid , pDropout = None )', 'start_line': 9, 'end_line': 26, 'full_parameters': ['self', ' inputDim', ' outputDim', ' hiddenDims', ' hidActivationFn = torch . sigmoid', ' outputActivationFn = torch . sigmoid', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 20, 'token_count': 172, 'name': '__init__', 'long_name': '__init__( self , inputDim : float , outputDim : float , hiddenDims : Sequence [ int ] , hidActivationFn : Callable [ [ torch . Tensor ] , torch . Tensor ] = torch . sigmoid , outputActivationFn : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] = torch . sigmoid , pDropout : Optional [ float ] = None )', 'start_line': 12, 'end_line': 31, 'full_parameters': ['self', ' inputDim : float', ' outputDim : float', ' hiddenDims : Sequence [ int ]', ' hidActivationFn : Callable [ [ torch . Tensor ]', ' torch . Tensor ] = torch . sigmoid', ' outputActivationFn : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ] = torch . sigmoid', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.37537795049750344,0.3780956668918516,"(tensor([0.9146]), tensor([0.9506]), tensor([0.9323]), tensor([0.9469]))"
"25             self.layers.append(nn.Linear(prevDim, dim))
26             prevDim = dim
27 
28     def __str__(self):
29         return objectRepr(self, dict(inputDim=self.inputDim, outputDim=self.outputDim, hiddenDims=self.hiddenDims,
30             hidActivationFn=self.hidActivationFn.__name__ if self.hidActivationFn is not None else None, outputActivationFn=self.outputActivationFn.__name__ if self.outputActivationFn is not None else None,
31             pDropout=self.pDropout))
32 
33     def forward(self, x):
","30             self.layers.append(nn.Linear(prevDim, dim))
31             prevDim = dim
32 
33     def __str__(self):
34         return objectRepr(self, dict(inputDim=self.inputDim, outputDim=self.outputDim, hiddenDims=self.hiddenDims,
35             hidActivationFn=self.hidActivationFn.__name__ if self.hidActivationFn is not None else None,
36             outputActivationFn=self.outputActivationFn.__name__ if self.outputActivationFn is not None else None,
37             pDropout=self.pDropout))
38 
39     def forward(self, x):
","Before: 30
After: 35, 36",fix bug in multilayerperceptron(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_modules.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,335,"{'module': 1, 'expression_statement': 2, 'call': 4, 'attribute': 13, 'identifier': 38, '.': 13, 'argument_list': 4, '(': 5, ',': 7, ')': 5, 'assignment': 1, '=': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'keyword_argument': 6, 'conditional_expression': 2, 'if': 2, 'comparison_operator': 2, 'is not': 4, 'none': 4, 'else': 2}","{'cyclomatic_complexity': 3, 'nloc': 18, 'token_count': 129, 'name': '__init__', 'long_name': '__init__( self , inputDim , outputDim , hiddenDims , hidActivationFn = torch . sigmoid , outputActivationFn = torch . sigmoid , pDropout = None )', 'start_line': 9, 'end_line': 26, 'full_parameters': ['self', ' inputDim', ' outputDim', ' hiddenDims', ' hidActivationFn = torch . sigmoid', ' outputActivationFn = torch . sigmoid', ' pDropout = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 20, 'token_count': 172, 'name': '__init__', 'long_name': '__init__( self , inputDim : float , outputDim : float , hiddenDims : Sequence [ int ] , hidActivationFn : Callable [ [ torch . Tensor ] , torch . Tensor ] = torch . sigmoid , outputActivationFn : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] = torch . sigmoid , pDropout : Optional [ float ] = None )', 'start_line': 12, 'end_line': 31, 'full_parameters': ['self', ' inputDim : float', ' outputDim : float', ' hiddenDims : Sequence [ int ]', ' hidActivationFn : Callable [ [ torch . Tensor ]', ' torch . Tensor ] = torch . sigmoid', ' outputActivationFn : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ] = torch . sigmoid', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7102144031492882,0.6907515582592882,"(tensor([0.9834]), tensor([0.9851]), tensor([0.9842]), tensor([0.9849]))"
"1 import logging
2 from typing import Sequence, Union
3 
4 import torch
5 
6 from .residualffn_modules import ResidualFeedForwardNetwork
","1 import logging
2 from typing import Sequence, Union, Optional
3 
4 import torch
5 
6 from .residualffn_modules import ResidualFeedForwardNetwork
","Before: 2
After: 2",fix typos in src/src/torch/torch_models/residualffn_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,16,"{'module': 1, 'import_statement': 2, 'import': 3, 'dotted_name': 5, 'identifier': 5, 'import_from_statement': 1, 'from': 1, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 60, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 64, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8232490471721702,0.8112275861486086,"(tensor([0.9911]), tensor([0.9961]), tensor([0.9936]), tensor([0.9956]))"
"6 from .residualffn_modules import ResidualFeedForwardNetwork
7 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel
8 from ...torch_opt import NNOptimiserParams
9 from ....normalisation import NormalisationMode
10 
11 log = logging.getLogger(__name__)
12 
13 
14 class ResidualFeedForwardNetworkTorchModel(VectorTorchModel):
15 
","6 from .residualffn_modules import ResidualFeedForwardNetwork
7 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel
8 from ...torch_opt import NNOptimiserParams
9 from ....normalisation import NormalisationMode
10 
11 log: logging.Logger = logging.getLogger(__name__)
12 
13 
14 class ResidualFeedForwardNetworkTorchModel(VectorTorchModel):
15 
","Before: 11
After: 11",fix typos in src/src/torch/torch_models/residualffn_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,83,"{'module': 1, 'import_from_statement': 4, 'from': 4, 'relative_import': 4, 'import_prefix': 4, '.': 12, 'dotted_name': 9, 'identifier': 15, 'import': 4, ',': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, 'argument_list': 2, '(': 2, ')': 2, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 60, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 64, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9184043388013005,0.9186684121785716,"(tensor([0.9912]), tensor([0.9943]), tensor([0.9928]), tensor([0.9940]))"
"13 
14 class ResidualFeedForwardNetworkTorchModel(VectorTorchModel):
15 
16     def __init__(self, cuda, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1, pDropout=None,
17             useBatchNormalisation: bool = False):
18         super().__init__(cuda=cuda)
19         self.hiddenDims = hiddenDims
20         self.bottleneckDimensionFactor = bottleneckDimensionFactor
21         self.pDropout = pDropout
22         self.useBatchNormalisation = useBatchNormalisation
23 
24     def createTorchModuleForDims(self, inputDim, outputDim) -> torch.nn.Module:
","13 
14 class ResidualFeedForwardNetworkTorchModel(VectorTorchModel):
15 
16     def __init__(self, cuda: bool, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1, pDropout=None,
17             useBatchNormalisation: bool = False) -> None:
18         super().__init__(cuda=cuda)
19         self.hiddenDims = hiddenDims
20         self.bottleneckDimensionFactor = bottleneckDimensionFactor
21         self.pDropout = pDropout
22         self.useBatchNormalisation = useBatchNormalisation
23 
24     def createTorchModuleForDims(self, inputDim: int, outputDim: int) -> torch.nn.Module:
","Before: 16, 17
After: 16, 17",fix typos in src/src/torch/torch_models/residualffn_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,125,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 29, 'argument_list': 3, '(': 4, ')': 4, ':': 5, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'typed_parameter': 1, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'typed_default_parameter': 2, '=': 8, 'integer': 1, 'default_parameter': 1, 'none': 1, 'false': 1, 'expression_statement': 5, 'call': 2, 'attribute': 5, '.': 5, 'keyword_argument': 1, 'assignment': 4}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 60, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 64, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8273237449741192,0.8303902095409667,"(tensor([0.9854]), tensor([0.9930]), tensor([0.9892]), tensor([0.9923]))"
"21         self.pDropout = pDropout
22         self.useBatchNormalisation = useBatchNormalisation
23 
24     def createTorchModuleForDims(self, inputDim, outputDim) -> torch.nn.Module:
25         return ResidualFeedForwardNetwork(inputDim, outputDim, self.hiddenDims, self.bottleneckDimensionFactor,
26             pDropout=self.pDropout, useBatchNormalisation=self.useBatchNormalisation)
27 
28 
","21         self.pDropout = pDropout
22         self.useBatchNormalisation = useBatchNormalisation
23 
24     def createTorchModuleForDims(self, inputDim: int, outputDim: int) -> torch.nn.Module:
25         return ResidualFeedForwardNetwork(inputDim, outputDim, self.hiddenDims, self.bottleneckDimensionFactor,
26             pDropout=self.pDropout, useBatchNormalisation=self.useBatchNormalisation)
27 
28 
","Before: 24
After: 24",fix typos in src/src/torch/torch_models/residualffn_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,207,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 8, 'identifier': 26, '.': 8, '=': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 7, ')': 2, '->': 1, 'type': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 60, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 64, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8458648134706074,0.8447443361325576,"(tensor([0.9879]), tensor([0.9958]), tensor([0.9919]), tensor([0.9950]))"
"28 
29 class ResidualFeedForwardNetworkVectorRegressionModel(TorchVectorRegressionModel):
30 
31     def __init__(self, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1, cuda=True, pDropout=None,
32             useBatchNormalisation: bool = False, normalisationMode=NormalisationMode.NONE,
33             nnOptimiserParams: Union[NNOptimiserParams, dict] = None):
34         super().__init__(ResidualFeedForwardNetworkTorchModel, [cuda, hiddenDims],
35             dict(bottleneckDimensionFactor=bottleneckDimensionFactor, pDropout=pDropout, useBatchNormalisation=useBatchNormalisation),
36             normalisationMode=normalisationMode, nnOptimiserParams=nnOptimiserParams)
","28 
29 class ResidualFeedForwardNetworkVectorRegressionModel(TorchVectorRegressionModel):
30 
31     def __init__(self, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1, cuda: bool = True, pDropout: Optional[float] = None,
32             useBatchNormalisation: bool = False, normalisationMode: NormalisationMode = NormalisationMode.NONE,
33             nnOptimiserParams: Union[NNOptimiserParams, dict, None] = None) -> None:
34         super().__init__(ResidualFeedForwardNetworkTorchModel, [cuda, hiddenDims],
35             dict(bottleneckDimensionFactor=bottleneckDimensionFactor, pDropout=pDropout, useBatchNormalisation=useBatchNormalisation),
36             normalisationMode=normalisationMode, nnOptimiserParams=nnOptimiserParams)
","Before: 31, 32, 33
After: 31, 32, 33",fix typos in src/src/torch/torch_models/residualffn_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_models.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,288,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 32, 'argument_list': 3, '(': 5, ')': 4, ':': 6, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 14, 'typed_parameter': 1, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'typed_default_parameter': 3, '=': 9, 'integer': 1, 'default_parameter': 3, 'true': 1, 'none': 2, 'false': 1, 'attribute': 2, '.': 2, 'ERROR': 1, 'call': 2, 'list': 1, 'keyword_argument': 3}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 60, 'name': '__init__', 'long_name': '__init__( self , cuda , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 64, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout = None , useBatchNormalisation : bool = False )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout = None', ' useBatchNormalisation : bool = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7045275820786797,0.7009101722269687,"(tensor([0.9817]), tensor([0.9902]), tensor([0.9859]), tensor([0.9894]))"
"1 from abc import ABC, abstractmethod
2 from typing import Sequence
3 
4 import torch
5 from torch import nn
6 
","1 from abc import ABC, abstractmethod
2 from typing import Sequence, Optional
3 
4 import torch
5 from torch import nn
6 
","Before: 2
After: 2",fix residualffn_modules.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,19,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 8, 'identifier': 8, 'import': 4, ',': 1, 'import_statement': 1}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.839587623092576,0.8256129008997986,"(tensor([0.9887]), tensor([0.9949]), tensor([0.9918]), tensor([0.9943]))"
"43         Internal Covariate Shift.â ArXiv:1502.03167 [Cs], March 2, 2015. http://arxiv.org/abs/1502.03167.
44     """"""
45 
46     def __init__(self, inputDim: int, outputDim: int, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1,
47             pDropout: float = None, useBatchNormalisation: bool = True):
48         """"""
49         :param inputDim: the input dimension of the model
50         :param outputDim: the output dimension of the model
51         :param hiddenDims: a list of dimensions; for each list item, a residual block with the corresponding dimension
52                 is created
53         :param bottleneckDimensionFactor: an optional factor that specifies the hidden dimension within each block
54         :param pDropout: the dropout probability to use during training (defaults to None for no dropout)
55         :param useBatchNormalisation: whether to use batch normalisation (defaults to True)
56         """"""
57         super().__init__()
58         self.inputDim = inputDim
59         self.outputDim = outputDim
60         self.hiddenDims = hiddenDims
61         self.useBatchNormalisation = useBatchNormalisation
62 
63         if pDropout is not None:
64             self.dropout = nn.Dropout(p=pDropout)
65 
66         self.inputLayer = nn.Linear(self.inputDim, self.hiddenDims[0])
67 
68         innerHiddenDims = lambda x: max(1, round(x * bottleneckDimensionFactor))
69         blocks = []
70         prevDim = self.hiddenDims[0]
71         for hiddenDim in self.hiddenDims[1:]:
72             if hiddenDim == prevDim:
73                 block = self._IdentityBlock(hiddenDim, innerHiddenDims(hiddenDim), self.dropout, useBatchNormalisation)
74             else:
75                 block = self._DenseBlock(prevDim, innerHiddenDims(hiddenDim), hiddenDim, self.dropout, useBatchNormalisation)
76             blocks.append(block)
77             prevDim = hiddenDim
78 
79         self.bnOutput = nn.BatchNorm1d(self.hiddenDims[-1]) if self.useBatchNormalisation else None
80         self.outputLayer = nn.Linear(self.hiddenDims[-1], outputDim)
81         self.blocks = nn.ModuleList(blocks)
82 
83     def forward(self, x):
","43         Internal Covariate Shift.â ArXiv:1502.03167 [Cs], March 2, 2015. http://arxiv.org/abs/1502.03167.
44     """"""
45 
46     def __init__(self, inputDim: int, outputDim: int, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1,
47             pDropout: Optional[float] = None, useBatchNormalisation: bool = True) -> None:
48         """"""
49         :param inputDim: the input dimension of the model
50         :param outputDim: the output dimension of the model
51         :param hiddenDims: a list of dimensions; for each list item, a residual block with the corresponding dimension
52                 is created
53         :param bottleneckDimensionFactor: an optional factor that specifies the hidden dimension within each block
54         :param pDropout: the dropout probability to use during training (defaults to None for no dropout)
55         :param useBatchNormalisation: whether to use batch normalisation (defaults to True)
56         """"""
57         super().__init__()
58         self.inputDim = inputDim
59         self.outputDim = outputDim
60         self.hiddenDims = hiddenDims
61         self.useBatchNormalisation = useBatchNormalisation
62 
63         if pDropout is not None:
64             self.dropout = nn.Dropout(p=pDropout)
65 
66         self.inputLayer = nn.Linear(self.inputDim, self.hiddenDims[0])
67 
68         innerHiddenDims = lambda x: max(1, round(x * bottleneckDimensionFactor))
69         blocks = []
70         prevDim = self.hiddenDims[0]
71         for hiddenDim in self.hiddenDims[1:]:
72             if hiddenDim == prevDim:
73                 block = self._IdentityBlock(hiddenDim, innerHiddenDims(hiddenDim), self.dropout, useBatchNormalisation)
74             else:
75                 block = self._DenseBlock(prevDim, innerHiddenDims(hiddenDim), hiddenDim, self.dropout, useBatchNormalisation)
76             blocks.append(block)
77             prevDim = hiddenDim
78 
79         self.bnOutput = nn.BatchNorm1d(self.hiddenDims[-1]) if self.useBatchNormalisation else None
80         self.outputLayer = nn.Linear(self.hiddenDims[-1], outputDim)
81         self.blocks = nn.ModuleList(blocks)
82 
83     def forward(self, x):
","Before: 47
After: 47",fix residualffn_modules.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,104,"{'module': 1, 'expression_statement': 11, 'assignment': 7, 'pattern_list': 1, 'attribute': 2, 'identifier': 81, 'ERROR': 25, '.': 3, ':': 14, 'subscript': 1, 'float': 3, '[': 1, ']': 1, ',': 3, 'integer': 1, '//': 1, 'type': 7, 'binary_operator': 2, '/': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ';': 1, 'call': 2, 'argument_list': 2, '(': 2, 'none': 1, 'for': 1, ')': 2, 'true': 1}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9729048454471695,0.9725676559936927,"(tensor([0.9889]), tensor([0.9914]), tensor([0.9901]), tensor([0.9911]))"
"97         A generic residual block which need to be specified by defining the skip path.
98         """"""
99 
100         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: nn.Dropout, useBatchNormalisation: bool):
101             super().__init__()
102             self.inputDim = inputDim
103             self.hiddenDim = hiddenDim
104             self.outputDim = outputDim
105             self.dropout = dropout
106             self.useBatchNormalisation = useBatchNormalisation
107             self.bnIn = nn.BatchNorm1d(self.inputDim) if useBatchNormalisation else None
108             self.denseIn = nn.Linear(self.inputDim, self.hiddenDim)
109             self.bnOut = nn.BatchNorm1d(self.hiddenDim) if useBatchNormalisation else None
110             self.denseOut = nn.Linear(self.hiddenDim, self.outputDim)
111 
112         def forward(self, x):
","97         A generic residual block which need to be specified by defining the skip path.
98         """"""
99 
100         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: nn.Dropout, useBatchNormalisation: bool) -> None:
101             super().__init__()
102             self.inputDim = inputDim
103             self.hiddenDim = hiddenDim
104             self.outputDim = outputDim
105             self.dropout = dropout
106             self.useBatchNormalisation = useBatchNormalisation
107             self.bnIn = nn.BatchNorm1d(self.inputDim) if useBatchNormalisation else None
108             self.denseIn = nn.Linear(self.inputDim, self.hiddenDim)
109             self.bnOut = nn.BatchNorm1d(self.hiddenDim) if useBatchNormalisation else None
110             self.denseOut = nn.Linear(self.hiddenDim, self.outputDim)
111 
112         def forward(self, x):
","Before: 100
After: 100",fix residualffn_modules.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,596,"{'module': 1, 'ERROR': 1, 'identifier': 14, '.': 1, 'string_start': 1}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9643025203494107,0.9598635297529062,"(tensor([0.9927]), tensor([0.9977]), tensor([0.9952]), tensor([0.9972]))"
"136         A residual block preserving the dimension of the input
137         """"""
138 
139         def __init__(self, inputOutputDim: int, hiddenDim: int, dropout: nn.Dropout, useBatchNormalisation: bool):
140             super().__init__(inputOutputDim, hiddenDim, inputOutputDim, dropout, useBatchNormalisation)
141 
142         def _skip(self, x):
","136         A residual block preserving the dimension of the input
137         """"""
138 
139         def __init__(self, inputOutputDim: int, hiddenDim: int, dropout: nn.Dropout, useBatchNormalisation: bool) -> None:
140             super().__init__(inputOutputDim, hiddenDim, inputOutputDim, dropout, useBatchNormalisation)
141 
142         def _skip(self, x):
","Before: 139
After: 139",fix residualffn_modules.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,991,"{'module': 1, 'ERROR': 2, 'identifier': 27, 'expression_statement': 2, 'string_start': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 8, 'typed_parameter': 4, ':': 5, 'type': 4, 'attribute': 2, '.': 2, ')': 3, 'block': 1, 'call': 2, 'argument_list': 2}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9308071478073938,0.9178728019360205,"(tensor([0.9840]), tensor([0.9970]), tensor([0.9905]), tensor([0.9957]))"
"150         A residual block changing the dimension of the input to the given value.
151         """"""
152 
153         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: nn.Dropout, useBatchNormalisation: bool):
154             super().__init__(inputDim, hiddenDim, outputDim, dropout, useBatchNormalisation)
155             self.denseSkip = nn.Linear(self.inputDim, self.outputDim)
156 
157         def _skip(self, x):
","150         A residual block changing the dimension of the input to the given value.
151         """"""
152 
153         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: nn.Dropout, useBatchNormalisation: bool) -> None:
154             super().__init__(inputDim, hiddenDim, outputDim, dropout, useBatchNormalisation)
155             self.denseSkip = nn.Linear(self.inputDim, self.outputDim)
156 
157         def _skip(self, x):
","Before: 153
After: 153",fix residualffn_modules.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,1088,"{'module': 1, 'ERROR': 1, 'identifier': 13, '.': 1, 'string_start': 1}","{'cyclomatic_complexity': 5, 'nloc': 23, 'token_count': 248, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : float = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : float = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9458134147569868,0.9379236163019692,"(tensor([0.9876]), tensor([0.9976]), tensor([0.9926]), tensor([0.9966]))"
"326             return ""CE""
327         else:
328             raise AssertionError(f""No selection criterion defined for loss function {self.lossFn}"")
329 
330 
331 class NNOptimiserParams:
332     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserClip=10., optimiserLR=0.001,
333             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
334             useShrinkage=True, **optimiserArgs):
335         """"""
","328             return ""CE""
329         else:
330             raise AssertionError(f""No selection criterion defined for loss function {self.lossFn}"")
331 
332 
333 class NNOptimiserParams(ToStringMixin):
334     REMOVED_PARAMS = {""cuda""}
335     RENAMED_PARAMS = {
336         ""optimiserClip"": ""shrinkageClip""
337     }
","Before: 331, 332
After: 333, 334, 335, 336, 337, 338, 339",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,2943,"{'module': 1, 'return_statement': 1, 'return': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'expression_statement': 1, 'assignment': 1, 'identifier': 20, ':': 4, 'ERROR': 1, 'type': 2, 'call': 1, 'argument_list': 1, '(': 2, 'interpolation': 1, '{': 1, 'attribute': 1, '.': 1, '}': 1, ')': 2, 'class_definition': 1, 'class': 1, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 11, 'typed_default_parameter': 1, '=': 10, 'none': 3, 'default_parameter': 9, 'float': 3, 'integer': 1, 'false': 1, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.28450072510346647,0.2849911045014766,"(tensor([0.8608]), tensor([0.8068]), tensor([0.8329]), tensor([0.8119]))"
"329 
330 
331 class NNOptimiserParams:
332     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserClip=10., optimiserLR=0.001,
333             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
334             useShrinkage=True, **optimiserArgs):
335         """"""
336         :param lossEvaluator: the loss evaluator to use
337         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained)
338         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
339         :param optimiserClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
340         :param optimiserLR: the optimiser's learning rate
341         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
342             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
343         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
344             If no validation is to be performed, pass 1.0.
345         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
346             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
347         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
348         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
349         """"""
350         if optimiser == 'lbfgs':
351             largeBatchSize = 1e12
352             if batchSize is not None:
353                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
354             batchSize = largeBatchSize
355         else:
356             if batchSize is None:
357                 log.debug(""No batch size was specified, using batch size 64 by default"")
358                 batchSize = 64
359 
360         self.epochs = epochs
361         self.batchSize = batchSize
362         self.optimiserLR = optimiserLR
363         self.optimiserClip = optimiserClip
364         self.optimiser = optimiser
365         self.gpu = gpu
366         self.trainFraction = trainFraction
367         self.scaledOutputs = scaledOutputs
368         self.lossEvaluator = lossEvaluator
369         self.optimiserArgs = optimiserArgs
370         self.useShrinkage = useShrinkage
371 
372     def __str__(self):
","336         ""optimiserClip"": ""shrinkageClip""
337     }
338 
339     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
340             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
341             useShrinkage=True, shrinkageClip=10., **optimiserArgs):
342         """"""
343         :param lossEvaluator: the loss evaluator to use
344         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
345         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
346         :param optimiserLR: the optimiser's learning rate
347         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
348             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
349         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
350             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
351         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
352             If no validation is to be performed, pass 1.0.
353         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
354             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
355         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
356         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
357         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
358         """"""
359         if optimiser == 'lbfgs':
360             largeBatchSize = 1e12
361             if batchSize is not None:
362                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
363             batchSize = largeBatchSize
364         else:
365             if batchSize is None:
366                 log.debug(""No batch size was specified, using batch size 64 by default"")
367                 batchSize = 64
368 
369         self.epochs = epochs
370         self.batchSize = batchSize
371         self.optimiserLR = optimiserLR
372         self.shrinkageClip = shrinkageClip
373         self.optimiser = optimiser
374         self.gpu = gpu
375         self.trainFraction = trainFraction
376         self.scaledOutputs = scaledOutputs
377         self.lossEvaluator = lossEvaluator
378         self.optimiserArgs = optimiserArgs
379         self.useShrinkage = useShrinkage
380         self.earlyStoppingEpochs = earlyStoppingEpochs
381 
382     @classmethod
","Before: 334
After: 341",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3012,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 60, ':': 7, 'block': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 11, 'typed_default_parameter': 1, 'type': 1, '=': 24, 'none': 5, 'default_parameter': 9, 'string': 5, 'string_start': 5, 'string_content': 6, 'string_end': 5, 'float': 4, 'integer': 2, 'false': 1, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 3, 'expression_statement': 17, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '==': 1, 'assignment': 14, 'is not': 2, 'call': 2, 'attribute': 13, '.': 13, 'argument_list': 2, 'interpolation': 1, '{': 1, '}': 1, 'else_clause': 1, 'else': 1, 'is': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6661285344921689,0.6605234862195797,"(tensor([0.9138]), tensor([0.9333]), tensor([0.9234]), tensor([0.9313]))"
"329 
330 
331 class NNOptimiserParams:
332     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserClip=10., optimiserLR=0.001,
333             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
334             useShrinkage=True, **optimiserArgs):
335         """"""
336         :param lossEvaluator: the loss evaluator to use
337         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained)
338         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
339         :param optimiserClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
340         :param optimiserLR: the optimiser's learning rate
341         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
342             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
343         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
344             If no validation is to be performed, pass 1.0.
345         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
346             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
347         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
348         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
349         """"""
350         if optimiser == 'lbfgs':
351             largeBatchSize = 1e12
352             if batchSize is not None:
353                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
354             batchSize = largeBatchSize
355         else:
356             if batchSize is None:
357                 log.debug(""No batch size was specified, using batch size 64 by default"")
358                 batchSize = 64
359 
360         self.epochs = epochs
361         self.batchSize = batchSize
362         self.optimiserLR = optimiserLR
363         self.optimiserClip = optimiserClip
364         self.optimiser = optimiser
365         self.gpu = gpu
366         self.trainFraction = trainFraction
367         self.scaledOutputs = scaledOutputs
368         self.lossEvaluator = lossEvaluator
369         self.optimiserArgs = optimiserArgs
370         self.useShrinkage = useShrinkage
371 
372     def __str__(self):
","336         ""optimiserClip"": ""shrinkageClip""
337     }
338 
339     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
340             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
341             useShrinkage=True, shrinkageClip=10., **optimiserArgs):
342         """"""
343         :param lossEvaluator: the loss evaluator to use
344         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
345         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
346         :param optimiserLR: the optimiser's learning rate
347         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
348             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
349         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
350             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
351         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
352             If no validation is to be performed, pass 1.0.
353         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
354             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
355         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
356         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
357         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
358         """"""
359         if optimiser == 'lbfgs':
360             largeBatchSize = 1e12
361             if batchSize is not None:
362                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
363             batchSize = largeBatchSize
364         else:
365             if batchSize is None:
366                 log.debug(""No batch size was specified, using batch size 64 by default"")
367                 batchSize = 64
368 
369         self.epochs = epochs
370         self.batchSize = batchSize
371         self.optimiserLR = optimiserLR
372         self.shrinkageClip = shrinkageClip
373         self.optimiser = optimiser
374         self.gpu = gpu
375         self.trainFraction = trainFraction
376         self.scaledOutputs = scaledOutputs
377         self.lossEvaluator = lossEvaluator
378         self.optimiserArgs = optimiserArgs
379         self.useShrinkage = useShrinkage
380         self.earlyStoppingEpochs = earlyStoppingEpochs
381 
382     @classmethod
","Before: 337
After: 344",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3062,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 60, ':': 7, 'block': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 11, 'typed_default_parameter': 1, 'type': 1, '=': 24, 'none': 5, 'default_parameter': 9, 'string': 5, 'string_start': 5, 'string_content': 6, 'string_end': 5, 'float': 4, 'integer': 2, 'false': 1, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 3, 'expression_statement': 17, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '==': 1, 'assignment': 14, 'is not': 2, 'call': 2, 'attribute': 13, '.': 13, 'argument_list': 2, 'interpolation': 1, '{': 1, '}': 1, 'else_clause': 1, 'else': 1, 'is': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6661285344921689,0.6605234862195797,"(tensor([0.9138]), tensor([0.9333]), tensor([0.9234]), tensor([0.9313]))"
"329 
330 
331 class NNOptimiserParams:
332     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserClip=10., optimiserLR=0.001,
333             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
334             useShrinkage=True, **optimiserArgs):
335         """"""
336         :param lossEvaluator: the loss evaluator to use
337         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained)
338         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
339         :param optimiserClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
340         :param optimiserLR: the optimiser's learning rate
341         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
342             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
343         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
344             If no validation is to be performed, pass 1.0.
345         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
346             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
347         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
348         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
349         """"""
350         if optimiser == 'lbfgs':
351             largeBatchSize = 1e12
352             if batchSize is not None:
353                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
354             batchSize = largeBatchSize
355         else:
356             if batchSize is None:
357                 log.debug(""No batch size was specified, using batch size 64 by default"")
358                 batchSize = 64
359 
360         self.epochs = epochs
361         self.batchSize = batchSize
362         self.optimiserLR = optimiserLR
363         self.optimiserClip = optimiserClip
364         self.optimiser = optimiser
365         self.gpu = gpu
366         self.trainFraction = trainFraction
367         self.scaledOutputs = scaledOutputs
368         self.lossEvaluator = lossEvaluator
369         self.optimiserArgs = optimiserArgs
370         self.useShrinkage = useShrinkage
371 
372     def __str__(self):
","336         ""optimiserClip"": ""shrinkageClip""
337     }
338 
339     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
340             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
341             useShrinkage=True, shrinkageClip=10., **optimiserArgs):
342         """"""
343         :param lossEvaluator: the loss evaluator to use
344         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
345         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
346         :param optimiserLR: the optimiser's learning rate
347         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
348             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
349         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
350             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
351         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
352             If no validation is to be performed, pass 1.0.
353         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
354             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
355         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
356         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
357         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
358         """"""
359         if optimiser == 'lbfgs':
360             largeBatchSize = 1e12
361             if batchSize is not None:
362                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
363             batchSize = largeBatchSize
364         else:
365             if batchSize is None:
366                 log.debug(""No batch size was specified, using batch size 64 by default"")
367                 batchSize = 64
368 
369         self.epochs = epochs
370         self.batchSize = batchSize
371         self.optimiserLR = optimiserLR
372         self.shrinkageClip = shrinkageClip
373         self.optimiser = optimiser
374         self.gpu = gpu
375         self.trainFraction = trainFraction
376         self.scaledOutputs = scaledOutputs
377         self.lossEvaluator = lossEvaluator
378         self.optimiserArgs = optimiserArgs
379         self.useShrinkage = useShrinkage
380         self.earlyStoppingEpochs = earlyStoppingEpochs
381 
382     @classmethod
","Before: 339
After: 347, 348",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3041,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 60, ':': 7, 'block': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 11, 'typed_default_parameter': 1, 'type': 1, '=': 24, 'none': 5, 'default_parameter': 9, 'string': 5, 'string_start': 5, 'string_content': 6, 'string_end': 5, 'float': 4, 'integer': 2, 'false': 1, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 3, 'expression_statement': 17, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '==': 1, 'assignment': 14, 'is not': 2, 'call': 2, 'attribute': 13, '.': 13, 'argument_list': 2, 'interpolation': 1, '{': 1, '}': 1, 'else_clause': 1, 'else': 1, 'is': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6661285344921689,0.6605234862195797,"(tensor([0.9138]), tensor([0.9333]), tensor([0.9234]), tensor([0.9313]))"
"329 
330 
331 class NNOptimiserParams:
332     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserClip=10., optimiserLR=0.001,
333             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
334             useShrinkage=True, **optimiserArgs):
335         """"""
336         :param lossEvaluator: the loss evaluator to use
337         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained)
338         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
339         :param optimiserClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
340         :param optimiserLR: the optimiser's learning rate
341         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
342             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
343         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
344             If no validation is to be performed, pass 1.0.
345         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
346             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
347         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
348         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
349         """"""
350         if optimiser == 'lbfgs':
351             largeBatchSize = 1e12
352             if batchSize is not None:
353                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
354             batchSize = largeBatchSize
355         else:
356             if batchSize is None:
357                 log.debug(""No batch size was specified, using batch size 64 by default"")
358                 batchSize = 64
359 
360         self.epochs = epochs
361         self.batchSize = batchSize
362         self.optimiserLR = optimiserLR
363         self.optimiserClip = optimiserClip
364         self.optimiser = optimiser
365         self.gpu = gpu
366         self.trainFraction = trainFraction
367         self.scaledOutputs = scaledOutputs
368         self.lossEvaluator = lossEvaluator
369         self.optimiserArgs = optimiserArgs
370         self.useShrinkage = useShrinkage
371 
372     def __str__(self):
","336         ""optimiserClip"": ""shrinkageClip""
337     }
338 
339     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
340             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
341             useShrinkage=True, shrinkageClip=10., **optimiserArgs):
342         """"""
343         :param lossEvaluator: the loss evaluator to use
344         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
345         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
346         :param optimiserLR: the optimiser's learning rate
347         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
348             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
349         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
350             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
351         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
352             If no validation is to be performed, pass 1.0.
353         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
354             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
355         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
356         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
357         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
358         """"""
359         if optimiser == 'lbfgs':
360             largeBatchSize = 1e12
361             if batchSize is not None:
362                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
363             batchSize = largeBatchSize
364         else:
365             if batchSize is None:
366                 log.debug(""No batch size was specified, using batch size 64 by default"")
367                 batchSize = 64
368 
369         self.epochs = epochs
370         self.batchSize = batchSize
371         self.optimiserLR = optimiserLR
372         self.shrinkageClip = shrinkageClip
373         self.optimiser = optimiser
374         self.gpu = gpu
375         self.trainFraction = trainFraction
376         self.scaledOutputs = scaledOutputs
377         self.lossEvaluator = lossEvaluator
378         self.optimiserArgs = optimiserArgs
379         self.useShrinkage = useShrinkage
380         self.earlyStoppingEpochs = earlyStoppingEpochs
381 
382     @classmethod
","Before: 363
After: 372",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3127,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 60, ':': 7, 'block': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 11, 'typed_default_parameter': 1, 'type': 1, '=': 24, 'none': 5, 'default_parameter': 9, 'string': 5, 'string_start': 5, 'string_content': 6, 'string_end': 5, 'float': 4, 'integer': 2, 'false': 1, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 3, 'expression_statement': 17, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '==': 1, 'assignment': 14, 'is not': 2, 'call': 2, 'attribute': 13, '.': 13, 'argument_list': 2, 'interpolation': 1, '{': 1, '}': 1, 'else_clause': 1, 'else': 1, 'is': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6661285344921689,0.6605234862195797,"(tensor([0.9138]), tensor([0.9333]), tensor([0.9234]), tensor([0.9313]))"
"369         self.optimiserArgs = optimiserArgs
370         self.useShrinkage = useShrinkage
371 
372     def __str__(self):
373         return f""{self.__class__.__name__}[optimiser={self.optimiser}, lossEvaluator={self.lossEvaluator}, epochs={self.epochs}, "" \
374                f""batchSize={self.batchSize}, LR={self.optimiserLR}, clip={self.optimiserClip}, gpu={self.gpu}, useShrinkage={self.useShrinkage}, "" \
375                f""optimiserArgs={self.optimiserArgs}]""
376 
377     @classmethod
","377         self.lossEvaluator = lossEvaluator
378         self.optimiserArgs = optimiserArgs
379         self.useShrinkage = useShrinkage
380         self.earlyStoppingEpochs = earlyStoppingEpochs
381 
382     @classmethod
383     def _updatedParams(cls, params: dict) -> dict:
384         return {cls.RENAMED_PARAMS.get(k, k): v for k, v in params.items() if k not in cls.REMOVED_PARAMS}
385 
386     def __setstate__(self, state):
","Before: 372, 373, 374, 375
After: 382, 383, 384, 385, 386, 387",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3192,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 13, 'identifier': 29, '.': 13, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ')': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'concatenated_string': 1, 'string': 3, 'string_start': 3, 'interpolation': 10, '{': 10, '}': 10, 'string_content': 12, 'string_end': 3}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",1.7386162784336124e-78,1.6454754283576622e-78,"(tensor([0.7831]), tensor([0.8450]), tensor([0.8129]), tensor([0.8384]))"
"379         if isinstance(nnOptimiserParams, NNOptimiserParams):
380             return nnOptimiserParams
381         else:
382             return cls.fromDict(nnOptimiserParams)
383 
384     @staticmethod
385     def fromDict(params: dict) -> ""NNOptimiserParams"":
386         removedParams = [""cuda""]
387         params = {k: v for k, v in params.items() if k not in removedParams}
388         return NNOptimiserParams(**params)
","391         if isinstance(nnOptimiserParams, NNOptimiserParams):
392             return nnOptimiserParams
393         else:
394             return cls.fromDict(nnOptimiserParams)
395 
396     @classmethod
397     def fromDict(cls, params: dict) -> ""NNOptimiserParams"":
398         return NNOptimiserParams(**cls._updatedParams(params))
399 
400     @classmethod
","Before: 384, 385, 386, 387, 388
After: 396, 397, 398",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3360,"{'module': 1, 'if_statement': 1, 'if': 2, 'call': 3, 'identifier': 21, 'argument_list': 3, '(': 4, ',': 2, ')': 4, ':': 5, 'block': 3, 'return_statement': 2, 'return': 2, 'else_clause': 1, 'else': 1, 'attribute': 2, '.': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 2, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'expression_statement': 2, 'assignment': 2, '=': 2, 'list': 1, '[': 1, ']': 1, 'dictionary_comprehension': 1, '{': 1, 'pair': 1, 'for_in_clause': 1, 'for': 1, 'pattern_list': 1, 'in': 1, 'if_clause': 1, 'comparison_operator': 1, 'not in': 2, '}': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3255748421114411,0.28509339173913767,"(tensor([0.9401]), tensor([0.8981]), tensor([0.9186]), tensor([0.9022]))"
"432     def __str__(self):
433         return f""{self.__class__.__name__}[params={self.params}]""
434 
435     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
436             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
437             createTorchModule=True):
438         """"""
439         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
440 
441             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
442               the `trainFraction` parameter of this object)
443             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
444             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
445               the validation set
446 
447         :param model: the model to be fitted
448         :param data: the data to use (see variants above)
449         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
450             If False, (re-)train the existing module.
451         """"""
452         self.cuda = model.cuda
453         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
454 
455         useValidation = self.params.trainFraction != 1.0
456 
457         def toDataSetProvider(d) -> TorchDataSetProvider:
458             if isinstance(d, TorchDataSetProvider):
459                 return d
460             elif isinstance(d, DataUtil):
461                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
462             else:
463                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
464 
465         # initialise data to be generated
466         self.trainingLog = []
467         self.bestEpoch = None
468 
469         def trainingLog(s):
470             self.log.info(s)
471             self.trainingLog.append(s)
472 
473         self._init_cuda()
474 
475         # Set the random seed manually for reproducibility.
476         seed = 42
477         torch.manual_seed(seed)
478         if self.cuda:
479             torchcuda.manual_seed_all(seed)
480         torch.backends.cudnn.benchmark = False
481         torch.backends.cudnn.deterministic = True
482 
483         # obtain data, splitting it into training and validation set(s)
484         validationSets = []
485         trainingSets = []
486         outputScalers = []
487         if type(data) != list:
488             data = [data]
489         self.log.info(""Obtaining input/output training instances"")
490         for idxDataItem, dataItem in enumerate(data):
491             if isinstance(dataItem, TorchDataSet):
492                 if useValidation:
493                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
494                 trainingSet = dataItem
495                 validationSet = None
496                 outputScaler = TensorScalerIdentity()
497             elif type(dataItem) == tuple:
498                 trainingSet, validationSet = dataItem
499                 outputScaler = TensorScalerIdentity()
500             else:
501                 dataSetProvider = toDataSetProvider(dataItem)
502                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
503                 outputScaler = dataSetProvider.getOutputTensorScaler()
504             trainingSets.append(trainingSet)
505             if validationSet is not None:
506                 validationSets.append(validationSet)
507             outputScalers.append(outputScaler)
508             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
509         trainingLog(""Number of validation sets: %d"" % len(validationSets))
510 
511         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
512         if self.cuda:
513             torchModel.cuda()
514         model.setTorchModule(torchModel)
515 
516         nParams = sum([p.nelement() for p in torchModel.parameters()])
517         self.log.info(f""Learning parameters of {model} via {self}"")
518         trainingLog('Number of parameters: %d' % nParams)
519 
520         lossEvaluator = self.params.lossEvaluator
521         criterion = lossEvaluator.getTrainingCriterion()
522 
523         if self.cuda:
524             criterion = criterion.cuda()
525 
526         best_val = 1e9
527         best_epoch = 0
528         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
529             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
530 
531         bestModelBytes = model.getModuleBytes()
532         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
533         validationMetricName = lossEvaluator.getValidationMetricName()
534         trainingLossValues = []
535         validationMetricValues = []
536         try:
537             self.log.info(f'Begin training with cuda={self.cuda}')
538             self.log.info('Press Ctrl+C to end training early')
539             for epoch in range(1, self.params.epochs + 1):
540                 epoch_start_time = time.time()
541 
542                 # perform training step, processing all the training data once
543                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
544                 trainingLossValues.append(train_loss)
545 
546                 # perform validation, computing the mean metrics across all validation sets (if more than one),
547                 # and check for new best result according to validation results
548                 isNewBest = False
549                 if useValidation:
550                     metricsSum = None
551                     metricsKeys = None
552                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
553                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
554                         metricsArray = np.array(list(metrics.values()))
555                         if i == 0:
556                             metricsSum = metricsArray
557                             metricsKeys = metrics.keys()
558                         else:
559                             metricsSum += metricsArray
560                     metricsSum /= len(validationSets)  # mean results
561                     metrics = dict(zip(metricsKeys, metricsSum))
562                     current_val = metrics[lossEvaluator.getValidationMetricName()]
563                     validationMetricValues.append(current_val)
564                     isNewBest = current_val < best_val
565                     if isNewBest:
566                         best_val = current_val
567                         best_epoch = epoch
568                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
569                     else:
570                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
571                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
572                 else:
573                     valStr = """"
574                 trainingLog(
575                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
576                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
577                 if useValidation and isNewBest:
578                     bestModelBytes = model.getModuleBytes()
579             trainingLog(""Training complete"")
580         except KeyboardInterrupt:
581             trainingLog('Exiting from training early')
582 
583         self.trainingLossSequence = trainingLossValues
584         self.validationMetricSequence = validationMetricValues
585 
586         # reload best model according to validation results
587         if useValidation:
588             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 437
After: 444",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3759,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 378, 'parameters': 4, '(': 82, ')': 82, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 18, '{': 18, 'attribute': 100, '.': 100, '}': 18, 'string_content': 34, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 59, 'true': 2, 'expression_statement': 82, 'assignment': 54, 'call': 76, 'argument_list': 76, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 7, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889804085258799,0.6864355055978623,"(tensor([0.9575]), tensor([0.9709]), tensor([0.9642]), tensor([0.9696]))"
"432     def __str__(self):
433         return f""{self.__class__.__name__}[params={self.params}]""
434 
435     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
436             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
437             createTorchModule=True):
438         """"""
439         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
440 
441             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
442               the `trainFraction` parameter of this object)
443             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
444             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
445               the validation set
446 
447         :param model: the model to be fitted
448         :param data: the data to use (see variants above)
449         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
450             If False, (re-)train the existing module.
451         """"""
452         self.cuda = model.cuda
453         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
454 
455         useValidation = self.params.trainFraction != 1.0
456 
457         def toDataSetProvider(d) -> TorchDataSetProvider:
458             if isinstance(d, TorchDataSetProvider):
459                 return d
460             elif isinstance(d, DataUtil):
461                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
462             else:
463                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
464 
465         # initialise data to be generated
466         self.trainingLog = []
467         self.bestEpoch = None
468 
469         def trainingLog(s):
470             self.log.info(s)
471             self.trainingLog.append(s)
472 
473         self._init_cuda()
474 
475         # Set the random seed manually for reproducibility.
476         seed = 42
477         torch.manual_seed(seed)
478         if self.cuda:
479             torchcuda.manual_seed_all(seed)
480         torch.backends.cudnn.benchmark = False
481         torch.backends.cudnn.deterministic = True
482 
483         # obtain data, splitting it into training and validation set(s)
484         validationSets = []
485         trainingSets = []
486         outputScalers = []
487         if type(data) != list:
488             data = [data]
489         self.log.info(""Obtaining input/output training instances"")
490         for idxDataItem, dataItem in enumerate(data):
491             if isinstance(dataItem, TorchDataSet):
492                 if useValidation:
493                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
494                 trainingSet = dataItem
495                 validationSet = None
496                 outputScaler = TensorScalerIdentity()
497             elif type(dataItem) == tuple:
498                 trainingSet, validationSet = dataItem
499                 outputScaler = TensorScalerIdentity()
500             else:
501                 dataSetProvider = toDataSetProvider(dataItem)
502                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
503                 outputScaler = dataSetProvider.getOutputTensorScaler()
504             trainingSets.append(trainingSet)
505             if validationSet is not None:
506                 validationSets.append(validationSet)
507             outputScalers.append(outputScaler)
508             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
509         trainingLog(""Number of validation sets: %d"" % len(validationSets))
510 
511         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
512         if self.cuda:
513             torchModel.cuda()
514         model.setTorchModule(torchModel)
515 
516         nParams = sum([p.nelement() for p in torchModel.parameters()])
517         self.log.info(f""Learning parameters of {model} via {self}"")
518         trainingLog('Number of parameters: %d' % nParams)
519 
520         lossEvaluator = self.params.lossEvaluator
521         criterion = lossEvaluator.getTrainingCriterion()
522 
523         if self.cuda:
524             criterion = criterion.cuda()
525 
526         best_val = 1e9
527         best_epoch = 0
528         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
529             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
530 
531         bestModelBytes = model.getModuleBytes()
532         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
533         validationMetricName = lossEvaluator.getValidationMetricName()
534         trainingLossValues = []
535         validationMetricValues = []
536         try:
537             self.log.info(f'Begin training with cuda={self.cuda}')
538             self.log.info('Press Ctrl+C to end training early')
539             for epoch in range(1, self.params.epochs + 1):
540                 epoch_start_time = time.time()
541 
542                 # perform training step, processing all the training data once
543                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
544                 trainingLossValues.append(train_loss)
545 
546                 # perform validation, computing the mean metrics across all validation sets (if more than one),
547                 # and check for new best result according to validation results
548                 isNewBest = False
549                 if useValidation:
550                     metricsSum = None
551                     metricsKeys = None
552                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
553                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
554                         metricsArray = np.array(list(metrics.values()))
555                         if i == 0:
556                             metricsSum = metricsArray
557                             metricsKeys = metrics.keys()
558                         else:
559                             metricsSum += metricsArray
560                     metricsSum /= len(validationSets)  # mean results
561                     metrics = dict(zip(metricsKeys, metricsSum))
562                     current_val = metrics[lossEvaluator.getValidationMetricName()]
563                     validationMetricValues.append(current_val)
564                     isNewBest = current_val < best_val
565                     if isNewBest:
566                         best_val = current_val
567                         best_epoch = epoch
568                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
569                     else:
570                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
571                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
572                 else:
573                     valStr = """"
574                 trainingLog(
575                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
576                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
577                 if useValidation and isNewBest:
578                     bestModelBytes = model.getModuleBytes()
579             trainingLog(""Training complete"")
580         except KeyboardInterrupt:
581             trainingLog('Exiting from training early')
582 
583         self.trainingLossSequence = trainingLossValues
584         self.validationMetricSequence = validationMetricValues
585 
586         # reload best model according to validation results
587         if useValidation:
588             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 465, 466, 467
After: 472",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3894,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 378, 'parameters': 4, '(': 82, ')': 82, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 18, '{': 18, 'attribute': 100, '.': 100, '}': 18, 'string_content': 34, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 59, 'true': 2, 'expression_statement': 82, 'assignment': 54, 'call': 76, 'argument_list': 76, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 7, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889804085258799,0.6864355055978623,"(tensor([0.9575]), tensor([0.9709]), tensor([0.9642]), tensor([0.9696]))"
"432     def __str__(self):
433         return f""{self.__class__.__name__}[params={self.params}]""
434 
435     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
436             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
437             createTorchModule=True):
438         """"""
439         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
440 
441             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
442               the `trainFraction` parameter of this object)
443             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
444             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
445               the validation set
446 
447         :param model: the model to be fitted
448         :param data: the data to use (see variants above)
449         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
450             If False, (re-)train the existing module.
451         """"""
452         self.cuda = model.cuda
453         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
454 
455         useValidation = self.params.trainFraction != 1.0
456 
457         def toDataSetProvider(d) -> TorchDataSetProvider:
458             if isinstance(d, TorchDataSetProvider):
459                 return d
460             elif isinstance(d, DataUtil):
461                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
462             else:
463                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
464 
465         # initialise data to be generated
466         self.trainingLog = []
467         self.bestEpoch = None
468 
469         def trainingLog(s):
470             self.log.info(s)
471             self.trainingLog.append(s)
472 
473         self._init_cuda()
474 
475         # Set the random seed manually for reproducibility.
476         seed = 42
477         torch.manual_seed(seed)
478         if self.cuda:
479             torchcuda.manual_seed_all(seed)
480         torch.backends.cudnn.benchmark = False
481         torch.backends.cudnn.deterministic = True
482 
483         # obtain data, splitting it into training and validation set(s)
484         validationSets = []
485         trainingSets = []
486         outputScalers = []
487         if type(data) != list:
488             data = [data]
489         self.log.info(""Obtaining input/output training instances"")
490         for idxDataItem, dataItem in enumerate(data):
491             if isinstance(dataItem, TorchDataSet):
492                 if useValidation:
493                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
494                 trainingSet = dataItem
495                 validationSet = None
496                 outputScaler = TensorScalerIdentity()
497             elif type(dataItem) == tuple:
498                 trainingSet, validationSet = dataItem
499                 outputScaler = TensorScalerIdentity()
500             else:
501                 dataSetProvider = toDataSetProvider(dataItem)
502                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
503                 outputScaler = dataSetProvider.getOutputTensorScaler()
504             trainingSets.append(trainingSet)
505             if validationSet is not None:
506                 validationSets.append(validationSet)
507             outputScalers.append(outputScaler)
508             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
509         trainingLog(""Number of validation sets: %d"" % len(validationSets))
510 
511         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
512         if self.cuda:
513             torchModel.cuda()
514         model.setTorchModule(torchModel)
515 
516         nParams = sum([p.nelement() for p in torchModel.parameters()])
517         self.log.info(f""Learning parameters of {model} via {self}"")
518         trainingLog('Number of parameters: %d' % nParams)
519 
520         lossEvaluator = self.params.lossEvaluator
521         criterion = lossEvaluator.getTrainingCriterion()
522 
523         if self.cuda:
524             criterion = criterion.cuda()
525 
526         best_val = 1e9
527         best_epoch = 0
528         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
529             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
530 
531         bestModelBytes = model.getModuleBytes()
532         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
533         validationMetricName = lossEvaluator.getValidationMetricName()
534         trainingLossValues = []
535         validationMetricValues = []
536         try:
537             self.log.info(f'Begin training with cuda={self.cuda}')
538             self.log.info('Press Ctrl+C to end training early')
539             for epoch in range(1, self.params.epochs + 1):
540                 epoch_start_time = time.time()
541 
542                 # perform training step, processing all the training data once
543                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
544                 trainingLossValues.append(train_loss)
545 
546                 # perform validation, computing the mean metrics across all validation sets (if more than one),
547                 # and check for new best result according to validation results
548                 isNewBest = False
549                 if useValidation:
550                     metricsSum = None
551                     metricsKeys = None
552                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
553                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
554                         metricsArray = np.array(list(metrics.values()))
555                         if i == 0:
556                             metricsSum = metricsArray
557                             metricsKeys = metrics.keys()
558                         else:
559                             metricsSum += metricsArray
560                     metricsSum /= len(validationSets)  # mean results
561                     metrics = dict(zip(metricsKeys, metricsSum))
562                     current_val = metrics[lossEvaluator.getValidationMetricName()]
563                     validationMetricValues.append(current_val)
564                     isNewBest = current_val < best_val
565                     if isNewBest:
566                         best_val = current_val
567                         best_epoch = epoch
568                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
569                     else:
570                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
571                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
572                 else:
573                     valStr = """"
574                 trainingLog(
575                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
576                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
577                 if useValidation and isNewBest:
578                     bestModelBytes = model.getModuleBytes()
579             trainingLog(""Training complete"")
580         except KeyboardInterrupt:
581             trainingLog('Exiting from training early')
582 
583         self.trainingLossSequence = trainingLossValues
584         self.validationMetricSequence = validationMetricValues
585 
586         # reload best model according to validation results
587         if useValidation:
588             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 471
After: 476",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,3947,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 378, 'parameters': 4, '(': 82, ')': 82, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 18, '{': 18, 'attribute': 100, '.': 100, '}': 18, 'string_content': 34, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 59, 'true': 2, 'expression_statement': 82, 'assignment': 54, 'call': 76, 'argument_list': 76, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 7, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889804085258799,0.6864355055978623,"(tensor([0.9575]), tensor([0.9709]), tensor([0.9642]), tensor([0.9696]))"
"432     def __str__(self):
433         return f""{self.__class__.__name__}[params={self.params}]""
434 
435     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
436             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
437             createTorchModule=True):
438         """"""
439         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
440 
441             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
442               the `trainFraction` parameter of this object)
443             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
444             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
445               the validation set
446 
447         :param model: the model to be fitted
448         :param data: the data to use (see variants above)
449         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
450             If False, (re-)train the existing module.
451         """"""
452         self.cuda = model.cuda
453         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
454 
455         useValidation = self.params.trainFraction != 1.0
456 
457         def toDataSetProvider(d) -> TorchDataSetProvider:
458             if isinstance(d, TorchDataSetProvider):
459                 return d
460             elif isinstance(d, DataUtil):
461                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
462             else:
463                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
464 
465         # initialise data to be generated
466         self.trainingLog = []
467         self.bestEpoch = None
468 
469         def trainingLog(s):
470             self.log.info(s)
471             self.trainingLog.append(s)
472 
473         self._init_cuda()
474 
475         # Set the random seed manually for reproducibility.
476         seed = 42
477         torch.manual_seed(seed)
478         if self.cuda:
479             torchcuda.manual_seed_all(seed)
480         torch.backends.cudnn.benchmark = False
481         torch.backends.cudnn.deterministic = True
482 
483         # obtain data, splitting it into training and validation set(s)
484         validationSets = []
485         trainingSets = []
486         outputScalers = []
487         if type(data) != list:
488             data = [data]
489         self.log.info(""Obtaining input/output training instances"")
490         for idxDataItem, dataItem in enumerate(data):
491             if isinstance(dataItem, TorchDataSet):
492                 if useValidation:
493                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
494                 trainingSet = dataItem
495                 validationSet = None
496                 outputScaler = TensorScalerIdentity()
497             elif type(dataItem) == tuple:
498                 trainingSet, validationSet = dataItem
499                 outputScaler = TensorScalerIdentity()
500             else:
501                 dataSetProvider = toDataSetProvider(dataItem)
502                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
503                 outputScaler = dataSetProvider.getOutputTensorScaler()
504             trainingSets.append(trainingSet)
505             if validationSet is not None:
506                 validationSets.append(validationSet)
507             outputScalers.append(outputScaler)
508             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
509         trainingLog(""Number of validation sets: %d"" % len(validationSets))
510 
511         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
512         if self.cuda:
513             torchModel.cuda()
514         model.setTorchModule(torchModel)
515 
516         nParams = sum([p.nelement() for p in torchModel.parameters()])
517         self.log.info(f""Learning parameters of {model} via {self}"")
518         trainingLog('Number of parameters: %d' % nParams)
519 
520         lossEvaluator = self.params.lossEvaluator
521         criterion = lossEvaluator.getTrainingCriterion()
522 
523         if self.cuda:
524             criterion = criterion.cuda()
525 
526         best_val = 1e9
527         best_epoch = 0
528         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
529             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
530 
531         bestModelBytes = model.getModuleBytes()
532         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
533         validationMetricName = lossEvaluator.getValidationMetricName()
534         trainingLossValues = []
535         validationMetricValues = []
536         try:
537             self.log.info(f'Begin training with cuda={self.cuda}')
538             self.log.info('Press Ctrl+C to end training early')
539             for epoch in range(1, self.params.epochs + 1):
540                 epoch_start_time = time.time()
541 
542                 # perform training step, processing all the training data once
543                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
544                 trainingLossValues.append(train_loss)
545 
546                 # perform validation, computing the mean metrics across all validation sets (if more than one),
547                 # and check for new best result according to validation results
548                 isNewBest = False
549                 if useValidation:
550                     metricsSum = None
551                     metricsKeys = None
552                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
553                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
554                         metricsArray = np.array(list(metrics.values()))
555                         if i == 0:
556                             metricsSum = metricsArray
557                             metricsKeys = metrics.keys()
558                         else:
559                             metricsSum += metricsArray
560                     metricsSum /= len(validationSets)  # mean results
561                     metrics = dict(zip(metricsKeys, metricsSum))
562                     current_val = metrics[lossEvaluator.getValidationMetricName()]
563                     validationMetricValues.append(current_val)
564                     isNewBest = current_val < best_val
565                     if isNewBest:
566                         best_val = current_val
567                         best_epoch = epoch
568                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
569                     else:
570                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
571                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
572                 else:
573                     valStr = """"
574                 trainingLog(
575                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
576                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
577                 if useValidation and isNewBest:
578                     bestModelBytes = model.getModuleBytes()
579             trainingLog(""Training complete"")
580         except KeyboardInterrupt:
581             trainingLog('Exiting from training early')
582 
583         self.trainingLossSequence = trainingLossValues
584         self.validationMetricSequence = validationMetricValues
585 
586         # reload best model according to validation results
587         if useValidation:
588             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 517
After: 522, 524",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,4445,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 378, 'parameters': 4, '(': 82, ')': 82, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 18, '{': 18, 'attribute': 100, '.': 100, '}': 18, 'string_content': 34, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 59, 'true': 2, 'expression_statement': 82, 'assignment': 54, 'call': 76, 'argument_list': 76, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 7, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889804085258799,0.6864355055978623,"(tensor([0.9575]), tensor([0.9709]), tensor([0.9642]), tensor([0.9696]))"
"432     def __str__(self):
433         return f""{self.__class__.__name__}[params={self.params}]""
434 
435     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
436             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
437             createTorchModule=True):
438         """"""
439         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
440 
441             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
442               the `trainFraction` parameter of this object)
443             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
444             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
445               the validation set
446 
447         :param model: the model to be fitted
448         :param data: the data to use (see variants above)
449         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
450             If False, (re-)train the existing module.
451         """"""
452         self.cuda = model.cuda
453         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
454 
455         useValidation = self.params.trainFraction != 1.0
456 
457         def toDataSetProvider(d) -> TorchDataSetProvider:
458             if isinstance(d, TorchDataSetProvider):
459                 return d
460             elif isinstance(d, DataUtil):
461                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
462             else:
463                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
464 
465         # initialise data to be generated
466         self.trainingLog = []
467         self.bestEpoch = None
468 
469         def trainingLog(s):
470             self.log.info(s)
471             self.trainingLog.append(s)
472 
473         self._init_cuda()
474 
475         # Set the random seed manually for reproducibility.
476         seed = 42
477         torch.manual_seed(seed)
478         if self.cuda:
479             torchcuda.manual_seed_all(seed)
480         torch.backends.cudnn.benchmark = False
481         torch.backends.cudnn.deterministic = True
482 
483         # obtain data, splitting it into training and validation set(s)
484         validationSets = []
485         trainingSets = []
486         outputScalers = []
487         if type(data) != list:
488             data = [data]
489         self.log.info(""Obtaining input/output training instances"")
490         for idxDataItem, dataItem in enumerate(data):
491             if isinstance(dataItem, TorchDataSet):
492                 if useValidation:
493                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
494                 trainingSet = dataItem
495                 validationSet = None
496                 outputScaler = TensorScalerIdentity()
497             elif type(dataItem) == tuple:
498                 trainingSet, validationSet = dataItem
499                 outputScaler = TensorScalerIdentity()
500             else:
501                 dataSetProvider = toDataSetProvider(dataItem)
502                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
503                 outputScaler = dataSetProvider.getOutputTensorScaler()
504             trainingSets.append(trainingSet)
505             if validationSet is not None:
506                 validationSets.append(validationSet)
507             outputScalers.append(outputScaler)
508             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
509         trainingLog(""Number of validation sets: %d"" % len(validationSets))
510 
511         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
512         if self.cuda:
513             torchModel.cuda()
514         model.setTorchModule(torchModel)
515 
516         nParams = sum([p.nelement() for p in torchModel.parameters()])
517         self.log.info(f""Learning parameters of {model} via {self}"")
518         trainingLog('Number of parameters: %d' % nParams)
519 
520         lossEvaluator = self.params.lossEvaluator
521         criterion = lossEvaluator.getTrainingCriterion()
522 
523         if self.cuda:
524             criterion = criterion.cuda()
525 
526         best_val = 1e9
527         best_epoch = 0
528         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
529             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
530 
531         bestModelBytes = model.getModuleBytes()
532         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
533         validationMetricName = lossEvaluator.getValidationMetricName()
534         trainingLossValues = []
535         validationMetricValues = []
536         try:
537             self.log.info(f'Begin training with cuda={self.cuda}')
538             self.log.info('Press Ctrl+C to end training early')
539             for epoch in range(1, self.params.epochs + 1):
540                 epoch_start_time = time.time()
541 
542                 # perform training step, processing all the training data once
543                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
544                 trainingLossValues.append(train_loss)
545 
546                 # perform validation, computing the mean metrics across all validation sets (if more than one),
547                 # and check for new best result according to validation results
548                 isNewBest = False
549                 if useValidation:
550                     metricsSum = None
551                     metricsKeys = None
552                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
553                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
554                         metricsArray = np.array(list(metrics.values()))
555                         if i == 0:
556                             metricsSum = metricsArray
557                             metricsKeys = metrics.keys()
558                         else:
559                             metricsSum += metricsArray
560                     metricsSum /= len(validationSets)  # mean results
561                     metrics = dict(zip(metricsKeys, metricsSum))
562                     current_val = metrics[lossEvaluator.getValidationMetricName()]
563                     validationMetricValues.append(current_val)
564                     isNewBest = current_val < best_val
565                     if isNewBest:
566                         best_val = current_val
567                         best_epoch = epoch
568                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
569                     else:
570                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
571                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
572                 else:
573                     valStr = """"
574                 trainingLog(
575                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
576                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
577                 if useValidation and isNewBest:
578                     bestModelBytes = model.getModuleBytes()
579             trainingLog(""Training complete"")
580         except KeyboardInterrupt:
581             trainingLog('Exiting from training early')
582 
583         self.trainingLossSequence = trainingLossValues
584         self.validationMetricSequence = validationMetricValues
585 
586         # reload best model according to validation results
587         if useValidation:
588             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 529
After: 536",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,4582,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 378, 'parameters': 4, '(': 82, ')': 82, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 18, '{': 18, 'attribute': 100, '.': 100, '}': 18, 'string_content': 34, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 59, 'true': 2, 'expression_statement': 82, 'assignment': 54, 'call': 76, 'argument_list': 76, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 7, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889804085258799,0.6864355055978623,"(tensor([0.9575]), tensor([0.9709]), tensor([0.9642]), tensor([0.9696]))"
"432     def __str__(self):
433         return f""{self.__class__.__name__}[params={self.params}]""
434 
435     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
436             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
437             createTorchModule=True):
438         """"""
439         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
440 
441             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
442               the `trainFraction` parameter of this object)
443             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
444             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
445               the validation set
446 
447         :param model: the model to be fitted
448         :param data: the data to use (see variants above)
449         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
450             If False, (re-)train the existing module.
451         """"""
452         self.cuda = model.cuda
453         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
454 
455         useValidation = self.params.trainFraction != 1.0
456 
457         def toDataSetProvider(d) -> TorchDataSetProvider:
458             if isinstance(d, TorchDataSetProvider):
459                 return d
460             elif isinstance(d, DataUtil):
461                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
462             else:
463                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
464 
465         # initialise data to be generated
466         self.trainingLog = []
467         self.bestEpoch = None
468 
469         def trainingLog(s):
470             self.log.info(s)
471             self.trainingLog.append(s)
472 
473         self._init_cuda()
474 
475         # Set the random seed manually for reproducibility.
476         seed = 42
477         torch.manual_seed(seed)
478         if self.cuda:
479             torchcuda.manual_seed_all(seed)
480         torch.backends.cudnn.benchmark = False
481         torch.backends.cudnn.deterministic = True
482 
483         # obtain data, splitting it into training and validation set(s)
484         validationSets = []
485         trainingSets = []
486         outputScalers = []
487         if type(data) != list:
488             data = [data]
489         self.log.info(""Obtaining input/output training instances"")
490         for idxDataItem, dataItem in enumerate(data):
491             if isinstance(dataItem, TorchDataSet):
492                 if useValidation:
493                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
494                 trainingSet = dataItem
495                 validationSet = None
496                 outputScaler = TensorScalerIdentity()
497             elif type(dataItem) == tuple:
498                 trainingSet, validationSet = dataItem
499                 outputScaler = TensorScalerIdentity()
500             else:
501                 dataSetProvider = toDataSetProvider(dataItem)
502                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
503                 outputScaler = dataSetProvider.getOutputTensorScaler()
504             trainingSets.append(trainingSet)
505             if validationSet is not None:
506                 validationSets.append(validationSet)
507             outputScalers.append(outputScaler)
508             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
509         trainingLog(""Number of validation sets: %d"" % len(validationSets))
510 
511         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
512         if self.cuda:
513             torchModel.cuda()
514         model.setTorchModule(torchModel)
515 
516         nParams = sum([p.nelement() for p in torchModel.parameters()])
517         self.log.info(f""Learning parameters of {model} via {self}"")
518         trainingLog('Number of parameters: %d' % nParams)
519 
520         lossEvaluator = self.params.lossEvaluator
521         criterion = lossEvaluator.getTrainingCriterion()
522 
523         if self.cuda:
524             criterion = criterion.cuda()
525 
526         best_val = 1e9
527         best_epoch = 0
528         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
529             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
530 
531         bestModelBytes = model.getModuleBytes()
532         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
533         validationMetricName = lossEvaluator.getValidationMetricName()
534         trainingLossValues = []
535         validationMetricValues = []
536         try:
537             self.log.info(f'Begin training with cuda={self.cuda}')
538             self.log.info('Press Ctrl+C to end training early')
539             for epoch in range(1, self.params.epochs + 1):
540                 epoch_start_time = time.time()
541 
542                 # perform training step, processing all the training data once
543                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
544                 trainingLossValues.append(train_loss)
545 
546                 # perform validation, computing the mean metrics across all validation sets (if more than one),
547                 # and check for new best result according to validation results
548                 isNewBest = False
549                 if useValidation:
550                     metricsSum = None
551                     metricsKeys = None
552                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
553                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
554                         metricsArray = np.array(list(metrics.values()))
555                         if i == 0:
556                             metricsSum = metricsArray
557                             metricsKeys = metrics.keys()
558                         else:
559                             metricsSum += metricsArray
560                     metricsSum /= len(validationSets)  # mean results
561                     metrics = dict(zip(metricsKeys, metricsSum))
562                     current_val = metrics[lossEvaluator.getValidationMetricName()]
563                     validationMetricValues.append(current_val)
564                     isNewBest = current_val < best_val
565                     if isNewBest:
566                         best_val = current_val
567                         best_epoch = epoch
568                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
569                     else:
570                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
571                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
572                 else:
573                     valStr = """"
574                 trainingLog(
575                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
576                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
577                 if useValidation and isNewBest:
578                     bestModelBytes = model.getModuleBytes()
579             trainingLog(""Training complete"")
580         except KeyboardInterrupt:
581             trainingLog('Exiting from training early')
582 
583         self.trainingLossSequence = trainingLossValues
584         self.validationMetricSequence = validationMetricValues
585 
586         # reload best model according to validation results
587         if useValidation:
588             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 577, 578
After: 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,5121,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 378, 'parameters': 4, '(': 82, ')': 82, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 18, '{': 18, 'attribute': 100, '.': 100, '}': 18, 'string_content': 34, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 59, 'true': 2, 'expression_statement': 82, 'assignment': 54, 'call': 76, 'argument_list': 76, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 7, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889804085258799,0.6864355055978623,"(tensor([0.9575]), tensor([0.9709]), tensor([0.9642]), tensor([0.9696]))"
"432     def __str__(self):
433         return f""{self.__class__.__name__}[params={self.params}]""
434 
435     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
436             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
437             createTorchModule=True):
438         """"""
439         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
440 
441             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
442               the `trainFraction` parameter of this object)
443             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
444             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
445               the validation set
446 
447         :param model: the model to be fitted
448         :param data: the data to use (see variants above)
449         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
450             If False, (re-)train the existing module.
451         """"""
452         self.cuda = model.cuda
453         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
454 
455         useValidation = self.params.trainFraction != 1.0
456 
457         def toDataSetProvider(d) -> TorchDataSetProvider:
458             if isinstance(d, TorchDataSetProvider):
459                 return d
460             elif isinstance(d, DataUtil):
461                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
462             else:
463                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
464 
465         # initialise data to be generated
466         self.trainingLog = []
467         self.bestEpoch = None
468 
469         def trainingLog(s):
470             self.log.info(s)
471             self.trainingLog.append(s)
472 
473         self._init_cuda()
474 
475         # Set the random seed manually for reproducibility.
476         seed = 42
477         torch.manual_seed(seed)
478         if self.cuda:
479             torchcuda.manual_seed_all(seed)
480         torch.backends.cudnn.benchmark = False
481         torch.backends.cudnn.deterministic = True
482 
483         # obtain data, splitting it into training and validation set(s)
484         validationSets = []
485         trainingSets = []
486         outputScalers = []
487         if type(data) != list:
488             data = [data]
489         self.log.info(""Obtaining input/output training instances"")
490         for idxDataItem, dataItem in enumerate(data):
491             if isinstance(dataItem, TorchDataSet):
492                 if useValidation:
493                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
494                 trainingSet = dataItem
495                 validationSet = None
496                 outputScaler = TensorScalerIdentity()
497             elif type(dataItem) == tuple:
498                 trainingSet, validationSet = dataItem
499                 outputScaler = TensorScalerIdentity()
500             else:
501                 dataSetProvider = toDataSetProvider(dataItem)
502                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
503                 outputScaler = dataSetProvider.getOutputTensorScaler()
504             trainingSets.append(trainingSet)
505             if validationSet is not None:
506                 validationSets.append(validationSet)
507             outputScalers.append(outputScaler)
508             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
509         trainingLog(""Number of validation sets: %d"" % len(validationSets))
510 
511         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
512         if self.cuda:
513             torchModel.cuda()
514         model.setTorchModule(torchModel)
515 
516         nParams = sum([p.nelement() for p in torchModel.parameters()])
517         self.log.info(f""Learning parameters of {model} via {self}"")
518         trainingLog('Number of parameters: %d' % nParams)
519 
520         lossEvaluator = self.params.lossEvaluator
521         criterion = lossEvaluator.getTrainingCriterion()
522 
523         if self.cuda:
524             criterion = criterion.cuda()
525 
526         best_val = 1e9
527         best_epoch = 0
528         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
529             max_grad_norm=self.params.optimiserClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
530 
531         bestModelBytes = model.getModuleBytes()
532         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
533         validationMetricName = lossEvaluator.getValidationMetricName()
534         trainingLossValues = []
535         validationMetricValues = []
536         try:
537             self.log.info(f'Begin training with cuda={self.cuda}')
538             self.log.info('Press Ctrl+C to end training early')
539             for epoch in range(1, self.params.epochs + 1):
540                 epoch_start_time = time.time()
541 
542                 # perform training step, processing all the training data once
543                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
544                 trainingLossValues.append(train_loss)
545 
546                 # perform validation, computing the mean metrics across all validation sets (if more than one),
547                 # and check for new best result according to validation results
548                 isNewBest = False
549                 if useValidation:
550                     metricsSum = None
551                     metricsKeys = None
552                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
553                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
554                         metricsArray = np.array(list(metrics.values()))
555                         if i == 0:
556                             metricsSum = metricsArray
557                             metricsKeys = metrics.keys()
558                         else:
559                             metricsSum += metricsArray
560                     metricsSum /= len(validationSets)  # mean results
561                     metrics = dict(zip(metricsKeys, metricsSum))
562                     current_val = metrics[lossEvaluator.getValidationMetricName()]
563                     validationMetricValues.append(current_val)
564                     isNewBest = current_val < best_val
565                     if isNewBest:
566                         best_val = current_val
567                         best_epoch = epoch
568                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
569                     else:
570                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
571                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
572                 else:
573                     valStr = """"
574                 trainingLog(
575                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
576                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
577                 if useValidation and isNewBest:
578                     bestModelBytes = model.getModuleBytes()
579             trainingLog(""Training complete"")
580         except KeyboardInterrupt:
581             trainingLog('Exiting from training early')
582 
583         self.trainingLossSequence = trainingLossValues
584         self.validationMetricSequence = validationMetricValues
585 
586         # reload best model according to validation results
587         if useValidation:
588             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 581, 582, 583, 584
After: 597",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,5156,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 378, 'parameters': 4, '(': 82, ')': 82, ':': 31, 'block': 29, 'return_statement': 3, 'return': 3, 'string': 24, 'string_start': 24, 'interpolation': 18, '{': 18, 'attribute': 100, '.': 100, '}': 18, 'string_content': 34, 'string_end': 24, ',': 44, 'typed_parameter': 2, 'type': 19, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 59, 'true': 2, 'expression_statement': 82, 'assignment': 54, 'call': 76, 'argument_list': 76, 'comparison_operator': 7, '!=': 2, 'float': 2, '->': 1, 'if_statement': 13, 'if': 15, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 7, 'raise_statement': 2, 'raise': 2, 'comment': 8, 'list': 7, 'none': 6, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, '==': 2, 'is not': 4, 'binary_operator': 6, '+': 2, 'conditional_expression': 2, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 4, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 1, 'boolean_operator': 1, 'and': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889804085258799,0.6864355055978623,"(tensor([0.9575]), tensor([0.9709]), tensor([0.9642]), tensor([0.9696]))"
"589             self.bestEpoch = best_epoch
590             model.setModuleBytes(bestModelBytes)
591 
592     def getTrainingLog(self):
593         return self.trainingLog
594 
595     def getTrainingLossSequence(self):
","439     def __str__(self):
440         return f""{self.__class__.__name__}[params={self.params}]""
441 
442     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
443             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
444             createTorchModule=True) -> ""TrainingInfo"":
445         """"""
446         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
447 
448             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
449               the `trainFraction` parameter of this object)
450             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
451             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
452               the validation set
453 
454         :param model: the model to be fitted
455         :param data: the data to use (see variants above)
456         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
457             If False, (re-)train the existing module.
458         """"""
459         self.cuda = model.cuda
460         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
461 
462         useValidation = self.params.trainFraction != 1.0
463 
464         def toDataSetProvider(d) -> TorchDataSetProvider:
465             if isinstance(d, TorchDataSetProvider):
466                 return d
467             elif isinstance(d, DataUtil):
468                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
469             else:
470                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
471 
472         trainingLogEntries = []
473 
474         def trainingLog(s):
475             self.log.info(s)
476             trainingLogEntries.append(s)
477 
478         self._init_cuda()
479 
480         # Set the random seed manually for reproducibility.
481         seed = 42
482         torch.manual_seed(seed)
483         if self.cuda:
484             torchcuda.manual_seed_all(seed)
485         torch.backends.cudnn.benchmark = False
486         torch.backends.cudnn.deterministic = True
487 
488         # obtain data, splitting it into training and validation set(s)
489         validationSets = []
490         trainingSets = []
491         outputScalers = []
492         if type(data) != list:
493             data = [data]
494         self.log.info(""Obtaining input/output training instances"")
495         for idxDataItem, dataItem in enumerate(data):
496             if isinstance(dataItem, TorchDataSet):
497                 if useValidation:
498                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
499                 trainingSet = dataItem
500                 validationSet = None
501                 outputScaler = TensorScalerIdentity()
502             elif type(dataItem) == tuple:
503                 trainingSet, validationSet = dataItem
504                 outputScaler = TensorScalerIdentity()
505             else:
506                 dataSetProvider = toDataSetProvider(dataItem)
507                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
508                 outputScaler = dataSetProvider.getOutputTensorScaler()
509             trainingSets.append(trainingSet)
510             if validationSet is not None:
511                 validationSets.append(validationSet)
512             outputScalers.append(outputScaler)
513             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
514         trainingLog(""Number of validation sets: %d"" % len(validationSets))
515 
516         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
517         if self.cuda:
518             torchModel.cuda()
519         model.setTorchModule(torchModel)
520 
521         nParams = sum([p.nelement() for p in torchModel.parameters()])
522         self.log.info(f""Learning parameters of {model}"")
523         trainingLog('Number of parameters: %d' % nParams)
524         trainingLog(f""Starting training process via {self}"")
525 
526         lossEvaluator = self.params.lossEvaluator
527         criterion = lossEvaluator.getTrainingCriterion()
528 
529         if self.cuda:
530             criterion = criterion.cuda()
531 
532         totalEpochs = None
533         best_val = 1e9
534         best_epoch = 0
535         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
536             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
537 
538         bestModelBytes = model.getModuleBytes()
539         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
540         validationMetricName = lossEvaluator.getValidationMetricName()
541         trainingLossValues = []
542         validationMetricValues = []
543         try:
544             self.log.info(f'Begin training with cuda={self.cuda}')
545             self.log.info('Press Ctrl+C to end training early')
546             for epoch in range(1, self.params.epochs + 1):
547                 epoch_start_time = time.time()
548 
549                 # perform training step, processing all the training data once
550                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
551                 trainingLossValues.append(train_loss)
552 
553                 # perform validation, computing the mean metrics across all validation sets (if more than one),
554                 # and check for new best result according to validation results
555                 isNewBest = False
556                 if useValidation:
557                     metricsSum = None
558                     metricsKeys = None
559                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
560                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
561                         metricsArray = np.array(list(metrics.values()))
562                         if i == 0:
563                             metricsSum = metricsArray
564                             metricsKeys = metrics.keys()
565                         else:
566                             metricsSum += metricsArray
567                     metricsSum /= len(validationSets)  # mean results
568                     metrics = dict(zip(metricsKeys, metricsSum))
569                     current_val = metrics[lossEvaluator.getValidationMetricName()]
570                     validationMetricValues.append(current_val)
571                     isNewBest = current_val < best_val
572                     if isNewBest:
573                         best_val = current_val
574                         best_epoch = epoch
575                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
576                     else:
577                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
578                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
579                 else:
580                     valStr = """"
581                 trainingLog(
582                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
583                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
584                 totalEpochs = epoch
585                 if useValidation:
586                     if isNewBest:
587                         bestModelBytes = model.getModuleBytes()
588 
589                     # check for early stopping
590                     numEpochsWithoutImprovement = epoch - best_epoch
591                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
592                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
593                         break
594 
595             trainingLog(""Training complete"")
596         except KeyboardInterrupt:
597             trainingLog('Exiting from training early because of keyboard interrupt')
598 
599         # reload best model according to validation results
600         if useValidation:
601             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
602             self.bestEpoch = best_epoch
603             model.setModuleBytes(bestModelBytes)
604 
605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
","Before: 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606
After: 605, 606",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,5230,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'attribute': 3, 'identifier': 10, '.': 3, '=': 1, 'call': 1, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.005062271621345378,0.005008629077369262,"(tensor([0.5965]), tensor([0.8285]), tensor([0.6936]), tensor([0.7974]))"
"676 
677 
678 class TrainingInfo:
679     def __init__(self, bestEpoch: int = None, log: str = None, trainingLossSequence: Sequence[float] = None, validationMetricSequence:
680             Sequence[float] = None):
681         self.validationMetricSequence = validationMetricSequence
682         self.trainingLossSequence = trainingLossSequence
683         self.log = log
684         self.bestEpoch = bestEpoch
685 
686     def getTrainingLossSeries(self):
","676 
677 
678 class TrainingInfo:
679     def __init__(self, bestEpoch: int = None, log: List[str] = None, trainingLossSequence: Sequence[float] = None, validationMetricSequence:
680             Sequence[float] = None, totalEpochs=None):
681         self.validationMetricSequence = validationMetricSequence
682         self.trainingLossSequence = trainingLossSequence
683         self.log = log
684         self.bestEpoch = bestEpoch
685         self.totalEpochs = totalEpochs
686 
687     def __setstate__(self, state):
","Before: 679, 680
After: 679, 680, 685, 686, 687, 688, 689, 690",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,6128,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 25, ':': 6, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 4, 'typed_default_parameter': 4, 'type': 6, '=': 8, 'none': 4, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, ')': 1, 'expression_statement': 4, 'assignment': 4, 'attribute': 4, '.': 4}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.741252523213636,0.738695253912949,"(tensor([0.9447]), tensor([0.9723]), tensor([0.9583]), tensor([0.9695]))"
"683         self.log = log
684         self.bestEpoch = bestEpoch
685 
686     def getTrainingLossSeries(self):
687         return pd.Series(self.trainingLossSequence, name=""training loss"")
688 
689     def getValidationMetricSeries(self):
","689             state[""totalEpochs""] = None
690         self.__dict__ = state
691 
692     def getTrainingLossSeries(self) -> pd.Series:
693         return pd.Series(self.trainingLossSequence, name=""training loss"")
694 
695     def getValidationMetricSeries(self) -> pd.Series:
","Before: 686
After: 692",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,6184,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'attribute': 4, 'identifier': 13, '.': 4, '=': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1, ',': 1, 'keyword_argument': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3745410033489862,0.317023313852343,"(tensor([0.9044]), tensor([0.9312]), tensor([0.9176]), tensor([0.9284]))"
"686     def getTrainingLossSeries(self):
687         return pd.Series(self.trainingLossSequence, name=""training loss"")
688 
689     def getValidationMetricSeries(self):
690         return pd.Series(self.validationMetricSequence, name=""validation metric"")
691 
692     def plotAll(self):
","692     def getTrainingLossSeries(self) -> pd.Series:
693         return pd.Series(self.trainingLossSequence, name=""training loss"")
694 
695     def getValidationMetricSeries(self) -> pd.Series:
696         return pd.Series(self.validationMetricSequence, name=""validation metric"")
697 
698     def plotAll(self) -> matplotlib.figure.Figure:
","Before: 689
After: 695",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,6215,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 14, 'parameters': 2, '(': 4, ')': 4, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 2, 'attribute': 4, '.': 4, 'argument_list': 2, ',': 2, 'keyword_argument': 2, '=': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5494978201382716,0.49365027857609584,"(tensor([0.9331]), tensor([0.9715]), tensor([0.9519]), tensor([0.9675]))"
"689     def getValidationMetricSeries(self):
690         return pd.Series(self.validationMetricSequence, name=""validation metric"")
691 
692     def plotAll(self):
693         """"""
694         Plots both the sequence of training loss values and the sequence of validation metric values
695         """"""
696         plt.figure()
697         pd.concat([self.getTrainingLossSeries(), self.getValidationMetricSeries()], axis=1).plot()
","695     def getValidationMetricSeries(self) -> pd.Series:
696         return pd.Series(self.validationMetricSequence, name=""validation metric"")
697 
698     def plotAll(self) -> matplotlib.figure.Figure:
699         """"""
700         Plots both the sequence of training loss values and the sequence of validation metric values
701         """"""
702         fig = plt.figure()
703         pd.concat([self.getTrainingLossSeries(), self.getValidationMetricSeries()], axis=1).plot()
704         return fig
","Before: 692
After: 698",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,6246,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 11, 'parameters': 2, '(': 4, ')': 4, ':': 2, 'block': 2, 'return_statement': 1, 'return': 1, 'call': 2, 'attribute': 3, '.': 3, 'argument_list': 2, ',': 1, 'keyword_argument': 1, '=': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'expression_statement': 2}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.610703863800358,0.5771449301794022,"(tensor([0.9420]), tensor([0.9704]), tensor([0.9560]), tensor([0.9675]))"
"689     def getValidationMetricSeries(self):
690         return pd.Series(self.validationMetricSequence, name=""validation metric"")
691 
692     def plotAll(self):
693         """"""
694         Plots both the sequence of training loss values and the sequence of validation metric values
695         """"""
696         plt.figure()
697         pd.concat([self.getTrainingLossSeries(), self.getValidationMetricSeries()], axis=1).plot()
","695     def getValidationMetricSeries(self) -> pd.Series:
696         return pd.Series(self.validationMetricSequence, name=""validation metric"")
697 
698     def plotAll(self) -> matplotlib.figure.Figure:
699         """"""
700         Plots both the sequence of training loss values and the sequence of validation metric values
701         """"""
702         fig = plt.figure()
703         pd.concat([self.getTrainingLossSeries(), self.getValidationMetricSeries()], axis=1).plot()
704         return fig
","Before: 696
After: 702, 704",fix nnoptimiserparams in src/src/torch_opt.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,6260,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 11, 'parameters': 2, '(': 4, ')': 4, ':': 2, 'block': 2, 'return_statement': 1, 'return': 1, 'call': 2, 'attribute': 3, '.': 3, 'argument_list': 2, ',': 1, 'keyword_argument': 1, '=': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'expression_statement': 2}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 31, 'end_line': 56, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.610703863800358,0.5771449301794022,"(tensor([0.9420]), tensor([0.9704]), tensor([0.9560]), tensor([0.9675]))"
"55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
59         """"""
60         Creates a string containing information on the objects state, which is the name and value of all attributes
61         without the attributes that are in the list provided by _toStringExclusions. It is used by the methods __str__
62         and __repr__. This method can be overwritten by sub-classes to provide a custom string.
63 
64         :return: a string containing all attribute names and values
65         """"""
66         return self._toStringProperties(exclude=self._toStringExcludes())
67 
68     def _toStringExcludes(self) -> List[str]:
","55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
59         """"""
60         Creates a string containing information on the objects state, which is the name and value of all attributes
61         without the attributes that are in the list provided by _toStringExclusions. It is used by the methods __str__
62         and __repr__. This method can be overwritten by sub-classes to provide a custom string.
63 
64         :return: a string containing all attribute names and values
65         """"""
66         return self._toStringProperties(exclude=self._toStringExcludes(), **self._toStringAdditionalEntries())
67 
68     def _toStringExcludes(self) -> List[str]:
","Before: 66
After: 66",add _tostringadditionalentries method,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,b10e220629f965c0e9574c84549e8a7d2f96bcf1,d166e5396bdc22a153691aeccc033117d3ee967c,0,516,"{'module': 1, 'expression_statement': 2, 'call': 4, 'attribute': 3, 'identifier': 13, '.': 3, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 1, '=': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9505578470896794,0.9506484324861515,"(tensor([0.9885]), tensor([0.9966]), tensor([0.9926]), tensor([0.9958]))"
"1 from collections import Sequence
2 from typing import Callable, Optional
3 
4 import torch
5 from torch import nn
","1 from typing import Callable, Optional, Sequence
2 
3 import torch
4 from torch import nn
5 
","Before: 1, 2
After: 1",remove unused imports,Fixed type annotation,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_modules.py,7e1887d5b0c71ea82d93df0b0bbc6f4afcfed6e2,b10e220629f965c0e9574c84549e8a7d2f96bcf1,0,9,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 6, 'identifier': 6, 'import': 3, ',': 1, 'import_statement': 1}","{'cyclomatic_complexity': 3, 'nloc': 20, 'token_count': 172, 'name': '__init__', 'long_name': '__init__( self , inputDim : float , outputDim : float , hiddenDims : Sequence [ int ] , hidActivationFn : Callable [ [ torch . Tensor ] , torch . Tensor ] = torch . sigmoid , outputActivationFn : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] = torch . sigmoid , pDropout : Optional [ float ] = None )', 'start_line': 12, 'end_line': 31, 'full_parameters': ['self', ' inputDim : float', ' outputDim : float', ' hiddenDims : Sequence [ int ]', ' hidActivationFn : Callable [ [ torch . Tensor ]', ' torch . Tensor ] = torch . sigmoid', ' outputActivationFn : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ] = torch . sigmoid', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 20, 'token_count': 172, 'name': '__init__', 'long_name': '__init__( self , inputDim : float , outputDim : float , hiddenDims : Sequence [ int ] , hidActivationFn : Callable [ [ torch . Tensor ] , torch . Tensor ] = torch . sigmoid , outputActivationFn : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] = torch . sigmoid , pDropout : Optional [ float ] = None )', 'start_line': 11, 'end_line': 30, 'full_parameters': ['self', ' inputDim : float', ' outputDim : float', ' hiddenDims : Sequence [ int ]', ' hidActivationFn : Callable [ [ torch . Tensor ]', ' torch . Tensor ] = torch . sigmoid', ' outputActivationFn : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ] = torch . sigmoid', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.427912671199037,0.43373595087629213,"(tensor([0.9642]), tensor([0.9581]), tensor([0.9612]), tensor([0.9587]))"
"12     ResNet [3]. Each residual block consists of two fully connected layers with (optionally) batch normalisation and
13     dropout, which can all be bypassed by a so called skip connection. The skip path and the non-skip path are added as
14     the last step within each block.
15 
16     More precisely, the non-skip path consists of the following layers:
17       batch normalization -> ReLU, dropout -> fully-connected -> batch normalization -> ReLU, dropout -> fully-connected
18     The use of the activation function before the connected layers is called ""pre-activation"" [4].
19 
20     The skip path does nothing for the case where the input dimension of the block equals the output dimension. If these
21     dimensions are different, the skip-path consists of a fully-connected layer, but with no activation, normalization,
","12     ResNet [3]. Each residual block consists of two fully connected layers with (optionally) batch normalisation and
13     dropout, which can all be bypassed by a so called skip connection. The skip path and the non-skip path are added as
14     the last step within each block.
15 
16     More precisely, the non-skip path consists of the following layers:
17     batch normalization -> ReLU, dropout -> fully-connected -> batch normalization -> ReLU, dropout -> fully-connected
18     The use of the activation function before the connected layers is called ""pre-activation"" [4].
19 
20     The skip path does nothing for the case where the input dimension of the block equals the output dimension. If these
21     dimensions are different, the skip-path consists of a fully-connected layer, but with no activation, normalization,
","Before: 17
After: 17",fix typos in src/src/torch_models/residualffn/residualffn_modules.py,Fixed docstring syntax issues,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,93263b222af3e34c7b7d66d9174cf1730e1d32cb,14548b13c3b8611c12f4455ff375a70bc56a72b4,0,241,"{'module': 1, 'ERROR': 15, 'attribute': 5, 'subscript': 2, 'identifier': 95, '[': 2, 'integer': 2, ']': 2, '.': 5, 'with_statement': 1, 'with': 1, 'parenthesized_expression': 1, '(': 1, ')': 1, 'with_clause': 1, 'with_item': 3, 'boolean_operator': 2, 'and': 2, ',': 4, 'as_pattern': 1, 'binary_operator': 8, '-': 8, 'as': 1, 'as_pattern_target': 1, ':': 1, 'block': 1, 'expression_statement': 2, '->': 1, '>': 4, 'comparison_operator': 1, 'is': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",1.0,1.0,"(tensor([0.9982]), tensor([0.9982]), tensor([0.9982]), tensor([0.9982]))"
"29     regularisation, but also normalises the distribution of the inputs of each layer and therefore addresses the problem
30     of ""internal covariate shift""[5]. The mechanism behind this is not yet fully understood (see e.g. the Wikipedia
31     article on batch normalisation for further references).
32 
33     References:
34       * [1] Chen, Dongwei et al. âDeep Residual Learning for Nonlinear Regression.â
35         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
36       * [2] Kiprijanovska, et al. âHousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.â
37         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
38       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âDeep Residual Learning for Image Recognition.â
","29     regularisation, but also normalises the distribution of the inputs of each layer and therefore addresses the problem
30     of ""internal covariate shift""[5]. The mechanism behind this is not yet fully understood (see e.g. the Wikipedia
31     article on batch normalisation for further references).
32 
33     References:
34 
35       * [1] Chen, Dongwei et al. ""Deep Residual Learning for Nonlinear Regression.""
36         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
37       * [2] Kiprijanovska, et al. ""HousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.""
38         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
","Before: 34
After: 34, 35",fix typos in src/src/torch_models/residualffn/residualffn_modules.py,Fixed docstring syntax issues,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,93263b222af3e34c7b7d66d9174cf1730e1d32cb,14548b13c3b8611c12f4455ff375a70bc56a72b4,0,120,"{'module': 1, 'expression_statement': 1, 'identifier': 78, ',': 5, 'ERROR': 21, 'boolean_operator': 1, 'and': 1, 'attribute': 10, 'subscript': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '[': 3, 'integer': 9, ']': 3, '.': 14, 'is': 1, 'not': 1, '(': 3, 'for': 1, 'binary_operator': 8, ')': 3, ':': 6, '*': 2, 'list': 2, 'call': 2, 'argument_list': 2, 'float': 4, '//': 2, '/': 4, '-': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7367575449366371,0.733665829508582,"(tensor([0.9836]), tensor([0.8844]), tensor([0.9314]), tensor([0.8934]))"
"31     article on batch normalisation for further references).
32 
33     References:
34       * [1] Chen, Dongwei et al. âDeep Residual Learning for Nonlinear Regression.â
35         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
36       * [2] Kiprijanovska, et al. âHousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.â
37         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
38       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âDeep Residual Learning for Image Recognition.â
39         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
40       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âIdentity Mappings in Deep Residual Networks.â
","32 
33     References:
34 
35       * [1] Chen, Dongwei et al. ""Deep Residual Learning for Nonlinear Regression.""
36         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
37       * [2] Kiprijanovska, et al. ""HousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.""
38         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
39       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Deep Residual Learning for Image Recognition.""
40         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
41       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Identity Mappings in Deep Residual Networks.""
","Before: 36
After: 37",fix typos in src/src/torch_models/residualffn/residualffn_modules.py,Fixed docstring syntax issues,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,93263b222af3e34c7b7d66d9174cf1730e1d32cb,14548b13c3b8611c12f4455ff375a70bc56a72b4,0,189,"{'module': 1, 'ERROR': 25, 'identifier': 71, 'for': 2, 'subscript': 1, 'attribute': 10, 'binary_operator': 12, ')': 3, '.': 15, ':': 8, '*': 3, 'list': 3, '[': 4, 'integer': 10, ']': 4, ',': 10, 'call': 2, 'argument_list': 2, '(': 2, 'float': 7, '//': 3, '/': 6, '-': 1, 'and': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7169667034026651,0.7100350826276225,"(tensor([0.9625]), tensor([0.8727]), tensor([0.9154]), tensor([0.8809]))"
"33     References:
34       * [1] Chen, Dongwei et al. âDeep Residual Learning for Nonlinear Regression.â
35         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
36       * [2] Kiprijanovska, et al. âHousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.â
37         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
38       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âDeep Residual Learning for Image Recognition.â
39         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
40       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âIdentity Mappings in Deep Residual Networks.â
41         ArXiv:1603.05027 [Cs], July 25, 2016. http://arxiv.org/abs/1603.05027.
42       * [5] Ioffe, Sergey, and Christian Szegedy. âBatch Normalization: Accelerating Deep Network Training by Reducing
","34 
35       * [1] Chen, Dongwei et al. ""Deep Residual Learning for Nonlinear Regression.""
36         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
37       * [2] Kiprijanovska, et al. ""HousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.""
38         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
39       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Deep Residual Learning for Image Recognition.""
40         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
41       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Identity Mappings in Deep Residual Networks.""
42         ArXiv:1603.05027 [Cs], July 25, 2016. http://arxiv.org/abs/1603.05027.
43       * [5] Ioffe, Sergey, and Christian Szegedy. ""Batch Normalization: Accelerating Deep Network Training by Reducing
","Before: 38
After: 39",fix typos in src/src/torch_models/residualffn/residualffn_modules.py,Fixed docstring syntax issues,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,93263b222af3e34c7b7d66d9174cf1730e1d32cb,14548b13c3b8611c12f4455ff375a70bc56a72b4,0,262,"{'module': 1, 'ERROR': 37, 'expression_statement': 2, 'assignment': 1, 'identifier': 89, ':': 10, 'type': 1, 'attribute': 12, 'list_splat': 1, '*': 4, 'list': 4, '[': 6, 'integer': 12, ']': 6, ',': 16, '.': 18, 'for_statement': 1, 'for': 1, 'pattern_list': 1, 'call': 2, 'argument_list': 2, '(': 2, ')': 2, 'float': 10, 'binary_operator': 14, '//': 4, '/': 8, '-': 1, 'subscript': 2, 'in': 1, 'block': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.726085318915499,0.7194113551952845,"(tensor([0.9639]), tensor([0.8869]), tensor([0.9238]), tensor([0.8941]))"
"35         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
36       * [2] Kiprijanovska, et al. âHousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.â
37         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
38       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âDeep Residual Learning for Image Recognition.â
39         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
40       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âIdentity Mappings in Deep Residual Networks.â
41         ArXiv:1603.05027 [Cs], July 25, 2016. http://arxiv.org/abs/1603.05027.
42       * [5] Ioffe, Sergey, and Christian Szegedy. âBatch Normalization: Accelerating Deep Network Training by Reducing
43         Internal Covariate Shift.â ArXiv:1502.03167 [Cs], March 2, 2015. http://arxiv.org/abs/1502.03167.
44     """"""
","36         Entropy 22, no. 2 (February 2020): 193. https://doi.org/10.3390/e22020193.
37       * [2] Kiprijanovska, et al. ""HousEEC: Day-Ahead Household Electrical Energy Consumption Forecasting Using Deep Learning.""
38         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
39       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Deep Residual Learning for Image Recognition.""
40         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
41       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Identity Mappings in Deep Residual Networks.""
42         ArXiv:1603.05027 [Cs], July 25, 2016. http://arxiv.org/abs/1603.05027.
43       * [5] Ioffe, Sergey, and Christian Szegedy. ""Batch Normalization: Accelerating Deep Network Training by Reducing
44         Internal Covariate Shift."" ArXiv:1502.03167 [Cs], March 2, 2015. http://arxiv.org/abs/1502.03167.
45     """"""
","Before: 40
After: 41",fix typos in src/src/torch_models/residualffn/residualffn_modules.py,Fixed docstring syntax issues,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,93263b222af3e34c7b7d66d9174cf1730e1d32cb,14548b13c3b8611c12f4455ff375a70bc56a72b4,0,337,"{'module': 1, 'expression_statement': 9, 'identifier': 102, 'ERROR': 43, 'integer': 13, ',': 19, '.': 20, 'call': 2, 'argument_list': 2, '(': 2, ')': 2, ':': 12, 'float': 13, 'assignment': 4, '//': 5, 'type': 6, 'binary_operator': 10, 'attribute': 13, '/': 10, 'list_splat': 4, '*': 4, 'list': 4, '[': 7, ']': 7, '-': 1, 'constrained_type': 1, 'subscript': 3, 'comparison_operator': 1, 'in': 1, 'pattern_list': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7326065551627728,0.7326934258726084,"(tensor([0.9757]), tensor([0.9090]), tensor([0.9412]), tensor([0.9153]))"
"37         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
38       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âDeep Residual Learning for Image Recognition.â
39         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
40       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. âIdentity Mappings in Deep Residual Networks.â
41         ArXiv:1603.05027 [Cs], July 25, 2016. http://arxiv.org/abs/1603.05027.
42       * [5] Ioffe, Sergey, and Christian Szegedy. âBatch Normalization: Accelerating Deep Network Training by Reducing
43         Internal Covariate Shift.â ArXiv:1502.03167 [Cs], March 2, 2015. http://arxiv.org/abs/1502.03167.
44     """"""
45 
46     def __init__(self, inputDim: int, outputDim: int, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1,
","38         Energies 13, no. 10 (January 2020): 2672. https://doi.org/10.3390/en13102672.
39       * [3] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Deep Residual Learning for Image Recognition.""
40         ArXiv:1512.03385 [Cs], December 10, 2015. http://arxiv.org/abs/1512.03385.
41       * [4] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. ""Identity Mappings in Deep Residual Networks.""
42         ArXiv:1603.05027 [Cs], July 25, 2016. http://arxiv.org/abs/1603.05027.
43       * [5] Ioffe, Sergey, and Christian Szegedy. ""Batch Normalization: Accelerating Deep Network Training by Reducing
44         Internal Covariate Shift."" ArXiv:1502.03167 [Cs], March 2, 2015. http://arxiv.org/abs/1502.03167.
45     """"""
46 
47     def __init__(self, inputDim: int, outputDim: int, hiddenDims: Sequence[int], bottleneckDimensionFactor: float = 1,
","Before: 42, 43
After: 43, 44",fix typos in src/src/torch_models/residualffn/residualffn_modules.py,Fixed docstring syntax issues,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,93263b222af3e34c7b7d66d9174cf1730e1d32cb,14548b13c3b8611c12f4455ff375a70bc56a72b4,0,398,"{'module': 1, 'expression_statement': 7, 'identifier': 79, 'ERROR': 35, 'integer': 9, ',': 17, '.': 15, 'call': 1, 'argument_list': 1, '(': 1, ')': 1, ':': 9, 'float': 11, 'assignment': 3, '//': 4, 'type': 5, 'binary_operator': 7, 'attribute': 10, '/': 8, 'list_splat': 3, '*': 3, 'list': 3, '[': 6, ']': 6, 'constrained_type': 1, 'subscript': 3, 'comparison_operator': 1, 'in': 1, 'pattern_list': 1, 'string_start': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 46, 'end_line': 81, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.760149279286996,0.7579500709049869,"(tensor([0.9802]), tensor([0.9216]), tensor([0.9500]), tensor([0.9272]))"
"2 import numpy as np
3 import seaborn as sns
4 from abc import abstractmethod, ABC
5 from matplotlib import pyplot as plt
6 from matplotlib.colors import LinearSegmentedColormap
7 from typing import List, Sequence
8 
9 from .eval_stats_base import PredictionEvalStats, Metric, EvalStatsCollection, PredictionArray
10 
11 log = logging.getLogger(__name__)
","2 import numpy as np
3 import seaborn as sns
4 from abc import abstractmethod, ABC
5 from matplotlib import pyplot as plt
6 from matplotlib.colors import LinearSegmentedColormap
7 from typing import List, Sequence, Optional
8 
9 from .eval_stats_base import PredictionEvalStats, Metric, EvalStatsCollection, PredictionArray
10 
11 log = logging.getLogger(__name__)
","Before: 7
After: 7",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,59,"{'module': 1, 'import_statement': 2, 'import': 7, 'aliased_import': 3, 'dotted_name': 17, 'identifier': 21, 'as': 3, 'import_from_statement': 5, 'from': 5, ',': 5, '.': 2, 'relative_import': 1, 'import_prefix': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9374009563674955,0.9379404679326143,"(tensor([0.9959]), tensor([0.9985]), tensor([0.9972]), tensor([0.9982]))"
"146         """"""Gets the standard deviation of the absolute error""""""
147         return self.computeMetricValue(RegressionMetricStdDevAE())
148 
149     def getEvalStatsCollection(self):
150         """"""
151         For the case where we collected data on multiple dimensions, obtain a stats collection where
152         each object in the collection holds stats on just one dimension
153         """"""
154         if self.y_true_multidim is None:
155             raise Exception(""No multi-dimensional data was collected"")
156         dim = len(self.y_true_multidim)
157         statsList = []
158         for i in range(dim):
159             stats = RegressionEvalStats(self.y_predicted_multidim[i], self.y_true_multidim[i])
160             statsList.append(stats)
161         return RegressionEvalStatsCollection(statsList)
162 
163     def plotErrorDistribution(self, bins=None, figure=True, titleAdd=None):
","146         """"""Gets the standard deviation of the absolute error""""""
147         return self.computeMetricValue(RegressionMetricStdDevAE())
148 
149     def getEvalStatsCollection(self) -> ""RegressionEvalStatsCollection"":
150         """"""
151         For the case where we collected data on multiple dimensions, obtain a stats collection where
152         each object in the collection holds stats on just one dimension
153         """"""
154         if self.y_true_multidim is None:
155             raise Exception(""No multi-dimensional data was collected"")
156         dim = len(self.y_true_multidim)
157         statsList = []
158         for i in range(dim):
159             stats = RegressionEvalStats(self.y_predicted_multidim[i], self.y_true_multidim[i])
160             statsList.append(stats)
161         return RegressionEvalStatsCollection(statsList)
162 
163     def plotErrorDistribution(self, bins=None, figure=True, titleAdd=None) -> Optional[plt.Figure]:
","Before: 149
After: 149",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1318,"{'module': 1, 'expression_statement': 6, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'return_statement': 2, 'return': 2, 'call': 8, 'attribute': 6, 'identifier': 29, '.': 6, 'argument_list': 8, '(': 9, ')': 9, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'raise_statement': 1, 'raise': 1, 'assignment': 3, '=': 3, 'list': 1, '[': 3, ']': 3, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 2, ',': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9152100758234881,0.9121646081148104,"(tensor([0.9793]), tensor([0.9954]), tensor([0.9873]), tensor([0.9938]))"
"160             statsList.append(stats)
161         return RegressionEvalStatsCollection(statsList)
162 
163     def plotErrorDistribution(self, bins=None, figure=True, titleAdd=None):
164         """"""
165         :param bins: if None, seaborns default binning will be used
166         :param figure: whether to plot in a separate figure
167         :param titleAdd: a string to add to the title (on a second line)
168 
169         :return: the resulting figure object or None
170         """"""
171         errors = np.array(self.y_predicted) - np.array(self.y_true)
172         fig = None
173         title = ""Prediction Error Distribution""
174         if titleAdd is not None:
175             title += ""\n"" + titleAdd
176         if figure:
177             fig = plt.figure(title.replace(""\n"", "" ""))
178         sns.distplot(errors, bins=bins)
179         plt.title(title)
180         plt.xlabel(""error (prediction - ground truth)"")
181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs):
","160             statsList.append(stats)
161         return RegressionEvalStatsCollection(statsList)
162 
163     def plotErrorDistribution(self, bins=None, figure=True, titleAdd=None) -> Optional[plt.Figure]:
164         """"""
165         :param bins: if None, seaborns default binning will be used
166         :param figure: whether to plot in a separate figure and return that figure
167         :param titleAdd: a string to add to the title (on a second line)
168 
169         :return: the resulting figure object or None
170         """"""
171         errors = np.array(self.y_predicted) - np.array(self.y_true)
172         fig = None
173         title = ""Prediction Error Distribution""
174         if titleAdd is not None:
175             title += ""\n"" + titleAdd
176         if figure:
177             fig = plt.figure(title.replace(""\n"", "" ""))
178         sns.distplot(errors, bins=bins)
179         plt.title(title)
180         plt.xlabel(""error (prediction - ground truth)"")
181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","Before: 163
After: 163",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1445,"{'module': 1, 'expression_statement': 11, 'call': 10, 'attribute': 11, 'identifier': 43, '.': 11, 'argument_list': 10, '(': 11, ')': 11, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'default_parameter': 3, '=': 8, 'none': 4, 'true': 1, ':': 3, 'block': 3, 'string': 7, 'string_start': 7, 'string_content': 7, 'string_end': 7, 'assignment': 4, 'binary_operator': 2, '-': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'escape_sequence': 2, '+': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9104640147292403,0.9003027011678756,"(tensor([0.9751]), tensor([0.9931]), tensor([0.9840]), tensor([0.9913]))"
"160             statsList.append(stats)
161         return RegressionEvalStatsCollection(statsList)
162 
163     def plotErrorDistribution(self, bins=None, figure=True, titleAdd=None):
164         """"""
165         :param bins: if None, seaborns default binning will be used
166         :param figure: whether to plot in a separate figure
167         :param titleAdd: a string to add to the title (on a second line)
168 
169         :return: the resulting figure object or None
170         """"""
171         errors = np.array(self.y_predicted) - np.array(self.y_true)
172         fig = None
173         title = ""Prediction Error Distribution""
174         if titleAdd is not None:
175             title += ""\n"" + titleAdd
176         if figure:
177             fig = plt.figure(title.replace(""\n"", "" ""))
178         sns.distplot(errors, bins=bins)
179         plt.title(title)
180         plt.xlabel(""error (prediction - ground truth)"")
181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs):
","160             statsList.append(stats)
161         return RegressionEvalStatsCollection(statsList)
162 
163     def plotErrorDistribution(self, bins=None, figure=True, titleAdd=None) -> Optional[plt.Figure]:
164         """"""
165         :param bins: if None, seaborns default binning will be used
166         :param figure: whether to plot in a separate figure and return that figure
167         :param titleAdd: a string to add to the title (on a second line)
168 
169         :return: the resulting figure object or None
170         """"""
171         errors = np.array(self.y_predicted) - np.array(self.y_true)
172         fig = None
173         title = ""Prediction Error Distribution""
174         if titleAdd is not None:
175             title += ""\n"" + titleAdd
176         if figure:
177             fig = plt.figure(title.replace(""\n"", "" ""))
178         sns.distplot(errors, bins=bins)
179         plt.title(title)
180         plt.xlabel(""error (prediction - ground truth)"")
181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","Before: 166
After: 166",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1478,"{'module': 1, 'expression_statement': 11, 'call': 10, 'attribute': 11, 'identifier': 43, '.': 11, 'argument_list': 10, '(': 11, ')': 11, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'default_parameter': 3, '=': 8, 'none': 4, 'true': 1, ':': 3, 'block': 3, 'string': 7, 'string_start': 7, 'string_content': 7, 'string_end': 7, 'assignment': 4, 'binary_operator': 2, '-': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'escape_sequence': 2, '+': 1, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9104640147292403,0.9003027011678756,"(tensor([0.9751]), tensor([0.9931]), tensor([0.9840]), tensor([0.9913]))"
"181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs):
185         """"""
186         :param figure: whether to plot in a separate figure
187         :param kwargs: will be passed to plt.scatter()
188 
189         :return:  the resulting figure object or None
190         """"""
191         fig = None
192         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
193         if titleAdd is not None:
194             title += ""\n"" + titleAdd
195         if figure:
196             fig = plt.figure(title.replace(""\n"", "" ""))
197         y_range = [min(self.y_true), max(self.y_true)]
198         plt.scatter(self.y_true, self.y_predicted, **kwargs)
199         plt.plot(y_range, y_range, 'k-', lw=2, label=""_not in legend"", color=""r"")
200         plt.xlabel(""ground truth"")
201         plt.ylabel(""prediction"")
202         plt.title(title)
203         return fig
204 
205     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs):
","181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
185         """"""
186         :param figure: whether to plot in a separate figure and return that figure
187         :param titleAdd: a string to be added to the title in a second line
188         :param kwargs: parameters to be passed on to plt.scatter()
189 
190         :return:  the resulting figure object or None
191         """"""
192         fig = None
193         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
194         if titleAdd is not None:
195             title += ""\n"" + titleAdd
196         if figure:
197             fig = plt.figure(title.replace(""\n"", "" ""))
198         y_range = [min(self.y_true), max(self.y_true)]
199         plt.scatter(self.y_true, self.y_predicted, **kwargs)
200         plt.plot(y_range, y_range, 'k-', lw=2, label=""_not in legend"", color=""r"")
201         plt.xlabel(""ground truth"")
202         plt.ylabel(""prediction"")
203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","Before: 184
After: 184",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1626,"{'module': 1, 'expression_statement': 12, 'call': 10, 'attribute': 12, 'identifier': 48, '.': 12, 'argument_list': 10, '(': 11, 'string': 11, 'string_start': 11, 'string_content': 11, 'string_end': 11, ')': 11, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 12, 'default_parameter': 2, '=': 9, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 2, ':': 3, 'block': 3, 'assignment': 4, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 1, '[': 1, ']': 1, 'dictionary_splat': 1, 'keyword_argument': 3, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6769429237275415,0.6620022339607192,"(tensor([0.9409]), tensor([0.9701]), tensor([0.9553]), tensor([0.9671]))"
"181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs):
185         """"""
186         :param figure: whether to plot in a separate figure
187         :param kwargs: will be passed to plt.scatter()
188 
189         :return:  the resulting figure object or None
190         """"""
191         fig = None
192         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
193         if titleAdd is not None:
194             title += ""\n"" + titleAdd
195         if figure:
196             fig = plt.figure(title.replace(""\n"", "" ""))
197         y_range = [min(self.y_true), max(self.y_true)]
198         plt.scatter(self.y_true, self.y_predicted, **kwargs)
199         plt.plot(y_range, y_range, 'k-', lw=2, label=""_not in legend"", color=""r"")
200         plt.xlabel(""ground truth"")
201         plt.ylabel(""prediction"")
202         plt.title(title)
203         return fig
204 
205     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs):
","181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
185         """"""
186         :param figure: whether to plot in a separate figure and return that figure
187         :param titleAdd: a string to be added to the title in a second line
188         :param kwargs: parameters to be passed on to plt.scatter()
189 
190         :return:  the resulting figure object or None
191         """"""
192         fig = None
193         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
194         if titleAdd is not None:
195             title += ""\n"" + titleAdd
196         if figure:
197             fig = plt.figure(title.replace(""\n"", "" ""))
198         y_range = [min(self.y_true), max(self.y_true)]
199         plt.scatter(self.y_true, self.y_predicted, **kwargs)
200         plt.plot(y_range, y_range, 'k-', lw=2, label=""_not in legend"", color=""r"")
201         plt.xlabel(""ground truth"")
202         plt.ylabel(""prediction"")
203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","Before: 186, 187
After: 186, 187, 188",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1645,"{'module': 1, 'expression_statement': 12, 'call': 10, 'attribute': 12, 'identifier': 48, '.': 12, 'argument_list': 10, '(': 11, 'string': 11, 'string_start': 11, 'string_content': 11, 'string_end': 11, ')': 11, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 12, 'default_parameter': 2, '=': 9, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 2, ':': 3, 'block': 3, 'assignment': 4, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 1, '[': 1, ']': 1, 'dictionary_splat': 1, 'keyword_argument': 3, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6769429237275415,0.6620022339607192,"(tensor([0.9409]), tensor([0.9701]), tensor([0.9553]), tensor([0.9671]))"
"202         plt.title(title)
203         return fig
204 
205     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs):
206         """"""
207         :param figure: whether to plot in a separate figure
208         :param cmap: value for corresponding parameter of plt.imshow() or None
209         :param bins: how many bins to use for construncting the heatmap
210         :param titleAdd: a string to add to the title (on a second line)
211         :param kwargs: will be passed to plt.imshow()
212 
213         :return:  the resulting figure object or None
214         """"""
215         fig = None
216         title = ""Heat Map of Ground Truth vs. Predicted Values""
217         if titleAdd:
218             title += ""\n"" + titleAdd
219         if figure:
220             fig = plt.figure(title.replace(""\n"", "" ""))
221         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
222         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
223         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
224         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
225         if cmap is None:
226             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
227         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
228 
229         plt.xlabel(""ground truth"")
230         plt.ylabel(""prediction"")
231         plt.title(title)
232         return fig
233 
234 
","203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
207         """"""
208         :param figure: whether to plot in a separate figure and return that figure
209         :param cmap: the colour map to use (see corresponding parameter of plt.imshow); if None use colour map from white to red
210         :param bins: how many bins to use for constructing the heatmap
211         :param titleAdd: a string to add to the title (on a second line)
212         :param kwargs: will be passed to plt.imshow()
213 
214         :return:  the resulting figure object or None
215         """"""
216         fig = None
217         title = ""Heat Map of Ground Truth vs. Predicted Values""
218         if titleAdd:
219             title += ""\n"" + titleAdd
220         if figure:
221             fig = plt.figure(title.replace(""\n"", "" ""))
222         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
223         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
224         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
225         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
226         if cmap is None:
227             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
228         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
229 
230         plt.xlabel(""ground truth"")
231         plt.ylabel(""prediction"")
232         plt.title(title)
233         return fig
234 
235 
","Before: 205
After: 206",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1858,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 17, 'identifier': 87, '.': 17, 'argument_list': 16, '(': 20, ')': 20, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 35, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 12, 'dictionary_splat_pattern': 1, '**': 2, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, '[': 7, ']': 7, 'keyword_argument': 10, 'float': 2, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'tuple': 3, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7361933106895228,0.7275901918920895,"(tensor([0.9624]), tensor([0.9710]), tensor([0.9667]), tensor([0.9701]))"
"202         plt.title(title)
203         return fig
204 
205     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs):
206         """"""
207         :param figure: whether to plot in a separate figure
208         :param cmap: value for corresponding parameter of plt.imshow() or None
209         :param bins: how many bins to use for construncting the heatmap
210         :param titleAdd: a string to add to the title (on a second line)
211         :param kwargs: will be passed to plt.imshow()
212 
213         :return:  the resulting figure object or None
214         """"""
215         fig = None
216         title = ""Heat Map of Ground Truth vs. Predicted Values""
217         if titleAdd:
218             title += ""\n"" + titleAdd
219         if figure:
220             fig = plt.figure(title.replace(""\n"", "" ""))
221         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
222         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
223         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
224         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
225         if cmap is None:
226             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
227         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
228 
229         plt.xlabel(""ground truth"")
230         plt.ylabel(""prediction"")
231         plt.title(title)
232         return fig
233 
234 
","203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
207         """"""
208         :param figure: whether to plot in a separate figure and return that figure
209         :param cmap: the colour map to use (see corresponding parameter of plt.imshow); if None use colour map from white to red
210         :param bins: how many bins to use for constructing the heatmap
211         :param titleAdd: a string to add to the title (on a second line)
212         :param kwargs: will be passed to plt.imshow()
213 
214         :return:  the resulting figure object or None
215         """"""
216         fig = None
217         title = ""Heat Map of Ground Truth vs. Predicted Values""
218         if titleAdd:
219             title += ""\n"" + titleAdd
220         if figure:
221             fig = plt.figure(title.replace(""\n"", "" ""))
222         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
223         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
224         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
225         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
226         if cmap is None:
227             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
228         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
229 
230         plt.xlabel(""ground truth"")
231         plt.ylabel(""prediction"")
232         plt.title(title)
233         return fig
234 
235 
","Before: 207, 208, 209
After: 208, 209, 210",fix eval_stats_regression.py -- a/src/sensai/evaluation/eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1877,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 17, 'identifier': 87, '.': 17, 'argument_list': 16, '(': 20, ')': 20, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 35, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 12, 'dictionary_splat_pattern': 1, '**': 2, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, '[': 7, ']': 7, 'keyword_argument': 10, 'float': 2, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'tuple': 3, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7361933106895228,0.7275901918920895,"(tensor([0.9624]), tensor([0.9710]), tensor([0.9667]), tensor([0.9701]))"
"145                 resultWriter = resultWriter.childWithAddedPrefix(addedFilenamePrefix)
146             return self.__class__(showPlots=self.showPlots, resultWriter=resultWriter)
147 
148     def createEvaluator(self, model: TModel = None, isRegression: bool = None) -> TEvaluator:
149         """"""
150         Creates an evaluator holding the current input-output data
151 
152         :param model: the model for which to create an evaluator (just for reading off regression or classification,
153             the resulting evaluator will work on other models as well)
154         :param isRegression: whether to create a regression model evaluator. Either this or model have to be specified
155         :return: an evaluator
156         """"""
157         return createVectorModelEvaluator(self.inputOutputData, model=model, isRegression=isRegression,
158                                          **self.evaluatorParams)
159 
160     def createCrossValidator(self, model: TModel = None, isRegression: bool = None) -> TCrossValidator:
","145                 resultWriter = resultWriter.childWithAddedPrefix(addedFilenamePrefix)
146             return self.__class__(showPlots=self.showPlots, resultWriter=resultWriter)
147 
148     def createEvaluator(self, model: TModel = None, isRegression: bool = None) -> TEvaluator:
149         """"""
150         Creates an evaluator holding the current input-output data
151 
152         :param model: the model for which to create an evaluator (just for reading off regression or classification,
153             the resulting evaluator will work on other models as well)
154         :param isRegression: whether to create a regression model evaluator. Either this or model have to be specified
155         :return: an evaluator
156         """"""
157         return createVectorModelEvaluator(self.inputOutputData, model=model, isRegression=isRegression, **self.evaluatorParams)
158 
159     def createCrossValidator(self, model: TModel = None, isRegression: bool = None) -> TCrossValidator:
","Before: 157, 158
After: 157",add usecrossvalidation to eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1477,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 27, '=': 7, 'call': 3, 'attribute': 5, '.': 5, 'argument_list': 3, '(': 4, ')': 4, 'return_statement': 2, 'return': 2, 'keyword_argument': 4, ',': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 2, ':': 3, 'type': 3, 'none': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'dictionary_splat': 1, '**': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9638749749879476,0.9637012646902907,"(tensor([0.9935]), tensor([0.9927]), tensor([0.9931]), tensor([0.9928]))"
"157         return createVectorModelEvaluator(self.inputOutputData, model=model, isRegression=isRegression,
158                                          **self.evaluatorParams)
159 
160     def createCrossValidator(self, model: TModel = None, isRegression: bool = None) -> TCrossValidator:
161         """"""
162         Creates a cross-validator holding the current input-output data
163 
164         :param model: the model for which to create a cross-validator (just for reading off regression or classification,
165             the resulting evaluator will work on other models as well)
166         :param isRegression: whether to create a regression model cross-validator. Either this or model have to be specified
167         :return: an evaluator
168         """"""
169         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression,
170                                          **self.crossValidatorParams)
171 
172     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
","156         """"""
157         return createVectorModelEvaluator(self.inputOutputData, model=model, isRegression=isRegression, **self.evaluatorParams)
158 
159     def createCrossValidator(self, model: TModel = None, isRegression: bool = None) -> TCrossValidator:
160         """"""
161         Creates a cross-validator holding the current input-output data
162 
163         :param model: the model for which to create a cross-validator (just for reading off regression or classification,
164             the resulting evaluator will work on other models as well)
165         :param isRegression: whether to create a regression model cross-validator. Either this or model have to be specified
166         :return: an evaluator
167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
","Before: 169, 170
After: 168",add usecrossvalidation to eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1538,"{'module': 1, 'return_statement': 2, 'return': 2, 'call': 2, 'identifier': 25, 'argument_list': 2, '(': 3, 'attribute': 4, '.': 4, ',': 8, 'keyword_argument': 4, '=': 6, 'dictionary_splat': 2, '**': 2, ')': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_default_parameter': 2, ':': 3, 'type': 3, 'none': 2, '->': 1, 'block': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7851512346943341,0.7861513432945515,"(tensor([0.9650]), tensor([0.9635]), tensor([0.9643]), tensor([0.9636]))"
"221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
225         """"""
226         Compares several models via cross-validation
227 
228         :param models: the models to compare
229         :param resultWriter: a writer with which to store results of the comparison
230         :return: a data frame containing evaluation metrics on all models
231         """"""
232         statsList = []
233         for model in models:
234             crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
235             stats = crossValidationResult.getEvalStatsCollection().aggStats()
236             stats[""modelName""] = model.getName()
237             statsList.append(stats)
238         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
239         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
240         log.info(strResults)
241         if resultWriter is not None:
242             resultWriter.writeTextFile(""model-comparison-results"", strResults)
243         return resultsDF
244 
245     # TODO: a parameter predictedVarName seemed to have been present at some point but has now disappeared,
","219         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
220         return crossValidationData
221 
222     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
223         """"""
224         Compares several models via simple evaluation or cross-validation
225 
226         :param models: the models to compare
227         :param resultWriter: a writer with which to store results of the comparison
228         :return: a data frame containing evaluation metrics on all models
229         """"""
230         statsList = []
231         for model in models:
232             if useCrossValidation:
233                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
234                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
235             else:
236                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter).getEvalStats()
237                 statsDict = evalStats.getAll()
238             statsDict[""modelName""] = model.getName()
239             statsList.append(statsDict)
240         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
241         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
242         log.info(strResults)
243         if resultWriter is not None:
244             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
245             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
246         return resultsDF
247 
248     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 224
After: 222",add usecrossvalidation to eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2158,"{'module': 1, 'expression_statement': 11, 'call': 11, 'attribute': 12, 'identifier': 53, '.': 12, 'argument_list': 11, '(': 12, ',': 6, 'keyword_argument': 3, '=': 10, ')': 12, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 5, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'block': 3, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'assignment': 6, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.505033404374703,0.5031212456192484,"(tensor([0.9200]), tensor([0.9310]), tensor([0.9255]), tensor([0.9299]))"
"221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
225         """"""
226         Compares several models via cross-validation
227 
228         :param models: the models to compare
229         :param resultWriter: a writer with which to store results of the comparison
230         :return: a data frame containing evaluation metrics on all models
231         """"""
232         statsList = []
233         for model in models:
234             crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
235             stats = crossValidationResult.getEvalStatsCollection().aggStats()
236             stats[""modelName""] = model.getName()
237             statsList.append(stats)
238         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
239         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
240         log.info(strResults)
241         if resultWriter is not None:
242             resultWriter.writeTextFile(""model-comparison-results"", strResults)
243         return resultsDF
244 
245     # TODO: a parameter predictedVarName seemed to have been present at some point but has now disappeared,
","219         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
220         return crossValidationData
221 
222     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
223         """"""
224         Compares several models via simple evaluation or cross-validation
225 
226         :param models: the models to compare
227         :param resultWriter: a writer with which to store results of the comparison
228         :return: a data frame containing evaluation metrics on all models
229         """"""
230         statsList = []
231         for model in models:
232             if useCrossValidation:
233                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
234                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
235             else:
236                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter).getEvalStats()
237                 statsDict = evalStats.getAll()
238             statsDict[""modelName""] = model.getName()
239             statsList.append(statsDict)
240         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
241         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
242         log.info(strResults)
243         if resultWriter is not None:
244             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
245             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
246         return resultsDF
247 
248     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 226
After: 224",add usecrossvalidation to eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2169,"{'module': 1, 'expression_statement': 11, 'call': 11, 'attribute': 12, 'identifier': 53, '.': 12, 'argument_list': 11, '(': 12, ',': 6, 'keyword_argument': 3, '=': 10, ')': 12, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 5, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'block': 3, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'assignment': 6, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.505033404374703,0.5031212456192484,"(tensor([0.9200]), tensor([0.9310]), tensor([0.9255]), tensor([0.9299]))"
"221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
225         """"""
226         Compares several models via cross-validation
227 
228         :param models: the models to compare
229         :param resultWriter: a writer with which to store results of the comparison
230         :return: a data frame containing evaluation metrics on all models
231         """"""
232         statsList = []
233         for model in models:
234             crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
235             stats = crossValidationResult.getEvalStatsCollection().aggStats()
236             stats[""modelName""] = model.getName()
237             statsList.append(stats)
238         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
239         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
240         log.info(strResults)
241         if resultWriter is not None:
242             resultWriter.writeTextFile(""model-comparison-results"", strResults)
243         return resultsDF
244 
245     # TODO: a parameter predictedVarName seemed to have been present at some point but has now disappeared,
","219         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
220         return crossValidationData
221 
222     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
223         """"""
224         Compares several models via simple evaluation or cross-validation
225 
226         :param models: the models to compare
227         :param resultWriter: a writer with which to store results of the comparison
228         :return: a data frame containing evaluation metrics on all models
229         """"""
230         statsList = []
231         for model in models:
232             if useCrossValidation:
233                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
234                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
235             else:
236                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter).getEvalStats()
237                 statsDict = evalStats.getAll()
238             statsDict[""modelName""] = model.getName()
239             statsList.append(statsDict)
240         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
241         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
242         log.info(strResults)
243         if resultWriter is not None:
244             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
245             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
246         return resultsDF
247 
248     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 234, 235, 236, 237
After: 232, 233, 234, 235, 236, 237, 238, 239",add usecrossvalidation to eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2195,"{'module': 1, 'expression_statement': 11, 'call': 11, 'attribute': 12, 'identifier': 53, '.': 12, 'argument_list': 11, '(': 12, ',': 6, 'keyword_argument': 3, '=': 10, ')': 12, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 5, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'block': 3, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'assignment': 6, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.505033404374703,0.5031212456192484,"(tensor([0.9200]), tensor([0.9310]), tensor([0.9255]), tensor([0.9299]))"
"221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
225         """"""
226         Compares several models via cross-validation
227 
228         :param models: the models to compare
229         :param resultWriter: a writer with which to store results of the comparison
230         :return: a data frame containing evaluation metrics on all models
231         """"""
232         statsList = []
233         for model in models:
234             crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
235             stats = crossValidationResult.getEvalStatsCollection().aggStats()
236             stats[""modelName""] = model.getName()
237             statsList.append(stats)
238         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
239         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
240         log.info(strResults)
241         if resultWriter is not None:
242             resultWriter.writeTextFile(""model-comparison-results"", strResults)
243         return resultsDF
244 
245     # TODO: a parameter predictedVarName seemed to have been present at some point but has now disappeared,
","219         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
220         return crossValidationData
221 
222     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
223         """"""
224         Compares several models via simple evaluation or cross-validation
225 
226         :param models: the models to compare
227         :param resultWriter: a writer with which to store results of the comparison
228         :return: a data frame containing evaluation metrics on all models
229         """"""
230         statsList = []
231         for model in models:
232             if useCrossValidation:
233                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
234                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
235             else:
236                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter).getEvalStats()
237                 statsDict = evalStats.getAll()
238             statsDict[""modelName""] = model.getName()
239             statsList.append(statsDict)
240         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
241         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
242         log.info(strResults)
243         if resultWriter is not None:
244             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
245             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
246         return resultsDF
247 
248     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 242
After: 244, 245",add usecrossvalidation to eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2321,"{'module': 1, 'expression_statement': 11, 'call': 11, 'attribute': 12, 'identifier': 53, '.': 12, 'argument_list': 11, '(': 12, ',': 6, 'keyword_argument': 3, '=': 10, ')': 12, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 5, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, 'typed_default_parameter': 1, 'none': 2, '->': 1, 'block': 3, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'assignment': 6, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 1, 'escape_sequence': 1, 'interpolation': 1, '{': 1, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.505033404374703,0.5031212456192484,"(tensor([0.9200]), tensor([0.9310]), tensor([0.9255]), tensor([0.9299]))"
"240         log.info(strResults)
241         if resultWriter is not None:
242             resultWriter.writeTextFile(""model-comparison-results"", strResults)
243         return resultsDF
244 
245     # TODO: a parameter predictedVarName seemed to have been present at some point but has now disappeared,
246     #   should we resuscitate it?
247     def createPlots(self, data: Union[TEvalData, TCrossValData], showPlots=True, resultWriter: Optional[ResultWriter] = None, subtitlePrefix: str = """"):
248         """"""
249         Creates default plots that visualise the results in the given evaluation data
","245             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
246         return resultsDF
247 
248     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
249         """"""
250         Compares several models via cross-validation
251 
252         :param models: the models to compare
253         :param resultWriter: a writer with which to store results of the comparison
254         :return: a data frame containing evaluation metrics on all models
255         """"""
256         return self.compareModels(models, resultWriter=resultWriter, useCrossValidation=True)
257 
258     def createPlots(self, data: Union[TEvalData, TCrossValData], showPlots=True, resultWriter: Optional[ResultWriter] = None, subtitlePrefix: str = """"):
","Before: 245, 246
After: 248, 249, 250, 251, 252, 253, 254, 255, 256, 257",add usecrossvalidation to eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2325,"{'module': 1, 'expression_statement': 2, 'call': 2, 'attribute': 2, 'identifier': 20, '.': 2, 'argument_list': 2, '(': 3, ')': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 2, ':': 5, 'block': 2, 'string': 2, 'string_start': 3, 'string_content': 1, 'string_end': 2, ',': 6, 'return_statement': 1, 'return': 1, 'comment': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 6, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'default_parameter': 1, '=': 3, 'true': 1, 'typed_default_parameter': 2, 'ERROR': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2926405616650702,0.281290826153482,"(tensor([0.8248]), tensor([0.8195]), tensor([0.8222]), tensor([0.8201]))"
"19     def __init__(self, dataFrame: pd.DataFrame, normalisationMode: NormalisationMode):
20         self.normalisationMode = normalisationMode
21         self.scale, self.translate = self._computeScalingParams(dataFrame.values, normalisationMode)
22         self.dimensionNames = list(dataFrame.columns)
23 
24     @staticmethod
25     def _computeScalingParams(rawArray: np.ndarray, normalisationMode: NormalisationMode):
26         """"""
27         :param rawArray: numpy array containing raw data
28         :param normalisationMode: the normalization mode (0=none, 1=by maximum in entire data set, 2=by separate maximum in each column)
","19     def __init__(self, dataFrame: pd.DataFrame, normalisationMode: NormalisationMode):
20         self.normalisationMode = normalisationMode
21         self.scale, self.translate = self._computeScalingParams(dataFrame.values, normalisationMode)
22         self.dimensionNames = list(dataFrame.columns)
23 
24     @classmethod
25     def _computeScalingParams(cls, rawArray: np.ndarray, normalisationMode: NormalisationMode):
26         """"""
27         :param rawArray: numpy array containing raw data
28         :param normalisationMode: the normalization mode (0=none, 1=by maximum in entire data set, 2=by separate maximum in each column)
","Before: 24, 25
After: 24, 25",move vectordatascaler to its own class,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/normalisation.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,173,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 38, 'parameters': 2, '(': 4, ',': 5, 'typed_parameter': 4, ':': 8, 'type': 5, 'attribute': 9, '.': 9, ')': 4, 'block': 2, 'expression_statement': 4, 'assignment': 4, '=': 3, 'pattern_list': 1, 'call': 2, 'argument_list': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'ERROR': 2, 'string_start': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 48, 'name': '__init__', 'long_name': '__init__( self , dataFrame : pd . DataFrame , normalisationMode : NormalisationMode )', 'start_line': 19, 'end_line': 22, 'full_parameters': ['self', ' dataFrame : pd . DataFrame', ' normalisationMode : NormalisationMode'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/normalisation.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 48, 'name': '__init__', 'long_name': '__init__( self , dataFrame : pd . DataFrame , normalisationMode : NormalisationMode )', 'start_line': 19, 'end_line': 22, 'full_parameters': ['self', ' dataFrame : pd . DataFrame', ' normalisationMode : NormalisationMode'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/normalisation.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9352076177156146,0.9331545801208964,"(tensor([0.9877]), tensor([0.9953]), tensor([0.9915]), tensor([0.9945]))"
"22         self.dimensionNames = list(dataFrame.columns)
23 
24     @staticmethod
25     def _computeScalingParams(rawArray: np.ndarray, normalisationMode: NormalisationMode):
26         """"""
27         :param rawArray: numpy array containing raw data
28         :param normalisationMode: the normalization mode (0=none, 1=by maximum in entire data set, 2=by separate maximum in each column)
29         """"""
30         if len(rawArray.shape) != 2:
31             raise ValueError(""Only 2D arrays are supported"")
32         dim = rawArray.shape[1]
33         translate = None
34         if normalisationMode == NormalisationMode.NONE:
35             scale = None
36         elif normalisationMode == NormalisationMode.MAX_ALL:
37             scale = np.ones(dim) * np.max(rawArray)
38         elif normalisationMode == NormalisationMode.MAX_BY_COLUMN:
39             scale = np.ones(dim)
40             for i in range(dim):
41                 scale[i] = np.max(np.abs(rawArray[:, i]))
42         elif normalisationMode == NormalisationMode.STANDARDISED:
43             standardScaler = sklearn.preprocessing.StandardScaler()
44             standardScaler.fit(rawArray)
45             translate = standardScaler.mean_
46             scale = standardScaler.scale_
47         else:
48             raise Exception(""Unknown normalization mode"")
49         return scale, translate
50 
51     @staticmethod
","22         self.dimensionNames = list(dataFrame.columns)
23 
24     @classmethod
25     def _computeScalingParams(cls, rawArray: np.ndarray, normalisationMode: NormalisationMode):
26         """"""
27         :param rawArray: numpy array containing raw data
28         :param normalisationMode: the normalization mode (0=none, 1=by maximum in entire data set, 2=by separate maximum in each column)
29         """"""
30         translate = None
31         scale = None
32         if normalisationMode != NormalisationMode.NONE:
33             if len(rawArray.shape) != 2:
34                 raise ValueError(f""Only 2D arrays are supported by {cls.__name__} with mode {normalisationMode}"")
35             dim = rawArray.shape[1]
36             if normalisationMode == NormalisationMode.MAX_ALL:
37                 scale = np.ones(dim) * np.max(rawArray)
38             elif normalisationMode == NormalisationMode.MAX_BY_COLUMN:
39                 scale = np.ones(dim)
40                 for i in range(dim):
41                     scale[i] = np.max(np.abs(rawArray[:, i]))
42             elif normalisationMode == NormalisationMode.STANDARDISED:
43                 standardScaler = sklearn.preprocessing.StandardScaler()
44                 standardScaler.fit(rawArray)
45                 translate = standardScaler.mean_
46                 scale = standardScaler.scale_
47             else:
48                 raise Exception(""Unknown normalization mode"")
49         return scale, translate
50 
51     @staticmethod
","Before: 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48
After: 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48",move vectordatascaler to its own class,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/normalisation.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,216,"{'module': 1, 'expression_statement': 12, 'assignment': 10, 'attribute': 19, 'identifier': 71, '.': 19, '=': 10, 'call': 12, 'argument_list': 12, '(': 13, ')': 13, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 11, 'type': 2, ',': 3, 'block': 8, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 5, '!=': 1, 'integer': 2, 'raise_statement': 2, 'raise': 2, 'subscript': 3, '[': 3, ']': 3, 'none': 2, '==': 4, 'elif_clause': 3, 'elif': 3, 'binary_operator': 1, '*': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'slice': 1, 'else_clause': 1, 'else': 1, 'return_statement': 1, 'return': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 48, 'name': '__init__', 'long_name': '__init__( self , dataFrame : pd . DataFrame , normalisationMode : NormalisationMode )', 'start_line': 19, 'end_line': 22, 'full_parameters': ['self', ' dataFrame : pd . DataFrame', ' normalisationMode : NormalisationMode'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/normalisation.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 48, 'name': '__init__', 'long_name': '__init__( self , dataFrame : pd . DataFrame , normalisationMode : NormalisationMode )', 'start_line': 19, 'end_line': 22, 'full_parameters': ['self', ' dataFrame : pd . DataFrame', ' normalisationMode : NormalisationMode'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/normalisation.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8272233944394548,0.8227987772874831,"(tensor([0.9546]), tensor([0.9704]), tensor([0.9624]), tensor([0.9688]))"
"8 import torch
9 from torch import nn
10 from torch.nn import functional as F
11 
12 from .torch_data import TensorScaler, VectorDataUtil, ClassificationVectorDataUtil, TorchDataSet, \
13     TorchDataSetProviderFromDataUtil, TorchDataSetProvider
14 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams, TrainingInfo
15 from ..normalisation import NormalisationMode
16 from ..util.dtype import toFloatArray
17 from ..util.string import objectRepr, ToStringMixin
","8 import torch
9 from torch import nn
10 from torch.nn import functional as F
11 
12 from .torch_data import TensorScaler, VectorDataUtil, ClassificationVectorDataUtil, TorchDataSet, \
13     TorchDataSetProviderFromDataUtil, TorchDataSetProvider, Tensoriser, TorchDataSetFromDataFrames
14 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams, TrainingInfo
15 from ..normalisation import NormalisationMode
16 from ..util.dtype import toFloatArray
17 from ..util.string import objectRepr, ToStringMixin
","Before: 13
After: 13",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,105,"{'module': 1, 'import_statement': 1, 'import': 7, 'dotted_name': 22, 'identifier': 25, 'import_from_statement': 6, 'from': 6, '.': 8, 'aliased_import': 1, 'as': 1, 'relative_import': 4, 'import_prefix': 4, ',': 9, 'line_continuation': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9192507668025022,0.9180240235658439,"(tensor([0.9860]), tensor([0.9936]), tensor([0.9898]), tensor([0.9929]))"
"61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: torch.Tensor, numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         results = []
74         self._enableMCDropout(True, pMCDropoutOverride=p)
75         try:
76             for i in range(numSamples):
77                 y = self(x)
78                 results.append(y)
79         finally:
80             self._enableMCDropout(False)
81         results = torch.stack(results)
82         mean = torch.mean(results, 0)
83         stddev = torch.std(results, 0, unbiased=False)
84         return mean, stddev
85 
86 
","61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: Union[torch.Tensor, Sequence[torch.Tensor]], numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input (a tensor or tuple/list of tensors)
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         if type(x) not in (tuple, list):
74             x = [x]
75         results = []
76         self._enableMCDropout(True, pMCDropoutOverride=p)
77         try:
78             for i in range(numSamples):
79                 y = self(*x)
80                 results.append(y)
81         finally:
82             self._enableMCDropout(False)
83         results = torch.stack(results)
84         mean = torch.mean(results, 0)
85         stddev = torch.std(results, 0, unbiased=False)
86         return mean, stddev
87 
88 
","Before: 64
After: 64",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,545,"{'module': 1, 'expression_statement': 11, 'assignment': 7, 'attribute': 11, 'identifier': 49, '.': 11, '=': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 9, 'typed_parameter': 1, ':': 5, 'type': 4, 'default_parameter': 1, 'none': 1, ')': 9, '->': 1, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'list': 1, 'call': 8, 'argument_list': 8, 'true': 1, 'keyword_argument': 2, 'try_statement': 1, 'try': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'finally_clause': 1, 'finally': 1, 'false': 2, 'integer': 2, 'return_statement': 1, 'return': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7196549426120102,0.7190822136823042,"(tensor([0.9518]), tensor([0.9779]), tensor([0.9647]), tensor([0.9752]))"
"61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: torch.Tensor, numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         results = []
74         self._enableMCDropout(True, pMCDropoutOverride=p)
75         try:
76             for i in range(numSamples):
77                 y = self(x)
78                 results.append(y)
79         finally:
80             self._enableMCDropout(False)
81         results = torch.stack(results)
82         mean = torch.mean(results, 0)
83         stddev = torch.std(results, 0, unbiased=False)
84         return mean, stddev
85 
86 
","61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: Union[torch.Tensor, Sequence[torch.Tensor]], numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input (a tensor or tuple/list of tensors)
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         if type(x) not in (tuple, list):
74             x = [x]
75         results = []
76         self._enableMCDropout(True, pMCDropoutOverride=p)
77         try:
78             for i in range(numSamples):
79                 y = self(*x)
80                 results.append(y)
81         finally:
82             self._enableMCDropout(False)
83         results = torch.stack(results)
84         mean = torch.mean(results, 0)
85         stddev = torch.std(results, 0, unbiased=False)
86         return mean, stddev
87 
88 
","Before: 68
After: 68, 73, 74",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,577,"{'module': 1, 'expression_statement': 11, 'assignment': 7, 'attribute': 11, 'identifier': 49, '.': 11, '=': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 9, 'typed_parameter': 1, ':': 5, 'type': 4, 'default_parameter': 1, 'none': 1, ')': 9, '->': 1, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'list': 1, 'call': 8, 'argument_list': 8, 'true': 1, 'keyword_argument': 2, 'try_statement': 1, 'try': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'finally_clause': 1, 'finally': 1, 'false': 2, 'integer': 2, 'return_statement': 1, 'return': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7196549426120102,0.7190822136823042,"(tensor([0.9518]), tensor([0.9779]), tensor([0.9647]), tensor([0.9752]))"
"61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: torch.Tensor, numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         results = []
74         self._enableMCDropout(True, pMCDropoutOverride=p)
75         try:
76             for i in range(numSamples):
77                 y = self(x)
78                 results.append(y)
79         finally:
80             self._enableMCDropout(False)
81         results = torch.stack(results)
82         mean = torch.mean(results, 0)
83         stddev = torch.std(results, 0, unbiased=False)
84         return mean, stddev
85 
86 
","61         self._applyMCDropout = enabled
62         self._pMCDropoutOverride = pMCDropoutOverride
63 
64     def inferMCDropout(self, x: Union[torch.Tensor, Sequence[torch.Tensor]], numSamples, p=None) -> Tuple[torch.Tensor, torch.Tensor]:
65         """"""
66         Applies inference using MC-Dropout, drawing the given number of samples.
67 
68         :param x: the model input (a tensor or tuple/list of tensors)
69         :param numSamples: the number of samples to draw with MC-Dropout
70         :param p: the dropout probability to apply, overriding the probability specified by the model's forward method; if None, use model's default
71         :return: a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
72         """"""
73         if type(x) not in (tuple, list):
74             x = [x]
75         results = []
76         self._enableMCDropout(True, pMCDropoutOverride=p)
77         try:
78             for i in range(numSamples):
79                 y = self(*x)
80                 results.append(y)
81         finally:
82             self._enableMCDropout(False)
83         results = torch.stack(results)
84         mean = torch.mean(results, 0)
85         stddev = torch.std(results, 0, unbiased=False)
86         return mean, stddev
87 
88 
","Before: 77
After: 79",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,600,"{'module': 1, 'expression_statement': 11, 'assignment': 7, 'attribute': 11, 'identifier': 49, '.': 11, '=': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 9, 'typed_parameter': 1, ':': 5, 'type': 4, 'default_parameter': 1, 'none': 1, ')': 9, '->': 1, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'list': 1, 'call': 8, 'argument_list': 8, 'true': 1, 'keyword_argument': 2, 'try_statement': 1, 'try': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'finally_clause': 1, 'finally': 1, 'false': 2, 'integer': 2, 'return_statement': 1, 'return': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7196549426120102,0.7190822136823042,"(tensor([0.9518]), tensor([0.9779]), tensor([0.9647]), tensor([0.9752]))"
"175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","177         if modelBytes is not None:
178             self.setModuleBytes(modelBytes)
179 
180     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
181             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
182             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
183         """"""
184         Applies the model to the given input tensor and returns the result (normalized)
185 
186         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
187             (if the model accepts more than one input).
188             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
189         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
190         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
191         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
192         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
193         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
194         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
195 
196         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
197             containing standard deviations
198         """"""
199         def extract(z):
200             if scaleOutput:
201                 z = self.scaledOutput(z)
202             if self._isCudaEnabled():
203                 z = z.cpu()
204             z = z.detach()
205             if asNumpy:
206                 z = z.numpy()
207             return z
208 
209         model = self.getTorchModule()
210         model.eval()
211 
212         if isinstance(X, TorchDataSet):
213             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
214         elif isinstance(X, np.ndarray):
215             X = toFloatArray(X)
216             X = torch.from_numpy(X).float()
217 
218         if type(X) not in (list, tuple):
219             inputs = [X]
220         else:
221             inputs = X
222 
223         if self._isCudaEnabled():
224             torch.cuda.set_device(self._gpu)
225             inputs = [t.cuda() for t in inputs]
226         if scaleInput:
227             inputs = [self.inputScaler.normalise(t) for t in inputs]
228         if createBatch:
229             inputs = [t.view(1, *X.size()) for t in inputs]
230 
231         maxValue = max([t.max().item() for t in inputs])
232         if maxValue > 2:
233             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
234 
235         if mcDropoutSamples is None:
236             y = model(*inputs)
237             return extract(y)
238         else:
239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 178
After: 180",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1584,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 129, 'is not': 2, 'none': 4, ':': 21, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 16, 'generic_type': 4, 'type_parameter': 4, '[': 4, ']': 4, 'typed_default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889195627668067,0.6847806938562975,"(tensor([0.9348]), tensor([0.9517]), tensor([0.9432]), tensor([0.9500]))"
"175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","177         if modelBytes is not None:
178             self.setModuleBytes(modelBytes)
179 
180     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
181             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
182             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
183         """"""
184         Applies the model to the given input tensor and returns the result (normalized)
185 
186         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
187             (if the model accepts more than one input).
188             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
189         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
190         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
191         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
192         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
193         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
194         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
195 
196         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
197             containing standard deviations
198         """"""
199         def extract(z):
200             if scaleOutput:
201                 z = self.scaledOutput(z)
202             if self._isCudaEnabled():
203                 z = z.cpu()
204             z = z.detach()
205             if asNumpy:
206                 z = z.numpy()
207             return z
208 
209         model = self.getTorchModule()
210         model.eval()
211 
212         if isinstance(X, TorchDataSet):
213             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
214         elif isinstance(X, np.ndarray):
215             X = toFloatArray(X)
216             X = torch.from_numpy(X).float()
217 
218         if type(X) not in (list, tuple):
219             inputs = [X]
220         else:
221             inputs = X
222 
223         if self._isCudaEnabled():
224             torch.cuda.set_device(self._gpu)
225             inputs = [t.cuda() for t in inputs]
226         if scaleInput:
227             inputs = [self.inputScaler.normalise(t) for t in inputs]
228         if createBatch:
229             inputs = [t.view(1, *X.size()) for t in inputs]
230 
231         maxValue = max([t.max().item() for t in inputs])
232         if maxValue > 2:
233             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
234 
235         if mcDropoutSamples is None:
236             y = model(*inputs)
237             return extract(y)
238         else:
239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 184, 185
After: 186, 187, 188",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1708,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 129, 'is not': 2, 'none': 4, ':': 21, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 16, 'generic_type': 4, 'type_parameter': 4, '[': 4, ']': 4, 'typed_default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889195627668067,0.6847806938562975,"(tensor([0.9348]), tensor([0.9517]), tensor([0.9432]), tensor([0.9500]))"
"175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","177         if modelBytes is not None:
178             self.setModuleBytes(modelBytes)
179 
180     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
181             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
182             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
183         """"""
184         Applies the model to the given input tensor and returns the result (normalized)
185 
186         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
187             (if the model accepts more than one input).
188             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
189         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
190         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
191         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
192         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
193         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
194         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
195 
196         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
197             containing standard deviations
198         """"""
199         def extract(z):
200             if scaleOutput:
201                 z = self.scaledOutput(z)
202             if self._isCudaEnabled():
203                 z = z.cpu()
204             z = z.detach()
205             if asNumpy:
206                 z = z.numpy()
207             return z
208 
209         model = self.getTorchModule()
210         model.eval()
211 
212         if isinstance(X, TorchDataSet):
213             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
214         elif isinstance(X, np.ndarray):
215             X = toFloatArray(X)
216             X = torch.from_numpy(X).float()
217 
218         if type(X) not in (list, tuple):
219             inputs = [X]
220         else:
221             inputs = X
222 
223         if self._isCudaEnabled():
224             torch.cuda.set_device(self._gpu)
225             inputs = [t.cuda() for t in inputs]
226         if scaleInput:
227             inputs = [self.inputScaler.normalise(t) for t in inputs]
228         if createBatch:
229             inputs = [t.view(1, *X.size()) for t in inputs]
230 
231         maxValue = max([t.max().item() for t in inputs])
232         if maxValue > 2:
233             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
234 
235         if mcDropoutSamples is None:
236             y = model(*inputs)
237             return extract(y)
238         else:
239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 217
After: 225",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1893,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 129, 'is not': 2, 'none': 4, ':': 21, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 16, 'generic_type': 4, 'type_parameter': 4, '[': 4, ']': 4, 'typed_default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889195627668067,0.6847806938562975,"(tensor([0.9348]), tensor([0.9517]), tensor([0.9432]), tensor([0.9500]))"
"175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","177         if modelBytes is not None:
178             self.setModuleBytes(modelBytes)
179 
180     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
181             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
182             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
183         """"""
184         Applies the model to the given input tensor and returns the result (normalized)
185 
186         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
187             (if the model accepts more than one input).
188             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
189         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
190         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
191         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
192         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
193         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
194         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
195 
196         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
197             containing standard deviations
198         """"""
199         def extract(z):
200             if scaleOutput:
201                 z = self.scaledOutput(z)
202             if self._isCudaEnabled():
203                 z = z.cpu()
204             z = z.detach()
205             if asNumpy:
206                 z = z.numpy()
207             return z
208 
209         model = self.getTorchModule()
210         model.eval()
211 
212         if isinstance(X, TorchDataSet):
213             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
214         elif isinstance(X, np.ndarray):
215             X = toFloatArray(X)
216             X = torch.from_numpy(X).float()
217 
218         if type(X) not in (list, tuple):
219             inputs = [X]
220         else:
221             inputs = X
222 
223         if self._isCudaEnabled():
224             torch.cuda.set_device(self._gpu)
225             inputs = [t.cuda() for t in inputs]
226         if scaleInput:
227             inputs = [self.inputScaler.normalise(t) for t in inputs]
228         if createBatch:
229             inputs = [t.view(1, *X.size()) for t in inputs]
230 
231         maxValue = max([t.max().item() for t in inputs])
232         if maxValue > 2:
233             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
234 
235         if mcDropoutSamples is None:
236             y = model(*inputs)
237             return extract(y)
238         else:
239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 219
After: 227",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1914,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 129, 'is not': 2, 'none': 4, ':': 21, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 16, 'generic_type': 4, 'type_parameter': 4, '[': 4, ']': 4, 'typed_default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889195627668067,0.6847806938562975,"(tensor([0.9348]), tensor([0.9517]), tensor([0.9432]), tensor([0.9500]))"
"175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","177         if modelBytes is not None:
178             self.setModuleBytes(modelBytes)
179 
180     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
181             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
182             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
183         """"""
184         Applies the model to the given input tensor and returns the result (normalized)
185 
186         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
187             (if the model accepts more than one input).
188             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
189         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
190         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
191         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
192         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
193         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
194         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
195 
196         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
197             containing standard deviations
198         """"""
199         def extract(z):
200             if scaleOutput:
201                 z = self.scaledOutput(z)
202             if self._isCudaEnabled():
203                 z = z.cpu()
204             z = z.detach()
205             if asNumpy:
206                 z = z.numpy()
207             return z
208 
209         model = self.getTorchModule()
210         model.eval()
211 
212         if isinstance(X, TorchDataSet):
213             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
214         elif isinstance(X, np.ndarray):
215             X = toFloatArray(X)
216             X = torch.from_numpy(X).float()
217 
218         if type(X) not in (list, tuple):
219             inputs = [X]
220         else:
221             inputs = X
222 
223         if self._isCudaEnabled():
224             torch.cuda.set_device(self._gpu)
225             inputs = [t.cuda() for t in inputs]
226         if scaleInput:
227             inputs = [self.inputScaler.normalise(t) for t in inputs]
228         if createBatch:
229             inputs = [t.view(1, *X.size()) for t in inputs]
230 
231         maxValue = max([t.max().item() for t in inputs])
232         if maxValue > 2:
233             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
234 
235         if mcDropoutSamples is None:
236             y = model(*inputs)
237             return extract(y)
238         else:
239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 221
After: 229",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1943,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 129, 'is not': 2, 'none': 4, ':': 21, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 16, 'generic_type': 4, 'type_parameter': 4, '[': 4, ']': 4, 'typed_default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889195627668067,0.6847806938562975,"(tensor([0.9348]), tensor([0.9517]), tensor([0.9432]), tensor([0.9500]))"
"175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","177         if modelBytes is not None:
178             self.setModuleBytes(modelBytes)
179 
180     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
181             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
182             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
183         """"""
184         Applies the model to the given input tensor and returns the result (normalized)
185 
186         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
187             (if the model accepts more than one input).
188             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
189         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
190         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
191         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
192         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
193         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
194         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
195 
196         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
197             containing standard deviations
198         """"""
199         def extract(z):
200             if scaleOutput:
201                 z = self.scaledOutput(z)
202             if self._isCudaEnabled():
203                 z = z.cpu()
204             z = z.detach()
205             if asNumpy:
206                 z = z.numpy()
207             return z
208 
209         model = self.getTorchModule()
210         model.eval()
211 
212         if isinstance(X, TorchDataSet):
213             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
214         elif isinstance(X, np.ndarray):
215             X = toFloatArray(X)
216             X = torch.from_numpy(X).float()
217 
218         if type(X) not in (list, tuple):
219             inputs = [X]
220         else:
221             inputs = X
222 
223         if self._isCudaEnabled():
224             torch.cuda.set_device(self._gpu)
225             inputs = [t.cuda() for t in inputs]
226         if scaleInput:
227             inputs = [self.inputScaler.normalise(t) for t in inputs]
228         if createBatch:
229             inputs = [t.view(1, *X.size()) for t in inputs]
230 
231         maxValue = max([t.max().item() for t in inputs])
232         if maxValue > 2:
233             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
234 
235         if mcDropoutSamples is None:
236             y = model(*inputs)
237             return extract(y)
238         else:
239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 223
After: 231",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1962,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 129, 'is not': 2, 'none': 4, ':': 21, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 16, 'generic_type': 4, 'type_parameter': 4, '[': 4, ']': 4, 'typed_default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889195627668067,0.6847806938562975,"(tensor([0.9348]), tensor([0.9517]), tensor([0.9432]), tensor([0.9500]))"
"175         if modelBytes is not None:
176             self.setModuleBytes(modelBytes)
177 
178     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], asNumpy: bool = True, createBatch: bool = False,
179             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
180             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
181         """"""
182         Applies the model to the given input tensor and returns the result (normalized)
183 
184         :param X: the input tensor (either a batch or, if createBatch=True, a single data point) or data set.
185             If it is a data set, a single tensor will be extracted from it, so the data set must not be too large to be processed at once.
186         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
187         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
188         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
189         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
190         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
191         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
192 
193         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
194             containing standard deviations
195         """"""
196         def extract(z):
197             if scaleOutput:
198                 z = self.scaledOutput(z)
199             if self._isCudaEnabled():
200                 z = z.cpu()
201             z = z.detach()
202             if asNumpy:
203                 z = z.numpy()
204             return z
205 
206         model = self.getTorchModule()
207         model.eval()
208 
209         if isinstance(X, TorchDataSet):
210             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
211         elif isinstance(X, np.ndarray):
212             X = toFloatArray(X)
213             X = torch.from_numpy(X).float()
214 
215         if self._isCudaEnabled():
216             torch.cuda.set_device(self._gpu)
217             X = X.cuda()
218         if scaleInput:
219             X = self.inputScaler.normalise(X)
220         if createBatch:
221             X = X.view(1, *X.size())
222 
223         maxValue = X.max().item()
224         if maxValue > 2:
225             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
226 
227         if mcDropoutSamples is None:
228             y = model(X)
229             return extract(y)
230         else:
231             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
232             return extract(y), extract(stddev)
233 
234     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","177         if modelBytes is not None:
178             self.setModuleBytes(modelBytes)
179 
180     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
181             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
182             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
183         """"""
184         Applies the model to the given input tensor and returns the result (normalized)
185 
186         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
187             (if the model accepts more than one input).
188             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
189         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
190         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
191         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
192         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
193         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
194         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
195 
196         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
197             containing standard deviations
198         """"""
199         def extract(z):
200             if scaleOutput:
201                 z = self.scaledOutput(z)
202             if self._isCudaEnabled():
203                 z = z.cpu()
204             z = z.detach()
205             if asNumpy:
206                 z = z.numpy()
207             return z
208 
209         model = self.getTorchModule()
210         model.eval()
211 
212         if isinstance(X, TorchDataSet):
213             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
214         elif isinstance(X, np.ndarray):
215             X = toFloatArray(X)
216             X = torch.from_numpy(X).float()
217 
218         if type(X) not in (list, tuple):
219             inputs = [X]
220         else:
221             inputs = X
222 
223         if self._isCudaEnabled():
224             torch.cuda.set_device(self._gpu)
225             inputs = [t.cuda() for t in inputs]
226         if scaleInput:
227             inputs = [self.inputScaler.normalise(t) for t in inputs]
228         if createBatch:
229             inputs = [t.view(1, *X.size()) for t in inputs]
230 
231         maxValue = max([t.max().item() for t in inputs])
232         if maxValue > 2:
233             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
234 
235         if mcDropoutSamples is None:
236             y = model(*inputs)
237             return extract(y)
238         else:
239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 228
After: 236",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2004,"{'module': 1, 'if_statement': 10, 'if': 10, 'comparison_operator': 3, 'identifier': 129, 'is not': 2, 'none': 4, ':': 21, 'block': 14, 'expression_statement': 19, 'call': 30, 'attribute': 30, '.': 30, 'argument_list': 30, '(': 32, ')': 32, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 20, 'typed_parameter': 1, 'type': 16, 'generic_type': 4, 'type_parameter': 4, '[': 4, ']': 4, 'typed_default_parameter': 6, '=': 23, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 14, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'integer': 2, 'list_splat': 1, '*': 1, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6889195627668067,0.6847806938562975,"(tensor([0.9348]), tensor([0.9517]), tensor([0.9432]), tensor([0.9500]))"
"305 
306 
307 class TorchModelFromModuleFactory(TorchModel):
308     def __init__(self, moduleFactory: Callable[[], torch.nn.Module], cuda: bool = True) -> None:
309         super().__init__(cuda)
310         self.moduleFactory = moduleFactory
311 
312     def createTorchModule(self) -> torch.nn.Module:
","313 
314 
315 class TorchModelFromModuleFactory(TorchModel):
316     def __init__(self, moduleFactory: Callable[..., torch.nn.Module], *args, cuda: bool = True, **kwargs) -> None:
317         super().__init__(cuda)
318         self.args = args
319         self.kwargs = kwargs
320         self.moduleFactory = moduleFactory
321 
322     def createTorchModule(self) -> torch.nn.Module:
","Before: 308
After: 316, 318, 319",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2727,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 17, 'argument_list': 3, '(': 4, ')': 4, ':': 4, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, 'typed_parameter': 1, 'type': 5, 'generic_type': 1, 'type_parameter': 1, '[': 2, 'list': 1, ']': 2, 'attribute': 4, '.': 4, 'typed_default_parameter': 1, '=': 2, 'true': 1, '->': 1, 'none': 1, 'expression_statement': 2, 'call': 2, 'assignment': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5063901700930162,0.4649222260826622,"(tensor([0.9018]), tensor([0.9594]), tensor([0.9297]), tensor([0.9533]))"
"309         super().__init__(cuda)
310         self.moduleFactory = moduleFactory
311 
312     def createTorchModule(self) -> torch.nn.Module:
313         return self.moduleFactory()
314 
315 
","319         self.kwargs = kwargs
320         self.moduleFactory = moduleFactory
321 
322     def createTorchModule(self) -> torch.nn.Module:
323         return self.moduleFactory(*self.args, **self.kwargs)
324 
325 
","Before: 313
After: 323",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2777,"{'module': 1, 'expression_statement': 2, 'call': 3, 'attribute': 5, 'identifier': 13, 'argument_list': 3, '(': 4, ')': 4, '.': 5, 'assignment': 1, '=': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3310670933889896,0.2783931320392681,"(tensor([0.8789]), tensor([0.9239]), tensor([0.9008]), tensor([0.9192]))"
"452     def _createTorchModel(self) -> VectorTorchModel:
453         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
454 
455     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
456         dataUtil = ClassificationVectorDataUtil(inputs, outputs, self.model.cuda, len(self._labels),
457             normalisationMode=self.normalisationMode)
458         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
459 
460     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","469     def _createTorchModel(self) -> VectorTorchModel:
470         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
471 
472     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
473         dataUtil = ClassificationVectorDataUtil(inputs, outputs, self.model.cuda, len(self._labels),
474             normalisationMode=self.normalisationMode, inputTensoriser=self.inputTensoriser)
475         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
476 
477     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","Before: 457
After: 474",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,4163,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 36, 'parameters': 2, '(': 6, ')': 6, '->': 2, 'type': 4, ':': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 4, 'attribute': 11, '.': 11, 'argument_list': 4, 'list_splat': 1, '*': 1, ',': 8, 'dictionary_splat': 1, '**': 1, 'typed_parameter': 2, 'expression_statement': 1, 'assignment': 1, '=': 2, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7641154479877998,0.732906610665208,"(tensor([0.9689]), tensor([0.9829]), tensor([0.9758]), tensor([0.9815]))"
"470         dataSetProvider = self._createDataSetProvider(inputs, outputs)
471         self.model.fit(dataSetProvider, self.nnOptimiserParams)
472 
473     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
474         results = []
475         i = 0
476         batchSize = 64
477         while i < len(inputs):
478             inputSlice = inputs.iloc[i:i+batchSize]
479             results.append(self.model.applyScaled(inputSlice.values, asNumpy=True))
480             i += batchSize
481         return np.concatenate(results)
482 
483     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","487         dataSetProvider = self._createDataSetProvider(inputs, outputs)
488         self.model.fit(dataSetProvider, self.nnOptimiserParams)
489 
490     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
491         batchSize = 64
492         results = []
493         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
494         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
495             results.append(self.model.applyScaled(inputBatch, asNumpy=True))
496         return np.concatenate(results)
497 
498     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 474, 475, 477, 478, 479, 480
After: 492, 493, 494, 495",fix typos in src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,4385,"{'module': 1, 'expression_statement': 8, 'assignment': 5, 'identifier': 43, '=': 6, 'call': 6, 'attribute': 12, '.': 12, 'argument_list': 6, '(': 7, ',': 4, ')': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 4, 'type': 2, '->': 1, 'block': 2, 'list': 1, '[': 2, ']': 2, 'integer': 2, 'while_statement': 1, 'while': 1, 'comparison_operator': 1, '<': 1, 'subscript': 1, 'slice': 1, 'binary_operator': 1, '+': 1, 'keyword_argument': 1, 'true': 1, 'augmented_assignment': 1, '+=': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4914418740419028,0.45264692767080916,"(tensor([0.9210]), tensor([0.9269]), tensor([0.9239]), tensor([0.9263]))"
"1 from abc import ABC, abstractmethod
2 from typing import Tuple, Sequence, Generator, Optional, Union
3 
4 from torch.autograd import Variable
5 import pandas as pd
6 import numpy as np
","1 from abc import ABC, abstractmethod
2 from typing import Tuple, Sequence, Generator, Optional, Union, List, Iterator
3 
4 from torch.autograd import Variable
5 import pandas as pd
6 import numpy as np
","Before: 2
After: 2",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,31,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 12, 'identifier': 14, 'import': 4, ',': 5, '.': 1, 'import_statement': 1, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8555308664663046,0.8513058935686583,"(tensor([0.9810]), tensor([0.9930]), tensor([0.9870]), tensor([0.9918]))"
"139 
140 
141 class VectorDataUtil(DataUtil):
142     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
143             differingOutputNormalisationMode=None):
144         """"""
145         :param inputs: the inputs
146         :param outputs: the outputs
147         :param cuda: whether to apply CUDA
148         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
149         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs
150         """"""
151         if inputs.shape[0] != outputs.shape[0]:
152             raise ValueError(""Output length must be equal to input length"")
153         self.inputs = inputs
154         self.outputs = outputs
155         self.normalisationMode = normalisationMode
156         self.inputValues = inputs.values
157         self.outputValues = outputs.values
158         inputScaler = normalisation.VectorDataScaler(self.inputs, self.normalisationMode)
159         self.inputValues = inputScaler.getNormalisedArray(self.inputs)
160         self.inputTensorScaler = TensorScalerFromVectorDataScaler(inputScaler, cuda)
161         outputScaler = normalisation.VectorDataScaler(self.outputs, self.normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
162         self.outputValues = outputScaler.getNormalisedArray(self.outputs)
163         self.outputTensorScaler = TensorScalerFromVectorDataScaler(outputScaler, cuda)
164 
165     def getOutputTensorScaler(self):
","171 
172 
173 class VectorDataUtil(DataUtil):
174     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
175             differingOutputNormalisationMode=None, inputTensoriser: Optional[Tensoriser] = None, outputTensoriser: Optional[Tensoriser] = None):
176         """"""
177         :param inputs: the data frame of inputs
178         :param outputs: the data frame of outputs
179         :param cuda: whether to apply CUDA
180         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
181         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs, overriding normalisationMode;
182             if None, use normalisationMode
183         """"""
184         if inputs.shape[0] != outputs.shape[0]:
185             raise ValueError(""Output length must be equal to input length"")
186         self.inputs = inputs
187         self.outputs = outputs
188         self.inputTensoriser = inputTensoriser if inputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
189         self.outputTensoriser = outputTensoriser if outputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
190         self.inputVectorDataScaler = normalisation.VectorDataScaler(self.inputs, normalisationMode)
191         self.inputTensorScaler = TensorScalerFromVectorDataScaler(self.inputVectorDataScaler, cuda)
192         self.outputVectorDataScaler = normalisation.VectorDataScaler(self.outputs, normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
193         self.outputTensorScaler = TensorScalerFromVectorDataScaler(self.outputVectorDataScaler, cuda)
194 
195     def getOutputTensorScaler(self):
","Before: 143
After: 175",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,955,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 77, 'argument_list': 8, '(': 9, ')': 9, ':': 6, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_parameter': 3, 'type': 3, 'attribute': 27, '.': 27, 'default_parameter': 2, '=': 13, 'none': 2, 'expression_statement': 12, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 2, 'comparison_operator': 2, 'subscript': 2, '[': 2, 'integer': 2, ']': 2, '!=': 1, 'raise_statement': 1, 'raise': 1, 'call': 7, 'assignment': 11, 'conditional_expression': 1, 'is': 1, 'else': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.48687805878188467,0.4556239635123242,"(tensor([0.9113]), tensor([0.9329]), tensor([0.9220]), tensor([0.9307]))"
"139 
140 
141 class VectorDataUtil(DataUtil):
142     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
143             differingOutputNormalisationMode=None):
144         """"""
145         :param inputs: the inputs
146         :param outputs: the outputs
147         :param cuda: whether to apply CUDA
148         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
149         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs
150         """"""
151         if inputs.shape[0] != outputs.shape[0]:
152             raise ValueError(""Output length must be equal to input length"")
153         self.inputs = inputs
154         self.outputs = outputs
155         self.normalisationMode = normalisationMode
156         self.inputValues = inputs.values
157         self.outputValues = outputs.values
158         inputScaler = normalisation.VectorDataScaler(self.inputs, self.normalisationMode)
159         self.inputValues = inputScaler.getNormalisedArray(self.inputs)
160         self.inputTensorScaler = TensorScalerFromVectorDataScaler(inputScaler, cuda)
161         outputScaler = normalisation.VectorDataScaler(self.outputs, self.normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
162         self.outputValues = outputScaler.getNormalisedArray(self.outputs)
163         self.outputTensorScaler = TensorScalerFromVectorDataScaler(outputScaler, cuda)
164 
165     def getOutputTensorScaler(self):
","171 
172 
173 class VectorDataUtil(DataUtil):
174     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
175             differingOutputNormalisationMode=None, inputTensoriser: Optional[Tensoriser] = None, outputTensoriser: Optional[Tensoriser] = None):
176         """"""
177         :param inputs: the data frame of inputs
178         :param outputs: the data frame of outputs
179         :param cuda: whether to apply CUDA
180         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
181         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs, overriding normalisationMode;
182             if None, use normalisationMode
183         """"""
184         if inputs.shape[0] != outputs.shape[0]:
185             raise ValueError(""Output length must be equal to input length"")
186         self.inputs = inputs
187         self.outputs = outputs
188         self.inputTensoriser = inputTensoriser if inputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
189         self.outputTensoriser = outputTensoriser if outputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
190         self.inputVectorDataScaler = normalisation.VectorDataScaler(self.inputs, normalisationMode)
191         self.inputTensorScaler = TensorScalerFromVectorDataScaler(self.inputVectorDataScaler, cuda)
192         self.outputVectorDataScaler = normalisation.VectorDataScaler(self.outputs, normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
193         self.outputTensorScaler = TensorScalerFromVectorDataScaler(self.outputVectorDataScaler, cuda)
194 
195     def getOutputTensorScaler(self):
","Before: 145, 146
After: 177, 178",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,966,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 77, 'argument_list': 8, '(': 9, ')': 9, ':': 6, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_parameter': 3, 'type': 3, 'attribute': 27, '.': 27, 'default_parameter': 2, '=': 13, 'none': 2, 'expression_statement': 12, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 2, 'comparison_operator': 2, 'subscript': 2, '[': 2, 'integer': 2, ']': 2, '!=': 1, 'raise_statement': 1, 'raise': 1, 'call': 7, 'assignment': 11, 'conditional_expression': 1, 'is': 1, 'else': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.48687805878188467,0.4556239635123242,"(tensor([0.9113]), tensor([0.9329]), tensor([0.9220]), tensor([0.9307]))"
"139 
140 
141 class VectorDataUtil(DataUtil):
142     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
143             differingOutputNormalisationMode=None):
144         """"""
145         :param inputs: the inputs
146         :param outputs: the outputs
147         :param cuda: whether to apply CUDA
148         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
149         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs
150         """"""
151         if inputs.shape[0] != outputs.shape[0]:
152             raise ValueError(""Output length must be equal to input length"")
153         self.inputs = inputs
154         self.outputs = outputs
155         self.normalisationMode = normalisationMode
156         self.inputValues = inputs.values
157         self.outputValues = outputs.values
158         inputScaler = normalisation.VectorDataScaler(self.inputs, self.normalisationMode)
159         self.inputValues = inputScaler.getNormalisedArray(self.inputs)
160         self.inputTensorScaler = TensorScalerFromVectorDataScaler(inputScaler, cuda)
161         outputScaler = normalisation.VectorDataScaler(self.outputs, self.normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
162         self.outputValues = outputScaler.getNormalisedArray(self.outputs)
163         self.outputTensorScaler = TensorScalerFromVectorDataScaler(outputScaler, cuda)
164 
165     def getOutputTensorScaler(self):
","171 
172 
173 class VectorDataUtil(DataUtil):
174     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
175             differingOutputNormalisationMode=None, inputTensoriser: Optional[Tensoriser] = None, outputTensoriser: Optional[Tensoriser] = None):
176         """"""
177         :param inputs: the data frame of inputs
178         :param outputs: the data frame of outputs
179         :param cuda: whether to apply CUDA
180         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
181         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs, overriding normalisationMode;
182             if None, use normalisationMode
183         """"""
184         if inputs.shape[0] != outputs.shape[0]:
185             raise ValueError(""Output length must be equal to input length"")
186         self.inputs = inputs
187         self.outputs = outputs
188         self.inputTensoriser = inputTensoriser if inputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
189         self.outputTensoriser = outputTensoriser if outputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
190         self.inputVectorDataScaler = normalisation.VectorDataScaler(self.inputs, normalisationMode)
191         self.inputTensorScaler = TensorScalerFromVectorDataScaler(self.inputVectorDataScaler, cuda)
192         self.outputVectorDataScaler = normalisation.VectorDataScaler(self.outputs, normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
193         self.outputTensorScaler = TensorScalerFromVectorDataScaler(self.outputVectorDataScaler, cuda)
194 
195     def getOutputTensorScaler(self):
","Before: 149
After: 181, 182",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1029,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 77, 'argument_list': 8, '(': 9, ')': 9, ':': 6, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_parameter': 3, 'type': 3, 'attribute': 27, '.': 27, 'default_parameter': 2, '=': 13, 'none': 2, 'expression_statement': 12, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 2, 'comparison_operator': 2, 'subscript': 2, '[': 2, 'integer': 2, ']': 2, '!=': 1, 'raise_statement': 1, 'raise': 1, 'call': 7, 'assignment': 11, 'conditional_expression': 1, 'is': 1, 'else': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.48687805878188467,0.4556239635123242,"(tensor([0.9113]), tensor([0.9329]), tensor([0.9220]), tensor([0.9307]))"
"139 
140 
141 class VectorDataUtil(DataUtil):
142     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
143             differingOutputNormalisationMode=None):
144         """"""
145         :param inputs: the inputs
146         :param outputs: the outputs
147         :param cuda: whether to apply CUDA
148         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
149         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs
150         """"""
151         if inputs.shape[0] != outputs.shape[0]:
152             raise ValueError(""Output length must be equal to input length"")
153         self.inputs = inputs
154         self.outputs = outputs
155         self.normalisationMode = normalisationMode
156         self.inputValues = inputs.values
157         self.outputValues = outputs.values
158         inputScaler = normalisation.VectorDataScaler(self.inputs, self.normalisationMode)
159         self.inputValues = inputScaler.getNormalisedArray(self.inputs)
160         self.inputTensorScaler = TensorScalerFromVectorDataScaler(inputScaler, cuda)
161         outputScaler = normalisation.VectorDataScaler(self.outputs, self.normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
162         self.outputValues = outputScaler.getNormalisedArray(self.outputs)
163         self.outputTensorScaler = TensorScalerFromVectorDataScaler(outputScaler, cuda)
164 
165     def getOutputTensorScaler(self):
","171 
172 
173 class VectorDataUtil(DataUtil):
174     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
175             differingOutputNormalisationMode=None, inputTensoriser: Optional[Tensoriser] = None, outputTensoriser: Optional[Tensoriser] = None):
176         """"""
177         :param inputs: the data frame of inputs
178         :param outputs: the data frame of outputs
179         :param cuda: whether to apply CUDA
180         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
181         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs, overriding normalisationMode;
182             if None, use normalisationMode
183         """"""
184         if inputs.shape[0] != outputs.shape[0]:
185             raise ValueError(""Output length must be equal to input length"")
186         self.inputs = inputs
187         self.outputs = outputs
188         self.inputTensoriser = inputTensoriser if inputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
189         self.outputTensoriser = outputTensoriser if outputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
190         self.inputVectorDataScaler = normalisation.VectorDataScaler(self.inputs, normalisationMode)
191         self.inputTensorScaler = TensorScalerFromVectorDataScaler(self.inputVectorDataScaler, cuda)
192         self.outputVectorDataScaler = normalisation.VectorDataScaler(self.outputs, normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
193         self.outputTensorScaler = TensorScalerFromVectorDataScaler(self.outputVectorDataScaler, cuda)
194 
195     def getOutputTensorScaler(self):
","Before: 155, 156, 157, 158, 159, 160, 161, 162, 163
After: 188, 189, 190, 191, 192, 193",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1017,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 77, 'argument_list': 8, '(': 9, ')': 9, ':': 6, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_parameter': 3, 'type': 3, 'attribute': 27, '.': 27, 'default_parameter': 2, '=': 13, 'none': 2, 'expression_statement': 12, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 2, 'comparison_operator': 2, 'subscript': 2, '[': 2, 'integer': 2, ']': 2, '!=': 1, 'raise_statement': 1, 'raise': 1, 'call': 7, 'assignment': 11, 'conditional_expression': 1, 'is': 1, 'else': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.48687805878188467,0.4556239635123242,"(tensor([0.9113]), tensor([0.9329]), tensor([0.9220]), tensor([0.9307]))"
"178         B = self._inputOutputPairs(indices_B)
179         return A, B
180 
181     def _inputOutputPairs(self, indices):
182         n = len(indices)
183         X = torch.zeros((n, self.inputDim()))
184         Y = torch.zeros((n, self.outputDim()), dtype=self._torchOutputDtype())
185 
186         for i, outputIdx in enumerate(indices):
187             inputData, outputData = self._inputOutputPair(outputIdx)
188             if i == 0:
189                 if inputData.size() != X[i].size():
190                     raise Exception(f""Unexpected input size: expected {X[i].size()}, got {inputData.size()}"")
191                 if outputData.size() != Y[i].size():
192                     raise Exception(f""Unexpected output size: expected {Y[i].size()}, got {outputData.size()}"")
193             X[i] = inputData
194             Y[i] = outputData
195 
196         return X, Y
197 
198     def _inputOutputPair(self, idx):
","208         B = self._inputOutputPairs(indices_B)
209         return A, B
210 
211     def _inputOutputPairs(self, indices):
212         inputDF = self.inputs.iloc[indices]
213         outputDF = self.outputs.iloc[indices]
214 
215         # apply normalisation (if any)
216         if self.inputVectorDataScaler.normalisationMode != normalisation.NormalisationMode.NONE:
217             inputDF = pd.DataFrame(self.inputVectorDataScaler.getNormalisedArray(inputDF), columns=inputDF.columns, index=inputDF.index)
218         if self.outputVectorDataScaler.normalisationMode != normalisation.NormalisationMode.NONE:
219             outputDF = pd.DataFrame(self.outputVectorDataScaler.getNormalisedArray(outputDF), columns=outputDF.columns, index=outputDF.index)
220 
221         return self.inputTensoriser.tensorise(inputDF), self.outputTensoriser.tensorise(outputDF)
222 
223     def inputDim(self):
","Before: 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201
After: 212, 213, 214, 215, 216, 217, 218, 219, 220, 221",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1316,"{'module': 1, 'expression_statement': 7, 'assignment': 7, 'identifier': 67, '=': 8, 'call': 19, 'attribute': 15, '.': 15, 'argument_list': 19, '(': 22, ')': 22, 'return_statement': 2, 'return': 2, 'expression_list': 2, ',': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 5, 'block': 5, 'tuple': 2, 'keyword_argument': 1, 'for_statement': 1, 'for': 1, 'pattern_list': 2, 'in': 1, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '==': 1, 'integer': 1, '!=': 2, 'subscript': 6, '[': 6, ']': 6, 'raise_statement': 2, 'raise': 2, 'string': 2, 'string_start': 2, 'string_content': 4, 'interpolation': 4, '{': 4, '}': 4, 'string_end': 2}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.07324484718825496,0.061190361358155455,"(tensor([0.7580]), tensor([0.7678]), tensor([0.7629]), tensor([0.7669]))"
"212     def modelOutputDim(self):
213         return self.outputDim()
214 
215     def _torchOutputDtype(self):
216         return None  # use default (some float)
217 
218 
","234 
235 
236 class ClassificationVectorDataUtil(VectorDataUtil):
237     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda, numClasses, normalisationMode=normalisation.NormalisationMode.NONE,
238             inputTensoriser: Tensoriser = None, outputTensoriser: Tensoriser = None):
239         if len(outputs.columns) != 1:
240             raise Exception(f""Exactly one output dimension (the class index) is required, got {len(outputs.columns)}"")
241         super().__init__(inputs, outputs, cuda, normalisationMode=normalisationMode,
242             differingOutputNormalisationMode=normalisation.NormalisationMode.NONE, inputTensoriser=inputTensoriser,
243             outputTensoriser=TensoriserClassLabelIndices() if outputTensoriser is None else outputTensoriser)
244         self.numClasses = numClasses
245 
246     def modelOutputDim(self):
","Before: 215, 216, 217, 220
After: 237, 238",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1709,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 6, 'parameters': 2, '(': 3, ')': 3, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 1, 'none': 1, 'comment': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.051955251543263234,0.02545666383022461,"(tensor([0.6010]), tensor([0.7361]), tensor([0.6617]), tensor([0.7199]))"
"217 
218 
219 class ClassificationVectorDataUtil(VectorDataUtil):
220     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda, numClasses, normalisationMode=normalisation.NormalisationMode.NONE):
221         if len(outputs.columns) != 1:
222             raise Exception(f""Exactly one output dimension (the class index) is required, got {len(outputs.columns)}"")
223         super().__init__(inputs, outputs, cuda, normalisationMode=normalisationMode, differingOutputNormalisationMode=normalisation.NormalisationMode.NONE)
224         self.numClasses = numClasses
225 
226     def modelOutputDim(self):
","234 
235 
236 class ClassificationVectorDataUtil(VectorDataUtil):
237     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda, numClasses, normalisationMode=normalisation.NormalisationMode.NONE,
238             inputTensoriser: Tensoriser = None, outputTensoriser: Tensoriser = None):
239         if len(outputs.columns) != 1:
240             raise Exception(f""Exactly one output dimension (the class index) is required, got {len(outputs.columns)}"")
241         super().__init__(inputs, outputs, cuda, normalisationMode=normalisationMode,
242             differingOutputNormalisationMode=normalisation.NormalisationMode.NONE, inputTensoriser=inputTensoriser,
243             outputTensoriser=TensoriserClassLabelIndices() if outputTensoriser is None else outputTensoriser)
244         self.numClasses = numClasses
245 
246     def modelOutputDim(self):
","Before: 223
After: 241, 242, 243",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1837,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 37, 'argument_list': 6, '(': 7, ')': 7, ':': 5, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 9, 'typed_parameter': 2, 'type': 2, 'attribute': 10, '.': 10, 'default_parameter': 1, '=': 4, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'call': 5, '!=': 1, 'integer': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 1, '}': 1, 'string_end': 1, 'expression_statement': 2, 'keyword_argument': 2, 'assignment': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.590484100823658,0.5672909034736712,"(tensor([0.8672]), tensor([0.9544]), tensor([0.9087]), tensor([0.9449]))"
"226     def modelOutputDim(self):
227         return self.numClasses
228 
229     def _torchOutputDtype(self):
230         return torch.long
231 
232     def _inputOutputPairs(self, indices):
","249 
250 class TorchDataSet:
251     @abstractmethod
252     def iterBatches(self, batchSize: int, shuffle: bool = False, inputOnly=False) -> Iterator[Union[Tuple[torch.Tensor, torch.Tensor],
253             Tuple[Sequence[torch.Tensor], torch.Tensor], torch.Tensor, Sequence[torch.Tensor]]]:
254         """"""
255         Provides an iterator over batches from the data set.
256 
257         :param batchSize: the maximum size of each batch
258         :param shuffle: whether to shuffle the data set
259         :param inputOnly: whether to provide only inputs (rather than inputs and corresponding outputs).
260             If true, provide only inputs, where inputs can either be a tensor or a tuple of tensors.
261             If false, provide a pair (i, o) with inputs and corresponding outputs (o is always a tensor).
262             Some data sets may only be able to provide inputs, in which case inputOnly=False should lead to an
263             exception.
264         """"""
265         pass
266 
267     @abstractmethod
","Before: 229, 230, 231, 232, 233, 234, 235, 236, 240
After: 252, 253",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,1869,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 8, 'parameters': 2, '(': 2, ')': 2, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'attribute': 2, '.': 2}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",5.200444976954816e-79,3.0688640339935355e-79,"(tensor([0.5655]), tensor([0.7202]), tensor([0.6335]), tensor([0.7010]))"
"237 
238 class TorchDataSet:
239     @abstractmethod
240     def iterBatches(self, batchSize: int, shuffle: bool = False, inputOnly=False) -> Generator[Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor], None, None]:
241         """"""
242         Provides an iterator over batches from the data set.
243 
244         :param batchSize: the maximum size of each batch
245         :param shuffle: whether to shuffle the data set
246         :param inputOnly: whether to provide only inputs (rather than inputs and corresponding outputs).
247             If true, provide a single tensor, which is to serve as a model input.
248             If false, provide a pair of tensors (i, o) with inputs and corresponding outputs.
249             Some data sets may only be able to provide inputs, in which case inputOnly=False should lead to an
250             exception.
251         """"""
252         pass
253 
254     @abstractmethod
","249 
250 class TorchDataSet:
251     @abstractmethod
252     def iterBatches(self, batchSize: int, shuffle: bool = False, inputOnly=False) -> Iterator[Union[Tuple[torch.Tensor, torch.Tensor],
253             Tuple[Sequence[torch.Tensor], torch.Tensor], torch.Tensor, Sequence[torch.Tensor]]]:
254         """"""
255         Provides an iterator over batches from the data set.
256 
257         :param batchSize: the maximum size of each batch
258         :param shuffle: whether to shuffle the data set
259         :param inputOnly: whether to provide only inputs (rather than inputs and corresponding outputs).
260             If true, provide only inputs, where inputs can either be a tensor or a tuple of tensors.
261             If false, provide a pair (i, o) with inputs and corresponding outputs (o is always a tensor).
262             Some data sets may only be able to provide inputs, in which case inputOnly=False should lead to an
263             exception.
264         """"""
265         pass
266 
267     @abstractmethod
","Before: 247, 248
After: 260, 261",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2085,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 18, ':': 4, 'block': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 7, 'typed_parameter': 1, 'type': 10, 'typed_default_parameter': 1, '=': 2, 'false': 2, 'default_parameter': 1, ')': 1, '->': 1, 'generic_type': 3, 'type_parameter': 3, '[': 3, 'attribute': 3, '.': 3, ']': 3, 'none': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5872080410776807,0.5846424003339779,"(tensor([0.9275]), tensor([0.9293]), tensor([0.9284]), tensor([0.9291]))"
"307 
308 
309 class TorchDataSetFromTensors(TorchDataSet):
310     def __init__(self, x: torch.Tensor, y: Optional[torch.Tensor], cuda: bool):
311         if y is not None and x.shape[0] != y.shape[0]:
312             raise ValueError(""Tensors are not of the same length"")
313         self.x = x
314         self.y = y
315         self.cuda = cuda
316 
317     def iterBatches(self, batchSize: int, shuffle: bool = False, inputOnly=False) -> Generator[Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor], None, None]:
","353 
354 
355 class TorchDataSetFromTensors(TorchDataSet):
356     def __init__(self, x: Union[torch.Tensor, Sequence[torch.Tensor]], y: Optional[torch.Tensor], cuda: bool):
357         """"""
358         :param x: the input tensor(s); if more than one, they must be of the same length (and a slice of each shall be provided to the
359             model as an input in each batch)
360         :param y: the output tensor
361         :param cuda: whether any generated tensors shall be moved to the selected CUDA device
362         """"""
363         x = TensorTuple(x)
364         y = TensorTuple(y) if y is not None else None
365         if y is not None and len(x) != len(y):
366             raise ValueError(""Tensors are not of the same length"")
367         self.x = x
368         self.y = y
369         self.cuda = cuda
370 
371     def iterBatches(self, batchSize: int, shuffle: bool = False, inputOnly=False) -> Iterator[Union[Tuple[torch.Tensor, torch.Tensor],
","Before: 310, 311
After: 356, 357, 358, 359, 360, 361, 362, 363, 364, 365",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2366,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 28, 'argument_list': 2, '(': 3, ')': 3, ':': 6, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, 'typed_parameter': 3, 'type': 4, 'attribute': 7, '.': 7, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'if_statement': 1, 'if': 1, 'boolean_operator': 1, 'comparison_operator': 2, 'is not': 2, 'none': 1, 'and': 1, 'subscript': 2, 'integer': 2, '!=': 1, 'raise_statement': 1, 'raise': 1, 'call': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'expression_statement': 3, 'assignment': 3, '=': 3}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.33412995793539074,0.3102112912869027,"(tensor([0.8035]), tensor([0.8736]), tensor([0.8371]), tensor([0.8660]))"
"314         self.y = y
315         self.cuda = cuda
316 
317     def iterBatches(self, batchSize: int, shuffle: bool = False, inputOnly=False) -> Generator[Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor], None, None]:
318         tensors = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
319         yield from self._get_batches(tensors, batchSize, shuffle)
320 
321     def _get_batches(self, tensors: Sequence[torch.Tensor], batch_size, shuffle):
","368         self.y = y
369         self.cuda = cuda
370 
371     def iterBatches(self, batchSize: int, shuffle: bool = False, inputOnly=False) -> Iterator[Union[Tuple[torch.Tensor, torch.Tensor],
372             Tuple[Sequence[torch.Tensor], torch.Tensor], torch.Tensor, Sequence[torch.Tensor]]]:
373         tensorTuples = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
374         yield from self._get_batches(tensorTuples, batchSize, shuffle)
375 
376     def _get_batches(self, tensorTuples: Sequence[TensorTuple], batch_size, shuffle):
","Before: 317, 318, 319
After: 371, 372, 373, 374",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2500,"{'module': 1, 'expression_statement': 4, 'assignment': 3, 'attribute': 10, 'identifier': 37, '.': 10, '=': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 4, ',': 11, 'typed_parameter': 1, ':': 3, 'type': 10, 'typed_default_parameter': 1, 'false': 2, 'default_parameter': 1, ')': 4, '->': 1, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 3, 'block': 1, 'conditional_expression': 1, 'tuple': 2, 'if': 1, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1, 'comparison_operator': 1, 'is not': 2, 'else': 1, 'yield': 2, 'from': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6207048558168978,0.6059333644770893,"(tensor([0.9309]), tensor([0.9439]), tensor([0.9373]), tensor([0.9426]))"
"318         tensors = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
319         yield from self._get_batches(tensors, batchSize, shuffle)
320 
321     def _get_batches(self, tensors: Sequence[torch.Tensor], batch_size, shuffle):
322         length = len(tensors[0])
323         if shuffle:
324             index = torch.randperm(length)
325         else:
326             index = torch.LongTensor(range(length))
327         start_idx = 0
328         while start_idx < length:
329             end_idx = min(length, start_idx + batch_size)
330             excerpt = index[start_idx:end_idx]
331             batch = []
332             for tensor in tensors:
333                 if len(tensor) != length:
334                     raise Exception(""Passed tensors of differing lengths"")
335                 t = tensor[excerpt]
336                 if self.cuda:
337                     t = t.cuda()
338                 batch.append(Variable(t))
339             if len(batch) == 1:
340                 yield batch[0]
341             else:
342                 yield tuple(batch)
343             start_idx += batch_size
344 
345     def size(self):
","373         tensorTuples = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
374         yield from self._get_batches(tensorTuples, batchSize, shuffle)
375 
376     def _get_batches(self, tensorTuples: Sequence[TensorTuple], batch_size, shuffle):
377         length = len(tensorTuples[0])
378         if shuffle:
379             index = torch.randperm(length)
380         else:
381             index = torch.LongTensor(range(length))
382         start_idx = 0
383         while start_idx < length:
384             end_idx = min(length, start_idx + batch_size)
385             excerpt = index[start_idx:end_idx]
386             batch = []
387             for tensorTuple in tensorTuples:
388                 if len(tensorTuple) != length:
389                     raise Exception(""Passed tensors of differing lengths"")
390                 t = tensorTuple[excerpt]
391                 if self.cuda:
392                     t = t.cuda()
393                 item = t.item()
394                 if type(item) == tuple:
395                     item = tuple(Variable(t) for t in item)
396                 else:
397                     item = Variable(item)
398                 batch.append(item)
399             if len(batch) == 1:
400                 yield batch[0]
401             else:
402                 yield tuple(batch)
403             start_idx += batch_size
404 
405     def size(self):
","Before: 321, 322
After: 376, 377",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2585,"{'module': 1, 'expression_statement': 15, 'assignment': 10, 'identifier': 74, '=': 10, 'conditional_expression': 1, 'tuple': 2, '(': 16, 'attribute': 11, '.': 11, ',': 8, ')': 16, 'if': 5, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1, 'comparison_operator': 4, 'is not': 2, 'none': 1, 'else': 3, 'yield': 6, 'from': 1, 'call': 13, 'argument_list': 13, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 6, ']': 6, 'block': 9, 'subscript': 4, 'integer': 4, 'if_statement': 4, 'else_clause': 2, 'while_statement': 1, 'while': 1, '<': 1, 'binary_operator': 1, '+': 1, 'slice': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '==': 1, 'augmented_assignment': 1, '+=': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.482489505996052,0.4584946607414112,"(tensor([0.9027]), tensor([0.9318]), tensor([0.9170]), tensor([0.9288]))"
"318         tensors = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
319         yield from self._get_batches(tensors, batchSize, shuffle)
320 
321     def _get_batches(self, tensors: Sequence[torch.Tensor], batch_size, shuffle):
322         length = len(tensors[0])
323         if shuffle:
324             index = torch.randperm(length)
325         else:
326             index = torch.LongTensor(range(length))
327         start_idx = 0
328         while start_idx < length:
329             end_idx = min(length, start_idx + batch_size)
330             excerpt = index[start_idx:end_idx]
331             batch = []
332             for tensor in tensors:
333                 if len(tensor) != length:
334                     raise Exception(""Passed tensors of differing lengths"")
335                 t = tensor[excerpt]
336                 if self.cuda:
337                     t = t.cuda()
338                 batch.append(Variable(t))
339             if len(batch) == 1:
340                 yield batch[0]
341             else:
342                 yield tuple(batch)
343             start_idx += batch_size
344 
345     def size(self):
","373         tensorTuples = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
374         yield from self._get_batches(tensorTuples, batchSize, shuffle)
375 
376     def _get_batches(self, tensorTuples: Sequence[TensorTuple], batch_size, shuffle):
377         length = len(tensorTuples[0])
378         if shuffle:
379             index = torch.randperm(length)
380         else:
381             index = torch.LongTensor(range(length))
382         start_idx = 0
383         while start_idx < length:
384             end_idx = min(length, start_idx + batch_size)
385             excerpt = index[start_idx:end_idx]
386             batch = []
387             for tensorTuple in tensorTuples:
388                 if len(tensorTuple) != length:
389                     raise Exception(""Passed tensors of differing lengths"")
390                 t = tensorTuple[excerpt]
391                 if self.cuda:
392                     t = t.cuda()
393                 item = t.item()
394                 if type(item) == tuple:
395                     item = tuple(Variable(t) for t in item)
396                 else:
397                     item = Variable(item)
398                 batch.append(item)
399             if len(batch) == 1:
400                 yield batch[0]
401             else:
402                 yield tuple(batch)
403             start_idx += batch_size
404 
405     def size(self):
","Before: 332, 333
After: 387, 388",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2693,"{'module': 1, 'expression_statement': 15, 'assignment': 10, 'identifier': 74, '=': 10, 'conditional_expression': 1, 'tuple': 2, '(': 16, 'attribute': 11, '.': 11, ',': 8, ')': 16, 'if': 5, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1, 'comparison_operator': 4, 'is not': 2, 'none': 1, 'else': 3, 'yield': 6, 'from': 1, 'call': 13, 'argument_list': 13, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 6, ']': 6, 'block': 9, 'subscript': 4, 'integer': 4, 'if_statement': 4, 'else_clause': 2, 'while_statement': 1, 'while': 1, '<': 1, 'binary_operator': 1, '+': 1, 'slice': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '==': 1, 'augmented_assignment': 1, '+=': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.482489505996052,0.4584946607414112,"(tensor([0.9027]), tensor([0.9318]), tensor([0.9170]), tensor([0.9288]))"
"318         tensors = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
319         yield from self._get_batches(tensors, batchSize, shuffle)
320 
321     def _get_batches(self, tensors: Sequence[torch.Tensor], batch_size, shuffle):
322         length = len(tensors[0])
323         if shuffle:
324             index = torch.randperm(length)
325         else:
326             index = torch.LongTensor(range(length))
327         start_idx = 0
328         while start_idx < length:
329             end_idx = min(length, start_idx + batch_size)
330             excerpt = index[start_idx:end_idx]
331             batch = []
332             for tensor in tensors:
333                 if len(tensor) != length:
334                     raise Exception(""Passed tensors of differing lengths"")
335                 t = tensor[excerpt]
336                 if self.cuda:
337                     t = t.cuda()
338                 batch.append(Variable(t))
339             if len(batch) == 1:
340                 yield batch[0]
341             else:
342                 yield tuple(batch)
343             start_idx += batch_size
344 
345     def size(self):
","373         tensorTuples = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
374         yield from self._get_batches(tensorTuples, batchSize, shuffle)
375 
376     def _get_batches(self, tensorTuples: Sequence[TensorTuple], batch_size, shuffle):
377         length = len(tensorTuples[0])
378         if shuffle:
379             index = torch.randperm(length)
380         else:
381             index = torch.LongTensor(range(length))
382         start_idx = 0
383         while start_idx < length:
384             end_idx = min(length, start_idx + batch_size)
385             excerpt = index[start_idx:end_idx]
386             batch = []
387             for tensorTuple in tensorTuples:
388                 if len(tensorTuple) != length:
389                     raise Exception(""Passed tensors of differing lengths"")
390                 t = tensorTuple[excerpt]
391                 if self.cuda:
392                     t = t.cuda()
393                 item = t.item()
394                 if type(item) == tuple:
395                     item = tuple(Variable(t) for t in item)
396                 else:
397                     item = Variable(item)
398                 batch.append(item)
399             if len(batch) == 1:
400                 yield batch[0]
401             else:
402                 yield tuple(batch)
403             start_idx += batch_size
404 
405     def size(self):
","Before: 335
After: 390",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2726,"{'module': 1, 'expression_statement': 15, 'assignment': 10, 'identifier': 74, '=': 10, 'conditional_expression': 1, 'tuple': 2, '(': 16, 'attribute': 11, '.': 11, ',': 8, ')': 16, 'if': 5, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1, 'comparison_operator': 4, 'is not': 2, 'none': 1, 'else': 3, 'yield': 6, 'from': 1, 'call': 13, 'argument_list': 13, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 6, ']': 6, 'block': 9, 'subscript': 4, 'integer': 4, 'if_statement': 4, 'else_clause': 2, 'while_statement': 1, 'while': 1, '<': 1, 'binary_operator': 1, '+': 1, 'slice': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '==': 1, 'augmented_assignment': 1, '+=': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.482489505996052,0.4584946607414112,"(tensor([0.9027]), tensor([0.9318]), tensor([0.9170]), tensor([0.9288]))"
"318         tensors = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
319         yield from self._get_batches(tensors, batchSize, shuffle)
320 
321     def _get_batches(self, tensors: Sequence[torch.Tensor], batch_size, shuffle):
322         length = len(tensors[0])
323         if shuffle:
324             index = torch.randperm(length)
325         else:
326             index = torch.LongTensor(range(length))
327         start_idx = 0
328         while start_idx < length:
329             end_idx = min(length, start_idx + batch_size)
330             excerpt = index[start_idx:end_idx]
331             batch = []
332             for tensor in tensors:
333                 if len(tensor) != length:
334                     raise Exception(""Passed tensors of differing lengths"")
335                 t = tensor[excerpt]
336                 if self.cuda:
337                     t = t.cuda()
338                 batch.append(Variable(t))
339             if len(batch) == 1:
340                 yield batch[0]
341             else:
342                 yield tuple(batch)
343             start_idx += batch_size
344 
345     def size(self):
","373         tensorTuples = (self.x, self.y) if not inputOnly and self.y is not None else (self.x,)
374         yield from self._get_batches(tensorTuples, batchSize, shuffle)
375 
376     def _get_batches(self, tensorTuples: Sequence[TensorTuple], batch_size, shuffle):
377         length = len(tensorTuples[0])
378         if shuffle:
379             index = torch.randperm(length)
380         else:
381             index = torch.LongTensor(range(length))
382         start_idx = 0
383         while start_idx < length:
384             end_idx = min(length, start_idx + batch_size)
385             excerpt = index[start_idx:end_idx]
386             batch = []
387             for tensorTuple in tensorTuples:
388                 if len(tensorTuple) != length:
389                     raise Exception(""Passed tensors of differing lengths"")
390                 t = tensorTuple[excerpt]
391                 if self.cuda:
392                     t = t.cuda()
393                 item = t.item()
394                 if type(item) == tuple:
395                     item = tuple(Variable(t) for t in item)
396                 else:
397                     item = Variable(item)
398                 batch.append(item)
399             if len(batch) == 1:
400                 yield batch[0]
401             else:
402                 yield tuple(batch)
403             start_idx += batch_size
404 
405     def size(self):
","Before: 338
After: 393, 394, 395, 396, 397, 398",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2761,"{'module': 1, 'expression_statement': 15, 'assignment': 10, 'identifier': 74, '=': 10, 'conditional_expression': 1, 'tuple': 2, '(': 16, 'attribute': 11, '.': 11, ',': 8, ')': 16, 'if': 5, 'boolean_operator': 1, 'not_operator': 1, 'not': 1, 'and': 1, 'comparison_operator': 4, 'is not': 2, 'none': 1, 'else': 3, 'yield': 6, 'from': 1, 'call': 13, 'argument_list': 13, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 11, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 6, ']': 6, 'block': 9, 'subscript': 4, 'integer': 4, 'if_statement': 4, 'else_clause': 2, 'while_statement': 1, 'while': 1, '<': 1, 'binary_operator': 1, '+': 1, 'slice': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '!=': 1, 'raise_statement': 1, 'raise': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '==': 1, 'augmented_assignment': 1, '+=': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.482489505996052,0.4584946607414112,"(tensor([0.9027]), tensor([0.9318]), tensor([0.9170]), tensor([0.9288]))"
"342                 yield tuple(batch)
343             start_idx += batch_size
344 
345     def size(self):
346         return self.y.shape[0]
347 
348 
","402                 yield tuple(batch)
403             start_idx += batch_size
404 
405     def size(self):
406         return len(self.x)
407 
408 
","Before: 346
After: 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421",add tensoriser and dataset iterator,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,2822,"{'module': 1, 'expression_statement': 2, 'yield': 2, 'call': 1, 'identifier': 9, 'argument_list': 1, '(': 2, ')': 2, 'augmented_assignment': 1, '+=': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'return_statement': 1, 'return': 1, 'subscript': 1, 'attribute': 2, '.': 2, '[': 1, 'integer': 1, ']': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 12, 'end_line': 22, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.368275540257269,0.2987984908801251,"(tensor([0.9243]), tensor([0.9093]), tensor([0.9167]), tensor([0.9107]))"
"605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, X, groundTruth, outputScaler: TensorScaler):
609         output = model(X)
610         if self.params.scaledOutputs:
611             output, groundTruth = self._scaledValues(output, groundTruth, outputScaler)
612         return output, groundTruth
613 
614     @classmethod
","605         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
606                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
607 
608     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
609         if isinstance(input, torch.Tensor):
610             output = model(input)
611         else:
612             output = model(*input)
613         if self.params.scaledOutputs:
614             output, groundTruth = self._scaledValues(output, groundTruth, outputScaler)
615         return output, groundTruth
616 
617     @classmethod
","Before: 608, 609
After: 608, 609, 610, 611, 612",fix nnoptimiser.py script,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,5331,"{'module': 1, 'return_statement': 2, 'return': 2, 'call': 3, 'identifier': 34, 'argument_list': 3, '(': 4, 'keyword_argument': 5, '=': 7, 'conditional_expression': 1, 'if': 2, 'else': 1, 'none': 1, ',': 12, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 1, 'block': 2, 'expression_statement': 2, 'assignment': 2, 'if_statement': 1, 'attribute': 3, '.': 3, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5320252951884084,0.5085398293820428,"(tensor([0.9294]), tensor([0.9732]), tensor([0.9508]), tensor([0.9687]))"
"625     else:
626         log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
627         return callFnAndCacheResult()
628 
629 
630 # TODO: I think this class deserves some documentation on the intended usage
631 class PickleCached(object):
632     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None, backend=""pickle""):
633         """"""
634 
","627         log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
628         return callFnAndCacheResult()
629 
630 
631 class PickleCached(object):
632     """"""
633     Function decorator for caching function results via pickle
634     """"""
635     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None, backend=""pickle""):
636         """"""
","Before: 630
After: 632, 633, 634",add a decorator for caching function results via pickle,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,4637,"{'module': 1, 'expression_statement': 1, 'assignment': 1, 'identifier': 17, ':': 6, 'type': 4, 'call': 2, 'attribute': 1, '.': 1, 'argument_list': 3, '(': 4, 'string': 2, 'string_start': 3, 'string_content': 4, 'interpolation': 2, '{': 2, '}': 2, 'string_end': 2, ')': 4, 'return_statement': 1, 'return': 1, 'comment': 1, 'class_definition': 1, 'class': 1, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, 'typed_parameter': 1, 'typed_default_parameter': 2, '=': 3, 'none': 2, 'default_parameter': 1, 'ERROR': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 26, 'end_line': 34, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6742685804563875,0.6600422213181906,"(tensor([0.9409]), tensor([0.9054]), tensor([0.9228]), tensor([0.9088]))"
"629 
630 # TODO: I think this class deserves some documentation on the intended usage
631 class PickleCached(object):
632     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None, backend=""pickle""):
633         """"""
634 
635         :param cacheBasePath:
636         :param filenamePrefix:
637         :param filename:
638         """"""
639         self.filename = filename
640         self.cacheBasePath = cacheBasePath
641         self.filenamePrefix = filenamePrefix
642         self.backend = backend
643 
644         if self.filenamePrefix is None:
645             self.filenamePrefix = """"
646         else:
647             self.filenamePrefix += ""-""
648 
649     def __call__(self, fn: Callable, *args, **kwargs):
","632     """"""
633     Function decorator for caching function results via pickle
634     """"""
635     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None, backend=""pickle""):
636         """"""
637         :param cacheBasePath: the directory where the pickle cache file will be stored
638         :param filenamePrefix: a prefix of the name of the cache file to be created, to which the function name and, where applicable,
639             a hash code of the function arguments will be appended and "".cache.pickle"" will be appended; if None, use """" (if filename
640             has not been provided)
641         :param filename: the full file name of the cache file to be created; this is admissible only if the function has no arguments
642         """"""
643         self.filename = filename
644         self.cacheBasePath = cacheBasePath
645         self.filenamePrefix = filenamePrefix
646         self.backend = backend
647 
648         if self.filenamePrefix is None:
649             self.filenamePrefix = """"
650         else:
651             self.filenamePrefix += ""-""
652 
653     def __call__(self, fn: Callable, *_args, **_kwargs):
","Before: 634, 635, 636, 637
After: 637, 638, 639, 640, 641",add a decorator for caching function results via pickle,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,4687,"{'module': 1, 'comment': 1, 'class_definition': 1, 'class': 1, 'identifier': 29, 'argument_list': 1, '(': 2, ')': 2, ':': 7, 'block': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, 'typed_parameter': 1, 'type': 3, 'typed_default_parameter': 2, '=': 8, 'none': 3, 'default_parameter': 1, 'string': 4, 'string_start': 4, 'string_content': 3, 'string_end': 4, 'expression_statement': 7, 'assignment': 5, 'attribute': 7, '.': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'augmented_assignment': 1, '+=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 26, 'end_line': 34, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.30805080075428004,0.29295254234916185,"(tensor([0.8118]), tensor([0.8869]), tensor([0.8477]), tensor([0.8787]))"
"646         else:
647             self.filenamePrefix += ""-""
648 
649     def __call__(self, fn: Callable, *args, **kwargs):
650         if self.filename is None:
651             self.filename = self.filenamePrefix + fn.__qualname__ + "".cache.pickle""
652         picklePath = os.path.join(self.cacheBasePath,  self.filename)
653         return lambda *args, **kwargs: cached(lambda: fn(*args, **kwargs), picklePath, functionName=fn.__name__, backend=self.backend)
654 
655 
","650         else:
651             self.filenamePrefix += ""-""
652 
653     def __call__(self, fn: Callable, *_args, **_kwargs):
654 
655         def wrapped(*args, **kwargs):
656             haveArgs = len(args) > 0 or len(kwargs) > 0
657             if self.filename is None:
658                 filename = self.filenamePrefix + fn.__qualname__
659                 if haveArgs:
660                     filename += ""-"" + pickleHash((args, kwargs))
661                 filename += "".cache.pickle""
662             else:
663                 if haveArgs:
664                     raise Exception(""Function called with arguments but full cache filename specified: specify a cache filename prefix only to account for argument values"")
665                 filename = self.filename
666             picklePath = os.path.join(self.cacheBasePath, filename)
667             return cached(lambda: fn(*args, **kwargs), picklePath, functionName=fn.__name__, backend=self.backend)
668 
669         return wrapped
670 
671 
","Before: 649, 650, 651, 652, 653
After: 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669",add a decorator for caching function results via pickle,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,4781,"{'module': 1, 'expression_statement': 3, 'assignment': 3, 'identifier': 38, ':': 6, 'type': 2, 'attribute': 11, '.': 11, 'ERROR': 1, '+': 3, '=': 5, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 4, ',': 9, 'typed_parameter': 1, 'list_splat_pattern': 2, '*': 3, 'dictionary_splat_pattern': 2, '**': 3, ')': 4, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'binary_operator': 2, 'call': 3, 'argument_list': 3, 'return_statement': 1, 'return': 1, 'lambda': 4, 'lambda_parameters': 1, 'list_splat': 1, 'dictionary_splat': 1, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 26, 'end_line': 34, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3162963131267781,0.29786918711889443,"(tensor([0.7989]), tensor([0.8938]), tensor([0.8437]), tensor([0.8833]))"
"1 from typing import Union, List, Dict, Any, Sequence, Iterable
2 import re
3 
4 
5 def dictString(d):
","1 from typing import Union, List, Dict, Any, Sequence, Iterable, Optional
2 import re
3 
4 from dcs.sensai.util import countNone
5 
","Before: 1
After: 1, 4, 5",improve string generation,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,24,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'dotted_name': 8, 'identifier': 8, 'import': 2, ',': 5, 'import_statement': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 7, 'end_line': 8, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6695960204886555,0.6442822497401405,"(tensor([0.8890]), tensor([0.9385]), tensor([0.9130]), tensor([0.9333]))"
"39     def _toStringClassName(self):
40         return type(self).__qualname__
41 
42     def _toStringProperties(self, exclude: Union[str, Iterable[str]] = None, **additionalEntries) -> str:
43         """"""
44         Creates a string of the class attributes, optionally excluding some and adding others.
45 
46         :param exclude: attributes to be excluded
47         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
48         :return: a string containing all attribute names and values
49         """"""
50         if exclude is None:
51             exclude = []
52         elif type(exclude) == str:
53             exclude = [exclude]
54         d = {k: v for k, v in self.__dict__.items() if k not in exclude}
55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
","41     def _toStringClassName(self):
42         return type(self).__qualname__
43 
44     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
45             **additionalEntries) -> str:
46         """"""
47         Creates a string of the class attributes, with optional exclusions/inclusions/additions
48 
49         :param exclude: attributes to be excluded; can only be non-empty if include is empty
50         :param include: attributes to be included; can only be non-empty if exclude is empty
51         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
52         :return: a string containing all attribute names and values
53         """"""
54         def mklist(x):
55             if x is None:
56                 return []
57             if type(x) == str:
58                 return [x]
59             return x
60 
61         exclude = mklist(exclude)
62         include = mklist(include)
63         if len(exclude) > 0 and len(include) > 0:
64             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
65 
66         if len(include) == 0:
67             attributeDict = self.__dict__
68         else:
69             attributeDict = {k: getattr(self, k) for k in include}
70         d = {k: v for k, v in attributeDict.items() if k not in exclude}
71         d.update(additionalEntries)
72         return dictString(d)
73 
74     def _toStringObjectInfo(self) -> str:
","Before: 42
After: 44, 45",improve string generation,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,383,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 36, 'parameters': 2, '(': 7, ')': 7, ':': 6, 'block': 4, 'return_statement': 2, 'return': 2, 'attribute': 4, 'call': 5, 'argument_list': 5, '.': 4, ',': 4, 'typed_default_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, '=': 4, 'none': 2, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 1, 'if': 2, 'comparison_operator': 3, 'is': 1, 'assignment': 3, 'list': 2, 'elif_clause': 1, 'elif': 1, '==': 1, 'dictionary_comprehension': 1, '{': 1, 'pair': 1, 'for_in_clause': 1, 'for': 1, 'pattern_list': 1, 'in': 1, 'if_clause': 1, 'not in': 2, '}': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 7, 'end_line': 8, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3626831649387075,0.3566264530266093,"(tensor([0.8301]), tensor([0.9157]), tensor([0.8708]), tensor([0.9063]))"
"39     def _toStringClassName(self):
40         return type(self).__qualname__
41 
42     def _toStringProperties(self, exclude: Union[str, Iterable[str]] = None, **additionalEntries) -> str:
43         """"""
44         Creates a string of the class attributes, optionally excluding some and adding others.
45 
46         :param exclude: attributes to be excluded
47         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
48         :return: a string containing all attribute names and values
49         """"""
50         if exclude is None:
51             exclude = []
52         elif type(exclude) == str:
53             exclude = [exclude]
54         d = {k: v for k, v in self.__dict__.items() if k not in exclude}
55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
","41     def _toStringClassName(self):
42         return type(self).__qualname__
43 
44     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
45             **additionalEntries) -> str:
46         """"""
47         Creates a string of the class attributes, with optional exclusions/inclusions/additions
48 
49         :param exclude: attributes to be excluded; can only be non-empty if include is empty
50         :param include: attributes to be included; can only be non-empty if exclude is empty
51         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
52         :return: a string containing all attribute names and values
53         """"""
54         def mklist(x):
55             if x is None:
56                 return []
57             if type(x) == str:
58                 return [x]
59             return x
60 
61         exclude = mklist(exclude)
62         include = mklist(include)
63         if len(exclude) > 0 and len(include) > 0:
64             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
65 
66         if len(include) == 0:
67             attributeDict = self.__dict__
68         else:
69             attributeDict = {k: getattr(self, k) for k in include}
70         d = {k: v for k, v in attributeDict.items() if k not in exclude}
71         d.update(additionalEntries)
72         return dictString(d)
73 
74     def _toStringObjectInfo(self) -> str:
","Before: 44
After: 47",improve string generation,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,402,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 36, 'parameters': 2, '(': 7, ')': 7, ':': 6, 'block': 4, 'return_statement': 2, 'return': 2, 'attribute': 4, 'call': 5, 'argument_list': 5, '.': 4, ',': 4, 'typed_default_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, '=': 4, 'none': 2, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 1, 'if': 2, 'comparison_operator': 3, 'is': 1, 'assignment': 3, 'list': 2, 'elif_clause': 1, 'elif': 1, '==': 1, 'dictionary_comprehension': 1, '{': 1, 'pair': 1, 'for_in_clause': 1, 'for': 1, 'pattern_list': 1, 'in': 1, 'if_clause': 1, 'not in': 2, '}': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 7, 'end_line': 8, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3626831649387075,0.3566264530266093,"(tensor([0.8301]), tensor([0.9157]), tensor([0.8708]), tensor([0.9063]))"
"39     def _toStringClassName(self):
40         return type(self).__qualname__
41 
42     def _toStringProperties(self, exclude: Union[str, Iterable[str]] = None, **additionalEntries) -> str:
43         """"""
44         Creates a string of the class attributes, optionally excluding some and adding others.
45 
46         :param exclude: attributes to be excluded
47         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
48         :return: a string containing all attribute names and values
49         """"""
50         if exclude is None:
51             exclude = []
52         elif type(exclude) == str:
53             exclude = [exclude]
54         d = {k: v for k, v in self.__dict__.items() if k not in exclude}
55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
","41     def _toStringClassName(self):
42         return type(self).__qualname__
43 
44     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
45             **additionalEntries) -> str:
46         """"""
47         Creates a string of the class attributes, with optional exclusions/inclusions/additions
48 
49         :param exclude: attributes to be excluded; can only be non-empty if include is empty
50         :param include: attributes to be included; can only be non-empty if exclude is empty
51         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
52         :return: a string containing all attribute names and values
53         """"""
54         def mklist(x):
55             if x is None:
56                 return []
57             if type(x) == str:
58                 return [x]
59             return x
60 
61         exclude = mklist(exclude)
62         include = mklist(include)
63         if len(exclude) > 0 and len(include) > 0:
64             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
65 
66         if len(include) == 0:
67             attributeDict = self.__dict__
68         else:
69             attributeDict = {k: getattr(self, k) for k in include}
70         d = {k: v for k, v in attributeDict.items() if k not in exclude}
71         d.update(additionalEntries)
72         return dictString(d)
73 
74     def _toStringObjectInfo(self) -> str:
","Before: 46
After: 49, 50",improve string generation,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,418,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 36, 'parameters': 2, '(': 7, ')': 7, ':': 6, 'block': 4, 'return_statement': 2, 'return': 2, 'attribute': 4, 'call': 5, 'argument_list': 5, '.': 4, ',': 4, 'typed_default_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, '=': 4, 'none': 2, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 1, 'if': 2, 'comparison_operator': 3, 'is': 1, 'assignment': 3, 'list': 2, 'elif_clause': 1, 'elif': 1, '==': 1, 'dictionary_comprehension': 1, '{': 1, 'pair': 1, 'for_in_clause': 1, 'for': 1, 'pattern_list': 1, 'in': 1, 'if_clause': 1, 'not in': 2, '}': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 7, 'end_line': 8, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3626831649387075,0.3566264530266093,"(tensor([0.8301]), tensor([0.9157]), tensor([0.8708]), tensor([0.9063]))"
"39     def _toStringClassName(self):
40         return type(self).__qualname__
41 
42     def _toStringProperties(self, exclude: Union[str, Iterable[str]] = None, **additionalEntries) -> str:
43         """"""
44         Creates a string of the class attributes, optionally excluding some and adding others.
45 
46         :param exclude: attributes to be excluded
47         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
48         :return: a string containing all attribute names and values
49         """"""
50         if exclude is None:
51             exclude = []
52         elif type(exclude) == str:
53             exclude = [exclude]
54         d = {k: v for k, v in self.__dict__.items() if k not in exclude}
55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
","41     def _toStringClassName(self):
42         return type(self).__qualname__
43 
44     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
45             **additionalEntries) -> str:
46         """"""
47         Creates a string of the class attributes, with optional exclusions/inclusions/additions
48 
49         :param exclude: attributes to be excluded; can only be non-empty if include is empty
50         :param include: attributes to be included; can only be non-empty if exclude is empty
51         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
52         :return: a string containing all attribute names and values
53         """"""
54         def mklist(x):
55             if x is None:
56                 return []
57             if type(x) == str:
58                 return [x]
59             return x
60 
61         exclude = mklist(exclude)
62         include = mklist(include)
63         if len(exclude) > 0 and len(include) > 0:
64             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
65 
66         if len(include) == 0:
67             attributeDict = self.__dict__
68         else:
69             attributeDict = {k: getattr(self, k) for k in include}
70         d = {k: v for k, v in attributeDict.items() if k not in exclude}
71         d.update(additionalEntries)
72         return dictString(d)
73 
74     def _toStringObjectInfo(self) -> str:
","Before: 50, 51, 52, 53, 54
After: 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70",improve string generation,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,396,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 36, 'parameters': 2, '(': 7, ')': 7, ':': 6, 'block': 4, 'return_statement': 2, 'return': 2, 'attribute': 4, 'call': 5, 'argument_list': 5, '.': 4, ',': 4, 'typed_default_parameter': 1, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, '=': 4, 'none': 2, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 1, 'if': 2, 'comparison_operator': 3, 'is': 1, 'assignment': 3, 'list': 2, 'elif_clause': 1, 'elif': 1, '==': 1, 'dictionary_comprehension': 1, '{': 1, 'pair': 1, 'for_in_clause': 1, 'for': 1, 'pattern_list': 1, 'in': 1, 'if_clause': 1, 'not in': 2, '}': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 7, 'end_line': 8, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3626831649387075,0.3566264530266093,"(tensor([0.8301]), tensor([0.9157]), tensor([0.8708]), tensor([0.9063]))"
"55         d.update(additionalEntries)
56         return dictString(d)
57 
58     def _toStringObjectInfo(self) -> str:
59         """"""
60         Creates a string containing information on the objects state, which is the name and value of all attributes
61         without the attributes that are in the list provided by _toStringExclusions. It is used by the methods __str__
62         and __repr__. This method can be overwritten by sub-classes to provide a custom string.
63 
64         :return: a string containing all attribute names and values
65         """"""
66         return self._toStringProperties(exclude=self._toStringExcludes(), **self._toStringAdditionalEntries())
67 
68     def _toStringExcludes(self) -> List[str]:
","71         d.update(additionalEntries)
72         return dictString(d)
73 
74     def _toStringObjectInfo(self) -> str:
75         """"""
76         Creates a string containing information on the objects state, which is the name and value of all attributes
77         without the attributes that are in the list provided by _toStringExclusions. It is used by the methods __str__
78         and __repr__. This method can be overwritten by sub-classes to provide a custom string.
79 
80         :return: a string containing all attribute names and values
81         """"""
82         return self._toStringProperties(exclude=self._toStringExcludes(), include=self._toStringIncludes(), **self._toStringAdditionalEntries())
83 
84     def _toStringExcludes(self) -> List[str]:
","Before: 66
After: 82",improve string generation,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,0,527,"{'module': 1, 'expression_statement': 2, 'call': 5, 'attribute': 4, 'identifier': 15, '.': 4, 'argument_list': 5, '(': 6, ')': 6, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 1, '=': 1, ',': 1, 'dictionary_splat': 1, '**': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 25, 'name': 'dictString', 'long_name': 'dictString( d )', 'start_line': 7, 'end_line': 8, 'full_parameters': ['d'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7411551969305041,0.7312045484143789,"(tensor([0.9743]), tensor([0.9770]), tensor([0.9756]), tensor([0.9767]))"
"469                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
470 
471     @abstractmethod
472     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
473         """"""
474         If you are implementing a probabilistic classifier, this method has to return a data frame with probabilities
475         (one column per label). The default implementation of _predict will then use the output of
476         this method and convert it to predicted labels (via argmax).
477 
478         In case you want to predict labels only or have a more efficient implementation of predicting labels than
479         using argmax, your will have to override _predict in your implementation. In the former case of a
480         non-probabilistic classifier, the implementation of this method should raise an exception, like the one below.
481         """"""
482         raise NotImplementedError(f""Model {self.__class__.__name__} does not support prediction of probabilities"")
483 
484     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
","469                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
470 
471     @abstractmethod
472     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
473         """"""
474         If you are implementing a probabilistic classifier, this method has to return a data frame with probabilities
475         (one column per label). The default implementation of _predict will then use the output of
476         this method and convert it to predicted labels (via argmax).
477 
478         In case you want to predict labels only or have a more efficient implementation of predicting labels than
479         using argmax, you may override _predict instead of implementing this method. In the case of a
480         non-probabilistic classifier, the implementation of this method should raise an exception.
481         """"""
482         raise NotImplementedError(f""{self.__class__.__name__} does not implement _predictClassProbabilities."")
483 
484     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 479, 480
After: 479, 480",update vector_model.py to use _predictclassprobabilities,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,1,3431,"{'module': 1, 'ERROR': 2, 'expression_statement': 2, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 4, '{': 4, 'identifier': 16, '}': 4, 'call': 2, 'argument_list': 2, '(': 3, ')': 4, 'string_end': 3, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 2, 'attribute': 4, '.': 4, '->': 1, 'block': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8651576553476641,0.8626286066619863,"(tensor([0.9855]), tensor([0.9756]), tensor([0.9805]), tensor([0.9766]))"
"469                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
470 
471     @abstractmethod
472     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
473         """"""
474         If you are implementing a probabilistic classifier, this method has to return a data frame with probabilities
475         (one column per label). The default implementation of _predict will then use the output of
476         this method and convert it to predicted labels (via argmax).
477 
478         In case you want to predict labels only or have a more efficient implementation of predicting labels than
479         using argmax, your will have to override _predict in your implementation. In the former case of a
480         non-probabilistic classifier, the implementation of this method should raise an exception, like the one below.
481         """"""
482         raise NotImplementedError(f""Model {self.__class__.__name__} does not support prediction of probabilities"")
483 
484     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
","469                     f""Probabilities data frame may not be correctly normalised: checked row {i}/{maxRowsToCheck} contains {list(valueSeries)}"")
470 
471     @abstractmethod
472     def _predictClassProbabilities(self, X: pd.DataFrame) -> pd.DataFrame:
473         """"""
474         If you are implementing a probabilistic classifier, this method has to return a data frame with probabilities
475         (one column per label). The default implementation of _predict will then use the output of
476         this method and convert it to predicted labels (via argmax).
477 
478         In case you want to predict labels only or have a more efficient implementation of predicting labels than
479         using argmax, you may override _predict instead of implementing this method. In the case of a
480         non-probabilistic classifier, the implementation of this method should raise an exception.
481         """"""
482         raise NotImplementedError(f""{self.__class__.__name__} does not implement _predictClassProbabilities."")
483 
484     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
","Before: 482
After: 482",update vector_model.py to use _predictclassprobabilities,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,1,3347,"{'module': 1, 'ERROR': 2, 'expression_statement': 2, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 4, '{': 4, 'identifier': 16, '}': 4, 'call': 2, 'argument_list': 2, '(': 3, ')': 4, 'string_end': 3, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 2, 'attribute': 4, '.': 4, '->': 1, 'block': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8651576553476641,0.8626286066619863,"(tensor([0.9855]), tensor([0.9756]), tensor([0.9805]), tensor([0.9766]))"
"481         """"""
482         raise NotImplementedError(f""Model {self.__class__.__name__} does not support prediction of probabilities"")
483 
484     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
485         try:
486             predictedProbabilitiesDf = self._predictClassProbabilities(x)
487         except Exception:
488             raise Exception(f""Wrong implementation of {self.__class__.__name__}. For non-probabilistic classifiers ""
489                             ""_predict has to be overrode!"")
490         return self.convertClassProbabilitiesToPredictions(predictedProbabilitiesDf)
491 
492 
","481         """"""
482         raise NotImplementedError(f""{self.__class__.__name__} does not implement _predictClassProbabilities."")
483 
484     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
485         predictedProbabilitiesDf = self._predictClassProbabilities(x)
486         return self.convertClassProbabilitiesToPredictions(predictedProbabilitiesDf)
487 
488 
","Before: 485, 486, 487, 488, 489
After: 485",update vector_model.py to use _predictclassprobabilities,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,c6ab3b72847012be1040ffe51d43bf052126f282,356c95e395747541357e0c264d5bbb1b026d268a,1,3374,"{'module': 1, 'ERROR': 1, 'string_start': 1, ')': 2, 'return': 1, 'expression_statement': 1, 'call': 1, 'attribute': 1, 'identifier': 3, '.': 1, 'argument_list': 1, '(': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.37237131120286465,0.3690221772544493,"(tensor([0.9580]), tensor([0.8775]), tensor([0.9160]), tensor([0.8849]))"
"197             return None
198         return resultWriter.childWithAddedPrefix(model.getName() + ""-"")
199 
200     def performCrossValidation(self, model: TModel, showPlots=False, logResults=True, resultWriter: Optional[ResultWriter] = None) -> TCrossValData:
201         """"""
202         Evaluates the given model via cross-validation
203 
204         :param model: the model to evaluate
205         :param showPlots: whether to show plots that visualise evaluation results (combining all folds)
206         :param logResults: whether to log evaluation results
207         :param resultWriter: a writer with which to store text files and plots. The evaluated model's name is added to each filename
208             automatically
209         :return: cross-validation result data
210         """"""
211         resultWriter = self._resultWriterForModel(resultWriter, model)
212         crossValidator = createVectorModelCrossValidator(self.inputOutputData, model=model, **self.crossValidatorParams)
213         crossValidationData = crossValidator.evalModel(model)
214         strEvalResults = str(crossValidationData.getEvalStatsCollection().aggStats())
215         if logResults:
216             log.info(f""Cross-validation results: {strEvalResults}"")
217         if resultWriter is not None:
218             resultWriter.writeTextFile(""crossval-results"", {strEvalResults})
219         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
220         return crossValidationData
221 
222     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
","197             return None
198         return resultWriter.childWithAddedPrefix(model.getName() + ""-"")
199 
200     def performCrossValidation(self, model: TModel, showPlots=False, logResults=True, resultWriter: Optional[ResultWriter] = None) -> TCrossValData:
201         """"""
202         Evaluates the given model via cross-validation
203 
204         :param model: the model to evaluate
205         :param showPlots: whether to show plots that visualise evaluation results (combining all folds)
206         :param logResults: whether to log evaluation results
207         :param resultWriter: a writer with which to store text files and plots. The evaluated model's name is added to each filename
208             automatically
209         :return: cross-validation result data
210         """"""
211         resultWriter = self._resultWriterForModel(resultWriter, model)
212         crossValidator = createVectorModelCrossValidator(self.inputOutputData, model=model, **self.crossValidatorParams)
213         crossValidationData = crossValidator.evalModel(model)
214         strEvalResults = str(crossValidationData.getEvalStatsCollection().aggStats())
215         if logResults:
216             log.info(f""Cross-validation results: {strEvalResults}"")
217         if resultWriter is not None:
218             resultWriter.writeTextFile(""crossval-results"", strEvalResults)
219         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
220         return crossValidationData
221 
222     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
","Before: 218
After: 218",fix linting errors,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,6d85d86d9188b04067aaaf38821cef423aab14e5,aca0f8fc18ee691d9fb3ac9dbbf9627d310910c5,0,2094,"{'module': 1, 'return_statement': 3, 'return': 3, 'none': 3, 'call': 11, 'attribute': 11, 'identifier': 52, '.': 11, 'argument_list': 11, '(': 12, 'binary_operator': 1, ')': 12, '+': 1, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 10, 'typed_parameter': 1, ':': 5, 'type': 4, 'default_parameter': 2, '=': 10, 'false': 1, 'true': 1, 'typed_default_parameter': 1, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, '->': 1, 'block': 3, 'expression_statement': 8, 'assignment': 4, 'keyword_argument': 3, 'dictionary_splat': 1, '**': 1, 'if_statement': 2, 'if': 2, 'interpolation': 1, '{': 2, '}': 2, 'comparison_operator': 1, 'is not': 2, 'set': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.982729309178603,0.982729309178603,"(tensor([0.9971]), tensor([0.9971]), tensor([0.9971]), tensor([0.9971]))"
"239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
243         """"""
244         applies the model to the given input tensor and returns the scaled result (i.e. in the original scale)
245 
246         :param X: the input tensor or data set
247         :param kwargs: parameters to pass on to apply
248 
249         :return: a scaled output tensor or, if MC-Dropout is applied, a pair (y, sd) of scaled tensors, where
250             y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
251         """"""
252         return self.apply(X, scaleOutput=True, scaleInput=True, **kwargs)
253 
254     def scaledOutput(self, output: torch.Tensor) -> torch.Tensor:
","239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], **kwargs) -> Union[torch.Tensor, np.ndarray]:
243         """"""
244         applies the model to the given input tensor and returns the scaled result (i.e. in the original scale)
245 
246         :param X: the input tensor(s) or data set
247         :param kwargs: parameters to pass on to apply
248 
249         :return: a scaled output tensor or, if MC-Dropout is applied, a pair (y, sd) of scaled tensors, where
250             y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
251         """"""
252         return self.apply(X, scaleOutput=True, scaleInput=True, **kwargs)
253 
254     def scaledOutput(self, output: torch.Tensor) -> torch.Tensor:
","Before: 242
After: 242",add inputtensoriser attribute to vectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,ec35df86cd8cf23ff14f971b172ccc282c875450,6a5db3528ca030568d642c19f82bfa11b3c08c57,0,2250,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'pattern_list': 1, 'identifier': 33, ',': 12, '=': 4, 'call': 4, 'attribute': 6, '.': 6, 'argument_list': 4, '(': 5, 'keyword_argument': 3, ')': 5, 'return_statement': 2, 'return': 2, 'expression_list': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'true': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9418025716152553,0.9436163335298609,"(tensor([0.9905]), tensor([0.9949]), tensor([0.9927]), tensor([0.9944]))"
"239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet], **kwargs) -> Union[torch.Tensor, np.ndarray]:
243         """"""
244         applies the model to the given input tensor and returns the scaled result (i.e. in the original scale)
245 
246         :param X: the input tensor or data set
247         :param kwargs: parameters to pass on to apply
248 
249         :return: a scaled output tensor or, if MC-Dropout is applied, a pair (y, sd) of scaled tensors, where
250             y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
251         """"""
252         return self.apply(X, scaleOutput=True, scaleInput=True, **kwargs)
253 
254     def scaledOutput(self, output: torch.Tensor) -> torch.Tensor:
","239             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
240             return extract(y), extract(stddev)
241 
242     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], **kwargs) -> Union[torch.Tensor, np.ndarray]:
243         """"""
244         applies the model to the given input tensor and returns the scaled result (i.e. in the original scale)
245 
246         :param X: the input tensor(s) or data set
247         :param kwargs: parameters to pass on to apply
248 
249         :return: a scaled output tensor or, if MC-Dropout is applied, a pair (y, sd) of scaled tensors, where
250             y the mean output tensor and sd is a tensor of the same dimension containing standard deviations
251         """"""
252         return self.apply(X, scaleOutput=True, scaleInput=True, **kwargs)
253 
254     def scaledOutput(self, output: torch.Tensor) -> torch.Tensor:
","Before: 246
After: 246",add inputtensoriser attribute to vectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,ec35df86cd8cf23ff14f971b172ccc282c875450,6a5db3528ca030568d642c19f82bfa11b3c08c57,0,2301,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'pattern_list': 1, 'identifier': 33, ',': 12, '=': 4, 'call': 4, 'attribute': 6, '.': 6, 'argument_list': 4, '(': 5, 'keyword_argument': 3, ')': 5, 'return_statement': 2, 'return': 2, 'expression_list': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 7, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'true': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9418025716152553,0.9436163335298609,"(tensor([0.9905]), tensor([0.9949]), tensor([0.9927]), tensor([0.9944]))"
"391     def _createTorchModel(self) -> TorchModel:
392         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
393 
394     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
395         dataUtil = VectorDataUtil(inputs, outputs, self.model.cuda, normalisationMode=self.normalisationMode)
396         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
397 
398     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","398     def _createTorchModel(self) -> TorchModel:
399         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
400 
401     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
402         dataUtil = VectorDataUtil(inputs, outputs, self.model.cuda, normalisationMode=self.normalisationMode, inputTensoriser=self.inputTensoriser)
403         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
404 
405     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","Before: 395
After: 402",add inputtensoriser attribute to vectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,ec35df86cd8cf23ff14f971b172ccc282c875450,6a5db3528ca030568d642c19f82bfa11b3c08c57,0,3586,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 33, 'parameters': 2, '(': 5, ')': 5, '->': 2, 'type': 4, ':': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 3, 'attribute': 10, '.': 10, 'argument_list': 3, 'list_splat': 1, '*': 1, ',': 7, 'dictionary_splat': 1, '**': 1, 'typed_parameter': 2, 'expression_statement': 1, 'assignment': 1, '=': 2, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7740066318780419,0.7475272180394044,"(tensor([0.9669]), tensor([0.9811]), tensor([0.9739]), tensor([0.9796]))"
"400         dataSetProvider = self._createDataSetProvider(inputs, outputs)
401         self.model.fit(dataSetProvider, self.nnOptimiserParams)
402 
403     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
404         results = []
405         i = 0
406         batchSize = 2**13
407         while i < len(inputs):
408             inputSlice = inputs.iloc[i:i+batchSize]
409             results.append(self.model.applyScaled(inputSlice.values, asNumpy=True))
410             i += batchSize
411         return np.concatenate(results)
412 
413     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
","407         dataSetProvider = self._createDataSetProvider(inputs, outputs)
408         self.model.fit(dataSetProvider, self.nnOptimiserParams)
409 
410     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
411         batchSize = 2**13
412         results = []
413         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
414         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
415             results.append(self.model.applyScaled(inputBatch, asNumpy=True))
416         return np.concatenate(results)
417 
418     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 404, 405, 407, 408, 409, 410
After: 412, 413, 414, 415",add inputtensoriser attribute to vectorregressionmodel,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,ec35df86cd8cf23ff14f971b172ccc282c875450,6a5db3528ca030568d642c19f82bfa11b3c08c57,0,3711,"{'module': 1, 'expression_statement': 8, 'assignment': 5, 'identifier': 43, '=': 6, 'call': 6, 'attribute': 12, '.': 12, 'argument_list': 6, '(': 7, ',': 4, ')': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 4, 'type': 2, '->': 1, 'block': 2, 'list': 1, '[': 2, ']': 2, 'integer': 3, 'binary_operator': 2, '**': 1, 'while_statement': 1, 'while': 1, 'comparison_operator': 1, '<': 1, 'subscript': 1, 'slice': 1, '+': 1, 'keyword_argument': 1, 'true': 1, 'augmented_assignment': 1, '+=': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5203167940836061,0.5022852552738793,"(tensor([0.9218]), tensor([0.9275]), tensor([0.9247]), tensor([0.9269]))"
"38 
39 
40 class MultiLayerPerceptronVectorRegressionModel(TorchVectorRegressionModel):
41     def __init__(self, hiddenDims: Sequence[int] = (5, 5), hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
42             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
43             normalisationMode: NormalisationMode = NormalisationMode .MAX_BY_COLUMN,
44             cuda: bool = True, pDropout: Optional[float] = None, nnOptimiserParams: Optional[NNOptimiserParams] = None,
45             **nnOptimiserDictParams) -> None:
46         """"""
47         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
48         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
49         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
50         :param normalisationMode: the normalisation mode to apply to input and output data
51         :param cuda: whether to use CUDA (GPU acceleration)
52         :param pDropout: the probability with which to apply dropouts after each hidden layer
53         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
54         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
55         """"""
56         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
57         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
58                 dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
59 
60 
","38 
39 
40 class MultiLayerPerceptronVectorRegressionModel(TorchVectorRegressionModel):
41     def __init__(self, hiddenDims: Sequence[int] = (5, 5), hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
42             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
43             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN,
44             cuda: bool = True, pDropout: Optional[float] = None, nnOptimiserParams: Optional[NNOptimiserParams] = None,
45             **nnOptimiserDictParams) -> None:
46         """"""
47         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
48         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
49         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
50         :param normalisationMode: the normalisation mode to apply to input and output data
51         :param cuda: whether to use CUDA (GPU acceleration)
52         :param pDropout: the probability with which to apply dropouts after each hidden layer
53         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
54         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
55         """"""
56         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
57         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
58                 dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
59 
60 
","Before: 43
After: 43",fix typo in src/src/sensai/torch/torch_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,ec35df86cd8cf23ff14f971b172ccc282c875450,6a5db3528ca030568d642c19f82bfa11b3c08c57,0,554,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 52, 'argument_list': 5, '(': 7, ')': 7, ':': 9, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 19, 'typed_default_parameter': 7, 'type': 16, 'generic_type': 6, 'type_parameter': 6, '[': 9, ']': 9, '=': 9, 'tuple': 1, 'integer': 2, 'list': 3, 'attribute': 8, '.': 8, 'none': 4, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 3, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 4, 'keyword_argument': 1}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9878572195474116,0.9872648434936355,"(tensor([0.9999]), tensor([0.9999]), tensor([0.9999]), tensor([0.9999]))"
"203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
207         """"""
208         :param figure: whether to plot in a separate figure and return that figure
209         :param cmap: the colour map to use (see corresponding parameter of plt.imshow); if None use colour map from white to red
210         :param bins: how many bins to use for constructing the heatmap
211         :param titleAdd: a string to add to the title (on a second line)
212         :param kwargs: will be passed to plt.imshow()
213 
214         :return:  the resulting figure object or None
215         """"""
216         fig = None
217         title = ""Heat Map of Ground Truth vs. Predicted Values""
218         if titleAdd:
219             title += ""\n"" + titleAdd
220         if figure:
221             fig = plt.figure(title.replace(""\n"", "" ""))
222         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
223         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
224         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
225         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
226         if cmap is None:
227             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
228         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
229 
230         plt.xlabel(""ground truth"")
231         plt.ylabel(""prediction"")
232         plt.title(title)
233         return fig
234 
235 
","203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
207         """"""
208         :param figure: whether to plot in a separate figure and return that figure
209         :param cmap: the colour map to use (see corresponding parameter of plt.imshow); if None use colour map from white to red
210         :param bins: how many bins to use for constructing the heatmap
211         :param titleAdd: a string to add to the title (on a second line)
212         :param kwargs: will be passed to plt.imshow()
213 
214         :return:  the resulting figure object or None
215         """"""
216         fig = None
217         title = ""Heat Map of Ground Truth vs. Predicted Values""
218         if titleAdd:
219             title += ""\n"" + titleAdd
220         if figure:
221             fig = plt.figure(title.replace(""\n"", "" ""))
222         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
223         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
224         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
225         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
226         if cmap is None:
227             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1.0, 0.95, 0.95)), (1, (0.7, 0, 0))))
228         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
229 
230         plt.xlabel(""ground truth"")
231         plt.ylabel(""prediction"")
232         plt.title(title)
233         return fig
234 
235 
","Before: 227
After: 227",update eval_stats_regression.py to 0.95/len(y_predicted),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2188,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 18, 'identifier': 90, '.': 18, 'argument_list': 16, '(': 20, ')': 20, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 35, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 12, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 10, 'float': 2, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'tuple': 3, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9365895430452653,0.9433035130647213,"(tensor([0.9863]), tensor([0.9932]), tensor([0.9898]), tensor([0.9925]))"
"428     Base class for the implementation of VectorClassificationModels based on TorchModels.
429     An instance of this class will have an instance of TorchModel as the underlying model.
430     """"""
431     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs: Sequence = (), modelKwArgs: Optional[dict] = None,
432             normalisationMode: NormalisationMode = NormalisationMode.NONE,
433             nnOptimiserParams: Union[dict, NNOptimiserParams, None] = None) -> None:
434         """"""
435         :param modelClass: the constructor with which to create the wrapped torch vector model
436         :param modelArgs: the constructor argument list to pass to modelClass
437         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
438         :param normalisationMode: the normalisation mode to apply to input data frames
439         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
440         """"""
441         super().__init__()
442         if modelKwArgs is None:
443             modelKwArgs = {}
444 
445         if nnOptimiserParams is None:
446             nnOptimiserParamsInstance = NNOptimiserParams()
447         else:
448             nnOptimiserParamsInstance = NNOptimiserParams.fromDictOrInstance(nnOptimiserParams)
449         if nnOptimiserParamsInstance.lossEvaluator is None:
450             nnOptimiserParamsInstance.lossEvaluator = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
451 
452         self.normalisationMode = normalisationMode
453         self.nnOptimiserParams = nnOptimiserParams
454         self.modelClass = modelClass
455         self.modelArgs = modelArgs
456         self.modelKwArgs = modelKwArgs
457         self.model: Optional[VectorTorchModel] = None
458         self.inputTensoriser = None
459 
460     def __setstate__(self, state) -> None:
","429     Base class for the implementation of VectorClassificationModels based on TorchModels.
430     An instance of this class will have an instance of TorchModel as the underlying model.
431     """"""
432     def __init__(self, outputMode: ClassificationOutputMode,
433             modelClass: Callable[..., VectorTorchModel], modelArgs: Sequence = (), modelKwArgs: Optional[dict] = None,
434             normalisationMode: NormalisationMode = NormalisationMode.NONE,
435             nnOptimiserParams: Union[dict, NNOptimiserParams, None] = None) -> None:
436         """"""
437         :param outputMode: specifies the nature of the output of the underlying neural network model
438         :param modelClass: the constructor with which to create the wrapped torch vector model
439         :param modelArgs: the constructor argument list to pass to modelClass
440         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
441         :param normalisationMode: the normalisation mode to apply to input data frames
442         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
443         """"""
444         super().__init__()
445         if modelKwArgs is None:
446             modelKwArgs = {}
447 
448         if nnOptimiserParams is None:
449             nnOptimiserParamsInstance = NNOptimiserParams()
450         else:
451             nnOptimiserParamsInstance = NNOptimiserParams.fromDictOrInstance(nnOptimiserParams)
452         if nnOptimiserParamsInstance.lossEvaluator is None:
453             lossFunction = NNLossEvaluatorClassification.LossFunction.defaultForOutputMode(outputMode)
454             nnOptimiserParamsInstance.lossEvaluator = NNLossEvaluatorClassification(lossFunction)
455 
456         self.outputMode = outputMode
457         self.normalisationMode = normalisationMode
458         self.nnOptimiserParams = nnOptimiserParams
459         self.modelClass = modelClass
460         self.modelArgs = modelArgs
461         self.modelKwArgs = modelKwArgs
462         self.model: Optional[VectorTorchModel] = None
463         self.inputTensoriser = None
464 
465     def __setstate__(self, state) -> None:
","Before: 431
After: 432, 433, 437",update torch_base.py for new version,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,4042,"{'module': 1, 'ERROR': 15, 'identifier': 76, 'for': 1, 'attribute': 1, '.': 2, 'class': 1, 'expression_statement': 8, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 10, 'assignment': 5, 'type': 5, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'block': 1, 'pass_statement': 1, 'pass': 1, 'comparison_operator': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6449905400141085,0.6441636943807988,"(tensor([0.9454]), tensor([0.9549]), tensor([0.9501]), tensor([0.9540]))"
"428     Base class for the implementation of VectorClassificationModels based on TorchModels.
429     An instance of this class will have an instance of TorchModel as the underlying model.
430     """"""
431     def __init__(self, modelClass: Callable[..., VectorTorchModel], modelArgs: Sequence = (), modelKwArgs: Optional[dict] = None,
432             normalisationMode: NormalisationMode = NormalisationMode.NONE,
433             nnOptimiserParams: Union[dict, NNOptimiserParams, None] = None) -> None:
434         """"""
435         :param modelClass: the constructor with which to create the wrapped torch vector model
436         :param modelArgs: the constructor argument list to pass to modelClass
437         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
438         :param normalisationMode: the normalisation mode to apply to input data frames
439         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
440         """"""
441         super().__init__()
442         if modelKwArgs is None:
443             modelKwArgs = {}
444 
445         if nnOptimiserParams is None:
446             nnOptimiserParamsInstance = NNOptimiserParams()
447         else:
448             nnOptimiserParamsInstance = NNOptimiserParams.fromDictOrInstance(nnOptimiserParams)
449         if nnOptimiserParamsInstance.lossEvaluator is None:
450             nnOptimiserParamsInstance.lossEvaluator = NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY)
451 
452         self.normalisationMode = normalisationMode
453         self.nnOptimiserParams = nnOptimiserParams
454         self.modelClass = modelClass
455         self.modelArgs = modelArgs
456         self.modelKwArgs = modelKwArgs
457         self.model: Optional[VectorTorchModel] = None
458         self.inputTensoriser = None
459 
460     def __setstate__(self, state) -> None:
","429     Base class for the implementation of VectorClassificationModels based on TorchModels.
430     An instance of this class will have an instance of TorchModel as the underlying model.
431     """"""
432     def __init__(self, outputMode: ClassificationOutputMode,
433             modelClass: Callable[..., VectorTorchModel], modelArgs: Sequence = (), modelKwArgs: Optional[dict] = None,
434             normalisationMode: NormalisationMode = NormalisationMode.NONE,
435             nnOptimiserParams: Union[dict, NNOptimiserParams, None] = None) -> None:
436         """"""
437         :param outputMode: specifies the nature of the output of the underlying neural network model
438         :param modelClass: the constructor with which to create the wrapped torch vector model
439         :param modelArgs: the constructor argument list to pass to modelClass
440         :param modelKwArgs: the dictionary of constructor keyword arguments to pass to modelClass
441         :param normalisationMode: the normalisation mode to apply to input data frames
442         :param nnOptimiserParams: the parameters to apply in NNOptimiser during training
443         """"""
444         super().__init__()
445         if modelKwArgs is None:
446             modelKwArgs = {}
447 
448         if nnOptimiserParams is None:
449             nnOptimiserParamsInstance = NNOptimiserParams()
450         else:
451             nnOptimiserParamsInstance = NNOptimiserParams.fromDictOrInstance(nnOptimiserParams)
452         if nnOptimiserParamsInstance.lossEvaluator is None:
453             lossFunction = NNLossEvaluatorClassification.LossFunction.defaultForOutputMode(outputMode)
454             nnOptimiserParamsInstance.lossEvaluator = NNLossEvaluatorClassification(lossFunction)
455 
456         self.outputMode = outputMode
457         self.normalisationMode = normalisationMode
458         self.nnOptimiserParams = nnOptimiserParams
459         self.modelClass = modelClass
460         self.modelArgs = modelArgs
461         self.modelKwArgs = modelKwArgs
462         self.model: Optional[VectorTorchModel] = None
463         self.inputTensoriser = None
464 
465     def __setstate__(self, state) -> None:
","Before: 450
After: 453, 454, 456",update torch_base.py for new version,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,4176,"{'module': 1, 'ERROR': 15, 'identifier': 76, 'for': 1, 'attribute': 1, '.': 2, 'class': 1, 'expression_statement': 8, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 10, 'assignment': 5, 'type': 5, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'block': 1, 'pass_statement': 1, 'pass': 1, 'comparison_operator': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6449905400141085,0.6441636943807988,"(tensor([0.9454]), tensor([0.9549]), tensor([0.9501]), tensor([0.9540]))"
"492         dataSetProvider = self._createDataSetProvider(inputs, outputs)
493         self.model.fit(dataSetProvider, self.nnOptimiserParams)
494 
495     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
496         batchSize = 64
497         results = []
498         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
499         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
500             results.append(self.model.applyScaled(inputBatch, asNumpy=True))
501         return np.concatenate(results)
502 
503     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","499         dataSetProvider = self._createDataSetProvider(inputs, outputs)
500         self.model.fit(dataSetProvider, self.nnOptimiserParams)
501 
502     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
503         batchSize = 64
504         results = []
505         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
506         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
507             results.append(self.model.applyScaled(inputBatch, asNumpy=False))
508         return torch.cat(results, dim=0)
509 
510     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 495
After: 502",update torch_base.py for new version,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,4707,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 44, '=': 7, 'call': 7, 'attribute': 14, '.': 14, 'argument_list': 7, '(': 8, ',': 8, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'integer': 1, 'list': 1, '[': 1, ']': 1, 'none': 1, 'keyword_argument': 3, 'for_statement': 1, 'for': 1, 'in': 1, 'true': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6451786030525621,0.6238343991134522,"(tensor([0.9649]), tensor([0.9646]), tensor([0.9647]), tensor([0.9646]))"
"492         dataSetProvider = self._createDataSetProvider(inputs, outputs)
493         self.model.fit(dataSetProvider, self.nnOptimiserParams)
494 
495     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
496         batchSize = 64
497         results = []
498         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
499         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
500             results.append(self.model.applyScaled(inputBatch, asNumpy=True))
501         return np.concatenate(results)
502 
503     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","499         dataSetProvider = self._createDataSetProvider(inputs, outputs)
500         self.model.fit(dataSetProvider, self.nnOptimiserParams)
501 
502     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
503         batchSize = 64
504         results = []
505         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
506         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
507             results.append(self.model.applyScaled(inputBatch, asNumpy=False))
508         return torch.cat(results, dim=0)
509 
510     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 500, 501
After: 507, 508",update torch_base.py for new version,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,4793,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 44, '=': 7, 'call': 7, 'attribute': 14, '.': 14, 'argument_list': 7, '(': 8, ',': 8, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'integer': 1, 'list': 1, '[': 1, ']': 1, 'none': 1, 'keyword_argument': 3, 'for_statement': 1, 'for': 1, 'in': 1, 'true': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6451786030525621,0.6238343991134522,"(tensor([0.9649]), tensor([0.9646]), tensor([0.9647]), tensor([0.9646]))"
"500             results.append(self.model.applyScaled(inputBatch, asNumpy=True))
501         return np.concatenate(results)
502 
503     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
504         y = self._predictOutputsForInputDataFrame(inputs)
505         normalisationConstants = y.sum(axis=1)
506         for i in range(y.shape[0]):
507             y[i,:] /= normalisationConstants[i]
508         return pd.DataFrame(y, columns=self._labels)
509 
510     def __str__(self) -> str:
","507             results.append(self.model.applyScaled(inputBatch, asNumpy=False))
508         return torch.cat(results, dim=0)
509 
510     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
511         y = self._predictOutputsForInputDataFrame(inputs)
512         if self.outputMode == ClassificationOutputMode.PROBABILITIES:
513             pass
514         elif self.outputMode == ClassificationOutputMode.LOG_PROBABILITIES:
515             y = y.exp()
516         elif self.outputMode == ClassificationOutputMode.UNNORMALISED_LOG_PROBABILITIES:
517             y = y.softmax(dim=1)
518         else:
519             raise ValueError(f""Unhandled output mode {self.outputMode}"")
520         return pd.DataFrame(y.numpy(), columns=self._labels)
521 
522     def __str__(self) -> str:
","Before: 505, 506, 507, 508
After: 512, 513, 514, 515, 516, 517, 518, 519, 520",update torch_base.py for new version,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,4857,"{'module': 1, 'expression_statement': 4, 'call': 7, 'attribute': 11, 'identifier': 39, '.': 11, 'argument_list': 7, '(': 8, ',': 4, 'keyword_argument': 3, '=': 5, 'true': 1, ')': 8, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 4, 'type': 2, '->': 1, 'block': 2, 'assignment': 2, 'integer': 2, 'for_statement': 1, 'for': 1, 'in': 1, 'subscript': 3, '[': 3, ']': 3, 'augmented_assignment': 1, 'slice': 1, '/=': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 30, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.327622412331606,0.28315210770733207,"(tensor([0.8354]), tensor([0.8865]), tensor([0.8602]), tensor([0.8811]))"
"1 import collections
2 import logging
3 import re
4 from typing import Optional, Union
5 
6 import numpy as np
7 import pandas as pd
8 import torch
9 
10 from .lstnet_modules import LSTNetwork
","5 
6 import pandas as pd
7 import torch
8 
9 from .lstnet_modules import LSTNetwork
10 from ...torch_base import TorchVectorClassificationModel, VectorTorchModel, ClassificationOutputMode
11 from ...torch_data import TorchDataSetProviderFromDataUtil, TensorScalerIdentity, TensorScaler, DataUtil
12 from ...torch_enums import ActivationFunction
13 from ...torch_opt import NNOptimiserParams
14 from ....util.string import objectRepr
","Before: 6, 11
After: 10, 12",update lstnet_models.py with new lstnet_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,31,"{'module': 1, 'import_statement': 6, 'import': 7, 'dotted_name': 9, 'identifier': 11, 'import_from_statement': 1, 'from': 1, ',': 1, 'aliased_import': 2, 'as': 2}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 182, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 58, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 12, 'token_count': 199, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ActivationFunction . LOG_SOFTMAX , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 59, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ActivationFunction . LOG_SOFTMAX', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.13259318860287828,0.11994148895186214,"(tensor([0.7636]), tensor([0.8426]), tensor([0.8012]), tensor([0.8340]))"
"27     For each N-digit prefix, we must have the same set of suffixes in the list of columns, i.e. we must have the same
28     features for each time slice in the input time series.
29     """"""
30     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numClasses: Optional[int] = None,
31             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
32             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", cuda=True,
33             nnOptimiserParams: Union[dict, NNOptimiserParams] = None):
34         """"""
35         :param numInputTimeSlices: the number of input time slices
36         :param inputDimPerTimeSlice: the dimension of the input data per time slice
37         :param numClasses: the number of classes considered by this classification problem; if None, determine from data
38         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
39             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
40         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
41             if it is 0, then the entire complex processing path is not applied.
42         :param hidRNN: the number of hidden output dimensions for the RNN stage
43         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
44         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
45         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
46             If it is 0, the highway component is not used.
47         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
48         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
49         :param outputActivation: the output activation function
50         :param nnOptimiserParams: parameters for NNOptimiser to use for training
51         """"""
52         self.cuda = cuda
53         self.numClasses = numClasses
54         lstnetArgs = dict(numInputTimeSlices=numInputTimeSlices, inputDimPerTimeSlice=inputDimPerTimeSlice, numOutputTimeSlices=1,
55             outputDimPerTimeSlice=numClasses, numConvolutions=numConvolutions, numCnnTimeSlices=numCnnTimeSlices, hidRNN=hidRNN,
56             hwWindow=hwWindow, hwCombine=hwCombine, dropout=dropout, outputActivation=outputActivation,
57             skip=skip, hidSkip=hidSkip, isClassification=True)
58         super().__init__(self._LSTNetworkModel, modelArgs=[self.cuda], modelKwArgs=lstnetArgs, nnOptimiserParams=nnOptimiserParams)
59 
60     class _LSTNetworkModel(VectorTorchModel):
","27     For each N-digit prefix, we must have the same set of suffixes in the list of columns, i.e. we must have the same
28     features for each time slice in the input time series.
29     """"""
30     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numClasses: Optional[int] = None,
31             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
32             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=ActivationFunction.LOG_SOFTMAX, cuda=True,
33             nnOptimiserParams: Union[dict, NNOptimiserParams] = None):
34         """"""
35         :param numInputTimeSlices: the number of input time slices
36         :param inputDimPerTimeSlice: the dimension of the input data per time slice
37         :param numClasses: the number of classes considered by this classification problem; if None, determine from data
38         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
39             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
40         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
41             if it is 0, then the entire complex processing path is not applied.
42         :param hidRNN: the number of hidden output dimensions for the RNN stage
43         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
44         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
45         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
46             If it is 0, the highway component is not used.
47         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
48         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
49         :param outputActivation: the output activation function
50         :param nnOptimiserParams: parameters for NNOptimiser to use for training
51         """"""
52         self.cuda = cuda
53         self.numClasses = numClasses
54         lstnetArgs = dict(numInputTimeSlices=numInputTimeSlices, inputDimPerTimeSlice=inputDimPerTimeSlice, numOutputTimeSlices=1,
55             outputDimPerTimeSlice=numClasses, numConvolutions=numConvolutions, numCnnTimeSlices=numCnnTimeSlices, hidRNN=hidRNN,
56             hwWindow=hwWindow, hwCombine=hwCombine, dropout=dropout, outputActivation=outputActivation,
57             skip=skip, hidSkip=hidSkip, isClassification=True)
58         outputMode = ClassificationOutputMode.forActivationFn(ActivationFunction.torchFunctionFromAny(outputActivation))
59         super().__init__(outputMode, self._LSTNetworkModel, modelArgs=[self.cuda], modelKwArgs=lstnetArgs, nnOptimiserParams=nnOptimiserParams)
60 
61     class _LSTNetworkModel(VectorTorchModel):
","Before: 32
After: 32",update lstnet_models.py with new lstnet_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,249,"{'module': 1, 'expression_statement': 5, 'binary_operator': 3, 'identifier': 245, 'ERROR': 29, '-': 3, ',': 12, 'comparison_operator': 3, 'in': 3, 'attribute': 15, '.': 17, 'for_statement': 1, 'for': 3, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 26, 'block': 1, 'assignment': 3, 'type': 3, ';': 2, 'none': 1, 'from': 1, '(': 2, 'is': 5, ')': 2, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 182, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 58, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 12, 'token_count': 199, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ActivationFunction . LOG_SOFTMAX , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 59, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ActivationFunction . LOG_SOFTMAX', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9615375554793484,0.959440456574726,"(tensor([0.9780]), tensor([0.9879]), tensor([0.9829]), tensor([0.9869]))"
"27     For each N-digit prefix, we must have the same set of suffixes in the list of columns, i.e. we must have the same
28     features for each time slice in the input time series.
29     """"""
30     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numClasses: Optional[int] = None,
31             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
32             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", cuda=True,
33             nnOptimiserParams: Union[dict, NNOptimiserParams] = None):
34         """"""
35         :param numInputTimeSlices: the number of input time slices
36         :param inputDimPerTimeSlice: the dimension of the input data per time slice
37         :param numClasses: the number of classes considered by this classification problem; if None, determine from data
38         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
39             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
40         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
41             if it is 0, then the entire complex processing path is not applied.
42         :param hidRNN: the number of hidden output dimensions for the RNN stage
43         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
44         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
45         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
46             If it is 0, the highway component is not used.
47         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
48         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
49         :param outputActivation: the output activation function
50         :param nnOptimiserParams: parameters for NNOptimiser to use for training
51         """"""
52         self.cuda = cuda
53         self.numClasses = numClasses
54         lstnetArgs = dict(numInputTimeSlices=numInputTimeSlices, inputDimPerTimeSlice=inputDimPerTimeSlice, numOutputTimeSlices=1,
55             outputDimPerTimeSlice=numClasses, numConvolutions=numConvolutions, numCnnTimeSlices=numCnnTimeSlices, hidRNN=hidRNN,
56             hwWindow=hwWindow, hwCombine=hwCombine, dropout=dropout, outputActivation=outputActivation,
57             skip=skip, hidSkip=hidSkip, isClassification=True)
58         super().__init__(self._LSTNetworkModel, modelArgs=[self.cuda], modelKwArgs=lstnetArgs, nnOptimiserParams=nnOptimiserParams)
59 
60     class _LSTNetworkModel(VectorTorchModel):
","27     For each N-digit prefix, we must have the same set of suffixes in the list of columns, i.e. we must have the same
28     features for each time slice in the input time series.
29     """"""
30     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numClasses: Optional[int] = None,
31             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
32             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=ActivationFunction.LOG_SOFTMAX, cuda=True,
33             nnOptimiserParams: Union[dict, NNOptimiserParams] = None):
34         """"""
35         :param numInputTimeSlices: the number of input time slices
36         :param inputDimPerTimeSlice: the dimension of the input data per time slice
37         :param numClasses: the number of classes considered by this classification problem; if None, determine from data
38         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
39             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
40         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
41             if it is 0, then the entire complex processing path is not applied.
42         :param hidRNN: the number of hidden output dimensions for the RNN stage
43         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
44         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
45         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
46             If it is 0, the highway component is not used.
47         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
48         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
49         :param outputActivation: the output activation function
50         :param nnOptimiserParams: parameters for NNOptimiser to use for training
51         """"""
52         self.cuda = cuda
53         self.numClasses = numClasses
54         lstnetArgs = dict(numInputTimeSlices=numInputTimeSlices, inputDimPerTimeSlice=inputDimPerTimeSlice, numOutputTimeSlices=1,
55             outputDimPerTimeSlice=numClasses, numConvolutions=numConvolutions, numCnnTimeSlices=numCnnTimeSlices, hidRNN=hidRNN,
56             hwWindow=hwWindow, hwCombine=hwCombine, dropout=dropout, outputActivation=outputActivation,
57             skip=skip, hidSkip=hidSkip, isClassification=True)
58         outputMode = ClassificationOutputMode.forActivationFn(ActivationFunction.torchFunctionFromAny(outputActivation))
59         super().__init__(outputMode, self._LSTNetworkModel, modelArgs=[self.cuda], modelKwArgs=lstnetArgs, nnOptimiserParams=nnOptimiserParams)
60 
61     class _LSTNetworkModel(VectorTorchModel):
","Before: 58
After: 58, 59",update lstnet_models.py with new lstnet_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,406,"{'module': 1, 'expression_statement': 5, 'binary_operator': 3, 'identifier': 245, 'ERROR': 29, '-': 3, ',': 12, 'comparison_operator': 3, 'in': 3, 'attribute': 15, '.': 17, 'for_statement': 1, 'for': 3, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 26, 'block': 1, 'assignment': 3, 'type': 3, ';': 2, 'none': 1, 'from': 1, '(': 2, 'is': 5, ')': 2, 'if': 1, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 182, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 58, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 12, 'token_count': 199, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ActivationFunction . LOG_SOFTMAX , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 59, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ActivationFunction . LOG_SOFTMAX', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9615375554793484,0.959440456574726,"(tensor([0.9780]), tensor([0.9879]), tensor([0.9829]), tensor([0.9869]))"
"89             raise ValueError(f""Output dimension {self.numClasses} per time time slice was specified, while the training data contains {len(self._labels)} classes"")
90         return TorchDataSetProviderFromDataUtil(self.DataUtil(inputs, outputs, self.numClasses), self.cuda)
91 
92     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
93         log.info(f""Predicting outputs for {len(inputs)} inputs"")
94         result = super()._predictOutputsForInputDataFrame(inputs)
95         return np.squeeze(result, 2)
96 
97     def _computeModelInputs(self, x: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","90             raise ValueError(f""Output dimension {self.numClasses} per time time slice was specified, while the training data contains {len(self._labels)} classes"")
91         return TorchDataSetProviderFromDataUtil(self.DataUtil(inputs, outputs, self.numClasses), self.cuda)
92 
93     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
94         log.info(f""Predicting outputs for {len(inputs)} inputs"")
95         result = super()._predictOutputsForInputDataFrame(inputs)
96         return result.squeeze(2)
97 
98     def _computeModelInputs(self, x: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","Before: 92
After: 93",update lstnet_models.py with new lstnet_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,809,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 9, 'identifier': 33, 'argument_list': 9, '(': 10, 'string': 2, 'string_start': 2, 'string_content': 5, 'interpolation': 3, '{': 3, 'attribute': 10, '.': 10, '}': 3, ')': 10, 'string_end': 2, 'return_statement': 2, 'return': 2, ',': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 2, '->': 1, 'block': 1, 'expression_statement': 2, 'assignment': 1, '=': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 182, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 58, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 12, 'token_count': 199, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ActivationFunction . LOG_SOFTMAX , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 59, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ActivationFunction . LOG_SOFTMAX', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8149633171840153,0.8095333670461701,"(tensor([0.9845]), tensor([0.9822]), tensor([0.9834]), tensor([0.9825]))"
"89             raise ValueError(f""Output dimension {self.numClasses} per time time slice was specified, while the training data contains {len(self._labels)} classes"")
90         return TorchDataSetProviderFromDataUtil(self.DataUtil(inputs, outputs, self.numClasses), self.cuda)
91 
92     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
93         log.info(f""Predicting outputs for {len(inputs)} inputs"")
94         result = super()._predictOutputsForInputDataFrame(inputs)
95         return np.squeeze(result, 2)
96 
97     def _computeModelInputs(self, x: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","90             raise ValueError(f""Output dimension {self.numClasses} per time time slice was specified, while the training data contains {len(self._labels)} classes"")
91         return TorchDataSetProviderFromDataUtil(self.DataUtil(inputs, outputs, self.numClasses), self.cuda)
92 
93     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
94         log.info(f""Predicting outputs for {len(inputs)} inputs"")
95         result = super()._predictOutputsForInputDataFrame(inputs)
96         return result.squeeze(2)
97 
98     def _computeModelInputs(self, x: pd.DataFrame, Y: pd.DataFrame = None, fit=False) -> pd.DataFrame:
","Before: 95
After: 96",update lstnet_models.py with new lstnet_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,862,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 9, 'identifier': 33, 'argument_list': 9, '(': 10, 'string': 2, 'string_start': 2, 'string_content': 5, 'interpolation': 3, '{': 3, 'attribute': 10, '.': 10, '}': 3, ')': 10, 'string_end': 2, 'return_statement': 2, 'return': 2, ',': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 2, '->': 1, 'block': 1, 'expression_statement': 2, 'assignment': 1, '=': 1, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 11, 'token_count': 182, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 58, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 12, 'token_count': 199, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numClasses : Optional [ int ] = None , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ActivationFunction . LOG_SOFTMAX , cuda = True , nnOptimiserParams : Union [ dict , NNOptimiserParams ] = None )', 'start_line': 30, 'end_line': 59, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numClasses : Optional [ int ] = None', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ActivationFunction . LOG_SOFTMAX', ' cuda = True', ' nnOptimiserParams : Union [ dict', ' NNOptimiserParams ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8149633171840153,0.8095333670461701,"(tensor([0.9845]), tensor([0.9822]), tensor([0.9834]), tensor([0.9825]))"
"40     if isClassification==True; the latter shape matches what is required by the multi-dimensional case of loss function
41     CrossEntropyLoss, for example, and therefore is suitable for classification use cases.
42     """"""
43     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1,
44             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
45             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", isClassification=False):
46         """"""
47         :param numInputTimeSlices: the number of input time slices
48         :param inputDimPerTimeSlice: the dimension of the input data per time slice
49         :param numOutputTimeSlices: the number of time slices predicted by the model
50         :param outputDimPerTimeSlice: the number of dimensions per output time slice. While this is the number of
51             target variables per time slice for regression problems, this must be the number of classes for classification problems.
52         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
53             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
54         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
55             if it is 0, then the entire complex processing path is not applied.
56         :param hidRNN: the number of hidden output dimensions for the RNN stage
57         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
58         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
59         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
60             If it is 0, the highway component is not used.
61         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
62         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
63         :param outputActivation: the output activation function
64         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
65             to suit loss functions such as CrossEntropyLoss
66         """"""
67         if numConvolutions == 0 and hwWindow == 0:
68             raise ValueError(""No processing paths remain"")
69         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
70             raise Exception(""Inconsistent numbers of times slices provided"")
71 
72         super().__init__()
73         self.inputDimPerTimeSlice = inputDimPerTimeSlice
74         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
75         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
76         self.numOutputTimeSlices = numOutputTimeSlices
77         self.window = numInputTimeSlices
78         self.hidRNN = hidRNN
79         self.numConv = numConvolutions
80         self.hidSkip = hidSkip
81         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
82         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
83         self.skip = skip
84         self.hw = hwWindow
85         self.pDropout = dropout
86         self.isClassification = isClassification
87 
88         # configure CNN-RNN path
89         if self.numConv > 0:
90             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
91             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
92             if self.skip > 0:
93                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
94                 if self.skipRnnSeqLength == 0:
95                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
96                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
97                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
98             else:
99                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
100 
101         # configure highway component
102         if self.hw > 0:
103             # direct mapping from all inputs to all outputs
104             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
105             if hwCombine == 'plus':
106                 self.highwayCombine = self._plus
107             elif hwCombine == 'product':
108                 self.highwayCombine = self._product
109             elif hwCombine == 'bilinear':
110                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
111             else:
112                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
113 
114         self.output = self._getOutputActivationFn(outputActivation)
115 
116     def forward(self, x):
","43     if isClassification==True; the latter shape matches what is required by the multi-dimensional case of loss function
44     CrossEntropyLoss, for example, and therefore is suitable for classification use cases.
45     """"""
46     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1,
47             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
48             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation: Union[str, ActivationFunction, Callable] = ""sigmoid"",
49             isClassification=False):
50         """"""
51         :param numInputTimeSlices: the number of input time slices
52         :param inputDimPerTimeSlice: the dimension of the input data per time slice
53         :param numOutputTimeSlices: the number of time slices predicted by the model
54         :param outputDimPerTimeSlice: the number of dimensions per output time slice. While this is the number of
55             target variables per time slice for regression problems, this must be the number of classes for classification problems.
56         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
57             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
58         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
59             if it is 0, then the entire complex processing path is not applied.
60         :param hidRNN: the number of hidden output dimensions for the RNN stage
61         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
62         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
63         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
64             If it is 0, the highway component is not used.
65         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
66         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
67         :param outputActivation: the output activation function
68         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
69             to suit loss functions such as CrossEntropyLoss
70         """"""
71         if numConvolutions == 0 and hwWindow == 0:
72             raise ValueError(""No processing paths remain"")
73         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
74             raise Exception(""Inconsistent numbers of times slices provided"")
75 
76         super().__init__()
77         self.inputDimPerTimeSlice = inputDimPerTimeSlice
78         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
79         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
80         self.numOutputTimeSlices = numOutputTimeSlices
81         self.window = numInputTimeSlices
82         self.hidRNN = hidRNN
83         self.numConv = numConvolutions
84         self.hidSkip = hidSkip
85         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
86         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
87         self.skip = skip
88         self.hw = hwWindow
89         self.pDropout = dropout
90         self.isClassification = isClassification
91 
92         # configure CNN-RNN path
93         if self.numConv > 0:
94             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
95             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
96             if self.skip > 0:
97                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
98                 if self.skipRnnSeqLength == 0:
99                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
100                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
101                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
102             else:
103                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
104 
105         # configure highway component
106         if self.hw > 0:
107             # direct mapping from all inputs to all outputs
108             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
109             if hwCombine == 'plus':
110                 self.highwayCombine = self._plus
111             elif hwCombine == 'product':
112                 self.highwayCombine = self._product
113             elif hwCombine == 'bilinear':
114                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
115             else:
116                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
117 
118         self.output = ActivationFunction.torchFunctionFromAny(outputActivation)
119 
120     def forward(self, x):
","Before: 45
After: 48, 49",convert lstnet_modules.py to py3 syntax,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_modules.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,151,"{'module': 1, 'if_statement': 1, 'if': 2, 'boolean_operator': 1, 'comparison_operator': 6, 'identifier': 280, '==': 1, 'ERROR': 33, 'true': 1, ';': 2, 'is': 9, 'binary_operator': 3, '-': 3, ',': 13, 'for': 4, 'and': 1, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 28, 'block': 1, 'expression_statement': 5, 'assignment': 4, 'type': 4, 'attribute': 15, '(': 2, 'pattern_list': 1, ')': 2, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'call': 1, 'argument_list': 1, 'in': 2, 'as': 2}","{'cyclomatic_complexity': 13, 'nloc': 44, 'token_count': 436, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numOutputTimeSlices = 1 , outputDimPerTimeSlice = 1 , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , isClassification = False )', 'start_line': 43, 'end_line': 114, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numOutputTimeSlices = 1', ' outputDimPerTimeSlice = 1', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' isClassification = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 13, 'nloc': 45, 'token_count': 445, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numOutputTimeSlices = 1 , outputDimPerTimeSlice = 1 , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation : Union [ str , ActivationFunction , Callable ] = ""sigmoid"" , isClassification = False )', 'start_line': 46, 'end_line': 118, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numOutputTimeSlices = 1', ' outputDimPerTimeSlice = 1', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation : Union [ str', ' ActivationFunction', ' Callable ] = ""sigmoid""', ' isClassification = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8053087789432415,0.8052042125208622,"(tensor([0.9687]), tensor([0.9716]), tensor([0.9702]), tensor([0.9713]))"
"40     if isClassification==True; the latter shape matches what is required by the multi-dimensional case of loss function
41     CrossEntropyLoss, for example, and therefore is suitable for classification use cases.
42     """"""
43     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1,
44             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
45             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation=""sigmoid"", isClassification=False):
46         """"""
47         :param numInputTimeSlices: the number of input time slices
48         :param inputDimPerTimeSlice: the dimension of the input data per time slice
49         :param numOutputTimeSlices: the number of time slices predicted by the model
50         :param outputDimPerTimeSlice: the number of dimensions per output time slice. While this is the number of
51             target variables per time slice for regression problems, this must be the number of classes for classification problems.
52         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
53             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
54         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
55             if it is 0, then the entire complex processing path is not applied.
56         :param hidRNN: the number of hidden output dimensions for the RNN stage
57         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
58         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
59         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
60             If it is 0, the highway component is not used.
61         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
62         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
63         :param outputActivation: the output activation function
64         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
65             to suit loss functions such as CrossEntropyLoss
66         """"""
67         if numConvolutions == 0 and hwWindow == 0:
68             raise ValueError(""No processing paths remain"")
69         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
70             raise Exception(""Inconsistent numbers of times slices provided"")
71 
72         super().__init__()
73         self.inputDimPerTimeSlice = inputDimPerTimeSlice
74         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
75         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
76         self.numOutputTimeSlices = numOutputTimeSlices
77         self.window = numInputTimeSlices
78         self.hidRNN = hidRNN
79         self.numConv = numConvolutions
80         self.hidSkip = hidSkip
81         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
82         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
83         self.skip = skip
84         self.hw = hwWindow
85         self.pDropout = dropout
86         self.isClassification = isClassification
87 
88         # configure CNN-RNN path
89         if self.numConv > 0:
90             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
91             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
92             if self.skip > 0:
93                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
94                 if self.skipRnnSeqLength == 0:
95                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
96                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
97                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
98             else:
99                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
100 
101         # configure highway component
102         if self.hw > 0:
103             # direct mapping from all inputs to all outputs
104             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
105             if hwCombine == 'plus':
106                 self.highwayCombine = self._plus
107             elif hwCombine == 'product':
108                 self.highwayCombine = self._product
109             elif hwCombine == 'bilinear':
110                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
111             else:
112                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
113 
114         self.output = self._getOutputActivationFn(outputActivation)
115 
116     def forward(self, x):
","43     if isClassification==True; the latter shape matches what is required by the multi-dimensional case of loss function
44     CrossEntropyLoss, for example, and therefore is suitable for classification use cases.
45     """"""
46     def __init__(self, numInputTimeSlices, inputDimPerTimeSlice, numOutputTimeSlices=1, outputDimPerTimeSlice=1,
47             numConvolutions: int = 100, numCnnTimeSlices: int = 6, hidRNN: int = 100, skip: int = 0, hidSkip: int = 5,
48             hwWindow: int = 0, hwCombine: str = ""plus"", dropout=0.2, outputActivation: Union[str, ActivationFunction, Callable] = ""sigmoid"",
49             isClassification=False):
50         """"""
51         :param numInputTimeSlices: the number of input time slices
52         :param inputDimPerTimeSlice: the dimension of the input data per time slice
53         :param numOutputTimeSlices: the number of time slices predicted by the model
54         :param outputDimPerTimeSlice: the number of dimensions per output time slice. While this is the number of
55             target variables per time slice for regression problems, this must be the number of classes for classification problems.
56         :param numCnnTimeSlices: the number of time slices considered by each convolution (i.e. it is one of the dimensions of the matrix used for
57             convolutions, the other dimension being inputDimPerTimeSlice), a.k.a. ""Ck""
58         :param numConvolutions: the number of separate convolutions to apply, i.e. the number of independent convolution matrices, a.k.a ""hidC"";
59             if it is 0, then the entire complex processing path is not applied.
60         :param hidRNN: the number of hidden output dimensions for the RNN stage
61         :param skip: the number of time slices to skip for the skip-RNN. If it is 0, then the skip-RNN is not used.
62         :param hidSkip: the number of output dimensions of each of the skip parallel RNNs
63         :param hwWindow: the number of time slices from the end of the input time series to consider as input for the highway component.
64             If it is 0, the highway component is not used.
65         :param hwCombine: {""plus"", ""product"", ""bilinear""} the function with which the highway component's output is combined with the complex path's output
66         :param dropout: the dropout probability to use during training (dropouts are applied after every major step in the evaluation path)
67         :param outputActivation: the output activation function
68         :param isClassification: whether the model is to serve as a classifier, in which case the output tensor dimension ordering is adapted
69             to suit loss functions such as CrossEntropyLoss
70         """"""
71         if numConvolutions == 0 and hwWindow == 0:
72             raise ValueError(""No processing paths remain"")
73         if numInputTimeSlices < numCnnTimeSlices or (hwWindow != 0 and hwWindow < numInputTimeSlices):
74             raise Exception(""Inconsistent numbers of times slices provided"")
75 
76         super().__init__()
77         self.inputDimPerTimeSlice = inputDimPerTimeSlice
78         self.timeSeriesDimPerTimeSlice = outputDimPerTimeSlice
79         self.totalOutputDim = self.timeSeriesDimPerTimeSlice * numOutputTimeSlices
80         self.numOutputTimeSlices = numOutputTimeSlices
81         self.window = numInputTimeSlices
82         self.hidRNN = hidRNN
83         self.numConv = numConvolutions
84         self.hidSkip = hidSkip
85         self.Ck = numCnnTimeSlices  # the ""height"" of the CNN filter/kernel; the ""width"" being inputDimPerTimeSlice
86         self.convSeqLength = self.window - self.Ck + 1  # the length of the output sequence produced by the CNN for each kernel matrix
87         self.skip = skip
88         self.hw = hwWindow
89         self.pDropout = dropout
90         self.isClassification = isClassification
91 
92         # configure CNN-RNN path
93         if self.numConv > 0:
94             self.conv1 = nn.Conv2d(1, self.numConv, kernel_size=(self.Ck, self.inputDimPerTimeSlice))  # produce numConv sequences using numConv kernel matrices of size (height=Ck, width=inputDimPerTimeSlice)
95             self.GRU1 = nn.GRU(self.numConv, self.hidRNN)
96             if self.skip > 0:
97                 self.skipRnnSeqLength = self.convSeqLength // self.skip  # we divide by skip to obtain the sequence length, because, in order to support skipping via a regrouping of the tensor, the Skip-RNN processes skip entries of the series in parallel to produce skip hidden output vectors
98                 if self.skipRnnSeqLength == 0:
99                     raise Exception(""Window size %d is not large enough for skip length %d; would result in Skip-RNN sequence length of 0!"" % (self.window, self.skip))
100                 self.GRUskip = nn.GRU(self.numConv, self.hidSkip)
101                 self.linear1 = nn.Linear(self.hidRNN + self.skip * self.hidSkip, self.totalOutputDim)
102             else:
103                 self.linear1 = nn.Linear(self.hidRNN, self.totalOutputDim)
104 
105         # configure highway component
106         if self.hw > 0:
107             # direct mapping from all inputs to all outputs
108             self.highway = nn.Linear(self.hw * self.inputDimPerTimeSlice, self.totalOutputDim)
109             if hwCombine == 'plus':
110                 self.highwayCombine = self._plus
111             elif hwCombine == 'product':
112                 self.highwayCombine = self._product
113             elif hwCombine == 'bilinear':
114                 self.highwayCombine = nn.Bilinear(self.totalOutputDim, self.totalOutputDim, self.totalOutputDim)
115             else:
116                 raise ValueError(""Unknown highway combination function '%s'"" % hwCombine)
117 
118         self.output = ActivationFunction.torchFunctionFromAny(outputActivation)
119 
120     def forward(self, x):
","Before: 114
After: 118",convert lstnet_modules.py to py3 syntax,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/lstnet/lstnet_modules.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,748,"{'module': 1, 'if_statement': 1, 'if': 2, 'boolean_operator': 1, 'comparison_operator': 6, 'identifier': 280, '==': 1, 'ERROR': 33, 'true': 1, ';': 2, 'is': 9, 'binary_operator': 3, '-': 3, ',': 13, 'for': 4, 'and': 1, '.': 17, 'string': 7, 'string_start': 8, 'string_content': 7, 'string_end': 7, ':': 28, 'block': 1, 'expression_statement': 5, 'assignment': 4, 'type': 4, 'attribute': 15, '(': 2, 'pattern_list': 1, ')': 2, 'integer': 3, 'not': 3, 'set': 1, '{': 1, '}': 1, 'call': 1, 'argument_list': 1, 'in': 2, 'as': 2}","{'cyclomatic_complexity': 13, 'nloc': 44, 'token_count': 436, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numOutputTimeSlices = 1 , outputDimPerTimeSlice = 1 , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation = ""sigmoid"" , isClassification = False )', 'start_line': 43, 'end_line': 114, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numOutputTimeSlices = 1', ' outputDimPerTimeSlice = 1', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation = ""sigmoid""', ' isClassification = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/lstnet/lstnet_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 13, 'nloc': 45, 'token_count': 445, 'name': '__init__', 'long_name': '__init__( self , numInputTimeSlices , inputDimPerTimeSlice , numOutputTimeSlices = 1 , outputDimPerTimeSlice = 1 , numConvolutions : int = 100 , numCnnTimeSlices : int = 6 , hidRNN : int = 100 , skip : int = 0 , hidSkip : int = 5 , hwWindow : int = 0 , hwCombine : str = ""plus"" , dropout = 0 . 2 , outputActivation : Union [ str , ActivationFunction , Callable ] = ""sigmoid"" , isClassification = False )', 'start_line': 46, 'end_line': 118, 'full_parameters': ['self', ' numInputTimeSlices', ' inputDimPerTimeSlice', ' numOutputTimeSlices = 1', ' outputDimPerTimeSlice = 1', ' numConvolutions : int = 100', ' numCnnTimeSlices : int = 6', ' hidRNN : int = 100', ' skip : int = 0', ' hidSkip : int = 5', ' hwWindow : int = 0', ' hwCombine : str = ""plus""', ' dropout = 0 . 2', ' outputActivation : Union [ str', ' ActivationFunction', ' Callable ] = ""sigmoid""', ' isClassification = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/lstnet/lstnet_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8053087789432415,0.8052042125208622,"(tensor([0.9687]), tensor([0.9716]), tensor([0.9702]), tensor([0.9713]))"
"1 import logging
2 from typing import Callable, Optional, Sequence
3 
4 import torch.nn.functional
5 
6 from .mlp_modules import MultiLayerPerceptron
","1 import logging
2 from typing import Callable, Optional, Sequence, Union
3 
4 import torch.nn.functional
5 
6 from .mlp_modules import MultiLayerPerceptron
","Before: 2
After: 2",fix coding style in src/src/torch/torch_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,19,"{'module': 1, 'import_statement': 2, 'import': 3, 'dotted_name': 6, 'identifier': 8, 'import_from_statement': 1, 'from': 1, ',': 2, '.': 2}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 100, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.839587623092576,0.8298797763231622,"(tensor([0.9914]), tensor([0.9962]), tensor([0.9938]), tensor([0.9957]))"
"2 from typing import Callable, Optional, Sequence
3 
4 import torch.nn.functional
5 
6 from .mlp_modules import MultiLayerPerceptron
7 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel, TorchVectorClassificationModel
8 from ...torch_opt import NNOptimiserParams
9 from .... import NormalisationMode
10 
11 log: logging.Logger = logging.getLogger(__name__)
","2 from typing import Callable, Optional, Sequence, Union
3 
4 import torch.nn.functional
5 
6 from .mlp_modules import MultiLayerPerceptron
7 from ...torch_base import VectorTorchModel, TorchVectorRegressionModel, TorchVectorClassificationModel, ClassificationOutputMode
8 from ...torch_enums import ActivationFunction
9 from ...torch_opt import NNOptimiserParams
10 from .... import NormalisationMode
11 
","Before: 7
After: 7, 8",fix coding style in src/src/torch/torch_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,55,"{'module': 1, 'import_from_statement': 5, 'from': 5, 'dotted_name': 14, 'identifier': 16, 'import': 6, ',': 4, 'import_statement': 1, '.': 13, 'relative_import': 4, 'import_prefix': 4}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 100, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6713938113687968,0.6741777019333914,"(tensor([0.9622]), tensor([0.9492]), tensor([0.9557]), tensor([0.9505]))"
"12 
13 
14 class MultiLayerPerceptronTorchModel(VectorTorchModel):
15     def __init__(self, cuda: bool, hiddenDims: Sequence[int], hidActivationFunction: Callable[[torch.Tensor], torch.Tensor],
16             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]], pDropout: Optional[float] = None) -> None:
17         super().__init__(cuda=cuda)
18         self.hidActivationFunction = hidActivationFunction
19         self.outputActivationFunction = outputActivationFunction
20         self.hiddenDims = hiddenDims
21         self.pDropout = pDropout
22 
23     def __str__(self) -> str:
","13 
14 
15 class MultiLayerPerceptronTorchModel(VectorTorchModel):
16     def __init__(self, cuda: bool, hiddenDims: Sequence[int], hidActivationFunction: Callable[[torch.Tensor], torch.Tensor],
17             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]], pDropout: Optional[float] = None) -> None:
18         super().__init__(cuda=cuda)
19         self.hidActivationFunction = ActivationFunction.torchFunctionFromAny(hidActivationFunction)
20         self.outputActivationFunction = ActivationFunction.torchFunctionFromAny(outputActivationFunction)
21         self.hiddenDims = hiddenDims
22         self.pDropout = pDropout
23 
24     def __str__(self) -> str:
","Before: 18, 19
After: 19, 20",fix coding style in src/src/torch/torch_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,229,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 41, 'argument_list': 3, '(': 4, ')': 4, ':': 7, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 7, 'typed_parameter': 4, 'type': 13, 'generic_type': 5, 'type_parameter': 5, '[': 7, ']': 7, 'list': 2, 'attribute': 9, '.': 9, 'typed_default_parameter': 1, '=': 6, 'none': 2, '->': 1, 'expression_statement': 5, 'call': 2, 'keyword_argument': 1, 'assignment': 4}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 100, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.719947294764957,0.7160290038417694,"(tensor([0.9623]), tensor([0.9824]), tensor([0.9722]), tensor([0.9803]))"
"59 
60 
61 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
62     def __init__(self, hiddenDims: Sequence[int] = (5, 5),
63             hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
64             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]] = torch.nn.functional.softmax,
65             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN, cuda: bool = True, pDropout: Optional[float] = None,
66             nnOptimiserParams: Optional[NNOptimiserParams] = None, **nnOptimiserDictParams) -> None:
67         """"""
68         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
69         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
70         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
71         :param normalisationMode: the normalisation mode to apply to input and output data
72         :param cuda: whether to use CUDA (GPU acceleration)
73         :param pDropout: the probability with which to apply dropouts after each hidden layer
74         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
75         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
76         """"""
77         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
78         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
79             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","60 
61 
62 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
63     def __init__(self, hiddenDims: Sequence[int] = (5, 5),
64             hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
65             outputActivationFunction: Optional[Union[Callable[[torch.Tensor], torch.Tensor], str, ActivationFunction]] = torch.nn.functional.log_softmax,
66             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN, cuda: bool = True, pDropout: Optional[float] = None,
67             nnOptimiserParams: Optional[NNOptimiserParams] = None, **nnOptimiserDictParams) -> None:
68         """"""
69         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
70         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
71         :param outputActivationFunction: the output activation function (function from torch.nn.functional.*, function name, enum instance or None)
72         :param normalisationMode: the normalisation mode to apply to input and output data
73         :param cuda: whether to use CUDA (GPU acceleration)
74         :param pDropout: the probability with which to apply dropouts after each hidden layer
75         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
76         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
77         """"""
78         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
79         outputMode = ClassificationOutputMode.forActivationFn(ActivationFunction.torchFunctionFromAny(outputActivationFunction))
80         super().__init__(outputMode, MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
81             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","Before: 64
After: 65",fix coding style in src/src/torch/torch_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,763,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 51, 'argument_list': 3, '(': 6, ')': 5, ':': 9, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 17, 'typed_default_parameter': 7, 'type': 16, 'generic_type': 6, 'type_parameter': 6, '[': 9, ']': 9, '=': 8, 'tuple': 1, 'integer': 2, 'list': 3, 'attribute': 11, '.': 11, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 2, 'ERROR': 1}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 100, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.759878605362527,0.7591588068258152,"(tensor([0.9618]), tensor([0.9787]), tensor([0.9702]), tensor([0.9770]))"
"59 
60 
61 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
62     def __init__(self, hiddenDims: Sequence[int] = (5, 5),
63             hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
64             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]] = torch.nn.functional.softmax,
65             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN, cuda: bool = True, pDropout: Optional[float] = None,
66             nnOptimiserParams: Optional[NNOptimiserParams] = None, **nnOptimiserDictParams) -> None:
67         """"""
68         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
69         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
70         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
71         :param normalisationMode: the normalisation mode to apply to input and output data
72         :param cuda: whether to use CUDA (GPU acceleration)
73         :param pDropout: the probability with which to apply dropouts after each hidden layer
74         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
75         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
76         """"""
77         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
78         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
79             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","60 
61 
62 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
63     def __init__(self, hiddenDims: Sequence[int] = (5, 5),
64             hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
65             outputActivationFunction: Optional[Union[Callable[[torch.Tensor], torch.Tensor], str, ActivationFunction]] = torch.nn.functional.log_softmax,
66             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN, cuda: bool = True, pDropout: Optional[float] = None,
67             nnOptimiserParams: Optional[NNOptimiserParams] = None, **nnOptimiserDictParams) -> None:
68         """"""
69         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
70         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
71         :param outputActivationFunction: the output activation function (function from torch.nn.functional.*, function name, enum instance or None)
72         :param normalisationMode: the normalisation mode to apply to input and output data
73         :param cuda: whether to use CUDA (GPU acceleration)
74         :param pDropout: the probability with which to apply dropouts after each hidden layer
75         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
76         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
77         """"""
78         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
79         outputMode = ClassificationOutputMode.forActivationFn(ActivationFunction.torchFunctionFromAny(outputActivationFunction))
80         super().__init__(outputMode, MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
81             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","Before: 70
After: 71",fix coding style in src/src/torch/torch_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,907,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 51, 'argument_list': 3, '(': 6, ')': 5, ':': 9, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 17, 'typed_default_parameter': 7, 'type': 16, 'generic_type': 6, 'type_parameter': 6, '[': 9, ']': 9, '=': 8, 'tuple': 1, 'integer': 2, 'list': 3, 'attribute': 11, '.': 11, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 2, 'ERROR': 1}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 100, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.759878605362527,0.7591588068258152,"(tensor([0.9618]), tensor([0.9787]), tensor([0.9702]), tensor([0.9770]))"
"59 
60 
61 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
62     def __init__(self, hiddenDims: Sequence[int] = (5, 5),
63             hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
64             outputActivationFunction: Optional[Callable[[torch.Tensor], torch.Tensor]] = torch.nn.functional.softmax,
65             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN, cuda: bool = True, pDropout: Optional[float] = None,
66             nnOptimiserParams: Optional[NNOptimiserParams] = None, **nnOptimiserDictParams) -> None:
67         """"""
68         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
69         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
70         :param outputActivationFunction: the output activation function (torch.nn.functional.* or torch.* or None)
71         :param normalisationMode: the normalisation mode to apply to input and output data
72         :param cuda: whether to use CUDA (GPU acceleration)
73         :param pDropout: the probability with which to apply dropouts after each hidden layer
74         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
75         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
76         """"""
77         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
78         super().__init__(MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
79             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","60 
61 
62 class MultiLayerPerceptronVectorClassificationModel(TorchVectorClassificationModel):
63     def __init__(self, hiddenDims: Sequence[int] = (5, 5),
64             hidActivationFunction: Callable[[torch.Tensor], torch.Tensor] = torch.sigmoid,
65             outputActivationFunction: Optional[Union[Callable[[torch.Tensor], torch.Tensor], str, ActivationFunction]] = torch.nn.functional.log_softmax,
66             normalisationMode: NormalisationMode = NormalisationMode.MAX_BY_COLUMN, cuda: bool = True, pDropout: Optional[float] = None,
67             nnOptimiserParams: Optional[NNOptimiserParams] = None, **nnOptimiserDictParams) -> None:
68         """"""
69         :param hiddenDims: sequence containing the number of neurons to use in hidden layers
70         :param hidActivationFunction: the activation function (torch.nn.functional.* or torch.*) to use for all hidden layers
71         :param outputActivationFunction: the output activation function (function from torch.nn.functional.*, function name, enum instance or None)
72         :param normalisationMode: the normalisation mode to apply to input and output data
73         :param cuda: whether to use CUDA (GPU acceleration)
74         :param pDropout: the probability with which to apply dropouts after each hidden layer
75         :param nnOptimiserParams: parameters for NNOptimiser; if None, use default (or what is specified in nnOptimiserDictParams)
76         :param nnOptimiserDictParams: [for backward compatibility] parameters for NNOptimiser (alternative to nnOptimiserParams)
77         """"""
78         nnOptimiserParams = NNOptimiserParams.fromEitherDictOrInstance(nnOptimiserDictParams, nnOptimiserParams)
79         outputMode = ClassificationOutputMode.forActivationFn(ActivationFunction.torchFunctionFromAny(outputActivationFunction))
80         super().__init__(outputMode, MultiLayerPerceptronTorchModel, [cuda, hiddenDims, hidActivationFunction, outputActivationFunction],
81             dict(pDropout=pDropout), normalisationMode, nnOptimiserParams)
","Before: 78
After: 79, 80",fix coding style in src/src/torch/torch_models.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/mlp/mlp_models.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,863,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 51, 'argument_list': 3, '(': 6, ')': 5, ':': 9, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 17, 'typed_default_parameter': 7, 'type': 16, 'generic_type': 6, 'type_parameter': 6, '[': 9, ']': 9, '=': 8, 'tuple': 1, 'integer': 2, 'list': 3, 'attribute': 11, '.': 11, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, 'call': 2, 'ERROR': 1}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 90, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 15, 'end_line': 21, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 7, 'token_count': 100, 'name': '__init__', 'long_name': '__init__( self , cuda : bool , hiddenDims : Sequence [ int ] , hidActivationFunction : Callable [ [ torch . Tensor ] , torch . Tensor ] , outputActivationFunction : Optional [ Callable [ [ torch . Tensor ] , torch . Tensor ] ] , pDropout : Optional [ float ] = None )', 'start_line': 16, 'end_line': 22, 'full_parameters': ['self', ' cuda : bool', ' hiddenDims : Sequence [ int ]', ' hidActivationFunction : Callable [ [ torch . Tensor ]', ' torch . Tensor ]', ' outputActivationFunction : Optional [ Callable [ [ torch . Tensor ]', ' torch . Tensor ] ]', ' pDropout : Optional [ float ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/mlp/mlp_models.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.759878605362527,0.7591588068258152,"(tensor([0.9618]), tensor([0.9787]), tensor([0.9702]), tensor([0.9770]))"
"98         A generic residual block which need to be specified by defining the skip path.
99         """"""
100 
101         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: nn.Dropout, useBatchNormalisation: bool) -> None:
102             super().__init__()
103             self.inputDim = inputDim
104             self.hiddenDim = hiddenDim
105             self.outputDim = outputDim
106             self.dropout = dropout
107             self.useBatchNormalisation = useBatchNormalisation
108             self.bnIn = nn.BatchNorm1d(self.inputDim) if useBatchNormalisation else None
109             self.denseIn = nn.Linear(self.inputDim, self.hiddenDim)
110             self.bnOut = nn.BatchNorm1d(self.hiddenDim) if useBatchNormalisation else None
111             self.denseOut = nn.Linear(self.hiddenDim, self.outputDim)
112 
113         def forward(self, x):
","100         A generic residual block which need to be specified by defining the skip path.
101         """"""
102 
103         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: Optional[nn.Dropout], useBatchNormalisation: bool) -> None:
104             super().__init__()
105             self.inputDim = inputDim
106             self.hiddenDim = hiddenDim
107             self.outputDim = outputDim
108             self.dropout = dropout
109             self.useBatchNormalisation = useBatchNormalisation
110             self.bnIn = nn.BatchNorm1d(self.inputDim) if useBatchNormalisation else None
111             self.denseIn = nn.Linear(self.inputDim, self.hiddenDim)
112             self.bnOut = nn.BatchNorm1d(self.hiddenDim) if useBatchNormalisation else None
113             self.denseOut = nn.Linear(self.hiddenDim, self.outputDim)
114 
115         def forward(self, x):
","Before: 101
After: 103",fix error in residualffn_modules.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,611,"{'module': 1, 'ERROR': 1, 'identifier': 14, '.': 1, 'string_start': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 34, 'token_count': 260, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 84, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7008381539534709,0.6954201452675487,"(tensor([0.9869]), tensor([0.9897]), tensor([0.9883]), tensor([0.9894]))"
"137         A residual block preserving the dimension of the input
138         """"""
139 
140         def __init__(self, inputOutputDim: int, hiddenDim: int, dropout: nn.Dropout, useBatchNormalisation: bool) -> None:
141             super().__init__(inputOutputDim, hiddenDim, inputOutputDim, dropout, useBatchNormalisation)
142 
143         def _skip(self, x):
","139         A residual block preserving the dimension of the input
140         """"""
141 
142         def __init__(self, inputOutputDim: int, hiddenDim: int, dropout: Optional[nn.Dropout], useBatchNormalisation: bool) -> None:
143             super().__init__(inputOutputDim, hiddenDim, inputOutputDim, dropout, useBatchNormalisation)
144 
145         def _skip(self, x):
","Before: 140
After: 142",fix error in residualffn_modules.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,1009,"{'module': 1, 'ERROR': 2, 'identifier': 27, 'expression_statement': 2, 'string_start': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 8, 'typed_parameter': 4, ':': 5, 'type': 5, 'attribute': 2, '.': 2, ')': 3, '->': 1, 'none': 1, 'block': 1, 'call': 2, 'argument_list': 2}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 34, 'token_count': 260, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 84, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7411140107664732,0.7255393385478159,"(tensor([0.9766]), tensor([0.9770]), tensor([0.9768]), tensor([0.9770]))"
"151         A residual block changing the dimension of the input to the given value.
152         """"""
153 
154         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: nn.Dropout, useBatchNormalisation: bool) -> None:
155             super().__init__(inputDim, hiddenDim, outputDim, dropout, useBatchNormalisation)
156             self.denseSkip = nn.Linear(self.inputDim, self.outputDim)
157 
158         def _skip(self, x):
","153         A residual block changing the dimension of the input to the given value.
154         """"""
155 
156         def __init__(self, inputDim: int, hiddenDim: int, outputDim: int, dropout: Optional[nn.Dropout], useBatchNormalisation: bool) -> None:
157             super().__init__(inputDim, hiddenDim, outputDim, dropout, useBatchNormalisation)
158             self.denseSkip = nn.Linear(self.inputDim, self.outputDim)
159 
160         def _skip(self, x):
","Before: 154
After: 156",fix error in residualffn_modules.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_models/residualffn/residualffn_modules.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,1109,"{'module': 1, 'ERROR': 1, 'identifier': 13, '.': 1, 'string_start': 1}","{'cyclomatic_complexity': 5, 'nloc': 32, 'token_count': 253, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 82, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 34, 'token_count': 260, 'name': '__init__', 'long_name': '__init__( self , inputDim : int , outputDim : int , hiddenDims : Sequence [ int ] , bottleneckDimensionFactor : float = 1 , pDropout : Optional [ float ] = None , useBatchNormalisation : bool = True )', 'start_line': 47, 'end_line': 84, 'full_parameters': ['self', ' inputDim : int', ' outputDim : int', ' hiddenDims : Sequence [ int ]', ' bottleneckDimensionFactor : float = 1', ' pDropout : Optional [ float ] = None', ' useBatchNormalisation : bool = True'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_models/residualffn/residualffn_modules.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7685826298747856,0.7540791484416651,"(tensor([0.9820]), tensor([0.9868]), tensor([0.9844]), tensor([0.9863]))"
"30     """"""
31     Wrapper for classes inherited from torch.optim.Optimizer
32     """"""
33     def _makeOptimizer(self):
34         optimiserArgs = dict(self.optimiserArgs)
35         optimiserArgs.update({'lr': self.lr})
36         if self.method == 'sgd':
37             self.optimizer = optim.SGD(self.params, **optimiserArgs)
38         elif self.method == 'asgd':
39             self.optimizer = optim.ASGD(self.params, **optimiserArgs)
40         elif self.method == 'adagrad':
41             self.optimizer = optim.Adagrad(self.params, **optimiserArgs)
42         elif self.method == 'adadelta':
43             self.optimizer = optim.Adadelta(self.params, **optimiserArgs)
44         elif self.method == 'adam':
45             self.optimizer = optim.Adam(self.params, **optimiserArgs)
46         elif self.method == 'adamw':
47             self.optimizer = optim.AdamW(self.params, **optimiserArgs)
48         elif self.method == 'adamax':
49             self.optimizer = optim.Adamax(self.params, **optimiserArgs)
50         elif self.method == 'rmsprop':
51             self.optimizer = optim.RMSprop(self.params, **optimiserArgs)
52         elif self.method == 'rprop':
53             self.optimizer = optim.Rprop(self.params, **optimiserArgs)
54         elif self.method == 'lbfgs':
55             self.use_shrinkage = False
56             self.optimizer = optim.LBFGS(self.params, **optimiserArgs)
57         else:
58             raise RuntimeError(""Invalid optim method: "" + self.method)
59 
60     def __init__(self, params, method, lr, max_grad_norm, use_shrinkage=True, **optimiserArgs):
","53     """"""
54     Wrapper for classes inherited from torch.optim.Optimizer
55     """"""
56     def __init__(self, params, method: Union[str, Optimiser], lr, max_grad_norm, use_shrinkage=True, **optimiserArgs):
57         """"""
58         :param params: an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized.
59         :param method: the optimiser to use
60         :param lr: learnig rate
61         :param max_grad_norm: gradient norm value beyond which to apply gradient shrinkage
62         :param optimiserArgs: keyword arguments to be used in actual torch optimiser
63         """"""
64         if type(method) == str:
65             self.method = Optimiser.fromName(method)
66         else:
67             self.method = method
68         self.params = list(params)  # careful: params may be a generator
69         self.last_ppl = None
70         self.lr = lr
71         self.max_grad_norm = max_grad_norm
72         self.start_decay = False
73         self.optimiserArgs = optimiserArgs
74         self.use_shrinkage = use_shrinkage
75 
76         # instantiate optimiser
77         optimiserArgs = dict(self.optimiserArgs)
78         optimiserArgs.update({'lr': self.lr})
79         if self.method == Optimiser.LBFGS:
80             self.use_shrinkage = False
81             self.optimizer = optim.LBFGS(self.params, **optimiserArgs)
82         else:
83             cons = self.method.value[1]
84             self.optimizer = cons(self.params, **optimiserArgs)
85 
86     def step(self, lossBackward: Callable):
","Before: 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60
After: 56",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,219,"{'module': 1, 'expression_statement': 14, 'string': 13, 'string_start': 13, 'string_content': 13, 'string_end': 13, 'function_definition': 1, 'def': 1, 'identifier': 105, 'parameters': 1, '(': 14, ')': 14, ':': 13, 'block': 12, 'assignment': 12, '=': 12, 'call': 13, 'argument_list': 13, 'attribute': 45, '.': 45, 'dictionary': 1, '{': 1, 'pair': 1, '}': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 10, '==': 10, ',': 10, 'dictionary_splat': 10, '**': 10, 'elif_clause': 9, 'elif': 9, 'false': 1, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1, 'binary_operator': 1, '+': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.23556766755777045,0.205134495688587,"(tensor([0.8016]), tensor([0.8582]), tensor([0.8289]), tensor([0.8522]))"
"57         else:
58             raise RuntimeError(""Invalid optim method: "" + self.method)
59 
60     def __init__(self, params, method, lr, max_grad_norm, use_shrinkage=True, **optimiserArgs):
61         """"""
62         :param params: an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized.
63         :param method: string identifier for optimiser method to use
64         :param lr: learnig rate
65         :param max_grad_norm: gradient norm value beyond which to apply gradient shrinkage
66         :param optimiserArgs: keyword arguments to be used in actual torch optimiser
67         """"""
68         self.params = list(params)  # careful: params may be a generator
69         self.last_ppl = None
70         self.lr = lr
71         self.max_grad_norm = max_grad_norm
72         self.method = method
73         self.start_decay = False
74         self.optimiserArgs = optimiserArgs
75         self.use_shrinkage = use_shrinkage
76         self._makeOptimizer()
77 
78     def step(self, lossBackward: Callable):
","53     """"""
54     Wrapper for classes inherited from torch.optim.Optimizer
55     """"""
56     def __init__(self, params, method: Union[str, Optimiser], lr, max_grad_norm, use_shrinkage=True, **optimiserArgs):
57         """"""
58         :param params: an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized.
59         :param method: the optimiser to use
60         :param lr: learnig rate
61         :param max_grad_norm: gradient norm value beyond which to apply gradient shrinkage
62         :param optimiserArgs: keyword arguments to be used in actual torch optimiser
63         """"""
64         if type(method) == str:
65             self.method = Optimiser.fromName(method)
66         else:
67             self.method = method
68         self.params = list(params)  # careful: params may be a generator
69         self.last_ppl = None
70         self.lr = lr
71         self.max_grad_norm = max_grad_norm
72         self.start_decay = False
73         self.optimiserArgs = optimiserArgs
74         self.use_shrinkage = use_shrinkage
75 
76         # instantiate optimiser
77         optimiserArgs = dict(self.optimiserArgs)
78         optimiserArgs.update({'lr': self.lr})
79         if self.method == Optimiser.LBFGS:
80             self.use_shrinkage = False
81             self.optimizer = optim.LBFGS(self.params, **optimiserArgs)
82         else:
83             cons = self.method.value[1]
84             self.optimizer = cons(self.params, **optimiserArgs)
85 
86     def step(self, lossBackward: Callable):
","Before: 63
After: 59, 64, 65, 66, 67",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,728,"{'module': 1, 'expression_statement': 11, 'assignment': 9, 'identifier': 38, ':': 2, 'ERROR': 1, 'type': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '+': 1, 'attribute': 10, '.': 10, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'default_parameter': 1, '=': 9, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, 'block': 1, 'comment': 1, 'none': 1, 'false': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4557715698595421,0.45083621345665614,"(tensor([0.8899]), tensor([0.9294]), tensor([0.9092]), tensor([0.9253]))"
"57         else:
58             raise RuntimeError(""Invalid optim method: "" + self.method)
59 
60     def __init__(self, params, method, lr, max_grad_norm, use_shrinkage=True, **optimiserArgs):
61         """"""
62         :param params: an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized.
63         :param method: string identifier for optimiser method to use
64         :param lr: learnig rate
65         :param max_grad_norm: gradient norm value beyond which to apply gradient shrinkage
66         :param optimiserArgs: keyword arguments to be used in actual torch optimiser
67         """"""
68         self.params = list(params)  # careful: params may be a generator
69         self.last_ppl = None
70         self.lr = lr
71         self.max_grad_norm = max_grad_norm
72         self.method = method
73         self.start_decay = False
74         self.optimiserArgs = optimiserArgs
75         self.use_shrinkage = use_shrinkage
76         self._makeOptimizer()
77 
78     def step(self, lossBackward: Callable):
","53     """"""
54     Wrapper for classes inherited from torch.optim.Optimizer
55     """"""
56     def __init__(self, params, method: Union[str, Optimiser], lr, max_grad_norm, use_shrinkage=True, **optimiserArgs):
57         """"""
58         :param params: an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized.
59         :param method: the optimiser to use
60         :param lr: learnig rate
61         :param max_grad_norm: gradient norm value beyond which to apply gradient shrinkage
62         :param optimiserArgs: keyword arguments to be used in actual torch optimiser
63         """"""
64         if type(method) == str:
65             self.method = Optimiser.fromName(method)
66         else:
67             self.method = method
68         self.params = list(params)  # careful: params may be a generator
69         self.last_ppl = None
70         self.lr = lr
71         self.max_grad_norm = max_grad_norm
72         self.start_decay = False
73         self.optimiserArgs = optimiserArgs
74         self.use_shrinkage = use_shrinkage
75 
76         # instantiate optimiser
77         optimiserArgs = dict(self.optimiserArgs)
78         optimiserArgs.update({'lr': self.lr})
79         if self.method == Optimiser.LBFGS:
80             self.use_shrinkage = False
81             self.optimizer = optim.LBFGS(self.params, **optimiserArgs)
82         else:
83             cons = self.method.value[1]
84             self.optimizer = cons(self.params, **optimiserArgs)
85 
86     def step(self, lossBackward: Callable):
","Before: 72, 76
After: 75, 76, 77, 78, 79, 80, 81, 82, 83, 84",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,730,"{'module': 1, 'expression_statement': 11, 'assignment': 9, 'identifier': 38, ':': 2, 'ERROR': 1, 'type': 1, 'call': 3, 'argument_list': 3, '(': 4, 'binary_operator': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '+': 1, 'attribute': 10, '.': 10, ')': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'default_parameter': 1, '=': 9, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, 'block': 1, 'comment': 1, 'none': 1, 'false': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4557715698595421,0.45083621345665614,"(tensor([0.8899]), tensor([0.9294]), tensor([0.9092]), tensor([0.9253]))"
"75         self.use_shrinkage = use_shrinkage
76         self._makeOptimizer()
77 
78     def step(self, lossBackward: Callable):
79         """"""
80 
81         :param lossBackward: callable, performs backward step and returns loss
82         :return:
83         """"""
84         if self.use_shrinkage:
85             def closureWithShrinkage():
86                 loss = lossBackward()
87 
88                 # Compute gradients norm.
89                 grad_norm = None
90                 for param in self.params:
91                     n = param.grad.data.norm()
92                     if grad_norm is None:
93                         grad_norm = n * n
94                     else:
95                         grad_norm += n * n
96                 grad_norm = torch.sqrt(grad_norm)
97 
98                 if grad_norm > self.max_grad_norm:
99                     shrinkage = self.max_grad_norm / grad_norm
100 
101                     for param in self.params:
102                         param.grad.data.mul_(shrinkage)
103 
104                 return loss
105 
106             closure = closureWithShrinkage
107         else:
108             closure = lossBackward
109 
110         loss = self.optimizer.step(closure)
111         return loss
112 
113 
","83             cons = self.method.value[1]
84             self.optimizer = cons(self.params, **optimiserArgs)
85 
86     def step(self, lossBackward: Callable):
87         """"""
88         :param lossBackward: callable, performs backward step and returns loss
89         :return: loss value
90         """"""
91         if self.use_shrinkage:
92             def closureWithShrinkage():
93                 loss = lossBackward()
94 
95                 # Compute gradients norm.
96                 grad_norm = None
97                 for param in self.params:
98                     n = param.grad.data.norm()
99                     if grad_norm is None:
100                         grad_norm = n * n
101                     else:
102                         grad_norm += n * n
103                 grad_norm = torch.sqrt(grad_norm)
104 
105                 if grad_norm > self.max_grad_norm:
106                     shrinkage = self.max_grad_norm / grad_norm
107 
108                     for param in self.params:
109                         param.grad.data.mul_(shrinkage)
110 
111                 return loss
112 
113             closure = closureWithShrinkage
114         else:
115             closure = lossBackward
116 
117         loss = self.optimizer.step(closure)
118         return loss
119 
120 
","Before: 80, 82
After: 89",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,780,"{'module': 1, 'expression_statement': 14, 'assignment': 10, 'attribute': 16, 'identifier': 60, '.': 16, '=': 10, 'call': 6, 'argument_list': 6, '(': 8, ')': 8, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 1, 'typed_parameter': 1, ':': 10, 'type': 1, 'block': 9, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 3, 'if': 3, 'comment': 1, 'none': 2, 'for_statement': 2, 'for': 2, 'in': 2, 'comparison_operator': 2, 'is': 1, 'binary_operator': 3, '*': 2, 'else_clause': 2, 'else': 2, 'augmented_assignment': 1, '+=': 1, '>': 1, '/': 1, 'return_statement': 2, 'return': 2}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4841535106676179,0.47435291350594083,"(tensor([0.9364]), tensor([0.9482]), tensor([0.9423]), tensor([0.9471]))"
"277 
278 class NNLossEvaluatorClassification(NNLossEvaluator):
279     """"""A loss evaluator for (multi-variate) regression""""""
280 
281     class LossFunction(Enum):
282         CROSSENTROPY = ""CrossEntropy""
283 
284     def __init__(self, lossFn: LossFunction = LossFunction.CROSSENTROPY):
285         if lossFn is None:
286             lossFn = self.LossFunction.CROSSENTROPY
","284 
285 class NNLossEvaluatorClassification(NNLossEvaluator):
286     """"""A loss evaluator for (multi-variate) regression""""""
287 
288     class LossFunction(Enum):
289         CROSSENTROPY = ""CrossEntropy"", ""CE""
290         NLL = ""NegativeLogLikelihood"", ""NLL""
291 
292         def createCriterion(self) -> Callable:
293             if self is self.CROSSENTROPY:
","Before: 282
After: 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2444,"{'module': 1, 'class_definition': 2, 'class': 2, 'identifier': 12, 'argument_list': 2, '(': 3, ')': 3, ':': 5, 'block': 4, 'expression_statement': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 1, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_default_parameter': 1, 'type': 1, 'attribute': 1, '.': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.41098197603695297,0.3772236330643423,"(tensor([0.8969]), tensor([0.9071]), tensor([0.9020]), tensor([0.9061]))"
"281     class LossFunction(Enum):
282         CROSSENTROPY = ""CrossEntropy""
283 
284     def __init__(self, lossFn: LossFunction = LossFunction.CROSSENTROPY):
285         if lossFn is None:
286             lossFn = self.LossFunction.CROSSENTROPY
287         try:
288             self.lossFn = self.LossFunction(lossFn)
289         except ValueError:
290             raise Exception(f""Loss function {lossFn} not supported. Available are: {[e.value for e in self.LossFunction]}"")
291 
292     def __str__(self):
","309             else:
310                 raise ValueError(f""No default specified for {outputMode}"")
311 
312     def __init__(self, lossFn: LossFunction = LossFunction.CROSSENTROPY):
313         if lossFn is None:
314             lossFn = self.LossFunction.CROSSENTROPY
315         self.lossFn: ""NNLossEvaluatorClassification.LossFunction"" = self.LossFunction(lossFn)
316 
317     def __str__(self):
","Before: 287, 288, 289, 290
After: 315",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2486,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 27, 'argument_list': 3, '(': 4, ')': 4, ':': 6, 'block': 5, 'expression_statement': 3, 'assignment': 3, '=': 4, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_default_parameter': 1, 'type': 1, 'attribute': 7, '.': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'try_statement': 1, 'try': 1, 'call': 2, 'except_clause': 1, 'except': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 2, '{': 2, '}': 2, 'list_comprehension': 1, '[': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, ']': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2930099242645659,0.265258189759841,"(tensor([0.8778]), tensor([0.8659]), tensor([0.8718]), tensor([0.8670]))"
"292     def __str__(self):
293         return f""{self.__class__.__name__}[{self.lossFn}]""
294 
295     def createValidationLossEvaluator(self, cuda):
296         return self.ValidationLossEvaluator(cuda)
297 
298     def getTrainingCriterion(self):
","317     def __str__(self):
318         return f""{self.__class__.__name__}[{self.lossFn}]""
319 
320     def createValidationLossEvaluator(self, cuda):
321         return self.ValidationLossEvaluator(cuda, self.lossFn)
322 
323     def getTrainingCriterion(self):
","Before: 296
After: 321",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2597,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 13, 'parameters': 2, '(': 3, ')': 3, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'string': 1, 'string_start': 1, 'interpolation': 2, '{': 2, 'attribute': 4, '.': 4, '}': 2, 'string_content': 2, 'string_end': 1, ',': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6323625553750277,0.5792776571305329,"(tensor([0.9670]), tensor([0.9696]), tensor([0.9683]), tensor([0.9693]))"
"295     def createValidationLossEvaluator(self, cuda):
296         return self.ValidationLossEvaluator(cuda)
297 
298     def getTrainingCriterion(self):
299         if self.lossFn is self.LossFunction.CROSSENTROPY:
300             criterion = nn.CrossEntropyLoss(reduction='sum')
301         else:
302             raise AssertionError(f""Loss function {self.lossFn} defined but instantiation not implemented."")
303         return criterion
304 
305     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
","320     def createValidationLossEvaluator(self, cuda):
321         return self.ValidationLossEvaluator(cuda, self.lossFn)
322 
323     def getTrainingCriterion(self):
324         return self.lossFn.createCriterion()
325 
326     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
","Before: 299, 300, 301, 302, 303
After: 324",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2623,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 21, 'parameters': 2, '(': 5, ',': 1, ')': 5, ':': 4, 'block': 4, 'return_statement': 2, 'return': 2, 'call': 3, 'attribute': 6, '.': 6, 'argument_list': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'expression_statement': 1, 'assignment': 1, '=': 2, 'keyword_argument': 1, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.21571069105627336,0.18407635979538087,"(tensor([0.9305]), tensor([0.8448]), tensor([0.8856]), tensor([0.8527]))"
"303         return criterion
304 
305     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
306         def __init__(self, cuda: bool):
307             self.totalLossCE = None
308             self.numValidationSamples = None
309             self.evaluateCE = nn.CrossEntropyLoss(reduction=""sum"")
310             if cuda:
311                 self.evaluateCE = self.evaluateCE.cuda()
312 
313         def startValidationCollection(self, groundTruthShape):
","324         return self.lossFn.createCriterion()
325 
326     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
327         def __init__(self, cuda: bool, lossFn: ""NNLossEvaluatorClassification.LossFunction""):
328             self.lossFn = lossFn
329             self.totalLoss = None
330             self.numValidationSamples = None
331             self.criterion = self.lossFn.createCriterion()
332             if cuda:
333                 self.criterion = self.criterion.cuda()
334 
335         def startValidationCollection(self, groundTruthShape):
","Before: 306, 307
After: 327, 328, 329",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2695,"{'module': 1, 'return_statement': 1, 'return': 1, 'identifier': 23, 'class_definition': 1, 'class': 1, 'argument_list': 3, '(': 4, 'attribute': 8, '.': 8, ')': 4, ':': 4, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, 'type': 1, 'expression_statement': 4, 'assignment': 4, '=': 5, 'none': 2, 'call': 2, 'keyword_argument': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 1, 'if': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3372093835976668,0.28521732360078705,"(tensor([0.8767]), tensor([0.8988]), tensor([0.8876]), tensor([0.8965]))"
"303         return criterion
304 
305     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
306         def __init__(self, cuda: bool):
307             self.totalLossCE = None
308             self.numValidationSamples = None
309             self.evaluateCE = nn.CrossEntropyLoss(reduction=""sum"")
310             if cuda:
311                 self.evaluateCE = self.evaluateCE.cuda()
312 
313         def startValidationCollection(self, groundTruthShape):
","324         return self.lossFn.createCriterion()
325 
326     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
327         def __init__(self, cuda: bool, lossFn: ""NNLossEvaluatorClassification.LossFunction""):
328             self.lossFn = lossFn
329             self.totalLoss = None
330             self.numValidationSamples = None
331             self.criterion = self.lossFn.createCriterion()
332             if cuda:
333                 self.criterion = self.criterion.cuda()
334 
335         def startValidationCollection(self, groundTruthShape):
","Before: 309
After: 331",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2733,"{'module': 1, 'return_statement': 1, 'return': 1, 'identifier': 23, 'class_definition': 1, 'class': 1, 'argument_list': 3, '(': 4, 'attribute': 8, '.': 8, ')': 4, ':': 4, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, 'type': 1, 'expression_statement': 4, 'assignment': 4, '=': 5, 'none': 2, 'call': 2, 'keyword_argument': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 1, 'if': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3372093835976668,0.28521732360078705,"(tensor([0.8767]), tensor([0.8988]), tensor([0.8876]), tensor([0.8965]))"
"303         return criterion
304 
305     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
306         def __init__(self, cuda: bool):
307             self.totalLossCE = None
308             self.numValidationSamples = None
309             self.evaluateCE = nn.CrossEntropyLoss(reduction=""sum"")
310             if cuda:
311                 self.evaluateCE = self.evaluateCE.cuda()
312 
313         def startValidationCollection(self, groundTruthShape):
","324         return self.lossFn.createCriterion()
325 
326     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
327         def __init__(self, cuda: bool, lossFn: ""NNLossEvaluatorClassification.LossFunction""):
328             self.lossFn = lossFn
329             self.totalLoss = None
330             self.numValidationSamples = None
331             self.criterion = self.lossFn.createCriterion()
332             if cuda:
333                 self.criterion = self.criterion.cuda()
334 
335         def startValidationCollection(self, groundTruthShape):
","Before: 311
After: 333",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2756,"{'module': 1, 'return_statement': 1, 'return': 1, 'identifier': 23, 'class_definition': 1, 'class': 1, 'argument_list': 3, '(': 4, 'attribute': 8, '.': 8, ')': 4, ':': 4, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, 'type': 1, 'expression_statement': 4, 'assignment': 4, '=': 5, 'none': 2, 'call': 2, 'keyword_argument': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 1, 'if': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3372093835976668,0.28521732360078705,"(tensor([0.8767]), tensor([0.8988]), tensor([0.8876]), tensor([0.8965]))"
"310             if cuda:
311                 self.evaluateCE = self.evaluateCE.cuda()
312 
313         def startValidationCollection(self, groundTruthShape):
314             self.totalLossCE = 0
315             self.numValidationSamples = 0
316 
317         def processValidationResultBatch(self, output, groundTruth):
","332             if cuda:
333                 self.criterion = self.criterion.cuda()
334 
335         def startValidationCollection(self, groundTruthShape):
336             self.totalLoss = 0
337             self.numValidationSamples = 0
338 
339         def processValidationResultBatch(self, output, groundTruth):
","Before: 314
After: 336",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2775,"{'module': 1, 'if_statement': 1, 'if': 1, 'identifier': 13, ':': 2, 'block': 2, 'expression_statement': 3, 'assignment': 3, 'attribute': 5, '.': 5, '=': 3, 'call': 1, 'argument_list': 1, '(': 2, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'integer': 2}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4861306425808154,0.4146810492128678,"(tensor([0.9460]), tensor([0.9400]), tensor([0.9430]), tensor([0.9406]))"
"314             self.totalLossCE = 0
315             self.numValidationSamples = 0
316 
317         def processValidationResultBatch(self, output, groundTruth):
318             self.totalLossCE += self.evaluateCE(output, groundTruth).item()
319             self.numValidationSamples += output.shape[0]
320 
321         def endValidationCollection(self):
","336             self.totalLoss = 0
337             self.numValidationSamples = 0
338 
339         def processValidationResultBatch(self, output, groundTruth):
340             self.totalLoss += self.criterion(output, groundTruth).item()
341             self.numValidationSamples += output.shape[0]
342 
343         def endValidationCollection(self):
","Before: 318
After: 340",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2821,"{'module': 1, 'expression_statement': 4, 'assignment': 2, 'attribute': 7, 'identifier': 19, '.': 7, '=': 2, 'integer': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, ',': 3, ')': 3, ':': 1, 'block': 1, 'augmented_assignment': 2, '+=': 2, 'call': 2, 'argument_list': 2, 'subscript': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5636960479073191,0.5005402493023005,"(tensor([0.9588]), tensor([0.9490]), tensor([0.9539]), tensor([0.9500]))"
"318             self.totalLossCE += self.evaluateCE(output, groundTruth).item()
319             self.numValidationSamples += output.shape[0]
320 
321         def endValidationCollection(self):
322             ce = self.totalLossCE / self.numValidationSamples
323             metrics = OrderedDict([(""CE"", ce), (""GeoMeanProbTrueClass"", math.exp(-ce))])
324             return metrics
325 
326     def getValidationMetricName(self):
","340             self.totalLoss += self.criterion(output, groundTruth).item()
341             self.numValidationSamples += output.shape[0]
342 
343         def endValidationCollection(self):
344             meanLoss = self.totalLoss / self.numValidationSamples
345             if isinstance(self.criterion, nn.CrossEntropyLoss):
346                 metrics = OrderedDict([(""CE"", meanLoss), (""GeoMeanProbTrueClass"", math.exp(-meanLoss))])
347             elif isinstance(self.criterion, nn.NLLLoss):
348                 metrics = {""NLL"": meanLoss}
349             else:
350                 raise ValueError()
351             return metrics
352 
353     def getValidationMetricName(self):
","Before: 322, 323
After: 344, 345, 346, 347, 348, 349, 350",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2859,"{'module': 1, 'expression_statement': 4, 'augmented_assignment': 2, 'attribute': 8, 'identifier': 25, '.': 8, '+=': 2, 'call': 4, 'argument_list': 4, '(': 7, ',': 4, ')': 7, 'subscript': 1, '[': 2, 'integer': 1, ']': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'assignment': 2, '=': 2, 'binary_operator': 1, '/': 1, 'list': 1, 'tuple': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'unary_operator': 1, '-': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.37555100483937487,0.3504659401656194,"(tensor([0.8548]), tensor([0.9325]), tensor([0.8920]), tensor([0.9241]))"
"323             metrics = OrderedDict([(""CE"", ce), (""GeoMeanProbTrueClass"", math.exp(-ce))])
324             return metrics
325 
326     def getValidationMetricName(self):
327         if self.lossFn is self.LossFunction.CROSSENTROPY:
328             return ""CE""
329         else:
330             raise AssertionError(f""No selection criterion defined for loss function {self.lossFn}"")
331 
332 
","350                 raise ValueError()
351             return metrics
352 
353     def getValidationMetricName(self):
354         return self.lossFn.getValidationMetricKey()
355 
356 
","Before: 327, 328, 329, 330
After: 354",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,2929,"{'module': 1, 'expression_statement': 1, 'assignment': 1, 'identifier': 17, '=': 1, 'call': 3, 'argument_list': 3, '(': 6, 'list': 1, '[': 1, 'tuple': 2, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, ',': 3, ')': 6, 'attribute': 5, '.': 5, 'unary_operator': 1, '-': 1, ']': 1, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.03255828195588729,0.02606432198878465,"(tensor([0.8569]), tensor([0.7356]), tensor([0.7916]), tensor([0.7462]))"
"336         ""optimiserClip"": ""shrinkageClip""
337     }
338 
339     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser=""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
340             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
341             useShrinkage=True, shrinkageClip=10., **optimiserArgs):
342         """"""
343         :param lossEvaluator: the loss evaluator to use
344         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
345         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
346         :param optimiserLR: the optimiser's learning rate
347         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
348             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
349         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
350             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
351         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
352             If no validation is to be performed, pass 1.0.
353         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
354             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
355         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
356         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
357         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
358         """"""
359         if optimiser == 'lbfgs':
360             largeBatchSize = 1e12
361             if batchSize is not None:
362                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
363             batchSize = largeBatchSize
364         else:
365             if batchSize is None:
366                 log.debug(""No batch size was specified, using batch size 64 by default"")
367                 batchSize = 64
368 
369         self.epochs = epochs
370         self.batchSize = batchSize
371         self.optimiserLR = optimiserLR
372         self.shrinkageClip = shrinkageClip
373         self.optimiser = optimiser
374         self.gpu = gpu
375         self.trainFraction = trainFraction
376         self.scaledOutputs = scaledOutputs
377         self.lossEvaluator = lossEvaluator
378         self.optimiserArgs = optimiserArgs
379         self.useShrinkage = useShrinkage
380         self.earlyStoppingEpochs = earlyStoppingEpochs
381 
382     @classmethod
","360         ""optimiserClip"": ""shrinkageClip""
361     }
362 
363     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser: Union[str, Optimiser] = ""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
364             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
365             useShrinkage=True, shrinkageClip=10., **optimiserArgs):
366         """"""
367         :param lossEvaluator: the loss evaluator to use
368         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
369         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
370         :param optimiserLR: the optimiser's learning rate
371         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
372             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
373         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
374             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
375         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
376             If no validation is to be performed, pass 1.0.
377         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
378             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
379         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
380         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
381         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
382         """"""
383         if optimiser == 'lbfgs':
384             largeBatchSize = 1e12
385             if batchSize is not None:
386                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
387             batchSize = largeBatchSize
388         else:
389             if batchSize is None:
390                 log.debug(""No batch size was specified, using batch size 64 by default"")
391                 batchSize = 64
392 
393         self.epochs = epochs
394         self.batchSize = batchSize
395         self.optimiserLR = optimiserLR
396         self.shrinkageClip = shrinkageClip
397         self.optimiser = optimiser
398         self.gpu = gpu
399         self.trainFraction = trainFraction
400         self.scaledOutputs = scaledOutputs
401         self.lossEvaluator = lossEvaluator
402         self.optimiserArgs = optimiserArgs
403         self.useShrinkage = useShrinkage
404         self.earlyStoppingEpochs = earlyStoppingEpochs
405 
406     @classmethod
","Before: 339
After: 363",add optimiser and enum,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,0,3031,"{'module': 1, 'ERROR': 2, 'string': 7, 'string_start': 7, 'string_content': 8, 'string_end': 7, ':': 7, 'expression_statement': 19, '}': 2, 'function_definition': 1, 'def': 1, 'identifier': 63, 'parameters': 1, '(': 3, ',': 12, 'typed_default_parameter': 1, 'type': 1, '=': 26, 'none': 6, 'default_parameter': 10, 'float': 4, 'integer': 2, 'false': 1, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 3, 'block': 5, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '==': 1, 'assignment': 15, 'is not': 2, 'call': 2, 'attribute': 14, '.': 14, 'argument_list': 2, 'interpolation': 1, '{': 1, 'else_clause': 1, 'else': 1, 'is': 1}","{'cyclomatic_complexity': 11, 'nloc': 26, 'token_count': 261, 'name': '_makeOptimizer', 'long_name': '_makeOptimizer( self )', 'start_line': 33, 'end_line': 58, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7608946987465013,0.7561787015567664,"(tensor([0.9754]), tensor([0.9782]), tensor([0.9768]), tensor([0.9779]))"
"176             X = self._inputTransformerChain.apply(X)
177         return X
178 
179     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
180         """"""
181         Performs a prediction for the given input data frame
182 
183         :param x: the input data
184         :return: a DataFrame with the same index as the input
185         """"""
186         if not self.isFitted():
187             raise Exception(f""Calling predict with unfitted model {self} ""
188                             f""(isUnderlyingModelFitted={self._isUnderlyingModelFitted()}, ""
189                             f""preProcessorsAreFitted={self._preProcessorsAreFitted()})"")
190         x = self._computeModelInputs(x)
191         self._checkModelInputColumns(x)
192         y = self._predict(x)
193         y.index = x.index
194         return y
195 
196     @abstractmethod
","176             X = self._inputTransformerChain.apply(X)
177         return X
178 
179     def predict(self, x: pd.DataFrame) -> pd.DataFrame:
180         """"""
181         Performs a prediction for the given input data frame
182 
183         :param x: the input data
184         :return: a DataFrame with the same index as the input
185         """"""
186         if not self.isFitted():
187             raise Exception(f""Calling predict with unfitted model {self} ""
188                             f""(isUnderlyingModelFitted={self._isUnderlyingModelFitted()}, ""
189                             f""preProcessorsAreFitted={self._preProcessorsAreFitted()})"")
190         x = self._computeModelInputs(x)
191         self._checkModelInputColumns(x)
192         y = self._predict(x)
193         return self._createOutputDataFrame(y, x.index)
194 
195     def _createOutputDataFrame(self, y: Union[pd.DataFrame, list], index):
","Before: 193, 194
After: 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204",improve vector_model performance,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,1,1208,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 37, '=': 4, 'call': 8, 'attribute': 12, '.': 12, 'argument_list': 8, '(': 9, ')': 9, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'string': 4, 'string_start': 4, 'string_content': 7, 'string_end': 4, 'if_statement': 1, 'if': 1, 'not_operator': 1, 'not': 1, 'raise_statement': 1, 'raise': 1, 'concatenated_string': 1, 'interpolation': 3, '{': 3, '}': 3}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8406230954211987,0.8434078700066272,"(tensor([0.9675]), tensor([0.9831]), tensor([0.9753]), tensor([0.9815]))"
"194         return y
195 
196     @abstractmethod
197     def _predict(self, x: pd.DataFrame) -> pd.DataFrame:
198         pass
199 
200     def _underlyingModelRequiresFitting(self) -> bool:
","204             return pd.DataFrame(pd.Series(y, name=predictedColumns[0], index=index))
205 
206     @abstractmethod
207     def _predict(self, x: pd.DataFrame) -> Union[pd.DataFrame, list]:
208         """"""
209         :param x: the input data frame
210         :return: the output data frame, or, for the case where a single column is to be predicted, the list of values for that column
211         """"""
212         pass
213 
214     def _underlyingModelRequiresFitting(self) -> bool:
","Before: 197
After: 207, 208, 209, 210, 211",improve vector_model performance,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,5002840360c22adf1a0046cc73907bd0a26f6536,ec35df86cd8cf23ff14f971b172ccc282c875450,1,1239,"{'module': 1, 'return_statement': 1, 'return': 1, 'identifier': 9, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 2, 'attribute': 2, '.': 2, ')': 1, '->': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.18884925959350285,0.16130426442813298,"(tensor([0.7613]), tensor([0.9049]), tensor([0.8269]), tensor([0.8881]))"
"284 
285 class NNLossEvaluatorClassification(NNLossEvaluator):
286     """"""A loss evaluator for (multi-variate) regression""""""
287 
288     class LossFunction(Enum):
289         CROSSENTROPY = ""CrossEntropy"", ""CE""
290         NLL = ""NegativeLogLikelihood"", ""NLL""
291 
292         def createCriterion(self) -> Callable:
293             if self is self.CROSSENTROPY:
","284 
285 class NNLossEvaluatorClassification(NNLossEvaluator):
286     """"""A loss evaluator for (multi-variate) regression""""""
287 
288     class LossFunction(Enum):
289         CROSSENTROPY = ""CrossEntropy""
290         NLL = ""NegativeLogLikelihood""
291 
292         def createCriterion(self) -> Callable:
293             if self is self.CROSSENTROPY:
","Before: 289, 290
After: 289, 290",fix typos in src/sensai/torch/torch_opt.py,Fixed backward compatibility of enum NNLossEvaluatorClassification.LossFunction,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b65aed4207b2ee6083899f774080ed7fa3293f72,7b8e4a4e8b9ca8ed196494d024b4e1def737da67,0,2441,"{'module': 1, 'class_definition': 2, 'class': 2, 'identifier': 9, 'argument_list': 2, '(': 3, ')': 3, ':': 3, 'block': 3, 'expression_statement': 3, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, 'assignment': 2, '=': 2, 'expression_list': 2, ',': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8518915901499531,0.8518915901499531,"(tensor([0.9938]), tensor([0.9866]), tensor([0.9902]), tensor([0.9873]))"
"295             elif self is self.NLL:
296                 return nn.NLLLoss(reduction=""sum"")
297 
298         def getValidationMetricKey(self) -> str:
299             return self.value[1]
300 
301         @classmethod
","295             elif self is self.NLL:
296                 return nn.NLLLoss(reduction=""sum"")
297 
298         def getValidationMetricKey(self) -> str:
299             if self is self.CROSSENTROPY:
300                 return ""CE""
301             elif self is self.NLL:
302                 return ""NLL""
303 
304         @classmethod
","Before: 299
After: 299, 300, 301, 302",fix typos in src/sensai/torch/torch_opt.py,Fixed backward compatibility of enum NNLossEvaluatorClassification.LossFunction,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,b65aed4207b2ee6083899f774080ed7fa3293f72,7b8e4a4e8b9ca8ed196494d024b4e1def737da67,0,2545,"{'module': 1, 'ERROR': 1, 'identifier': 12, 'comparison_operator': 1, 'is': 1, 'attribute': 3, '.': 3, ':': 2, 'return_statement': 2, 'return': 2, 'call': 1, 'argument_list': 1, '(': 2, 'keyword_argument': 1, '=': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, 'block': 1, 'subscript': 1, '[': 1, 'integer': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5314731724141766,0.5426854933448799,"(tensor([0.8995]), tensor([0.9454]), tensor([0.9219]), tensor([0.9406]))"
"1 from dataclasses import dataclass
2 from enum import Enum
3 from typing import Optional, Callable, Union
4 
5 from torch.nn import functional as F
","1 from enum import Enum
2 import functools
3 from typing import Optional, Callable, Union
4 
5 from torch.nn import functional as F
6 
","Before: 1
After: 2",replace dataclasses/torch_enums.py with functools.partial,ActivationFunction:,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,c5ecaf655e868f4942238421f96786f093cb17e2,b65aed4207b2ee6083899f774080ed7fa3293f72,0,9,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 8, 'identifier': 8, 'import': 3, ',': 2}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.735408443636839,0.7095641012424608,"(tensor([0.9410]), tensor([0.9328]), tensor([0.9369]), tensor([0.9336]))"
"3 from typing import Optional, Callable, Union
4 
5 from torch.nn import functional as F
6 
7 
8 @dataclass
9 class _ActivationFunction:
10     name: str
11     fn: Optional[Callable]
12 
","4 
5 from torch.nn import functional as F
6 
7 
8 class ActivationFunction(Enum):
9     NONE = ""none""
10     SIGMOID = ""sigmoid""
11     RELU = ""relu""
12     TANH = ""tanh""
13     LOG_SOFTMAX = ""log_softmax""
","Before: 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20
After: 9, 10, 11, 12, 13, 14",replace dataclasses/torch_enums.py with functools.partial,ActivationFunction:,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,c5ecaf655e868f4942238421f96786f093cb17e2,b65aed4207b2ee6083899f774080ed7fa3293f72,0,45,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 6, 'identifier': 15, 'import': 2, ',': 2, '.': 1, 'aliased_import': 1, 'as': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'class_definition': 1, 'class': 1, ':': 3, 'block': 1, 'expression_statement': 2, 'assignment': 2, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.23124115743136087,0.25580166785952446,"(tensor([0.7865]), tensor([0.8146]), tensor([0.8003]), tensor([0.8117]))"
"26                 return item
27         raise ValueError(f""No function found for name '{name}'"")
28 
29     def getTorchFunction(self) -> Callable:
30         return self.value.fn
31 
32     def getName(self) -> str:
","20                 return item
21         raise ValueError(f""No function found for name '{name}'"")
22 
23     def getTorchFunction(self) -> Callable:
24         return {
25                 ActivationFunction.NONE: None,
26                 ActivationFunction.SIGMOID: F.sigmoid,
27                 ActivationFunction.RELU: F.relu,
28                 ActivationFunction.TANH: F.tanh,
29                 ActivationFunction.LOG_SOFTMAX: functools.partial(F.log_softmax, dim=1),
30                 ActivationFunction.SOFTMAX: functools.partial(F.softmax, dim=1)
31             }[self]
32 
33     def getName(self) -> str:
","Before: 30
After: 24, 25, 26, 27, 28, 29, 30, 31",replace dataclasses/torch_enums.py with functools.partial,ActivationFunction:,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,c5ecaf655e868f4942238421f96786f093cb17e2,b65aed4207b2ee6083899f774080ed7fa3293f72,0,264,"{'module': 1, 'return_statement': 2, 'return': 2, 'identifier': 9, 'raise_statement': 1, 'raise': 1, 'call': 1, 'argument_list': 1, '(': 2, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 1, '{': 1, '}': 1, 'string_end': 1, ')': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1, 'attribute': 2, '.': 2}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3675918209957688,0.3640051304594415,"(tensor([0.7063]), tensor([0.8914]), tensor([0.7882]), tensor([0.8686]))"
"29     def getTorchFunction(self) -> Callable:
30         return self.value.fn
31 
32     def getName(self) -> str:
33         return self.value.name
34 
35     @classmethod
","30                 ActivationFunction.SOFTMAX: functools.partial(F.softmax, dim=1)
31             }[self]
32 
33     def getName(self) -> str:
34         return self.value
35 
36     @classmethod
","Before: 33
After: 34",replace dataclasses/torch_enums.py with functools.partial,ActivationFunction:,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,c5ecaf655e868f4942238421f96786f093cb17e2,b65aed4207b2ee6083899f774080ed7fa3293f72,0,285,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 12, 'parameters': 2, '(': 2, ')': 2, '->': 2, 'type': 2, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'attribute': 4, '.': 4}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3133075705270091,0.2762803649791817,"(tensor([0.8032]), tensor([0.8588]), tensor([0.8301]), tensor([0.8529]))"
"33         return self.value.name
34 
35     @classmethod
36     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable]) -> Callable:
37         """"""
38         Gets the torch activation for the given argument
39 
40         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
41         :return: a function that can be applied to tensors
42         """"""
43         if isinstance(f, str):
44             return getattr(F, f)
45         elif isinstance(f, ActivationFunction):
46             return f.getTorchFunction()
47         elif callable(f):
48             return f
49         else:
50             raise ValueError()
51 
52 
","34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable]) -> Callable:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors
43         """"""
44         if isinstance(f, str):
45             try:
46                 return cls.fromName(f).getTorchFunction()
47             except ValueError:
48                 return getattr(F, f)
49         elif isinstance(f, ActivationFunction):
50             return f.getTorchFunction()
51         elif callable(f):
52             return f
53         else:
54             raise ValueError()
55 
56 
","Before: 44
After: 45, 46, 47, 48",replace dataclasses/torch_enums.py with functools.partial,ActivationFunction:,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,c5ecaf655e868f4942238421f96786f093cb17e2,b65aed4207b2ee6083899f774080ed7fa3293f72,0,349,"{'module': 1, 'return_statement': 4, 'return': 4, 'attribute': 3, 'identifier': 26, '.': 3, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 7, ',': 6, 'typed_parameter': 1, ':': 6, 'type': 5, 'generic_type': 1, 'type_parameter': 1, '[': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 1, ')': 7, '->': 1, 'block': 5, 'expression_statement': 1, 'if_statement': 1, 'if': 1, 'call': 6, 'argument_list': 6, 'elif_clause': 2, 'elif': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6410082502806552,0.6417738911909676,"(tensor([0.9540]), tensor([0.9682]), tensor([0.9610]), tensor([0.9667]))"
"56     UNNORMALISED_LOG_PROBABILITIES = ""unnormalised_log_probabilities""
57 
58     @classmethod
59     def forActivationFn(cls, fn: Optional[Callable]):
60         if fn is None:
61             return cls.UNNORMALISED_LOG_PROBABILITIES
62         name = fn.__name__
63         if name in (""sigmoid"", ""relu""):
64             raise ValueError(f""The activation function {fn} is not suitable as an output activation function for classifcation"")
65         elif name in (""log_softmax"",):
66             return cls.LOG_PROBABILITIES
67         elif name in (""softmax"",):
68             return cls.PROBABILITIES
69         else:
70             raise ValueError(f""Unhandled function {fn}"")","60     UNNORMALISED_LOG_PROBABILITIES = ""unnormalised_log_probabilities""
61 
62     @classmethod
63     def forActivationFn(cls, fn: Optional[Callable]):
64         if fn is None:
65             return cls.UNNORMALISED_LOG_PROBABILITIES
66         name = fn.__name__
67         if name in (""sigmoid"", ""relu"", ""tanh""):
68             raise ValueError(f""The activation function {fn} is not suitable as an output activation function for classification"")
69         elif name in (""log_softmax"",):
70             return cls.LOG_PROBABILITIES
71         elif name in (""softmax"",):
72             return cls.PROBABILITIES
73         else:
74             raise ValueError(f""Unhandled function {fn}"")
","Before: 63, 64
After: 67, 68",replace dataclasses/torch_enums.py with functools.partial,ActivationFunction:,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,c5ecaf655e868f4942238421f96786f093cb17e2,b65aed4207b2ee6083899f774080ed7fa3293f72,0,494,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 22, '=': 2, 'string': 6, 'string_start': 6, 'string_content': 7, 'string_end': 6, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 5, ',': 4, 'typed_parameter': 1, ':': 6, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ')': 5, 'block': 5, 'if_statement': 2, 'if': 2, 'comparison_operator': 4, 'is': 1, 'none': 1, 'return_statement': 3, 'return': 3, 'attribute': 4, '.': 4, 'in': 3, 'tuple': 3, 'raise_statement': 1, 'raise': 1, 'call': 1, 'argument_list': 1, 'interpolation': 1, '{': 1, '}': 1, 'elif_clause': 2, 'elif': 2}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.683651110519284,0.6803067244423805,"(tensor([0.9788]), tensor([0.9765]), tensor([0.9776]), tensor([0.9767]))"
"56     UNNORMALISED_LOG_PROBABILITIES = ""unnormalised_log_probabilities""
57 
58     @classmethod
59     def forActivationFn(cls, fn: Optional[Callable]):
60         if fn is None:
61             return cls.UNNORMALISED_LOG_PROBABILITIES
62         name = fn.__name__
63         if name in (""sigmoid"", ""relu""):
64             raise ValueError(f""The activation function {fn} is not suitable as an output activation function for classifcation"")
65         elif name in (""log_softmax"",):
66             return cls.LOG_PROBABILITIES
67         elif name in (""softmax"",):
68             return cls.PROBABILITIES
69         else:
70             raise ValueError(f""Unhandled function {fn}"")","60     UNNORMALISED_LOG_PROBABILITIES = ""unnormalised_log_probabilities""
61 
62     @classmethod
63     def forActivationFn(cls, fn: Optional[Callable]):
64         if fn is None:
65             return cls.UNNORMALISED_LOG_PROBABILITIES
66         name = fn.__name__
67         if name in (""sigmoid"", ""relu"", ""tanh""):
68             raise ValueError(f""The activation function {fn} is not suitable as an output activation function for classification"")
69         elif name in (""log_softmax"",):
70             return cls.LOG_PROBABILITIES
71         elif name in (""softmax"",):
72             return cls.PROBABILITIES
73         else:
74             raise ValueError(f""Unhandled function {fn}"")
","Before: 70
After: 74",replace dataclasses/torch_enums.py with functools.partial,ActivationFunction:,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,c5ecaf655e868f4942238421f96786f093cb17e2,b65aed4207b2ee6083899f774080ed7fa3293f72,0,571,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 22, '=': 2, 'string': 6, 'string_start': 6, 'string_content': 7, 'string_end': 6, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 5, ',': 4, 'typed_parameter': 1, ':': 6, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ')': 5, 'block': 5, 'if_statement': 2, 'if': 2, 'comparison_operator': 4, 'is': 1, 'none': 1, 'return_statement': 3, 'return': 3, 'attribute': 4, '.': 4, 'in': 3, 'tuple': 3, 'raise_statement': 1, 'raise': 1, 'call': 1, 'argument_list': 1, 'interpolation': 1, '{': 1, '}': 1, 'elif_clause': 2, 'elif': 2}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 23, 'end_line': 27, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.683651110519284,0.6803067244423805,"(tensor([0.9788]), tensor([0.9765]), tensor([0.9776]), tensor([0.9767]))"
"167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False) -> TEvalData:
172         resultWriter = self._resultWriterForModel(resultWriter, model)
173         evaluator = self.createEvaluator(model)
174         evaluator.fitModel(model)
175 
176         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
177             strEvalResults = f""{model}\n\n""
178             for predictedVarName in model.getPredictedVariableNames():
179                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
180                 if logResults:
181                     log.info(f""Evaluation results for {predictedVarName}: {strEvalResult}"")
182                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
183             if resultWriter is not None:
184                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
185             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
186 
187         evalResultData = evaluator.evalModel(model)
188         gatherResults(evalResultData, resultWriter)
189         if additionalEvaluationOnTrainingData:
190             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
191             gatherResults(evalResultDataTrain, resultWriter.childWithAddedPrefix(""-onTrain-""), subtitlePrefix=""[onTrain] "")
192 
193         return evalResultData
194 
195     @staticmethod
","167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False) -> TEvalData:
172         resultWriter = self._resultWriterForModel(resultWriter, model)
173         evaluator = self.createEvaluator(model)
174         evaluator.fitModel(model)
175 
176         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
177             strEvalResults = f""{model}\n\n""
178             for predictedVarName in model.getPredictedVariableNames():
179                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
180                 if logResults:
181                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
182                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
183             if resultWriter is not None:
184                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
185             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
186 
187         evalResultData = evaluator.evalModel(model)
188         gatherResults(evalResultData, resultWriter)
189         if additionalEvaluationOnTrainingData:
190             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
191             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
192             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
193 
194         return evalResultData
195 
196     @staticmethod
","Before: 181
After: 181",fix eval_util.py -- a/src/sensai/evaluation/eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,1717,"{'ERROR': 1, 'string_start': 1, 'string_content': 1, 'escape_sequence': 3, ')': 1, 'return': 1, 'identifier': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8923658801304896,0.8843772734315022,"(tensor([0.9796]), tensor([0.9882]), tensor([0.9839]), tensor([0.9874]))"
"167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False) -> TEvalData:
172         resultWriter = self._resultWriterForModel(resultWriter, model)
173         evaluator = self.createEvaluator(model)
174         evaluator.fitModel(model)
175 
176         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
177             strEvalResults = f""{model}\n\n""
178             for predictedVarName in model.getPredictedVariableNames():
179                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
180                 if logResults:
181                     log.info(f""Evaluation results for {predictedVarName}: {strEvalResult}"")
182                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
183             if resultWriter is not None:
184                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
185             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
186 
187         evalResultData = evaluator.evalModel(model)
188         gatherResults(evalResultData, resultWriter)
189         if additionalEvaluationOnTrainingData:
190             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
191             gatherResults(evalResultDataTrain, resultWriter.childWithAddedPrefix(""-onTrain-""), subtitlePrefix=""[onTrain] "")
192 
193         return evalResultData
194 
195     @staticmethod
","167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False) -> TEvalData:
172         resultWriter = self._resultWriterForModel(resultWriter, model)
173         evaluator = self.createEvaluator(model)
174         evaluator.fitModel(model)
175 
176         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
177             strEvalResults = f""{model}\n\n""
178             for predictedVarName in model.getPredictedVariableNames():
179                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
180                 if logResults:
181                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
182                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
183             if resultWriter is not None:
184                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
185             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
186 
187         evalResultData = evaluator.evalModel(model)
188         gatherResults(evalResultData, resultWriter)
189         if additionalEvaluationOnTrainingData:
190             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
191             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
192             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
193 
194         return evalResultData
195 
196     @staticmethod
","Before: 191
After: 191, 192",fix eval_util.py -- a/src/sensai/evaluation/eval_util.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,1860,"{'ERROR': 1, 'string_start': 1, 'string_content': 1, 'escape_sequence': 3, ')': 1, 'return': 1, 'identifier': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8923658801304896,0.8843772734315022,"(tensor([0.9796]), tensor([0.9882]), tensor([0.9839]), tensor([0.9874]))"
"408         dataSetProvider = self._createDataSetProvider(inputs, outputs)
409         self.model.fit(dataSetProvider, self.nnOptimiserParams)
410 
411     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
412         batchSize = 2**13
413         results = []
414         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
415         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
416             results.append(self.model.applyScaled(inputBatch, asNumpy=True))
417         return np.concatenate(results)
418 
419     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
","408         dataSetProvider = self._createDataSetProvider(inputs, outputs)
409         self.model.fit(dataSetProvider, self.nnOptimiserParams)
410 
411     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
412         batchSize = self.nnOptimiserParams.batchSize
413         results = []
414         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
415         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
416             results.append(self.model.applyScaled(inputBatch, asNumpy=True))
417         return np.concatenate(results)
418 
419     def _predict(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 412
After: 412",fix batchsize in src/src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,3803,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 44, '=': 7, 'call': 7, 'attribute': 14, '.': 14, 'argument_list': 7, '(': 8, ',': 8, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'binary_operator': 1, 'integer': 2, '**': 1, 'list': 1, '[': 1, ']': 1, 'none': 1, 'keyword_argument': 3, 'for_statement': 1, 'for': 1, 'in': 1, 'true': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9427902756012181,0.9401079012877441,"(tensor([0.9881]), tensor([0.9835]), tensor([0.9858]), tensor([0.9840]))"
"499         dataSetProvider = self._createDataSetProvider(inputs, outputs)
500         self.model.fit(dataSetProvider, self.nnOptimiserParams)
501 
502     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
503         batchSize = 64
504         results = []
505         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
506         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
507             results.append(self.model.applyScaled(inputBatch, asNumpy=False))
508         return torch.cat(results, dim=0)
509 
510     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","499         dataSetProvider = self._createDataSetProvider(inputs, outputs)
500         self.model.fit(dataSetProvider, self.nnOptimiserParams)
501 
502     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
503         batchSize = self.nnOptimiserParams.batchSize
504         results = []
505         dataSet = TorchDataSetFromDataFrames(inputs, None, self.model.cuda, inputTensoriser=self.inputTensoriser)
506         for inputBatch in dataSet.iterBatches(batchSize, inputOnly=True):
507             results.append(self.model.applyScaled(inputBatch, asNumpy=False))
508         return torch.cat(results, dim=0)
509 
510     def _predictClassProbabilities(self, inputs: pd.DataFrame) -> pd.DataFrame:
","Before: 503
After: 503",fix batchsize in src/src/sensai/torch/torch_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,4773,"{'module': 1, 'expression_statement': 6, 'assignment': 4, 'identifier': 45, '=': 8, 'call': 7, 'attribute': 14, '.': 14, 'argument_list': 7, '(': 8, ',': 9, ')': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 3, 'type': 2, '->': 1, 'block': 2, 'integer': 2, 'list': 1, '[': 1, ']': 1, 'none': 1, 'keyword_argument': 4, 'for_statement': 1, 'for': 1, 'in': 1, 'true': 1, 'false': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9736071380988617,0.9708985847963453,"(tensor([0.9891]), tensor([0.9920]), tensor([0.9905]), tensor([0.9917]))"
"34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable]) -> Callable:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors
43         """"""
44         if isinstance(f, str):
45             try:
46                 return cls.fromName(f).getTorchFunction()
47             except ValueError:
48                 return getattr(F, f)
49         elif isinstance(f, ActivationFunction):
50             return f.getTorchFunction()
51         elif callable(f):
52             return f
53         else:
54             raise ValueError()
55 
56 
","34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable, None]) -> Optional[Callable]:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors (or None)
43         """"""
44         if f is None:
45             return None
46         elif isinstance(f, str):
47             try:
48                 return cls.fromName(f).getTorchFunction()
49             except ValueError:
50                 return getattr(F, f)
51         elif isinstance(f, ActivationFunction):
52             return f.getTorchFunction()
53         elif callable(f):
54             return f
55         else:
56             raise ValueError(f""Could not determine torch function from {f} of type {type(f)}"")
57 
58 
","Before: 37
After: 37",fix error message in torch_enums.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,320,"{'module': 1, 'return_statement': 5, 'return': 5, 'attribute': 4, 'identifier': 30, '.': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 5, 'generic_type': 1, 'type_parameter': 1, '[': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 1, ')': 9, '->': 1, 'block': 7, 'expression_statement': 1, 'if_statement': 1, 'if': 1, 'call': 8, 'argument_list': 8, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'elif_clause': 2, 'elif': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6437750763784877,0.6424162199974895,"(tensor([0.9423]), tensor([0.9791]), tensor([0.9604]), tensor([0.9753]))"
"34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable]) -> Callable:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors
43         """"""
44         if isinstance(f, str):
45             try:
46                 return cls.fromName(f).getTorchFunction()
47             except ValueError:
48                 return getattr(F, f)
49         elif isinstance(f, ActivationFunction):
50             return f.getTorchFunction()
51         elif callable(f):
52             return f
53         else:
54             raise ValueError()
55 
56 
","34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable, None]) -> Optional[Callable]:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors (or None)
43         """"""
44         if f is None:
45             return None
46         elif isinstance(f, str):
47             try:
48                 return cls.fromName(f).getTorchFunction()
49             except ValueError:
50                 return getattr(F, f)
51         elif isinstance(f, ActivationFunction):
52             return f.getTorchFunction()
53         elif callable(f):
54             return f
55         else:
56             raise ValueError(f""Could not determine torch function from {f} of type {type(f)}"")
57 
58 
","Before: 42
After: 42",fix error message in torch_enums.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,373,"{'module': 1, 'return_statement': 5, 'return': 5, 'attribute': 4, 'identifier': 30, '.': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 5, 'generic_type': 1, 'type_parameter': 1, '[': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 1, ')': 9, '->': 1, 'block': 7, 'expression_statement': 1, 'if_statement': 1, 'if': 1, 'call': 8, 'argument_list': 8, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'elif_clause': 2, 'elif': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6437750763784877,0.6424162199974895,"(tensor([0.9423]), tensor([0.9791]), tensor([0.9604]), tensor([0.9753]))"
"34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable]) -> Callable:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors
43         """"""
44         if isinstance(f, str):
45             try:
46                 return cls.fromName(f).getTorchFunction()
47             except ValueError:
48                 return getattr(F, f)
49         elif isinstance(f, ActivationFunction):
50             return f.getTorchFunction()
51         elif callable(f):
52             return f
53         else:
54             raise ValueError()
55 
56 
","34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable, None]) -> Optional[Callable]:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors (or None)
43         """"""
44         if f is None:
45             return None
46         elif isinstance(f, str):
47             try:
48                 return cls.fromName(f).getTorchFunction()
49             except ValueError:
50                 return getattr(F, f)
51         elif isinstance(f, ActivationFunction):
52             return f.getTorchFunction()
53         elif callable(f):
54             return f
55         else:
56             raise ValueError(f""Could not determine torch function from {f} of type {type(f)}"")
57 
58 
","Before: 44
After: 44, 45, 46",fix error message in torch_enums.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,337,"{'module': 1, 'return_statement': 5, 'return': 5, 'attribute': 4, 'identifier': 30, '.': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 5, 'generic_type': 1, 'type_parameter': 1, '[': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 1, ')': 9, '->': 1, 'block': 7, 'expression_statement': 1, 'if_statement': 1, 'if': 1, 'call': 8, 'argument_list': 8, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'elif_clause': 2, 'elif': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6437750763784877,0.6424162199974895,"(tensor([0.9423]), tensor([0.9791]), tensor([0.9604]), tensor([0.9753]))"
"34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable]) -> Callable:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors
43         """"""
44         if isinstance(f, str):
45             try:
46                 return cls.fromName(f).getTorchFunction()
47             except ValueError:
48                 return getattr(F, f)
49         elif isinstance(f, ActivationFunction):
50             return f.getTorchFunction()
51         elif callable(f):
52             return f
53         else:
54             raise ValueError()
55 
56 
","34         return self.value
35 
36     @classmethod
37     def torchFunctionFromAny(cls, f: Union[str, ""ActivationFunction"", Callable, None]) -> Optional[Callable]:
38         """"""
39         Gets the torch activation for the given argument
40 
41         :param f: either an instance of ActivationFunction, the name of a function from torch.nn.functional or an actual function
42         :return: a function that can be applied to tensors (or None)
43         """"""
44         if f is None:
45             return None
46         elif isinstance(f, str):
47             try:
48                 return cls.fromName(f).getTorchFunction()
49             except ValueError:
50                 return getattr(F, f)
51         elif isinstance(f, ActivationFunction):
52             return f.getTorchFunction()
53         elif callable(f):
54             return f
55         else:
56             raise ValueError(f""Could not determine torch function from {f} of type {type(f)}"")
57 
58 
","Before: 54
After: 56",fix error message in torch_enums.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_enums.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,420,"{'module': 1, 'return_statement': 5, 'return': 5, 'attribute': 4, 'identifier': 30, '.': 4, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 9, ',': 6, 'typed_parameter': 1, ':': 8, 'type': 5, 'generic_type': 1, 'type_parameter': 1, '[': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ']': 1, ')': 9, '->': 1, 'block': 7, 'expression_statement': 1, 'if_statement': 1, 'if': 1, 'call': 8, 'argument_list': 8, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'elif_clause': 2, 'elif': 2, 'else_clause': 1, 'else': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 5, 'token_count': 31, 'name': 'fromName', 'long_name': 'fromName( cls , name )', 'start_line': 17, 'end_line': 21, 'full_parameters': ['cls', ' name'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_enums.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6437750763784877,0.6424162199974895,"(tensor([0.9423]), tensor([0.9791]), tensor([0.9604]), tensor([0.9753]))"
"83             cons = self.method.value[1]
84             self.optimizer = cons(self.params, **optimiserArgs)
85 
86     def step(self, lossBackward: Callable):
87         """"""
88         :param lossBackward: callable, performs backward step and returns loss
89         :return: loss value
90         """"""
91         if self.use_shrinkage:
92             def closureWithShrinkage():
93                 loss = lossBackward()
94 
95                 # Compute gradients norm.
96                 grad_norm = None
97                 for param in self.params:
98                     n = param.grad.data.norm()
99                     if grad_norm is None:
100                         grad_norm = n * n
101                     else:
102                         grad_norm += n * n
103                 grad_norm = torch.sqrt(grad_norm)
104 
105                 if grad_norm > self.max_grad_norm:
106                     shrinkage = self.max_grad_norm / grad_norm
107 
108                     for param in self.params:
109                         param.grad.data.mul_(shrinkage)
110 
111                 return loss
112 
113             closure = closureWithShrinkage
114         else:
115             closure = lossBackward
116 
117         loss = self.optimizer.step(closure)
118         return loss
119 
120 
","83             cons = self.method.value[1]
84             self.optimizer = cons(self.params, **optimiserArgs)
85 
86     def step(self, lossBackward: Callable):
87         """"""
88         :param lossBackward: callable, performs backward step and returns loss
89         :return: loss value
90         """"""
91         if self.use_shrinkage:
92             def closureWithShrinkage():
93                 loss = lossBackward()
94                 torch.nn.utils.clip_grad_norm_(self.params, self.max_grad_norm)
95                 return loss
96 
97             closure = closureWithShrinkage
98         else:
99             closure = lossBackward
100 
101         loss = self.optimizer.step(closure)
102         return loss
103 
104 
","Before: 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110
After: 94",use clip_grad_norm_ in optimiser,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,799,"{'module': 1, 'expression_statement': 14, 'assignment': 11, 'identifier': 65, '=': 11, 'subscript': 1, 'attribute': 18, '.': 18, '[': 1, 'integer': 1, ']': 1, 'call': 6, 'argument_list': 6, '(': 8, ',': 2, 'dictionary_splat': 1, '**': 1, ')': 8, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 1, ':': 10, 'type': 1, 'block': 9, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'if_statement': 3, 'if': 3, 'comment': 1, 'none': 2, 'for_statement': 2, 'for': 2, 'in': 2, 'comparison_operator': 2, 'is': 1, 'binary_operator': 3, '*': 2, 'else_clause': 2, 'else': 2, 'augmented_assignment': 1, '+=': 1, '>': 1, '/': 1, 'return_statement': 2, 'return': 2}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.41706134529327954,0.41728691457929573,"(tensor([0.9339]), tensor([0.8550]), tensor([0.8927]), tensor([0.8623]))"
"363         ""optimiserClip"": ""shrinkageClip""
364     }
365 
366     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser: Union[str, Optimiser] = ""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
367             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
368             useShrinkage=True, shrinkageClip=10., **optimiserArgs):
369         """"""
370         :param lossEvaluator: the loss evaluator to use
371         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
372         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
373         :param optimiserLR: the optimiser's learning rate
374         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
375             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
376         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
377             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
378         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
379             If no validation is to be performed, pass 1.0.
380         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
381             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
382         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
383         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
384         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
385         """"""
386         if optimiser == 'lbfgs':
387             largeBatchSize = 1e12
388             if batchSize is not None:
389                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
390             batchSize = largeBatchSize
391         else:
392             if batchSize is None:
393                 log.debug(""No batch size was specified, using batch size 64 by default"")
394                 batchSize = 64
395 
396         self.epochs = epochs
397         self.batchSize = batchSize
398         self.optimiserLR = optimiserLR
399         self.shrinkageClip = shrinkageClip
400         self.optimiser = optimiser
401         self.gpu = gpu
402         self.trainFraction = trainFraction
403         self.scaledOutputs = scaledOutputs
404         self.lossEvaluator = lossEvaluator
405         self.optimiserArgs = optimiserArgs
406         self.useShrinkage = useShrinkage
407         self.earlyStoppingEpochs = earlyStoppingEpochs
408 
409     @classmethod
","347         ""optimiserClip"": ""shrinkageClip""
348     }
349 
350     def __init__(self, lossEvaluator: NNLossEvaluator = None, gpu=None, optimiser: Union[str, Optimiser] = ""adam"", optimiserLR=0.001, earlyStoppingEpochs=None,
351             batchSize=None, epochs=1000, trainFraction=0.75, scaledOutputs=False,
352             useShrinkage=True, shrinkageClip=10., shuffle=True, **optimiserArgs):
353         """"""
354         :param lossEvaluator: the loss evaluator to use
355         :param gpu: the index of the GPU to be used (if CUDA is enabled for the model to be trained); if None, default to first GPU
356         :param optimiser: the name of the optimizer to be used; defaults to ""adam""
357         :param optimiserLR: the optimiser's learning rate
358         :param earlyStoppingEpochs: the number of epochs without validation score improvement after which to abort training and
359             use the best epoch's model (early stopping); if None, never abort training before all epochs are completed
360         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
361             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
362         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
363             If no validation is to be performed, pass 1.0.
364         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
365             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
366         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
367         :param shrinkageClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
368         :param shuffle: whether to shuffle the training data
369         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
370         """"""
371         if optimiser == 'lbfgs':
372             largeBatchSize = 1e12
373             if batchSize is not None:
374                 log.warning(f""LBFGS does not make use of batches, therefore using large batch size {largeBatchSize} to achieve use of a single batch"")
375             batchSize = largeBatchSize
376         else:
377             if batchSize is None:
378                 log.debug(""No batch size was specified, using batch size 64 by default"")
379                 batchSize = 64
380 
381         self.epochs = epochs
382         self.batchSize = batchSize
383         self.optimiserLR = optimiserLR
384         self.shrinkageClip = shrinkageClip
385         self.optimiser = optimiser
386         self.gpu = gpu
387         self.trainFraction = trainFraction
388         self.scaledOutputs = scaledOutputs
389         self.lossEvaluator = lossEvaluator
390         self.optimiserArgs = optimiserArgs
391         self.useShrinkage = useShrinkage
392         self.earlyStoppingEpochs = earlyStoppingEpochs
393         self.shuffle = shuffle
394 
395     @classmethod
","Before: 368
After: 352",use clip_grad_norm_ in optimiser,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,3261,"{'module': 1, 'ERROR': 2, 'string': 7, 'string_start': 7, 'string_content': 8, 'string_end': 7, ':': 8, 'expression_statement': 19, '}': 2, 'function_definition': 1, 'def': 1, 'identifier': 66, 'parameters': 1, '(': 3, ',': 13, 'typed_default_parameter': 2, 'type': 4, '=': 26, 'none': 6, 'default_parameter': 9, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, 'float': 4, 'integer': 2, 'false': 1, 'true': 1, 'dictionary_splat_pattern': 1, '**': 1, ')': 3, 'block': 5, 'if_statement': 3, 'if': 3, 'comparison_operator': 3, '==': 1, 'assignment': 15, 'is not': 2, 'call': 2, 'attribute': 14, '.': 14, 'argument_list': 2, 'interpolation': 1, '{': 1, 'else_clause': 1, 'else': 1, 'is': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7604490512452633,0.7587951233459514,"(tensor([0.9750]), tensor([0.9782]), tensor([0.9766]), tensor([0.9779]))"
"647         scaledTruth = outputScaler.denormalise(groundTruth)
648         return scaledOutput, scaledTruth
649 
650     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, criterion: nn.modules.loss._Loss,
651             optim: _Optimiser, batch_size: int, cuda: bool, outputScalers: Sequence[TensorScaler]):
652         """"""Performs one training epoch""""""
653         model.train()
654         total_loss = 0
655         n_samples = 0
656         numOutputsPerDataPoint = None
657         for dataSet, outputScaler in zip(dataSets, outputScalers):
658             for X, Y in dataSet.iterBatches(batch_size, shuffle=True):
659                 if numOutputsPerDataPoint is None:
660                     outputShape = Y.shape[1:]
661                     numOutputsPerDataPoint = functools.reduce(lambda x, y: x * y, outputShape, 1)
662 
663                 def closure():
664                     model.zero_grad()
665                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
666                     loss = criterion(output, groundTruth)
667                     loss.backward()
668                     return loss
669 
670                 loss = optim.step(closure)
671                 total_loss += loss.item()
672                 numDataPointsInBatch = Y.size(0)
673                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
674         return total_loss / n_samples
675 
676     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
","635         scaledTruth = outputScaler.denormalise(groundTruth)
636         return scaledOutput, scaledTruth
637 
638     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, criterion: nn.modules.loss._Loss,
639             optim: _Optimiser, batch_size: int, cuda: bool, outputScalers: Sequence[TensorScaler]):
640         """"""Performs one training epoch""""""
641         model.train()
642         total_loss = 0
643         n_samples = 0
644         numOutputsPerDataPoint = None
645         for dataSet, outputScaler in zip(dataSets, outputScalers):
646             for X, Y in dataSet.iterBatches(batch_size, shuffle=self.params.shuffle):
647                 if numOutputsPerDataPoint is None:
648                     outputShape = Y.shape[1:]
649                     numOutputsPerDataPoint = functools.reduce(lambda x, y: x * y, outputShape, 1)
650 
651                 def closure():
652                     model.zero_grad()
653                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
654                     loss = criterion(output, groundTruth)
655                     loss.backward()
656                     return loss
657 
658                 loss = optim.step(closure)
659                 total_loss += loss.item()
660                 numDataPointsInBatch = Y.size(0)
661                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
662         return total_loss / n_samples
663 
664     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
","Before: 658
After: 646",use clip_grad_norm_ in optimiser,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,ba3bd66f102f9a575c3a61ea6e94fff718d8620c,e453941c03d5bdd1c83c4485b4cd9b229fd01291,0,5822,"{'module': 1, 'expression_statement': 16, 'assignment': 10, 'identifier': 89, '=': 11, 'call': 12, 'attribute': 15, '.': 15, 'argument_list': 12, '(': 14, ')': 14, 'return_statement': 3, 'return': 3, 'expression_list': 1, ',': 20, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 7, ':': 14, 'type': 9, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'integer': 5, 'none': 2, 'for_statement': 2, 'for': 2, 'pattern_list': 3, 'in': 2, 'keyword_argument': 1, 'true': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, 'slice': 1, 'lambda': 2, 'lambda_parameters': 1, 'binary_operator': 3, '*': 2, 'augmented_assignment': 2, '+=': 2, '/': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7073972533908952,0.6979946016673834,"(tensor([0.9666]), tensor([0.9720]), tensor([0.9693]), tensor([0.9715]))"
"1 from .greedy_clustering import GreedyAgglomerativeClustering
2 from sensai.clustering.clustering_base import EuclideanClusterer
3 from .sklearn_clustering import SkLearnEuclideanClusterer
","1 from .greedy_clustering import GreedyAgglomerativeClustering
2 from .clustering_base import EuclideanClusterer
3 from .sklearn_clustering import SkLearnEuclideanClusterer
","Before: 2
After: 2",fix typo in src/sensai/clustering/__init__.py,Fixed absolute import,https://github.com/opcode81/sensAI,src/sensai/clustering/__init__.py,05be11c198e01ca4edd0c37a2f84ed68754664a0,fa85533c53c1226d9e413f067172be1e8a8a0b94,0,23,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'relative_import': 1, 'import_prefix': 1, '.': 3, 'dotted_name': 4, 'identifier': 6, 'import': 2}",{},{},0.8003203203844999,0,"(tensor([0.9937]), tensor([0.9816]), tensor([0.9876]), tensor([0.9828]))"
"26     irisClassificationTestCase.testMinAccuracy(model, 0.8)
27 
28 
29 def test_NNOptimiserWithoutValidation_MLPClassifier(irisDataSet):
30     """"""
31     Tests and demonstrates the use of NNOptimiser without validation on the iris data set, using low-level
32     interfaces (i.e. without high-level abstractions such as VectorModel)
33     """"""
34     iodata = irisDataSet.getInputOutputData()
35     outputs = iodata.outputs.iloc[:, 0]
36     classLabels = list(outputs.unique())
37     outputTensor = torch.tensor([classLabels.index(l) for l in outputs])
38     scaler = normalisation.VectorDataScaler(iodata.inputs, normalisation.NormalisationMode.MAX_BY_COLUMN)
39     inputTensor = torch.tensor(scaler.getNormalisedArray(iodata.inputs), dtype=torch.float32)
40     dataSet = TorchDataSetFromTensors(inputTensor, outputTensor, False)
41     model = TorchModelFromModuleFactory(lambda: MultiLayerPerceptron(inputTensor.shape[1], len(classLabels),
42             (4, 3), hidActivationFn=torch.tanh, outputActivationFn=torch.nn.Softmax()), cuda=False)
43     NNOptimiser(NNOptimiserParams(lossEvaluator=NNLossEvaluatorClassification(), trainFraction=1.0, epochs=300,
44             optimiser=""adam"")).fit(model, dataSet)
45     modelOutputs = model.apply(inputTensor, asNumpy=False)
46     accuracy = torch.sum(torch.argmax(modelOutputs, 1) == outputTensor).item() / len(outputTensor)
47     assert accuracy > 0.9","26     irisClassificationTestCase.testMinAccuracy(model, 0.8)
27 
28 
29 def test_NNOptimiserWithoutValidation_MLPClassifier(irisDataSet):
30     """"""
31     Tests and demonstrates the use of NNOptimiser without validation on the iris data set, using low-level
32     interfaces (i.e. without high-level abstractions such as VectorModel)
33     """"""
34     iodata = irisDataSet.getInputOutputData()
35     outputs = iodata.outputs.iloc[:, 0]
36     classLabels = list(outputs.unique())
37     outputTensor = torch.tensor([classLabels.index(l) for l in outputs])
38     scaler = normalisation.VectorDataScaler(iodata.inputs, normalisation.NormalisationMode.MAX_BY_COLUMN)
39     inputTensor = torch.tensor(scaler.getNormalisedArray(iodata.inputs), dtype=torch.float32)
40     dataSet = TorchDataSetFromTensors(inputTensor, outputTensor, False)
41     model = TorchModelFromModuleFactory(lambda: MultiLayerPerceptron(inputTensor.shape[1], len(classLabels),
42             (4, 3), hidActivationFn=torch.tanh, outputActivationFn=None), cuda=False)
43     NNOptimiser(NNOptimiserParams(lossEvaluator=NNLossEvaluatorClassification(NNLossEvaluatorClassification.LossFunction.CROSSENTROPY), trainFraction=1.0, epochs=300,
44             optimiser=""adam"")).fit(model, dataSet)
45     modelOutputs = model.apply(inputTensor, asNumpy=False)
46     accuracy = torch.sum(torch.argmax(modelOutputs, 1) == outputTensor).item() / len(outputTensor)
47     assert accuracy > 0.9","Before: 42, 43
After: 42, 43",update test_nn_nounptimiserwithoutvalidation_mlpclassifier to use croSSENTROPY,Fixed test: loss function must now be specified explicitly,https://github.com/opcode81/sensAI,tests/frameworks/torch/test_torch.py,474bbdf3efee37cf5ace25ccee3763f19207a43d,e918acd6dd1f1ea566a22eb191245f09e99f906a,0,528,"{'module': 1, 'expression_statement': 12, 'call': 19, 'attribute': 21, 'identifier': 76, '.': 21, 'argument_list': 19, '(': 21, ',': 17, 'float': 2, ')': 21, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 9, '=': 18, 'subscript': 2, '[': 3, 'slice': 1, 'integer': 5, ']': 3, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'keyword_argument': 9, 'false': 3, 'lambda': 2, 'tuple': 1}","{'cyclomatic_complexity': 2, 'nloc': 9, 'token_count': 130, 'name': 'test_MLPClassifier', 'long_name': 'test_MLPClassifier( irisDataSet , irisClassificationTestCase , testResources )', 'start_line': 18, 'end_line': 26, 'full_parameters': ['irisDataSet', ' irisClassificationTestCase', ' testResources'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/tests/frameworks/torch/test_torch.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 9, 'token_count': 130, 'name': 'test_MLPClassifier', 'long_name': 'test_MLPClassifier( irisDataSet , irisClassificationTestCase , testResources )', 'start_line': 18, 'end_line': 26, 'full_parameters': ['irisDataSet', ' irisClassificationTestCase', ' testResources'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/tests/frameworks/torch/test_torch.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9728264463882839,0.9710002024956096,"(tensor([0.9829]), tensor([0.9913]), tensor([0.9871]), tensor([0.9905]))"
"24 
25 
26 class SkLearnMLPVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
27     def __init__(self, **modelArgs):
28         super().__init__(sklearn.neural_network.MLPClassifier, **modelArgs)
29 
30 
","24 
25 
26 class SkLearnMLPVectorClassificationModel(AbstractSkLearnVectorClassificationModel):
27     def __init__(self, random_state=42, **modelArgs):
28         super().__init__(sklearn.neural_network.MLPClassifier, random_state=random_state, **modelArgs)
29 
30 
","Before: 27, 28
After: 27, 28",fix sklearn vector classification model,Fixed: No fixed random seed in SkLearnMLPVectorClassificationModel,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_classification.py,0ce6878f2c7c6f12a4387fa2089b2e73ca96d80f,a5eb7622c30e05e7a410ee0a223664fd109492dd,0,207,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 11, 'argument_list': 3, '(': 4, ')': 4, ':': 2, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'dictionary_splat_pattern': 1, '**': 2, 'expression_statement': 1, 'call': 2, 'attribute': 3, '.': 3, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 15, 'end_line': 17, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 39, 'name': '__init__', 'long_name': '__init__( self , min_samples_leaf = 8 , random_state = 42 , ** modelArgs )', 'start_line': 15, 'end_line': 17, 'full_parameters': ['self', ' min_samples_leaf = 8', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_classification.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8463120071054943,0.8377722878823317,"(tensor([0.9574]), tensor([0.9914]), tensor([0.9741]), tensor([0.9879]))"
"181         plt.ylabel(""probability density"")
182         return fig
183 
184     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
185         """"""
186         :param figure: whether to plot in a separate figure and return that figure
187         :param titleAdd: a string to be added to the title in a second line
188         :param kwargs: parameters to be passed on to plt.scatter()
189 
190         :return:  the resulting figure object or None
191         """"""
192         fig = None
193         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
194         if titleAdd is not None:
195             title += ""\n"" + titleAdd
196         if figure:
197             fig = plt.figure(title.replace(""\n"", "" ""))
198         y_range = [min(self.y_true), max(self.y_true)]
199         plt.scatter(self.y_true, self.y_predicted, **kwargs)
200         plt.plot(y_range, y_range, '-', lw=2, label=""_not in legend"", color=""r"")
201         plt.xlabel(""ground truth"")
202         plt.ylabel(""prediction"")
203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","185         plt.ylabel(""probability density"")
186         return fig
187 
188     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
189         """"""
190         :param figure: whether to plot in a separate figure and return that figure
191         :param titleAdd: a string to be added to the title in a second line
192         :param kwargs: parameters to be passed on to plt.scatter()
193 
194         :return:  the resulting figure object or None
195         """"""
196         fig = None
197         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
198         if titleAdd is not None:
199             title += ""\n"" + titleAdd
200         if figure:
201             fig = plt.figure(title.replace(""\n"", "" ""))
202         y_range = [min(self.y_true), max(self.y_true)]
203         plt.scatter(self.y_true, self.y_predicted, c=(0, 0, 1, self.SCATTER_PLOT_POINT_TRANSPARENCY), zorder=2, **kwargs)
204         plt.plot(y_range, y_range, '-', lw=1, label=""_not in legend"", color=""green"", zorder=1)
205         plt.xlabel(""ground truth"")
206         plt.ylabel(""prediction"")
207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","Before: 199, 200
After: 203, 204",update eval_stats_regression.py for new features,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,1781,"{'module': 1, 'expression_statement': 12, 'call': 10, 'attribute': 13, 'identifier': 51, '.': 13, 'argument_list': 10, '(': 11, 'string': 11, 'string_start': 11, 'string_content': 11, 'string_end': 11, ')': 11, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 12, 'default_parameter': 2, '=': 9, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, ':': 3, 'block': 3, 'assignment': 4, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 1, 'dictionary_splat': 1, 'keyword_argument': 3, 'integer': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7093883655854358,0.7087599759480074,"(tensor([0.9472]), tensor([0.9767]), tensor([0.9617]), tensor([0.9737]))"
"203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
207         """"""
208         :param figure: whether to plot in a separate figure and return that figure
209         :param cmap: the colour map to use (see corresponding parameter of plt.imshow); if None use colour map from white to red
210         :param bins: how many bins to use for constructing the heatmap
211         :param titleAdd: a string to add to the title (on a second line)
212         :param kwargs: will be passed to plt.imshow()
213 
214         :return:  the resulting figure object or None
215         """"""
216         fig = None
217         title = ""Heat Map of Ground Truth vs. Predicted Values""
218         if titleAdd:
219             title += ""\n"" + titleAdd
220         if figure:
221             fig = plt.figure(title.replace(""\n"", "" ""))
222         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
223         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
224         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
225         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
226         if cmap is None:
227             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1.0, 0.95, 0.95)), (1, (0.7, 0, 0))))
228         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
229 
230         plt.xlabel(""ground truth"")
231         plt.ylabel(""prediction"")
232         plt.title(title)
233         return fig
234 
235 
","207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","Before: 209
After: 213, 214",update eval_stats_regression.py for new features,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,1965,"{'module': 1, 'expression_statement': 15, 'call': 17, 'attribute': 19, 'identifier': 93, '.': 19, 'argument_list': 17, '(': 25, ')': 25, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 41, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 15, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 2, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 10, 'float': 5, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'tuple': 7, '/': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.676961188769471,0.6718914806303168,"(tensor([0.9507]), tensor([0.9450]), tensor([0.9478]), tensor([0.9455]))"
"203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
207         """"""
208         :param figure: whether to plot in a separate figure and return that figure
209         :param cmap: the colour map to use (see corresponding parameter of plt.imshow); if None use colour map from white to red
210         :param bins: how many bins to use for constructing the heatmap
211         :param titleAdd: a string to add to the title (on a second line)
212         :param kwargs: will be passed to plt.imshow()
213 
214         :return:  the resulting figure object or None
215         """"""
216         fig = None
217         title = ""Heat Map of Ground Truth vs. Predicted Values""
218         if titleAdd:
219             title += ""\n"" + titleAdd
220         if figure:
221             fig = plt.figure(title.replace(""\n"", "" ""))
222         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
223         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
224         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
225         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
226         if cmap is None:
227             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1.0, 0.95, 0.95)), (1, (0.7, 0, 0))))
228         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
229 
230         plt.xlabel(""ground truth"")
231         plt.ylabel(""prediction"")
232         plt.title(title)
233         return fig
234 
235 
","207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","Before: 223
After: 228",update eval_stats_regression.py for new features,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,2072,"{'module': 1, 'expression_statement': 15, 'call': 17, 'attribute': 19, 'identifier': 93, '.': 19, 'argument_list': 17, '(': 25, ')': 25, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 41, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 15, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 2, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 10, 'float': 5, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'tuple': 7, '/': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.676961188769471,0.6718914806303168,"(tensor([0.9507]), tensor([0.9450]), tensor([0.9478]), tensor([0.9455]))"
"203         plt.title(title)
204         return fig
205 
206     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
207         """"""
208         :param figure: whether to plot in a separate figure and return that figure
209         :param cmap: the colour map to use (see corresponding parameter of plt.imshow); if None use colour map from white to red
210         :param bins: how many bins to use for constructing the heatmap
211         :param titleAdd: a string to add to the title (on a second line)
212         :param kwargs: will be passed to plt.imshow()
213 
214         :return:  the resulting figure object or None
215         """"""
216         fig = None
217         title = ""Heat Map of Ground Truth vs. Predicted Values""
218         if titleAdd:
219             title += ""\n"" + titleAdd
220         if figure:
221             fig = plt.figure(title.replace(""\n"", "" ""))
222         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
223         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
224         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
225         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
226         if cmap is None:
227             cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1.0, 0.95, 0.95)), (1, (0.7, 0, 0))))
228         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
229 
230         plt.xlabel(""ground truth"")
231         plt.ylabel(""prediction"")
232         plt.title(title)
233         return fig
234 
235 
","207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, 'k-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","Before: 227
After: 232",update eval_stats_regression.py for new features,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,2223,"{'module': 1, 'expression_statement': 15, 'call': 17, 'attribute': 19, 'identifier': 93, '.': 19, 'argument_list': 17, '(': 25, ')': 25, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 41, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 15, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 2, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 10, 'float': 5, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'tuple': 7, '/': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.676961188769471,0.6718914806303168,"(tensor([0.9507]), tensor([0.9450]), tensor([0.9478]), tensor([0.9455]))"
"101         super().__init__(modelConstructor, **modelArgs)
102         self.models = {}
103 
104     def __str__(self):
105         if len(self.models) > 0:
106             modelStr = str(next(iter(self.models.values())))
107         else:
108             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
109         return f""{self.__class__.__name__}[{modelStr}]""
110 
111     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","105         super().__init__(modelConstructor, **modelArgs)
106         self.models = {}
107 
108     def _toStringExcludes(self) -> List[str]:
109         return super()._toStringExcludes() + [""models""]
110 
111     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
","Before: 104
After: 108, 109, 110, 111, 112",update sklearn_base.py for new api,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,703,"{'module': 1, 'expression_statement': 4, 'call': 7, 'attribute': 10, 'identifier': 28, 'argument_list': 7, '(': 8, ')': 8, '.': 10, ',': 1, 'dictionary_splat': 1, '**': 1, 'assignment': 3, '=': 3, 'dictionary': 1, '{': 5, '}': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'else_clause': 1, 'else': 1, 'string': 2, 'string_start': 2, 'interpolation': 4, 'string_end': 2, 'return_statement': 1, 'return': 1, 'string_content': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.17717571422058803,0.16341350117589165,"(tensor([0.7806]), tensor([0.7659]), tensor([0.7731]), tensor([0.7673]))"
"101         super().__init__(modelConstructor, **modelArgs)
102         self.models = {}
103 
104     def __str__(self):
105         if len(self.models) > 0:
106             modelStr = str(next(iter(self.models.values())))
107         else:
108             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
109         return f""{self.__class__.__name__}[{modelStr}]""
110 
111     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","108     def _toStringExcludes(self) -> List[str]:
109         return super()._toStringExcludes() + [""models""]
110 
111     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
112         d = super()._toStringAdditionalEntries()
113         if len(self.models) > 0:
114             d[""model[0]""] = str(next(iter(self.models.values())))
115         else:
116             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
117         return d
118 
119     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 106
After: 114",update sklearn_base.py for new api,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,749,"{'module': 1, 'expression_statement': 4, 'call': 7, 'attribute': 10, 'identifier': 28, 'argument_list': 7, '(': 8, ')': 8, '.': 10, ',': 1, 'dictionary_splat': 1, '**': 1, 'assignment': 3, '=': 3, 'dictionary': 1, '{': 5, '}': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'else_clause': 1, 'else': 1, 'string': 2, 'string_start': 2, 'interpolation': 4, 'string_end': 2, 'return_statement': 1, 'return': 1, 'string_content': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.38022473593536715,0.3639786744395658,"(tensor([0.8192]), tensor([0.8959]), tensor([0.8559]), tensor([0.8876]))"
"101         super().__init__(modelConstructor, **modelArgs)
102         self.models = {}
103 
104     def __str__(self):
105         if len(self.models) > 0:
106             modelStr = str(next(iter(self.models.values())))
107         else:
108             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
109         return f""{self.__class__.__name__}[{modelStr}]""
110 
111     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","108     def _toStringExcludes(self) -> List[str]:
109         return super()._toStringExcludes() + [""models""]
110 
111     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
112         d = super()._toStringAdditionalEntries()
113         if len(self.models) > 0:
114             d[""model[0]""] = str(next(iter(self.models.values())))
115         else:
116             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
117         return d
118 
119     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 108, 109
After: 116, 117",update sklearn_base.py for new api,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,777,"{'module': 1, 'expression_statement': 4, 'call': 7, 'attribute': 10, 'identifier': 28, 'argument_list': 7, '(': 8, ')': 8, '.': 10, ',': 1, 'dictionary_splat': 1, '**': 1, 'assignment': 3, '=': 3, 'dictionary': 1, '{': 5, '}': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'else_clause': 1, 'else': 1, 'string': 2, 'string_start': 2, 'interpolation': 4, 'string_end': 2, 'return_statement': 1, 'return': 1, 'string_content': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.38022473593536715,0.3639786744395658,"(tensor([0.8192]), tensor([0.8959]), tensor([0.8559]), tensor([0.8876]))"
"132         super().__init__(modelConstructor, **modelArgs)
133         self.model = None
134 
135     def __str__(self):
136         if self.model is not None:
137             modelStr = str(self.model)
138         else:
139             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
140         return f""{self.__class__.__name__}[{modelStr}]""
141 
142     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","140         super().__init__(modelConstructor, **modelArgs)
141         self.model = None
142 
143     def _toStringExcludes(self) -> List[str]:
144         return super()._toStringExcludes() + [""model""]
145 
146     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
","Before: 135
After: 143, 144, 145, 146, 147",update sklearn_base.py for new api,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,1057,"{'module': 1, 'expression_statement': 4, 'call': 3, 'attribute': 9, 'identifier': 24, 'argument_list': 3, '(': 4, ')': 4, '.': 9, ',': 1, 'dictionary_splat': 1, '**': 1, 'assignment': 3, '=': 3, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'else_clause': 1, 'else': 1, 'string': 2, 'string_start': 2, 'interpolation': 4, '{': 4, '}': 4, 'string_end': 2, 'return_statement': 1, 'return': 1, 'string_content': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.1737171341281636,0.15210439323593228,"(tensor([0.7719]), tensor([0.7729]), tensor([0.7724]), tensor([0.7728]))"
"132         super().__init__(modelConstructor, **modelArgs)
133         self.model = None
134 
135     def __str__(self):
136         if self.model is not None:
137             modelStr = str(self.model)
138         else:
139             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
140         return f""{self.__class__.__name__}[{modelStr}]""
141 
142     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","143     def _toStringExcludes(self) -> List[str]:
144         return super()._toStringExcludes() + [""model""]
145 
146     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
147         d = super()._toStringAdditionalEntries()
148         if self.model is not None:
149             d[""model""] = str(self.model)
150         else:
151             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
152         return d
153 
154     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 137
After: 149",update sklearn_base.py for new api,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,1082,"{'module': 1, 'expression_statement': 4, 'call': 3, 'attribute': 9, 'identifier': 24, 'argument_list': 3, '(': 4, ')': 4, '.': 9, ',': 1, 'dictionary_splat': 1, '**': 1, 'assignment': 3, '=': 3, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'else_clause': 1, 'else': 1, 'string': 2, 'string_start': 2, 'interpolation': 4, '{': 4, '}': 4, 'string_end': 2, 'return_statement': 1, 'return': 1, 'string_content': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3124575887547057,0.2899897266018814,"(tensor([0.8189]), tensor([0.8950]), tensor([0.8553]), tensor([0.8868]))"
"132         super().__init__(modelConstructor, **modelArgs)
133         self.model = None
134 
135     def __str__(self):
136         if self.model is not None:
137             modelStr = str(self.model)
138         else:
139             modelStr = f""{self.modelConstructor.__name__}{self.modelArgs}""
140         return f""{self.__class__.__name__}[{modelStr}]""
141 
142     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","143     def _toStringExcludes(self) -> List[str]:
144         return super()._toStringExcludes() + [""model""]
145 
146     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
147         d = super()._toStringAdditionalEntries()
148         if self.model is not None:
149             d[""model""] = str(self.model)
150         else:
151             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
152         return d
153 
154     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 139, 140
After: 151, 152",update sklearn_base.py for new api,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,1110,"{'module': 1, 'expression_statement': 4, 'call': 3, 'attribute': 9, 'identifier': 24, 'argument_list': 3, '(': 4, ')': 4, '.': 9, ',': 1, 'dictionary_splat': 1, '**': 1, 'assignment': 3, '=': 3, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 3, 'block': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'else_clause': 1, 'else': 1, 'string': 2, 'string_start': 2, 'interpolation': 4, '{': 4, '}': 4, 'string_end': 2, 'return_statement': 1, 'return': 1, 'string_content': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 14, 'end_line': 18, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3124575887547057,0.2899897266018814,"(tensor([0.8189]), tensor([0.8950]), tensor([0.8553]), tensor([0.8868]))"
"423         yArray = self._predictOutputsForInputDataFrame(inputs)
424         return pd.DataFrame(yArray, columns=self.getModelOutputVariableNames())
425 
426     def __str__(self) -> str:
427         return objectRepr(self, [""model"", ""normalisationMode"", ""nnOptimiserParams""])
428 
429 
","423         yArray = self._predictOutputsForInputDataFrame(inputs)
424         return pd.DataFrame(yArray, columns=self.getModelOutputVariableNames())
425 
426     def _toStringExcludes(self) -> List[str]:
427         return super()._toStringExcludes() + [""modelClass"", ""modelArgs"", ""modelKwArgs"", ""inputTensoriser""]
428 
429 
","Before: 426, 427
After: 426, 427",remove unnecessary `__str__` and `_tostrings`,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,4025,"{'module': 1, 'expression_statement': 1, 'assignment': 1, 'identifier': 15, '=': 2, 'call': 4, 'attribute': 3, '.': 3, 'argument_list': 4, '(': 5, ')': 5, 'return_statement': 2, 'return': 2, ',': 4, 'keyword_argument': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1, 'list': 1, '[': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.49238651727522326,0.46811695472432113,"(tensor([0.8854]), tensor([0.9125]), tensor([0.8987]), tensor([0.9097]))"
"526             raise ValueError(f""Unhandled output mode {self.outputMode}"")
527         return pd.DataFrame(y.numpy(), columns=self._labels)
528 
529     def __str__(self) -> str:
530         return objectRepr(self, [""model"", ""normalisationMode"", ""nnOptimiserParams""])
","526             raise ValueError(f""Unhandled output mode {self.outputMode}"")
527         return pd.DataFrame(y.numpy(), columns=self._labels)
528 
529     def _toStringExcludes(self) -> List[str]:
530         return super()._toStringExcludes() + [""modelClass"", ""modelArgs"", ""modelKwArgs"", ""inputTensoriser""]
","Before: 529, 530
After: 529, 530",remove unnecessary `__str__` and `_tostrings`,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,0,5149,"{'module': 1, 'raise_statement': 1, 'raise': 1, 'call': 3, 'identifier': 13, 'argument_list': 3, '(': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'interpolation': 1, '{': 1, 'attribute': 4, '.': 4, '}': 1, 'string_end': 1, ')': 4, 'return_statement': 1, 'return': 1, ',': 1, 'keyword_argument': 1, '=': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 1, ':': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5601033466690303,0.5460555138491869,"(tensor([0.8776]), tensor([0.9106]), tensor([0.8938]), tensor([0.9072]))"
"71     @abstractmethod
72     def isFitted(self) -> bool:
73         pass
74 
75 
76 class VectorModel(FittableModel, PickleLoadSaveMixin, ABC):
77     """"""
78     Base class for models that map data frames to predictions and can be fitted on data frames
79     """"""
80 
","73     @abstractmethod
74     def isFitted(self) -> bool:
75         pass
76 
77 
78 class VectorModel(FittableModel, PickleLoadSaveMixin, ToStringMixin, ABC):
79     """"""
80     Base class for models that map data frames to predictions and can be fitted on data frames
81     """"""
82 
","Before: 76
After: 78",fix vector_model.py for python3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,8a69f70fa51f0c382f7118b6a9c1e941a2013235,fa9ddc22638bc8fb3ead3a270d566265c5343146,1,413,"{'module': 1, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 8, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, '->': 1, 'type': 1, ':': 2, 'block': 2, 'pass_statement': 1, 'pass': 1, 'class_definition': 1, 'class': 1, 'argument_list': 1, ',': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 33, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 34, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6681109016391983,0.663255152760135,"(tensor([0.9666]), tensor([0.9747]), tensor([0.9706]), tensor([0.9739]))"
"185         plt.ylabel(""probability density"")
186         return fig
187 
188     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
189         """"""
190         :param figure: whether to plot in a separate figure and return that figure
191         :param titleAdd: a string to be added to the title in a second line
192         :param kwargs: parameters to be passed on to plt.scatter()
193 
194         :return:  the resulting figure object or None
195         """"""
196         fig = None
197         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
198         if titleAdd is not None:
199             title += ""\n"" + titleAdd
200         if figure:
201             fig = plt.figure(title.replace(""\n"", "" ""))
202         y_range = [min(self.y_true), max(self.y_true)]
203         plt.scatter(self.y_true, self.y_predicted, c=(0, 0, 1, self.SCATTER_PLOT_POINT_TRANSPARENCY), zorder=2, **kwargs)
204         plt.plot(y_range, y_range, '-', lw=1, label=""_not in legend"", color=""green"", zorder=1)
205         plt.xlabel(""ground truth"")
206         plt.ylabel(""prediction"")
207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","185         plt.ylabel(""probability density"")
186         return fig
187 
188     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
189         """"""
190         :param figure: whether to plot in a separate figure and return that figure
191         :param titleAdd: a string to be added to the title in a second line
192         :param kwargs: parameters to be passed on to plt.scatter()
193 
194         :return:  the resulting figure object or None
195         """"""
196         fig = None
197         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
198         if titleAdd is not None:
199             title += ""\n"" + titleAdd
200         if figure:
201             fig = plt.figure(title.replace(""\n"", "" ""))
202         y_range = [min(self.y_true), max(self.y_true)]
203         plt.scatter(self.y_true, self.y_predicted, c=[(0, 0, 1, self.SCATTER_PLOT_POINT_TRANSPARENCY)], zorder=2, **kwargs)
204         plt.plot(y_range, y_range, '-', lw=1, label=""_not in legend"", color=""green"", zorder=1)
205         plt.xlabel(""ground truth"")
206         plt.ylabel(""prediction"")
207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","Before: 203
After: 203",fix typo in eval_stats_regression.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,a451d3c02c6e8860b7e4396c7333c702a58834f0,b695ffc38f2d202f6322a3fe1235b29629c30b25,0,1886,"{'module': 1, 'expression_statement': 12, 'call': 10, 'attribute': 14, 'identifier': 56, '.': 14, 'argument_list': 10, '(': 12, 'string': 11, 'string_start': 11, 'string_content': 11, 'string_end': 11, ')': 12, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 18, 'default_parameter': 2, '=': 12, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 2, ']': 2, ':': 3, 'block': 3, 'assignment': 4, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 1, 'keyword_argument': 6, 'tuple': 1, 'integer': 6, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.981023212727076,0.9797704639654082,"(tensor([0.9967]), tensor([0.9967]), tensor([0.9967]), tensor([0.9967]))"
"65 
66 
67 class VectorModelCrossValidator(MetricsDictProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None):
69         """"""
70         :param data: the data set
71         :param folds: the number of folds
72         :param randomSeed: the random seed to use
73         :param returnTrainedModels: whether to create a copy of the model for each fold and return each of the models
74             (requires that models can be deep-copied); if False, the model that is passed to evalModel is fitted several times
75         :param evaluatorParams: keyword parameters with which to instantiate model evaluators
76         """"""
77         self.returnTrainedModels = returnTrainedModels
78         self.evaluatorParams = evaluatorParams if evaluatorParams is not None else {}
79         numDataPoints = len(data)
80         permutedIndices = np.random.RandomState(randomSeed).permutation(numDataPoints)
81         numTestPoints = numDataPoints // folds
82         self.modelEvaluators = []
83         for i in range(folds):
84             testStartIdx = i * numTestPoints
85             testEndIdx = testStartIdx + numTestPoints
86             testIndices = permutedIndices[testStartIdx:testEndIdx]
87             trainIndices = np.concatenate((permutedIndices[:testStartIdx], permutedIndices[testEndIdx:]))
88             self.modelEvaluators.append(self._createModelEvaluator(data.filterIndices(trainIndices), data.filterIndices(testIndices)))
89 
90     @staticmethod
","65 
66 
67 class VectorModelCrossValidator(MetricsDictProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None,
69             shuffle=True):
70         """"""
71         :param data: the data set
72         :param folds: the number of folds
73         :param randomSeed: the random seed to use
74         :param returnTrainedModels: whether to create a copy of the model for each fold and return each of the models
75             (requires that models can be deep-copied); if False, the model that is passed to evalModel is fitted several times
76         :param evaluatorParams: keyword parameters with which to instantiate model evaluators
77         :param shuffle: whether to shuffle the data (using randomSeed) before creating the folds
78         """"""
79         self.returnTrainedModels = returnTrainedModels
80         self.evaluatorParams = evaluatorParams if evaluatorParams is not None else {}
81         numDataPoints = len(data)
82         if shuffle:
83             indices = np.random.RandomState(randomSeed).permutation(numDataPoints)
84         else:
85             indices = list(range(numDataPoints))
86         numTestPoints = numDataPoints // folds
87         self.modelEvaluators = []
88         for i in range(folds):
89             testStartIdx = i * numTestPoints
90             testEndIdx = testStartIdx + numTestPoints
91             testIndices = indices[testStartIdx:testEndIdx]
92             trainIndices = np.concatenate((indices[:testStartIdx], indices[testEndIdx:]))
93             self.modelEvaluators.append(self._createModelEvaluator(data.filterIndices(trainIndices), data.filterIndices(testIndices)))
94 
95     @staticmethod
","Before: 68
After: 68, 69",add shuffle option to vectormodelcrossvalidator,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,741,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 68, 'argument_list': 10, '(': 12, ',': 9, 'subscript': 4, '[': 5, ']': 5, ')': 12, ':': 9, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 3, 'typed_default_parameter': 2, '=': 14, 'integer': 2, 'default_parameter': 2, 'false': 1, 'none': 2, 'expression_statement': 12, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 10, 'attribute': 12, '.': 12, 'conditional_expression': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'else': 1, 'dictionary': 1, '{': 1, '}': 1, 'call': 9, 'binary_operator': 3, '//': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '*': 1, '+': 1, 'slice': 3, 'tuple': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6626412953140762,0.6629024554583839,"(tensor([0.9503]), tensor([0.9587]), tensor([0.9545]), tensor([0.9579]))"
"65 
66 
67 class VectorModelCrossValidator(MetricsDictProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None):
69         """"""
70         :param data: the data set
71         :param folds: the number of folds
72         :param randomSeed: the random seed to use
73         :param returnTrainedModels: whether to create a copy of the model for each fold and return each of the models
74             (requires that models can be deep-copied); if False, the model that is passed to evalModel is fitted several times
75         :param evaluatorParams: keyword parameters with which to instantiate model evaluators
76         """"""
77         self.returnTrainedModels = returnTrainedModels
78         self.evaluatorParams = evaluatorParams if evaluatorParams is not None else {}
79         numDataPoints = len(data)
80         permutedIndices = np.random.RandomState(randomSeed).permutation(numDataPoints)
81         numTestPoints = numDataPoints // folds
82         self.modelEvaluators = []
83         for i in range(folds):
84             testStartIdx = i * numTestPoints
85             testEndIdx = testStartIdx + numTestPoints
86             testIndices = permutedIndices[testStartIdx:testEndIdx]
87             trainIndices = np.concatenate((permutedIndices[:testStartIdx], permutedIndices[testEndIdx:]))
88             self.modelEvaluators.append(self._createModelEvaluator(data.filterIndices(trainIndices), data.filterIndices(testIndices)))
89 
90     @staticmethod
","65 
66 
67 class VectorModelCrossValidator(MetricsDictProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None,
69             shuffle=True):
70         """"""
71         :param data: the data set
72         :param folds: the number of folds
73         :param randomSeed: the random seed to use
74         :param returnTrainedModels: whether to create a copy of the model for each fold and return each of the models
75             (requires that models can be deep-copied); if False, the model that is passed to evalModel is fitted several times
76         :param evaluatorParams: keyword parameters with which to instantiate model evaluators
77         :param shuffle: whether to shuffle the data (using randomSeed) before creating the folds
78         """"""
79         self.returnTrainedModels = returnTrainedModels
80         self.evaluatorParams = evaluatorParams if evaluatorParams is not None else {}
81         numDataPoints = len(data)
82         if shuffle:
83             indices = np.random.RandomState(randomSeed).permutation(numDataPoints)
84         else:
85             indices = list(range(numDataPoints))
86         numTestPoints = numDataPoints // folds
87         self.modelEvaluators = []
88         for i in range(folds):
89             testStartIdx = i * numTestPoints
90             testEndIdx = testStartIdx + numTestPoints
91             testIndices = indices[testStartIdx:testEndIdx]
92             trainIndices = np.concatenate((indices[:testStartIdx], indices[testEndIdx:]))
93             self.modelEvaluators.append(self._createModelEvaluator(data.filterIndices(trainIndices), data.filterIndices(testIndices)))
94 
95     @staticmethod
","Before: 80
After: 82, 83, 84, 85",add shuffle option to vectormodelcrossvalidator,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,807,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 68, 'argument_list': 10, '(': 12, ',': 9, 'subscript': 4, '[': 5, ']': 5, ')': 12, ':': 9, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 3, 'typed_default_parameter': 2, '=': 14, 'integer': 2, 'default_parameter': 2, 'false': 1, 'none': 2, 'expression_statement': 12, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 10, 'attribute': 12, '.': 12, 'conditional_expression': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'else': 1, 'dictionary': 1, '{': 1, '}': 1, 'call': 9, 'binary_operator': 3, '//': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '*': 1, '+': 1, 'slice': 3, 'tuple': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6626412953140762,0.6629024554583839,"(tensor([0.9503]), tensor([0.9587]), tensor([0.9545]), tensor([0.9579]))"
"65 
66 
67 class VectorModelCrossValidator(MetricsDictProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None):
69         """"""
70         :param data: the data set
71         :param folds: the number of folds
72         :param randomSeed: the random seed to use
73         :param returnTrainedModels: whether to create a copy of the model for each fold and return each of the models
74             (requires that models can be deep-copied); if False, the model that is passed to evalModel is fitted several times
75         :param evaluatorParams: keyword parameters with which to instantiate model evaluators
76         """"""
77         self.returnTrainedModels = returnTrainedModels
78         self.evaluatorParams = evaluatorParams if evaluatorParams is not None else {}
79         numDataPoints = len(data)
80         permutedIndices = np.random.RandomState(randomSeed).permutation(numDataPoints)
81         numTestPoints = numDataPoints // folds
82         self.modelEvaluators = []
83         for i in range(folds):
84             testStartIdx = i * numTestPoints
85             testEndIdx = testStartIdx + numTestPoints
86             testIndices = permutedIndices[testStartIdx:testEndIdx]
87             trainIndices = np.concatenate((permutedIndices[:testStartIdx], permutedIndices[testEndIdx:]))
88             self.modelEvaluators.append(self._createModelEvaluator(data.filterIndices(trainIndices), data.filterIndices(testIndices)))
89 
90     @staticmethod
","65 
66 
67 class VectorModelCrossValidator(MetricsDictProvider, Generic[TCrossValData], ABC):
68     def __init__(self, data: InputOutputData, folds: int = 5, randomSeed=42, returnTrainedModels=False, evaluatorParams: dict = None,
69             shuffle=True):
70         """"""
71         :param data: the data set
72         :param folds: the number of folds
73         :param randomSeed: the random seed to use
74         :param returnTrainedModels: whether to create a copy of the model for each fold and return each of the models
75             (requires that models can be deep-copied); if False, the model that is passed to evalModel is fitted several times
76         :param evaluatorParams: keyword parameters with which to instantiate model evaluators
77         :param shuffle: whether to shuffle the data (using randomSeed) before creating the folds
78         """"""
79         self.returnTrainedModels = returnTrainedModels
80         self.evaluatorParams = evaluatorParams if evaluatorParams is not None else {}
81         numDataPoints = len(data)
82         if shuffle:
83             indices = np.random.RandomState(randomSeed).permutation(numDataPoints)
84         else:
85             indices = list(range(numDataPoints))
86         numTestPoints = numDataPoints // folds
87         self.modelEvaluators = []
88         for i in range(folds):
89             testStartIdx = i * numTestPoints
90             testEndIdx = testStartIdx + numTestPoints
91             testIndices = indices[testStartIdx:testEndIdx]
92             trainIndices = np.concatenate((indices[:testStartIdx], indices[testEndIdx:]))
93             self.modelEvaluators.append(self._createModelEvaluator(data.filterIndices(trainIndices), data.filterIndices(testIndices)))
94 
95     @staticmethod
","Before: 86, 87
After: 91, 92",add shuffle option to vectormodelcrossvalidator,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/crossval.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,865,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 68, 'argument_list': 10, '(': 12, ',': 9, 'subscript': 4, '[': 5, ']': 5, ')': 12, ':': 9, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 3, 'typed_default_parameter': 2, '=': 14, 'integer': 2, 'default_parameter': 2, 'false': 1, 'none': 2, 'expression_statement': 12, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 10, 'attribute': 12, '.': 12, 'conditional_expression': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'else': 1, 'dictionary': 1, '{': 1, '}': 1, 'call': 9, 'binary_operator': 3, '//': 1, 'list': 1, 'for_statement': 1, 'for': 1, 'in': 1, '*': 1, '+': 1, 'slice': 3, 'tuple': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , predictorModels : List [ TModel ] , evalDataList : List [ TEvalData ] , predictedVarNames : List [ str ] , testIndicesList = None )', 'start_line': 28, 'end_line': 32, 'full_parameters': ['self', ' predictorModels : List [ TModel ]', ' evalDataList : List [ TEvalData ]', ' predictedVarNames : List [ str ]', ' testIndicesList = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/crossval.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6626412953140762,0.6629024554583839,"(tensor([0.9503]), tensor([0.9587]), tensor([0.9545]), tensor([0.9579]))"
"98         return np.median(cls.computeAbsErrors(y_true, y_predicted))
99 
100 
101 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
102     # class members controlling plot appearince, which can be centrally overridden by a user if necessary
103     HEATMAP_COLORMAP_FACTORY = lambda self: LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1.0, 0.96, 0.96)), (1, (0.7, 0, 0))))
104     SCATTER_PLOT_POINT_TRANSPARENCY = 0.05
105 
106     """"""
107     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
","98         return np.median(cls.computeAbsErrors(y_true, y_predicted))
99 
100 
101 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
102     # class members controlling plot appearince, which can be centrally overridden by a user if necessary
103     HEATMAP_COLORMAP_FACTORY = lambda self: LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1, 0.96, 0.96)), (1, (0.7, 0, 0))), len(self.y_predicted))
104     SCATTER_PLOT_POINT_TRANSPARENCY = 0.05
105 
106     """"""
107     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
","Before: 103
After: 103",fix typo in eval_stats_regression.py,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,1038,"{'module': 1, 'return_statement': 1, 'return': 1, 'call': 4, 'attribute': 4, 'identifier': 16, '.': 4, 'argument_list': 5, '(': 12, ',': 13, ')': 12, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, 'string': 2, 'string_start': 3, 'string_content': 2, 'string_end': 2, ']': 1, ':': 2, 'comment': 1, 'block': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'lambda': 2, 'lambda_parameters': 1, 'tuple': 7, 'integer': 8, 'binary_operator': 1, '/': 1, 'float': 5, 'ERROR': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9323624660730864,0.9393477082816275,"(tensor([0.9933]), tensor([0.9933]), tensor([0.9933]), tensor([0.9933]))"
"207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","Before: 229
After: 229",fix typo in eval_stats_regression.py,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,2226,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 18, 'identifier': 90, '.': 18, 'argument_list': 16, '(': 17, ')': 17, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 29, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 7, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 11, 'string_start': 11, 'string_content': 11, 'string_end': 11, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 10, 'float': 1, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9739477644340556,0.9753166332571754,"(tensor([0.9924]), tensor([0.9973]), tensor([0.9949]), tensor([0.9968]))"
"207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","Before: 233
After: 233",fix typo in eval_stats_regression.py,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,2316,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 18, 'identifier': 90, '.': 18, 'argument_list': 16, '(': 17, ')': 17, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 29, 'default_parameter': 4, '=': 21, 'true': 1, 'none': 4, 'integer': 7, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 11, 'string_start': 11, 'string_content': 11, 'string_end': 11, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 10, 'float': 1, 'pattern_list': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9739477644340556,0.9753166332571754,"(tensor([0.9924]), tensor([0.9973]), tensor([0.9949]), tensor([0.9968]))"
"1 from typing import Sequence, Union, Optional
2 import logging
3 import lightgbm
4 import pandas as pd
5 import re
","1 from typing import Sequence, Union, Optional, Dict
2 import logging
3 import lightgbm
4 import pandas as pd
5 import re
","Before: 1
After: 1",add feature importances to lightgbm,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,1,15,"{'module': 1, 'import_from_statement': 1, 'from': 1, 'dotted_name': 7, 'identifier': 8, 'import': 4, ',': 2, 'import_statement': 3, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8466657105524215,0.839415306175675,"(tensor([0.9747]), tensor([0.9948]), tensor([0.9846]), tensor([0.9927]))"
"69                 categoricalFeatureNameRegex = None
70         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
71 
72     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
73         if self._categoricalFeatureNameRegex is not None:
74             cols = list(inputs.columns)
75             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
76             colIndices = [cols.index(f) for f in categoricalFeatureNames]
77             args = {""cat_column"": colIndices}
78             self.log.info(f""Updating model parameters with {args}"")
79             self.modelArgs.update(args)","72                 categoricalFeatureNameRegex = None
73         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
74 
75     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
76         if self._categoricalFeatureNameRegex is not None:
77             cols = list(inputs.columns)
78             categoricalFeatureNames = [col for col in cols if re.match(self._categoricalFeatureNameRegex, col)]
79             colIndices = [cols.index(f) for f in categoricalFeatureNames]
80             args = {""cat_column"": colIndices}
81             self.log.info(f""Updating model parameters with {args}"")
82             self.modelArgs.update(args)
83 
84     def getFeatureImportances(self) -> Dict[str, Dict[str, int]]:
","Before: 79
After: 82, 83, 84, 85",add feature importances to lightgbm,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,1,727,"{'module': 1, 'expression_statement': 6, 'assignment': 6, 'identifier': 36, '=': 6, 'none': 2, 'attribute': 8, '.': 8, ':': 6, 'type': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 4, ',': 3, 'typed_parameter': 2, ')': 4, 'block': 2, 'if_statement': 1, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'call': 3, 'argument_list': 3, 'list_comprehension': 2, '[': 2, 'for_in_clause': 2, 'for': 2, 'in': 2, 'if_clause': 1, ']': 2, 'dictionary': 1, '{': 1, 'pair': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, '}': 1}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6221235855905648,0.6199213436527589,"(tensor([0.9508]), tensor([0.9868]), tensor([0.9685]), tensor([0.9831]))"
"1 import logging
2 from typing import Union, Optional
3 
4 import sklearn.ensemble
5 import sklearn.linear_model
6 import sklearn.neighbors
","1 import logging
2 from typing import Union, Optional, Dict
3 
4 import sklearn.ensemble
5 import sklearn.linear_model
6 import sklearn.neighbors
","Before: 2
After: 2",add feature importances to sklearn_regression,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_regression.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,16,"{'module': 1, 'import_statement': 3, 'import': 4, 'dotted_name': 6, 'identifier': 8, 'import_from_statement': 1, 'from': 1, ',': 1, '.': 2}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 17, 'end_line': 19, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 3, 'token_count': 47, 'name': '__init__', 'long_name': '__init__( self , n_estimators = 100 , min_samples_leaf = 10 , random_state = 42 , ** modelArgs )', 'start_line': 17, 'end_line': 19, 'full_parameters': ['self', ' n_estimators = 100', ' min_samples_leaf = 10', ' random_state = 42', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8232490471721702,0.8112275861486086,"(tensor([0.9785]), tensor([0.9946]), tensor([0.9865]), tensor([0.9930]))"
"13     TorchDataSetProviderFromDataUtil, TorchDataSetProvider, Tensoriser, TorchDataSetFromDataFrames
14 from .torch_enums import ClassificationOutputMode
15 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams, TrainingInfo
16 from ..normalisation import NormalisationMode
17 from ..util.dtype import toFloatArray
18 from ..util.string import objectRepr, ToStringMixin
19 from ..vector_model import VectorRegressionModel, VectorClassificationModel
20 
21 log: logging.Logger = logging.getLogger(__name__)
22 
","13     TorchDataSetProviderFromDataUtil, TorchDataSetProvider, Tensoriser, TorchDataSetFromDataFrames
14 from .torch_enums import ClassificationOutputMode
15 from .torch_opt import NNOptimiser, NNLossEvaluatorRegression, NNLossEvaluatorClassification, NNOptimiserParams, TrainingInfo
16 from ..normalisation import NormalisationMode
17 from ..util.dtype import toFloatArray
18 from ..util.string import ToStringMixin
19 from ..vector_model import VectorRegressionModel, VectorClassificationModel, TrainingContext
20 
21 log: logging.Logger = logging.getLogger(__name__)
22 
","Before: 18, 19
After: 18, 19",add withtorchdatasetproviderfactory and withtorchdatasetproviderfactory,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,183,"{'module': 1, 'expression_statement': 2, 'identifier': 30, ',': 9, 'import_from_statement': 6, 'from': 6, 'relative_import': 6, 'import_prefix': 6, '.': 14, 'dotted_name': 18, 'import': 6, 'assignment': 1, ':': 1, 'type': 1, 'attribute': 2, '=': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9235141787567294,0.9193820206289121,"(tensor([0.9941]), tensor([0.9893]), tensor([0.9917]), tensor([0.9898]))"
"399     def _createTorchModel(self) -> TorchModel:
400         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
401 
402     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
403         dataUtil = VectorDataUtil(inputs, outputs, self.model.cuda, normalisationMode=self.normalisationMode, inputTensoriser=self.inputTensoriser)
404         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
405 
406     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","404     def _createTorchModel(self) -> TorchModel:
405         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
406 
407     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
408         factory = self.torchDataSetProviderFactory
409         if factory is None:
410             factory = DefaultRegressionTorchDataSetProviderFactory()
411         return factory.createDataSetProvider(inputs, outputs, self, self._trainingContext)
412 
413     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","Before: 403, 404
After: 408, 409, 410, 411",add withtorchdatasetproviderfactory and withtorchdatasetproviderfactory,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,3686,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 36, 'parameters': 2, '(': 5, ')': 5, '->': 2, 'type': 4, ':': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 3, 'attribute': 11, '.': 11, 'argument_list': 3, 'list_splat': 1, '*': 1, ',': 8, 'dictionary_splat': 1, '**': 1, 'typed_parameter': 2, 'expression_statement': 1, 'assignment': 1, '=': 3, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6086058357990287,0.5605171627809532,"(tensor([0.9241]), tensor([0.8964]), tensor([0.9101]), tensor([0.8991]))"
"484     def _createTorchModel(self) -> VectorTorchModel:
485         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
486 
487     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
488         dataUtil = ClassificationVectorDataUtil(inputs, outputs, self.model.cuda, len(self._labels),
489             normalisationMode=self.normalisationMode, inputTensoriser=self.inputTensoriser)
490         return TorchDataSetProviderFromDataUtil(dataUtil, self.model.cuda)
491 
492     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","496     def _createTorchModel(self) -> VectorTorchModel:
497         return self.modelClass(*self.modelArgs, **self.modelKwArgs)
498 
499     def _createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> TorchDataSetProvider:
500         factory = self.torchDataSetProviderFactory
501         if factory is None:
502             factory = DefaultClassificationTorchDataSetProviderFactory()
503         return factory.createDataSetProvider(inputs, outputs, self, self._trainingContext)
504 
505     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
","Before: 488, 489, 490
After: 500, 501, 502, 503",add withtorchdatasetproviderfactory and withtorchdatasetproviderfactory,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,4610,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 39, 'parameters': 2, '(': 6, ')': 6, '->': 2, 'type': 4, ':': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'call': 4, 'attribute': 12, '.': 12, 'argument_list': 4, 'list_splat': 1, '*': 1, ',': 9, 'dictionary_splat': 1, '**': 1, 'typed_parameter': 2, 'expression_statement': 1, 'assignment': 1, '=': 3, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5947993902354152,0.535918855971256,"(tensor([0.9246]), tensor([0.8911]), tensor([0.9075]), tensor([0.8943]))"
"186 
187 
188 class VectorDataUtil(DataUtil):
189     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
190             differingOutputNormalisationMode=None, inputTensoriser: Optional[Tensoriser] = None, outputTensoriser: Optional[Tensoriser] = None):
191         """"""
192         :param inputs: the data frame of inputs
193         :param outputs: the data frame of outputs
194         :param cuda: whether to apply CUDA
195         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
196         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs, overriding normalisationMode;
197             if None, use normalisationMode
198         """"""
199         if inputs.shape[0] != outputs.shape[0]:
200             raise ValueError(""Output length must be equal to input length"")
201         self.inputs = inputs
202         self.outputs = outputs
203         self.inputTensoriser = inputTensoriser if inputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
204         self.outputTensoriser = outputTensoriser if outputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
205         self.inputVectorDataScaler = normalisation.VectorDataScaler(self.inputs, normalisationMode)
206         self.inputTensorScaler = TensorScalerFromVectorDataScaler(self.inputVectorDataScaler, cuda)
207         self.outputVectorDataScaler = normalisation.VectorDataScaler(self.outputs, normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
208         self.outputTensorScaler = TensorScalerFromVectorDataScaler(self.outputVectorDataScaler, cuda)
209 
210     def getOutputTensorScaler(self):
","187 
188 
189 class VectorDataUtil(DataUtil):
190     def __init__(self, inputs: pd.DataFrame, outputs: pd.DataFrame, cuda: bool, normalisationMode=normalisation.NormalisationMode.NONE,
191             differingOutputNormalisationMode=None, inputTensoriser: Optional[Tensoriser] = None, outputTensoriser: Optional[Tensoriser] = None,
192             dataFrameSplitter: Optional[DataFrameSplitter] = None):
193         """"""
194         :param inputs: the data frame of inputs
195         :param outputs: the data frame of outputs
196         :param cuda: whether to apply CUDA
197         :param normalisationMode: the normalisation mode to use for inputs and (unless differingOutputNormalisationMode is specified) outputs
198         :param differingOutputNormalisationMode: the normalisation mode to apply to outputs, overriding normalisationMode;
199             if None, use normalisationMode
200         """"""
201         if inputs.shape[0] != outputs.shape[0]:
202             raise ValueError(""Output length must be equal to input length"")
203         self.inputs = inputs
204         self.outputs = outputs
205         self.inputTensoriser = inputTensoriser if inputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
206         self.outputTensoriser = outputTensoriser if outputTensoriser is not None else TensoriserDataFrameFloatValuesMatrix()
207         self.inputVectorDataScaler = normalisation.VectorDataScaler(self.inputs, normalisationMode)
208         self.inputTensorScaler = TensorScalerFromVectorDataScaler(self.inputVectorDataScaler, cuda)
209         self.outputVectorDataScaler = normalisation.VectorDataScaler(self.outputs, normalisationMode if differingOutputNormalisationMode is None else differingOutputNormalisationMode)
210         self.outputTensorScaler = TensorScalerFromVectorDataScaler(self.outputVectorDataScaler, cuda)
211         self.dataFrameSplitter = dataFrameSplitter
212 
213     def getOutputTensorScaler(self):
","Before: 190
After: 191, 192",add dataframesplitter to vectordatautil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,1450,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 72, 'argument_list': 8, '(': 9, ')': 9, ':': 8, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 11, 'typed_parameter': 3, 'type': 7, 'attribute': 20, '.': 20, 'default_parameter': 2, '=': 12, 'none': 6, 'typed_default_parameter': 2, 'generic_type': 2, 'type_parameter': 2, '[': 4, ']': 4, 'expression_statement': 9, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 4, 'comparison_operator': 4, 'subscript': 2, 'integer': 2, '!=': 1, 'raise_statement': 1, 'raise': 1, 'call': 7, 'assignment': 8, 'conditional_expression': 3, 'is not': 4, 'else': 3, 'is': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 14, 'end_line': 24, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7525528214026476,0.7516921329065482,"(tensor([0.9653]), tensor([0.9751]), tensor([0.9702]), tensor([0.9741]))"
"213     def getInputTensorScaler(self):
214         return self.inputTensorScaler
215 
216     def splitInputOutputPairs(self, fractionalSizeOfFirstSet):
217         n = self.inputs.shape[0]
218         sizeA = int(n * fractionalSizeOfFirstSet)
219         indices = list(range(n))
220         indices_A = indices[:sizeA]
221         indices_B = indices[sizeA:]
222         A = self._inputOutputPairs(indices_A)
223         B = self._inputOutputPairs(indices_B)
224         return A, B
225 
226     def _inputOutputPairs(self, indices):
","216     def getInputTensorScaler(self):
217         return self.inputTensorScaler
218 
219     def splitInputOutputPairs(self, fractionalSizeOfFirstSet):
220         splitter = self.dataFrameSplitter
221         if splitter is None:
222             splitter = DataFrameSplitterFractional()
223         indices_A, indices_B = splitter.computeSplitIndices(self.inputs, fractionalSizeOfFirstSet)
224         A = self._inputOutputPairs(indices_A)
225         B = self._inputOutputPairs(indices_B)
226         return A, B
227 
228     def _inputOutputPairs(self, indices):
","Before: 217, 218, 219, 220, 221
After: 220, 221, 222, 223",add dataframesplitter to vectordatautil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,0,1688,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 35, 'parameters': 2, '(': 7, ')': 7, ':': 4, 'block': 2, 'return_statement': 2, 'return': 2, 'attribute': 5, '.': 5, ',': 2, 'expression_statement': 7, 'assignment': 7, '=': 7, 'subscript': 3, '[': 3, 'integer': 1, ']': 3, 'call': 5, 'argument_list': 5, 'binary_operator': 1, '*': 1, 'slice': 2, 'expression_list': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 13, 'end_line': 23, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 14, 'end_line': 24, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.37251103202999486,0.35201691001824875,"(tensor([0.9057]), tensor([0.9246]), tensor([0.9151]), tensor([0.9227]))"
"236                 X = self._featureGenerator.fitGenerate(X, Y, self)
237         self._inputTransformerChain.fit(X)
238 
239     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
240         """"""
241         Fits the model using the given data
242 
243         :param X: a data frame containing input data
244         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
245             fitting, e.g. with rule-based models
246         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
247             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
248             an exception will be raised.
249         """"""
250         log.info(f""Training {self.__class__.__name__}"")
251         self._predictedVariableNames = list(Y.columns)
252         if not self._underlyingModelRequiresFitting():
253             self._fitPreprocessors(X, Y=Y)
254         else:
255             if Y is None:
256                 raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
257             X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
258             self._modelInputVariableNames = list(X.columns)
259             log.info(
260                 f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, inputs[{len(self._modelInputVariableNames)}]=[{', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])}]"")
261             self._fit(X, Y)
262             self._isFitted = True
263 
264     @abstractmethod
","269                 X = self._featureGenerator.fitGenerate(X, Y, self)
270         self._inputTransformerChain.fit(X)
271 
272     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
273         """"""
274         Fits the model using the given data
275 
276         :param X: a data frame containing input data
277         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
278             fitting, e.g. with rule-based models
279         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
280             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
281             an exception will be raised.
282         """"""
283         self._trainingContext = TrainingContext(X, Y)
284         try:
285             log.info(f""Training {self.__class__.__name__}"")
286             self._predictedVariableNames = list(Y.columns)
287             if not self._underlyingModelRequiresFitting():
288                 self._fitPreprocessors(X, Y=Y)
289             else:
290                 if Y is None:
291                     raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
292                 X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
293                 self._modelInputVariableNames = list(X.columns)
294                 inputsWithTypes = ', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])
295                 log.info(f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, ""
296                     f""inputs[{len(self._modelInputVariableNames)}]=[{inputsWithTypes}]; N={len(X)} data points"")
297                 self._fit(X, Y)
298                 self._isFitted = True
299         finally:
300             self._trainingContext = None
301 
302     def isBeingFitted(self) -> bool:
","Before: 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262
After: 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303",fix vector_model.py -- a/b/c,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,de2d3e5fb8e113f22a6aadba76771b9229d04b64,2382312f61e3ba0ac722a5e7e16b403784b9328a,1,1647,"{'module': 1, 'expression_statement': 11, 'assignment': 5, 'identifier': 79, '=': 9, 'call': 15, 'attribute': 26, '.': 26, 'argument_list': 15, '(': 16, ',': 9, ')': 16, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 6, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 1, 'true': 2, 'block': 4, 'string': 6, 'string_start': 6, 'string_content': 10, 'string_end': 6, 'interpolation': 5, '{': 5, '}': 5, 'if_statement': 2, 'if': 2, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 1, 'else': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'raise_statement': 1, 'raise': 1, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 34, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 34, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6586897229008493,0.6500254540819677,"(tensor([0.9298]), tensor([0.9525]), tensor([0.9410]), tensor([0.9502]))"
"99 
100 
101 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
102     # class members controlling plot appearince, which can be centrally overridden by a user if necessary
103     HEATMAP_COLORMAP_FACTORY = lambda self: LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1, 0.96, 0.96)), (1, (0.7, 0, 0))), len(self.y_predicted))
104     SCATTER_PLOT_POINT_TRANSPARENCY = 0.05
105 
106     """"""
107     Collects data for the evaluation of predicted continuous values and computes corresponding metrics
108     """"""
","99 
100 
101 class RegressionEvalStats(PredictionEvalStats[""RegressionMetric""]):
102     # class members controlling plot appearince, which can be centrally overridden by a user if necessary
103     HEATMAP_COLORMAP_FACTORY = lambda self: LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/len(self.y_predicted), (1, 0.96, 0.96)), (1, (0.7, 0, 0))), len(self.y_predicted))
104     HEATMAP_DIAGONAL_COLOR = ""green""
105     HEATMAP_ERROR_BOUNDARY_VALUE = None
106     HEATMAP_ERROR_BOUNDARY_COLOR = (0.8, 0.8, 0.8)
107     SCATTER_PLOT_POINT_COLOR = (0, 0, 1, 0.05)
108 
","Before: 104
After: 104, 105, 106, 107",update eval_stats_regression.py for new features,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1053,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 25, 'argument_list': 4, '(': 11, 'subscript': 1, '[': 1, 'string': 2, 'string_start': 3, 'string_content': 2, 'string_end': 2, ']': 1, ')': 11, ':': 2, 'comment': 1, 'block': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'lambda': 2, 'lambda_parameters': 1, 'call': 3, 'attribute': 3, '.': 3, ',': 13, 'tuple': 7, 'integer': 9, 'binary_operator': 1, '/': 1, 'float': 4, 'ERROR': 2, 'for': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7522803294655702,0.7431849430580908,"(tensor([0.9185]), tensor([0.9255]), tensor([0.9220]), tensor([0.9248]))"
"185         plt.ylabel(""probability density"")
186         return fig
187 
188     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
189         """"""
190         :param figure: whether to plot in a separate figure and return that figure
191         :param titleAdd: a string to be added to the title in a second line
192         :param kwargs: parameters to be passed on to plt.scatter()
193 
194         :return:  the resulting figure object or None
195         """"""
196         fig = None
197         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
198         if titleAdd is not None:
199             title += ""\n"" + titleAdd
200         if figure:
201             fig = plt.figure(title.replace(""\n"", "" ""))
202         y_range = [min(self.y_true), max(self.y_true)]
203         plt.scatter(self.y_true, self.y_predicted, c=[(0, 0, 1, self.SCATTER_PLOT_POINT_TRANSPARENCY)], zorder=2, **kwargs)
204         plt.plot(y_range, y_range, '-', lw=1, label=""_not in legend"", color=""green"", zorder=1)
205         plt.xlabel(""ground truth"")
206         plt.ylabel(""prediction"")
207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
","188         plt.ylabel(""probability density"")
189         return fig
190 
191     def plotScatterGroundTruthPredictions(self, figure=True, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
192         """"""
193         :param figure: whether to plot in a separate figure and return that figure
194         :param titleAdd: a string to be added to the title in a second line
195         :param kwargs: parameters to be passed on to plt.scatter()
196 
197         :return:  the resulting figure object or None
198         """"""
199         fig = None
200         title = ""Scatter Plot of Ground Truth vs. Predicted Values""
201         if titleAdd is not None:
202             title += ""\n"" + titleAdd
203         if figure:
204             fig = plt.figure(title.replace(""\n"", "" ""))
205         y_range = [min(self.y_true), max(self.y_true)]
206         plt.scatter(self.y_true, self.y_predicted, c=[self.SCATTER_PLOT_POINT_COLOR], zorder=2, **kwargs)
207         plt.plot(y_range, y_range, '-', lw=1, label=""_not in legend"", color=""green"", zorder=1)
208         plt.xlabel(""ground truth"")
209         plt.ylabel(""prediction"")
210         plt.title(title)
211         return fig
212 
213     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, errorBoundary: Optional[float] = None,
","Before: 203
After: 206",update eval_stats_regression.py for new features,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1899,"{'module': 1, 'expression_statement': 12, 'call': 10, 'attribute': 14, 'identifier': 56, '.': 14, 'argument_list': 10, '(': 12, 'string': 11, 'string_start': 11, 'string_content': 11, 'string_end': 11, ')': 12, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 18, 'default_parameter': 2, '=': 12, 'true': 1, 'none': 3, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, ':': 3, 'block': 3, 'assignment': 4, 'if_statement': 2, 'if': 2, 'comparison_operator': 1, 'is not': 2, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 2, 'keyword_argument': 6, 'tuple': 1, 'integer': 6, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7155972072225981,0.7130142546489214,"(tensor([0.9759]), tensor([0.9711]), tensor([0.9735]), tensor([0.9716]))"
"207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","210         plt.title(title)
211         return fig
212 
213     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, errorBoundary: Optional[float] = None,
214             **kwargs) -> Optional[plt.Figure]:
215         """"""
216         :param figure: whether to plot in a separate figure and return that figure
217         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
218             defined in HEATMAP_COLORMAP_FACTORY (which can be centrally set to achieve custom behaviour throughout an application)
219         :param bins: how many bins to use for constructing the heatmap
220         :param titleAdd: a string to add to the title (on a second line)
221         :param errorBoundary: if not None, add two lines (above and below the diagonal) indicating this absolute regression error boundary;
222             if None (default), use static member HEATMAP_ERROR_BOUNDARY_VALUE (which is also None by default, but can be centrally set
223             to achieve custom behaviour throughout an application)
224 
225         :param kwargs: will be passed to plt.imshow()
226 
227         :return:  the resulting figure object or None
228         """"""
229         fig = None
230         title = ""Heat Map of Ground Truth vs. Predicted Values""
231         if titleAdd:
232             title += ""\n"" + titleAdd
233         if figure:
234             fig = plt.figure(title.replace(""\n"", "" ""))
235         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
236 
237         # diagonal
238         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_DIAGONAL_COLOR, zorder=2)
239 
240         # error boundaries
241         if errorBoundary is None:
242             errorBoundary = self.HEATMAP_ERROR_BOUNDARY_VALUE
243         if errorBoundary is not None:
244             d = np.array(y_range)
245             offs = np.array([errorBoundary, errorBoundary])
246             plt.plot(d, d + offs, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_ERROR_BOUNDARY_COLOR, zorder=2)
247             plt.plot(d, d - offs, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_ERROR_BOUNDARY_COLOR, zorder=2)
248 
249         # heat map
250         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
251         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
252         if cmap is None:
253             cmap = self.HEATMAP_COLORMAP_FACTORY()
254         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
255 
256         plt.xlabel(""ground truth"")
257         plt.ylabel(""prediction"")
258         plt.title(title)
259         return fig
260 
261 
","Before: 210
After: 213, 214",update eval_stats_regression.py for new features,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,2026,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 18, 'identifier': 92, '.': 18, 'argument_list': 16, '(': 17, ')': 17, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 31, 'default_parameter': 4, '=': 23, 'true': 1, 'none': 4, 'integer': 7, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 12, 'float': 1, 'pattern_list': 1, 'false': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5225661738158648,0.512124346382337,"(tensor([0.8714]), tensor([0.9416]), tensor([0.9051]), tensor([0.9341]))"
"207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","210         plt.title(title)
211         return fig
212 
213     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, errorBoundary: Optional[float] = None,
214             **kwargs) -> Optional[plt.Figure]:
215         """"""
216         :param figure: whether to plot in a separate figure and return that figure
217         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
218             defined in HEATMAP_COLORMAP_FACTORY (which can be centrally set to achieve custom behaviour throughout an application)
219         :param bins: how many bins to use for constructing the heatmap
220         :param titleAdd: a string to add to the title (on a second line)
221         :param errorBoundary: if not None, add two lines (above and below the diagonal) indicating this absolute regression error boundary;
222             if None (default), use static member HEATMAP_ERROR_BOUNDARY_VALUE (which is also None by default, but can be centrally set
223             to achieve custom behaviour throughout an application)
224 
225         :param kwargs: will be passed to plt.imshow()
226 
227         :return:  the resulting figure object or None
228         """"""
229         fig = None
230         title = ""Heat Map of Ground Truth vs. Predicted Values""
231         if titleAdd:
232             title += ""\n"" + titleAdd
233         if figure:
234             fig = plt.figure(title.replace(""\n"", "" ""))
235         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
236 
237         # diagonal
238         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_DIAGONAL_COLOR, zorder=2)
239 
240         # error boundaries
241         if errorBoundary is None:
242             errorBoundary = self.HEATMAP_ERROR_BOUNDARY_VALUE
243         if errorBoundary is not None:
244             d = np.array(y_range)
245             offs = np.array([errorBoundary, errorBoundary])
246             plt.plot(d, d + offs, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_ERROR_BOUNDARY_COLOR, zorder=2)
247             plt.plot(d, d - offs, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_ERROR_BOUNDARY_COLOR, zorder=2)
248 
249         # heat map
250         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
251         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
252         if cmap is None:
253             cmap = self.HEATMAP_COLORMAP_FACTORY()
254         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
255 
256         plt.xlabel(""ground truth"")
257         plt.ylabel(""prediction"")
258         plt.title(title)
259         return fig
260 
261 
","Before: 214
After: 218, 221, 222, 223, 224",update eval_stats_regression.py for new features,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,2095,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 18, 'identifier': 92, '.': 18, 'argument_list': 16, '(': 17, ')': 17, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 31, 'default_parameter': 4, '=': 23, 'true': 1, 'none': 4, 'integer': 7, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 12, 'float': 1, 'pattern_list': 1, 'false': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5225661738158648,0.512124346382337,"(tensor([0.8714]), tensor([0.9416]), tensor([0.9051]), tensor([0.9341]))"
"207         plt.title(title)
208         return fig
209 
210     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, **kwargs) -> Optional[plt.Figure]:
211         """"""
212         :param figure: whether to plot in a separate figure and return that figure
213         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
214             defined in HEATMAP_COLORMAP_FACTORY
215         :param bins: how many bins to use for constructing the heatmap
216         :param titleAdd: a string to add to the title (on a second line)
217         :param kwargs: will be passed to plt.imshow()
218 
219         :return:  the resulting figure object or None
220         """"""
221         fig = None
222         title = ""Heat Map of Ground Truth vs. Predicted Values""
223         if titleAdd:
224             title += ""\n"" + titleAdd
225         if figure:
226             fig = plt.figure(title.replace(""\n"", "" ""))
227         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
228         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=""green"", zorder=2)
229         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
230         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
231         if cmap is None:
232             cmap = self.HEATMAP_COLORMAP_FACTORY()
233         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
234 
235         plt.xlabel(""ground truth"")
236         plt.ylabel(""prediction"")
237         plt.title(title)
238         return fig
239 
240 
","210         plt.title(title)
211         return fig
212 
213     def plotHeatmapGroundTruthPredictions(self, figure=True, cmap=None, bins=60, titleAdd=None, errorBoundary: Optional[float] = None,
214             **kwargs) -> Optional[plt.Figure]:
215         """"""
216         :param figure: whether to plot in a separate figure and return that figure
217         :param cmap: the colour map to use (see corresponding parameter of plt.imshow for further information); if None, use factory
218             defined in HEATMAP_COLORMAP_FACTORY (which can be centrally set to achieve custom behaviour throughout an application)
219         :param bins: how many bins to use for constructing the heatmap
220         :param titleAdd: a string to add to the title (on a second line)
221         :param errorBoundary: if not None, add two lines (above and below the diagonal) indicating this absolute regression error boundary;
222             if None (default), use static member HEATMAP_ERROR_BOUNDARY_VALUE (which is also None by default, but can be centrally set
223             to achieve custom behaviour throughout an application)
224 
225         :param kwargs: will be passed to plt.imshow()
226 
227         :return:  the resulting figure object or None
228         """"""
229         fig = None
230         title = ""Heat Map of Ground Truth vs. Predicted Values""
231         if titleAdd:
232             title += ""\n"" + titleAdd
233         if figure:
234             fig = plt.figure(title.replace(""\n"", "" ""))
235         y_range = [min(min(self.y_true), min(self.y_predicted)), max(max(self.y_true), max(self.y_predicted))]
236 
237         # diagonal
238         plt.plot(y_range, y_range, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_DIAGONAL_COLOR, zorder=2)
239 
240         # error boundaries
241         if errorBoundary is None:
242             errorBoundary = self.HEATMAP_ERROR_BOUNDARY_VALUE
243         if errorBoundary is not None:
244             d = np.array(y_range)
245             offs = np.array([errorBoundary, errorBoundary])
246             plt.plot(d, d + offs, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_ERROR_BOUNDARY_COLOR, zorder=2)
247             plt.plot(d, d - offs, '-', lw=0.75, label=""_not in legend"", color=self.HEATMAP_ERROR_BOUNDARY_COLOR, zorder=2)
248 
249         # heat map
250         heatmap, _, _ = np.histogram2d(self.y_true, self.y_predicted, range=[y_range, y_range], bins=bins, density=False)
251         extent = [y_range[0], y_range[1], y_range[0], y_range[1]]
252         if cmap is None:
253             cmap = self.HEATMAP_COLORMAP_FACTORY()
254         plt.imshow(heatmap.T, extent=extent, origin='lower', interpolation=""none"", cmap=cmap, zorder=1, **kwargs)
255 
256         plt.xlabel(""ground truth"")
257         plt.ylabel(""prediction"")
258         plt.title(title)
259         return fig
260 
261 
","Before: 228
After: 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249",update eval_stats_regression.py for new features,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_regression.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,2195,"{'module': 1, 'expression_statement': 15, 'call': 16, 'attribute': 18, 'identifier': 92, '.': 18, 'argument_list': 16, '(': 17, ')': 17, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 31, 'default_parameter': 4, '=': 23, 'true': 1, 'none': 4, 'integer': 7, 'dictionary_splat_pattern': 1, '**': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 8, ']': 8, ':': 4, 'block': 4, 'string': 12, 'string_start': 12, 'string_content': 12, 'string_end': 12, 'assignment': 7, 'if_statement': 3, 'if': 3, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, 'escape_sequence': 2, '+': 1, 'list': 3, 'keyword_argument': 12, 'float': 1, 'pattern_list': 1, 'false': 1, 'subscript': 4, 'comparison_operator': 1, 'is': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 32, 'name': 'computeValueForEvalStats', 'long_name': 'computeValueForEvalStats( self , evalStats : ""RegressionEvalStats"" )', 'start_line': 15, 'end_line': 16, 'full_parameters': ['self', ' evalStats : ""RegressionEvalStats""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_regression.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5225661738158648,0.512124346382337,"(tensor([0.8714]), tensor([0.9416]), tensor([0.9051]), tensor([0.9341]))"
"167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False) -> TEvalData:
172         resultWriter = self._resultWriterForModel(resultWriter, model)
173         evaluator = self.createEvaluator(model)
174         evaluator.fitModel(model)
175 
176         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
177             strEvalResults = f""{model}\n\n""
178             for predictedVarName in model.getPredictedVariableNames():
179                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
180                 if logResults:
181                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
182                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
183             if resultWriter is not None:
184                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
185             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
186 
187         evalResultData = evaluator.evalModel(model)
188         gatherResults(evalResultData, resultWriter)
189         if additionalEvaluationOnTrainingData:
190             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
191             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
192             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
193 
194         return evalResultData
195 
196     @staticmethod
","167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, createPlots=True, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False, fitModel=True) -> TEvalData:
172         if showPlots and not createPlots:
173             raise ValueError(""showPlots=True requires createPlots=True"")
174         resultWriter = self._resultWriterForModel(resultWriter, model)
175         evaluator = self.createEvaluator(model)
176         if fitModel:
177             evaluator.fitModel(model)
178 
179         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
180             strEvalResults = f""{model}\n\n""
181             for predictedVarName in model.getPredictedVariableNames():
182                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
183                 if logResults:
184                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
185                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
186             if resultWriter is not None:
187                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
188             if createPlots:
189                 self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
190 
191         evalResultData = evaluator.evalModel(model)
192         gatherResults(evalResultData, resultWriter)
193         if additionalEvaluationOnTrainingData:
194             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
195             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
196             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
197 
198         return evalResultData
199 
200     @staticmethod
","Before: 170, 171
After: 170, 171, 172, 173",add fitmodel argument to evaluationutil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1577,"{'ERROR': 1, 'string_start': 1, 'string_content': 1, 'escape_sequence': 3, ')': 1, 'return': 1, 'identifier': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6984720322480494,0.6972382374095932,"(tensor([0.9407]), tensor([0.9571]), tensor([0.9488]), tensor([0.9554]))"
"167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False) -> TEvalData:
172         resultWriter = self._resultWriterForModel(resultWriter, model)
173         evaluator = self.createEvaluator(model)
174         evaluator.fitModel(model)
175 
176         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
177             strEvalResults = f""{model}\n\n""
178             for predictedVarName in model.getPredictedVariableNames():
179                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
180                 if logResults:
181                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
182                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
183             if resultWriter is not None:
184                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
185             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
186 
187         evalResultData = evaluator.evalModel(model)
188         gatherResults(evalResultData, resultWriter)
189         if additionalEvaluationOnTrainingData:
190             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
191             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
192             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
193 
194         return evalResultData
195 
196     @staticmethod
","167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, createPlots=True, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False, fitModel=True) -> TEvalData:
172         if showPlots and not createPlots:
173             raise ValueError(""showPlots=True requires createPlots=True"")
174         resultWriter = self._resultWriterForModel(resultWriter, model)
175         evaluator = self.createEvaluator(model)
176         if fitModel:
177             evaluator.fitModel(model)
178 
179         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
180             strEvalResults = f""{model}\n\n""
181             for predictedVarName in model.getPredictedVariableNames():
182                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
183                 if logResults:
184                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
185                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
186             if resultWriter is not None:
187                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
188             if createPlots:
189                 self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
190 
191         evalResultData = evaluator.evalModel(model)
192         gatherResults(evalResultData, resultWriter)
193         if additionalEvaluationOnTrainingData:
194             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
195             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
196             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
197 
198         return evalResultData
199 
200     @staticmethod
","Before: 174
After: 176, 177",add fitmodel argument to evaluationutil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1626,"{'ERROR': 1, 'string_start': 1, 'string_content': 1, 'escape_sequence': 3, ')': 1, 'return': 1, 'identifier': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6984720322480494,0.6972382374095932,"(tensor([0.9407]), tensor([0.9571]), tensor([0.9488]), tensor([0.9554]))"
"167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False) -> TEvalData:
172         resultWriter = self._resultWriterForModel(resultWriter, model)
173         evaluator = self.createEvaluator(model)
174         evaluator.fitModel(model)
175 
176         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
177             strEvalResults = f""{model}\n\n""
178             for predictedVarName in model.getPredictedVariableNames():
179                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
180                 if logResults:
181                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
182                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
183             if resultWriter is not None:
184                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
185             self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
186 
187         evalResultData = evaluator.evalModel(model)
188         gatherResults(evalResultData, resultWriter)
189         if additionalEvaluationOnTrainingData:
190             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
191             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
192             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
193 
194         return evalResultData
195 
196     @staticmethod
","167         """"""
168         return createVectorModelCrossValidator(self.inputOutputData, model=model, isRegression=isRegression, **self.crossValidatorParams)
169 
170     def performSimpleEvaluation(self, model: TModel, createPlots=True, showPlots=False, logResults=True, resultWriter: ResultWriter = None,
171             additionalEvaluationOnTrainingData=False, fitModel=True) -> TEvalData:
172         if showPlots and not createPlots:
173             raise ValueError(""showPlots=True requires createPlots=True"")
174         resultWriter = self._resultWriterForModel(resultWriter, model)
175         evaluator = self.createEvaluator(model)
176         if fitModel:
177             evaluator.fitModel(model)
178 
179         def gatherResults(evalResultData, resultWriter, subtitlePrefix=""""):
180             strEvalResults = f""{model}\n\n""
181             for predictedVarName in model.getPredictedVariableNames():
182                 strEvalResult = str(evalResultData.getEvalStats(predictedVarName))
183                 if logResults:
184                     log.info(f""{subtitlePrefix}Evaluation results for {predictedVarName}: {strEvalResult}"")
185                 strEvalResults += predictedVarName + "": "" + strEvalResult + ""\n""
186             if resultWriter is not None:
187                 resultWriter.writeTextFile(""evaluator-results"", strEvalResults)
188             if createPlots:
189                 self.createPlots(evalResultData, showPlots=showPlots, resultWriter=resultWriter, subtitlePrefix=subtitlePrefix)
190 
191         evalResultData = evaluator.evalModel(model)
192         gatherResults(evalResultData, resultWriter)
193         if additionalEvaluationOnTrainingData:
194             evalResultDataTrain = evaluator.evalModel(model, onTrainingData=True)
195             additionalResultWriter = resultWriter.childWithAddedPrefix(""-onTrain-"") if resultWriter is not None else None
196             gatherResults(evalResultDataTrain, additionalResultWriter, subtitlePrefix=""[onTrain] "")
197 
198         return evalResultData
199 
200     @staticmethod
","Before: 185
After: 188, 189",add fitmodel argument to evaluationutil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1791,"{'ERROR': 1, 'string_start': 1, 'string_content': 1, 'escape_sequence': 3, ')': 1, 'return': 1, 'identifier': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6984720322480494,0.6972382374095932,"(tensor([0.9407]), tensor([0.9571]), tensor([0.9488]), tensor([0.9554]))"
"221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
225         """"""
226         Compares several models via simple evaluation or cross-validation
227 
228         :param models: the models to compare
229         :param resultWriter: a writer with which to store results of the comparison
230         :return: a data frame containing evaluation metrics on all models
231         """"""
232         statsList = []
233         for model in models:
234             if useCrossValidation:
235                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
236                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
237             else:
238                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter).getEvalStats()
239                 statsDict = evalStats.getAll()
240             statsDict[""modelName""] = model.getName()
241             statsList.append(statsDict)
242         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
243         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
244         log.info(strResults)
245         if resultWriter is not None:
246             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
247             strResults += ""\n\n"" + ""\n\n"".join([f""{model.getName()} = {str(model)}"" for model in models])
248             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
249         return resultsDF
250 
251     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","225         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
226         return crossValidationData
227 
228     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False,
229             fitModels=True, writeIndividualResults=True, sortColumn: Optional[str] = None, sortAscending: bool = True) -> pd.DataFrame:
230         """"""
231         Compares several models via simple evaluation or cross-validation
232 
233         :param models: the models to compare
234         :param resultWriter: a writer with which to store results of the comparison
235         :param useCrossValidation: whether to use cross-validation in order to evaluate models; if False, use a simple evaluation
236             on test data (single split)
237         :param fitModels: whether to fit models before evaluating them; this can only be False if useCrossValidation=False
238         :param writeIndividualResults: whether to write results files on each individual model (in addition to the comparison
239             summary)
240         :param sortColumn: column/metric name by which to sort
241         :param sortAscending: whether to sort in ascending order
242         :return: a data frame containing evaluation metrics on all models
243         """"""
244         statsList = []
245         for model in models:
246             if useCrossValidation:
247                 if not fitModels:
248                     raise ValueError(""Cross-validation necessitates that models be retrained; got fitModels=False"")
249                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter if writeIndividualResults else None)
250                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
251             else:
252                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter if writeIndividualResults else None,
253                     fitModel=fitModels).getEvalStats()
254                 statsDict = evalStats.getAll()
255             statsDict[""modelName""] = model.getName()
256             statsList.append(statsDict)
257         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
258         if sortColumn is not None:
259             if sortColumn not in resultsDF.columns:
260                 log.warning(f""Requested sort column '{sortColumn}' not in list of columns {list(resultsDF.columns)}"")
261             else:
262                 resultsDF.sort_values(sortColumn, ascending=sortAscending, inplace=True)
263         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
264         log.info(strResults)
265         if resultWriter is not None:
266             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
267             strResults += ""\n\n"" + ""\n\n"".join([f""{model.getName()} = {str(model)}"" for model in models])
268             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
269         return resultsDF
270 
271     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 224
After: 228, 229, 235, 236, 237, 238, 239, 240, 241",add fitmodel argument to evaluationutil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,2193,"{'module': 1, 'expression_statement': 15, 'call': 17, 'attribute': 17, 'identifier': 77, '.': 17, 'argument_list': 17, '(': 18, ',': 8, 'keyword_argument': 4, '=': 15, ')': 18, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 8, 'type': 6, 'generic_type': 2, 'type_parameter': 2, '[': 5, ']': 5, 'typed_default_parameter': 1, 'none': 2, 'default_parameter': 1, 'false': 1, '->': 1, 'block': 5, 'string': 10, 'string_start': 10, 'string_content': 10, 'string_end': 10, 'assignment': 9, 'list': 1, 'for_statement': 1, 'for': 2, 'in': 2, 'if_statement': 2, 'if': 3, 'else_clause': 1, 'else': 2, 'subscript': 1, 'escape_sequence': 5, 'interpolation': 4, '{': 4, '}': 4, 'comparison_operator': 1, 'is not': 2, 'conditional_expression': 1, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, '+': 1, 'list_comprehension': 1, 'for_in_clause': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4567496083876986,0.44257642724828034,"(tensor([0.8650]), tensor([0.9465]), tensor([0.9039]), tensor([0.9376]))"
"221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
225         """"""
226         Compares several models via simple evaluation or cross-validation
227 
228         :param models: the models to compare
229         :param resultWriter: a writer with which to store results of the comparison
230         :return: a data frame containing evaluation metrics on all models
231         """"""
232         statsList = []
233         for model in models:
234             if useCrossValidation:
235                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
236                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
237             else:
238                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter).getEvalStats()
239                 statsDict = evalStats.getAll()
240             statsDict[""modelName""] = model.getName()
241             statsList.append(statsDict)
242         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
243         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
244         log.info(strResults)
245         if resultWriter is not None:
246             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
247             strResults += ""\n\n"" + ""\n\n"".join([f""{model.getName()} = {str(model)}"" for model in models])
248             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
249         return resultsDF
250 
251     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","225         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
226         return crossValidationData
227 
228     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False,
229             fitModels=True, writeIndividualResults=True, sortColumn: Optional[str] = None, sortAscending: bool = True) -> pd.DataFrame:
230         """"""
231         Compares several models via simple evaluation or cross-validation
232 
233         :param models: the models to compare
234         :param resultWriter: a writer with which to store results of the comparison
235         :param useCrossValidation: whether to use cross-validation in order to evaluate models; if False, use a simple evaluation
236             on test data (single split)
237         :param fitModels: whether to fit models before evaluating them; this can only be False if useCrossValidation=False
238         :param writeIndividualResults: whether to write results files on each individual model (in addition to the comparison
239             summary)
240         :param sortColumn: column/metric name by which to sort
241         :param sortAscending: whether to sort in ascending order
242         :return: a data frame containing evaluation metrics on all models
243         """"""
244         statsList = []
245         for model in models:
246             if useCrossValidation:
247                 if not fitModels:
248                     raise ValueError(""Cross-validation necessitates that models be retrained; got fitModels=False"")
249                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter if writeIndividualResults else None)
250                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
251             else:
252                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter if writeIndividualResults else None,
253                     fitModel=fitModels).getEvalStats()
254                 statsDict = evalStats.getAll()
255             statsDict[""modelName""] = model.getName()
256             statsList.append(statsDict)
257         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
258         if sortColumn is not None:
259             if sortColumn not in resultsDF.columns:
260                 log.warning(f""Requested sort column '{sortColumn}' not in list of columns {list(resultsDF.columns)}"")
261             else:
262                 resultsDF.sort_values(sortColumn, ascending=sortAscending, inplace=True)
263         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
264         log.info(strResults)
265         if resultWriter is not None:
266             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
267             strResults += ""\n\n"" + ""\n\n"".join([f""{model.getName()} = {str(model)}"" for model in models])
268             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
269         return resultsDF
270 
271     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 235
After: 247, 248, 249",add fitmodel argument to evaluationutil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,2235,"{'module': 1, 'expression_statement': 15, 'call': 17, 'attribute': 17, 'identifier': 77, '.': 17, 'argument_list': 17, '(': 18, ',': 8, 'keyword_argument': 4, '=': 15, ')': 18, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 8, 'type': 6, 'generic_type': 2, 'type_parameter': 2, '[': 5, ']': 5, 'typed_default_parameter': 1, 'none': 2, 'default_parameter': 1, 'false': 1, '->': 1, 'block': 5, 'string': 10, 'string_start': 10, 'string_content': 10, 'string_end': 10, 'assignment': 9, 'list': 1, 'for_statement': 1, 'for': 2, 'in': 2, 'if_statement': 2, 'if': 3, 'else_clause': 1, 'else': 2, 'subscript': 1, 'escape_sequence': 5, 'interpolation': 4, '{': 4, '}': 4, 'comparison_operator': 1, 'is not': 2, 'conditional_expression': 1, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, '+': 1, 'list_comprehension': 1, 'for_in_clause': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4567496083876986,0.44257642724828034,"(tensor([0.8650]), tensor([0.9465]), tensor([0.9039]), tensor([0.9376]))"
"221         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
222         return crossValidationData
223 
224     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False) -> pd.DataFrame:
225         """"""
226         Compares several models via simple evaluation or cross-validation
227 
228         :param models: the models to compare
229         :param resultWriter: a writer with which to store results of the comparison
230         :return: a data frame containing evaluation metrics on all models
231         """"""
232         statsList = []
233         for model in models:
234             if useCrossValidation:
235                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter)
236                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
237             else:
238                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter).getEvalStats()
239                 statsDict = evalStats.getAll()
240             statsDict[""modelName""] = model.getName()
241             statsList.append(statsDict)
242         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
243         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
244         log.info(strResults)
245         if resultWriter is not None:
246             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
247             strResults += ""\n\n"" + ""\n\n"".join([f""{model.getName()} = {str(model)}"" for model in models])
248             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
249         return resultsDF
250 
251     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","225         self.createPlots(crossValidationData, showPlots=showPlots, resultWriter=resultWriter)
226         return crossValidationData
227 
228     def compareModels(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None, useCrossValidation=False,
229             fitModels=True, writeIndividualResults=True, sortColumn: Optional[str] = None, sortAscending: bool = True) -> pd.DataFrame:
230         """"""
231         Compares several models via simple evaluation or cross-validation
232 
233         :param models: the models to compare
234         :param resultWriter: a writer with which to store results of the comparison
235         :param useCrossValidation: whether to use cross-validation in order to evaluate models; if False, use a simple evaluation
236             on test data (single split)
237         :param fitModels: whether to fit models before evaluating them; this can only be False if useCrossValidation=False
238         :param writeIndividualResults: whether to write results files on each individual model (in addition to the comparison
239             summary)
240         :param sortColumn: column/metric name by which to sort
241         :param sortAscending: whether to sort in ascending order
242         :return: a data frame containing evaluation metrics on all models
243         """"""
244         statsList = []
245         for model in models:
246             if useCrossValidation:
247                 if not fitModels:
248                     raise ValueError(""Cross-validation necessitates that models be retrained; got fitModels=False"")
249                 crossValidationResult = self.performCrossValidation(model, resultWriter=resultWriter if writeIndividualResults else None)
250                 statsDict = crossValidationResult.getEvalStatsCollection().aggStats()
251             else:
252                 evalStats: EvalStats = self.performSimpleEvaluation(model, resultWriter=resultWriter if writeIndividualResults else None,
253                     fitModel=fitModels).getEvalStats()
254                 statsDict = evalStats.getAll()
255             statsDict[""modelName""] = model.getName()
256             statsList.append(statsDict)
257         resultsDF = pd.DataFrame(statsList).set_index(""modelName"")
258         if sortColumn is not None:
259             if sortColumn not in resultsDF.columns:
260                 log.warning(f""Requested sort column '{sortColumn}' not in list of columns {list(resultsDF.columns)}"")
261             else:
262                 resultsDF.sort_values(sortColumn, ascending=sortAscending, inplace=True)
263         strResults = f""Model comparison results:\n{resultsDF.to_string()}""
264         log.info(strResults)
265         if resultWriter is not None:
266             suffix = ""crossval"" if useCrossValidation else ""simple-eval""
267             strResults += ""\n\n"" + ""\n\n"".join([f""{model.getName()} = {str(model)}"" for model in models])
268             resultWriter.writeTextFile(f""model-comparison-results-{suffix}"", strResults)
269         return resultsDF
270 
271     def compareModelsCrossValidation(self, models: Sequence[TModel], resultWriter: Optional[ResultWriter] = None) -> pd.DataFrame:
","Before: 238
After: 252, 253, 258, 259, 260, 261, 262",add fitmodel argument to evaluationutil,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_util.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,2286,"{'module': 1, 'expression_statement': 15, 'call': 17, 'attribute': 17, 'identifier': 77, '.': 17, 'argument_list': 17, '(': 18, ',': 8, 'keyword_argument': 4, '=': 15, ')': 18, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 8, 'type': 6, 'generic_type': 2, 'type_parameter': 2, '[': 5, ']': 5, 'typed_default_parameter': 1, 'none': 2, 'default_parameter': 1, 'false': 1, '->': 1, 'block': 5, 'string': 10, 'string_start': 10, 'string_content': 10, 'string_end': 10, 'assignment': 9, 'list': 1, 'for_statement': 1, 'for': 2, 'in': 2, 'if_statement': 2, 'if': 3, 'else_clause': 1, 'else': 2, 'subscript': 1, 'escape_sequence': 5, 'interpolation': 4, '{': 4, '}': 4, 'comparison_operator': 1, 'is not': 2, 'conditional_expression': 1, 'augmented_assignment': 1, '+=': 1, 'binary_operator': 1, '+': 1, 'list_comprehension': 1, 'for_in_clause': 1}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 6, 'nloc': 7, 'token_count': 61, 'name': '_isRegression', 'long_name': '_isRegression( model : Optional [ VectorModel ] , isRegression : Optional [ bool ] )', 'start_line': 41, 'end_line': 48, 'full_parameters': ['model : Optional [ VectorModel ]', ' isRegression : Optional [ bool ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_util.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.4567496083876986,0.44257642724828034,"(tensor([0.8650]), tensor([0.9465]), tensor([0.9039]), tensor([0.9376]))"
"13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31, **modelArgs):
17         """"""
18         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
19             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
20             need not be specified (should be inferred automatically).
21             In general, passing categorical features is preferable to using one-hot encoding, for example.
22         :param random_state: the random seed to use
23         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
24         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
25         """"""
26         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
27 
28         if type(categoricalFeatureNames) == str:
29             categoricalFeatureNameRegex = categoricalFeatureNames
30         else:
31             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
32                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
33             else:
34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31,
17             max_depth=-1, n_estimators=100, min_child_samples=20, importance_type=""gain"", **modelArgs):
18         """"""
19         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
20             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
21             need not be specified (should be inferred automatically).
22             In general, passing categorical features is preferable to using one-hot encoding, for example.
23         :param random_state: the random seed to use
24         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
25         :param max_depth: maximum tree depth for base learners, <=0 means no limit
26         :param n_estimators: number of boosted trees to fit
27         :param min_child_samples: minimum number of data needed in a child (leaf)
28         :param importance_type: the type of feature importance to be filled into feature_importances_.
29             If â€˜splitâ€™, result contains numbers of times the feature is used in a model.
30             If â€˜gainâ€™, result contains total gains of splits which use the feature.
31         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
32         """"""
33         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, importance_type=importance_type,
34             max_depth=max_depth, n_estimators=n_estimators, min_child_samples=min_child_samples,
35             **modelArgs)
36 
37         if type(categoricalFeatureNames) == str:
38             categoricalFeatureNameRegex = categoricalFeatureNames
39         else:
40             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
41                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
42             else:
43                 categoricalFeatureNameRegex = None
44         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
45 
46     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 16
After: 16, 17",add missing parameters to lightgbmvectorregressionmodel,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,1,151,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 43, 'argument_list': 7, '(': 8, ')': 8, ':': 8, 'block': 6, 'expression_statement': 7, 'assignment': 5, '=': 10, 'call': 6, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'typed_default_parameter': 1, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 3, 'default_parameter': 2, 'integer': 3, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '==': 1, 'else_clause': 2, 'else': 2, 'boolean_operator': 1, 'is not': 2, 'and': 1, '>': 1}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 13, 'token_count': 134, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , max_depth = - 1 , n_estimators = 100 , min_child_samples = 20 , importance_type = ""gain"" , ** modelArgs )', 'start_line': 16, 'end_line': 44, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' max_depth = - 1', ' n_estimators = 100', ' min_child_samples = 20', ' importance_type = ""gain""', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5445105817951753,0.5401443957041291,"(tensor([0.8769]), tensor([0.9510]), tensor([0.9124]), tensor([0.9430]))"
"13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31, **modelArgs):
17         """"""
18         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
19             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
20             need not be specified (should be inferred automatically).
21             In general, passing categorical features is preferable to using one-hot encoding, for example.
22         :param random_state: the random seed to use
23         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
24         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
25         """"""
26         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
27 
28         if type(categoricalFeatureNames) == str:
29             categoricalFeatureNameRegex = categoricalFeatureNames
30         else:
31             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
32                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
33             else:
34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31,
17             max_depth=-1, n_estimators=100, min_child_samples=20, importance_type=""gain"", **modelArgs):
18         """"""
19         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
20             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
21             need not be specified (should be inferred automatically).
22             In general, passing categorical features is preferable to using one-hot encoding, for example.
23         :param random_state: the random seed to use
24         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
25         :param max_depth: maximum tree depth for base learners, <=0 means no limit
26         :param n_estimators: number of boosted trees to fit
27         :param min_child_samples: minimum number of data needed in a child (leaf)
28         :param importance_type: the type of feature importance to be filled into feature_importances_.
29             If â€˜splitâ€™, result contains numbers of times the feature is used in a model.
30             If â€˜gainâ€™, result contains total gains of splits which use the feature.
31         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
32         """"""
33         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, importance_type=importance_type,
34             max_depth=max_depth, n_estimators=n_estimators, min_child_samples=min_child_samples,
35             **modelArgs)
36 
37         if type(categoricalFeatureNames) == str:
38             categoricalFeatureNameRegex = categoricalFeatureNames
39         else:
40             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
41                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
42             else:
43                 categoricalFeatureNameRegex = None
44         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
45 
46     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 24
After: 25, 26, 27, 28, 29, 30, 31",add missing parameters to lightgbmvectorregressionmodel,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,1,324,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 43, 'argument_list': 7, '(': 8, ')': 8, ':': 8, 'block': 6, 'expression_statement': 7, 'assignment': 5, '=': 10, 'call': 6, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'typed_default_parameter': 1, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 3, 'default_parameter': 2, 'integer': 3, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '==': 1, 'else_clause': 2, 'else': 2, 'boolean_operator': 1, 'is not': 2, 'and': 1, '>': 1}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 13, 'token_count': 134, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , max_depth = - 1 , n_estimators = 100 , min_child_samples = 20 , importance_type = ""gain"" , ** modelArgs )', 'start_line': 16, 'end_line': 44, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' max_depth = - 1', ' n_estimators = 100', ' min_child_samples = 20', ' importance_type = ""gain""', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5445105817951753,0.5401443957041291,"(tensor([0.8769]), tensor([0.9510]), tensor([0.9124]), tensor([0.9430]))"
"13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31, **modelArgs):
17         """"""
18         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
19             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
20             need not be specified (should be inferred automatically).
21             In general, passing categorical features is preferable to using one-hot encoding, for example.
22         :param random_state: the random seed to use
23         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
24         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/Parameters.html
25         """"""
26         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, **modelArgs)
27 
28         if type(categoricalFeatureNames) == str:
29             categoricalFeatureNameRegex = categoricalFeatureNames
30         else:
31             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
32                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
33             else:
34                 categoricalFeatureNameRegex = None
35         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
36 
37     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31,
17             max_depth=-1, n_estimators=100, min_child_samples=20, importance_type=""gain"", **modelArgs):
18         """"""
19         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
20             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
21             need not be specified (should be inferred automatically).
22             In general, passing categorical features is preferable to using one-hot encoding, for example.
23         :param random_state: the random seed to use
24         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
25         :param max_depth: maximum tree depth for base learners, <=0 means no limit
26         :param n_estimators: number of boosted trees to fit
27         :param min_child_samples: minimum number of data needed in a child (leaf)
28         :param importance_type: the type of feature importance to be filled into feature_importances_.
29             If â€˜splitâ€™, result contains numbers of times the feature is used in a model.
30             If â€˜gainâ€™, result contains total gains of splits which use the feature.
31         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
32         """"""
33         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, importance_type=importance_type,
34             max_depth=max_depth, n_estimators=n_estimators, min_child_samples=min_child_samples,
35             **modelArgs)
36 
37         if type(categoricalFeatureNames) == str:
38             categoricalFeatureNameRegex = categoricalFeatureNames
39         else:
40             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
41                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
42             else:
43                 categoricalFeatureNameRegex = None
44         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
45 
46     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 26
After: 33, 34, 35",add missing parameters to lightgbmvectorregressionmodel,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,1,190,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 43, 'argument_list': 7, '(': 8, ')': 8, ':': 8, 'block': 6, 'expression_statement': 7, 'assignment': 5, '=': 10, 'call': 6, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 8, 'typed_default_parameter': 1, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 3, 'default_parameter': 2, 'integer': 3, 'dictionary_splat_pattern': 1, '**': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'keyword_argument': 2, 'dictionary_splat': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '==': 1, 'else_clause': 2, 'else': 2, 'boolean_operator': 1, 'is not': 2, 'and': 1, '>': 1}","{'cyclomatic_complexity': 4, 'nloc': 10, 'token_count': 101, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , ** modelArgs )', 'start_line': 16, 'end_line': 35, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 13, 'token_count': 134, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , max_depth = - 1 , n_estimators = 100 , min_child_samples = 20 , importance_type = ""gain"" , ** modelArgs )', 'start_line': 16, 'end_line': 44, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' max_depth = - 1', ' n_estimators = 100', ' min_child_samples = 20', ' importance_type = ""gain""', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5445105817951753,0.5401443957041291,"(tensor([0.8769]), tensor([0.9510]), tensor([0.9124]), tensor([0.9430]))"
"410             factory = DefaultRegressionTorchDataSetProviderFactory()
411         return factory.createDataSetProvider(inputs, outputs, self, self._trainingContext)
412 
413     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
414         if self.inputTensoriser is not None:
415             log.info(f""Fitting {self.inputTensoriser} ..."")
416             self.inputTensoriser.fit(inputs)
417         self.model = self._createTorchModel()
418         dataSetProvider = self._createDataSetProvider(inputs, outputs)
419         self.model.fit(dataSetProvider, self.nnOptimiserParams)
420 
421     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","410             factory = DefaultRegressionTorchDataSetProviderFactory()
411         return factory.createDataSetProvider(inputs, outputs, self, self._trainingContext)
412 
413     def _fit(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
414         if self.inputTensoriser is not None:
415             log.info(f""Fitting {self.inputTensoriser} ..."")
416             self.inputTensoriser.fit(inputs, model=self)
417         self.model = self._createTorchModel()
418         dataSetProvider = self._createDataSetProvider(inputs, outputs)
419         self.model.fit(dataSetProvider, self.nnOptimiserParams)
420 
421     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> np.ndarray:
","Before: 416
After: 416",fix bug in vector classification and regression,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,3811,"{'module': 1, 'expression_statement': 6, 'assignment': 3, 'identifier': 42, '=': 3, 'call': 7, 'argument_list': 7, '(': 8, ')': 8, 'return_statement': 1, 'return': 1, 'attribute': 15, '.': 15, ',': 7, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 4, 'type': 3, '->': 1, 'none': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'string': 1, 'string_start': 1, 'string_content': 2, 'interpolation': 1, '{': 1, '}': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9642618757124934,0.9640263739546189,"(tensor([0.9949]), tensor([0.9971]), tensor([0.9960]), tensor([0.9969]))"
"502             factory = DefaultClassificationTorchDataSetProviderFactory()
503         return factory.createDataSetProvider(inputs, outputs, self, self._trainingContext)
504 
505     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
506         if len(outputs.columns) != 1:
507             raise ValueError(""Expected one output dimension: the class labels"")
508 
509         if self.inputTensoriser is not None:
510             log.info(f""Fitting {self.inputTensoriser} ..."")
511             self.inputTensoriser.fit(inputs)
512 
513         # transform outputs: for each data point, the new output shall be the index in the list of labels
514         labels: pd.Series = outputs.iloc[:, 0]
515         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
516 
517         self.model = self._createTorchModel()
518 
519         dataSetProvider = self._createDataSetProvider(inputs, outputs)
520         self.model.fit(dataSetProvider, self.nnOptimiserParams)
521 
522     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
","502             factory = DefaultClassificationTorchDataSetProviderFactory()
503         return factory.createDataSetProvider(inputs, outputs, self, self._trainingContext)
504 
505     def _fitClassifier(self, inputs: pd.DataFrame, outputs: pd.DataFrame) -> None:
506         if len(outputs.columns) != 1:
507             raise ValueError(""Expected one output dimension: the class labels"")
508 
509         if self.inputTensoriser is not None:
510             log.info(f""Fitting {self.inputTensoriser} ..."")
511             self.inputTensoriser.fit(inputs, model=self)
512 
513         # transform outputs: for each data point, the new output shall be the index in the list of labels
514         labels: pd.Series = outputs.iloc[:, 0]
515         outputs = pd.DataFrame([self._labels.index(l) for l in labels], columns=outputs.columns, index=outputs.index)
516 
517         self.model = self._createTorchModel()
518 
519         dataSetProvider = self._createDataSetProvider(inputs, outputs)
520         self.model.fit(dataSetProvider, self.nnOptimiserParams)
521 
522     def _predictOutputsForInputDataFrame(self, inputs: pd.DataFrame) -> torch.Tensor:
","Before: 511
After: 511",fix bug in vector classification and regression,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,4804,"{'module': 1, 'expression_statement': 8, 'assignment': 5, 'identifier': 66, '=': 7, 'call': 11, 'argument_list': 11, '(': 12, ')': 12, 'return_statement': 1, 'return': 1, 'attribute': 23, '.': 23, ',': 10, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 7, 'type': 4, '->': 1, 'none': 2, 'block': 3, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, '!=': 1, 'integer': 2, 'raise_statement': 1, 'raise': 1, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'is not': 2, 'interpolation': 1, '{': 1, '}': 1, 'comment': 1, 'subscript': 1, '[': 2, 'slice': 1, ']': 2, 'list_comprehension': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'keyword_argument': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 31, 'end_line': 34, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.980549970922414,0.9808902624783801,"(tensor([0.9948]), tensor([0.9964]), tensor([0.9956]), tensor([0.9962]))"
"116         pass
117 
118     @abstractmethod
119     def fit(self, df: pd.DataFrame):
120         pass
121 
122 
","116         pass
117 
118     @abstractmethod
119     def fit(self, df: pd.DataFrame, model=None):
120         """"""
121         :param df: the data frame with which to fit this tensoriser
122         :param model: the model in the context of which the fitting takes place (if any).
123             The fitting process may set parameters within the model that can only be determined from the (pre-tensorised) data.
124         """"""
125         pass
126 
127 
","Before: 119
After: 119, 120, 121, 122, 123, 124",add model parameter to tensoriser and rulebasedtensoriser,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1025,"{'module': 1, 'pass_statement': 2, 'pass': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'identifier': 6, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 1, 'attribute': 1, '.': 1, ')': 1, 'block': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 14, 'end_line': 24, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 14, 'end_line': 24, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.178210100184271,0.1742620416210406,"(tensor([0.6828]), tensor([0.9295]), tensor([0.7873]), tensor([0.8971]))"
"124     """"""
125     Base class for tensorisers which transforms data frames into tensors based on a predefined set of rules and does not require fitting
126     """"""
127     def fit(self, df: pd.DataFrame):
128         pass
129 
130 
","129     """"""
130     Base class for tensorisers which transforms data frames into tensors based on a predefined set of rules and does not require fitting
131     """"""
132     def fit(self, df: pd.DataFrame, model=None):
133         pass
134 
135 
","Before: 127
After: 132",add model parameter to tensoriser and rulebasedtensoriser,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_data.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1061,"{'module': 1, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'function_definition': 1, 'def': 1, 'identifier': 5, 'parameters': 1, '(': 1, ',': 1, 'typed_parameter': 1, ':': 2, 'type': 1, 'attribute': 1, '.': 1, ')': 1, 'block': 1, 'pass_statement': 1, 'pass': 1}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 14, 'end_line': 24, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 5, 'nloc': 11, 'token_count': 88, 'name': 'toTensor', 'long_name': 'toTensor( d : Union [ torch . Tensor , np . ndarray , list ] , cuda = False )', 'start_line': 14, 'end_line': 24, 'full_parameters': ['d : Union [ torch . Tensor', ' np . ndarray', ' list ]', ' cuda = False'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_data.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6642619976565798,0.6478056385486898,"(tensor([0.9700]), tensor([0.9822]), tensor([0.9761]), tensor([0.9810]))"
"90             plt.close(fig)
91         return p
92 
93     def writeFigures(self, figures: Sequence[Tuple[str, matplotlib.figure.Figure]], closeFigures=False):
94         for name, fig in figures:
95             self.writeFigure(name, fig, closeFigure=closeFigures)","91             plt.close(fig)
92         return p
93 
94     def writeFigures(self, figures: Sequence[Tuple[str, matplotlib.figure.Figure]], closeFigures=False):
95         for name, fig in figures:
96             self.writeFigure(name, fig, closeFigure=closeFigures)
97 
98     def writePickle(self, filenameSuffix, obj):
","Before: 95
After: 96, 97, 98, 99, 100",add dumppickle utility function,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/util/io.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,869,"{'module': 1, 'expression_statement': 1, 'call': 1, 'attribute': 3, 'identifier': 14, '.': 3, 'argument_list': 1, '(': 2, ')': 2, 'return_statement': 1, 'return': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 3, 'typed_parameter': 1, ':': 2, 'type': 4, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'default_parameter': 1, '=': 1, 'false': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 31, 'name': '__init__', 'long_name': '__init__( self , resultDir , filenamePrefix = """" )', 'start_line': 16, 'end_line': 19, 'full_parameters': ['self', ' resultDir', ' filenamePrefix = """"'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/io.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 31, 'name': '__init__', 'long_name': '__init__( self , resultDir , filenamePrefix = """" )', 'start_line': 17, 'end_line': 20, 'full_parameters': ['self', ' resultDir', ' filenamePrefix = """"'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/io.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6009461144102306,0.5849296540390128,"(tensor([0.9329]), tensor([0.9825]), tensor([0.9571]), tensor([0.9773]))"
"84 
85 
86 class ScatterPlot(Plot):
87     def __init__(self, x, y, **kwargs):
88         super().__init__(lambda: plt.scatter(x, y, **kwargs))
89 
90 
","84 
85 
86 class ScatterPlot(Plot):
87     def __init__(self, x, y, c=((0, 0, 1, 0.05),), x_label=None, y_label=None, **kwargs):
88         assert len(x) == len(y)
89         if x_label is None and hasattr(x, ""name""):
90             x_label = x.name
91         if y_label is None and hasattr(y, ""name""):
92             y_label = y.name
93 
94         def draw():
95             if x_label is not None:
96                 plt.xlabel(x_label)
97             if x_label is not None:
98                 plt.ylabel(y_label)
99             return plt.scatter(x, y, c=c, **kwargs)
100 
101         super().__init__(draw)
102 
103 
","Before: 87, 88
After: 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101",add scatter/heatmap support to plot.py,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,793,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 14, 'argument_list': 4, '(': 5, ')': 5, ':': 3, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 5, 'dictionary_splat_pattern': 1, '**': 2, 'expression_statement': 1, 'call': 3, 'attribute': 2, '.': 2, 'lambda': 2, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.23827985803589546,0.21884740395485333,"(tensor([0.6775]), tensor([0.8669]), tensor([0.7606]), tensor([0.8433]))"
"89 
90 
91 class HeatMapPlot(Plot):
92     def __init__(self, x, y, bins=60, cmap=None, **kwargs):
93 
94         def draw():
95             nonlocal cmap
96             x_range = [min(x), max(x)]
97             y_range = [min(y), max(y)]
98             heatmap, _, _ = np.histogram2d(x, y, range=[x_range, y_range], bins=bins)
99             extent = [x_range[0], x_range[1], y_range[0], y_range[1]]
100             if cmap is None:
101                 cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
102             return plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, aspect=""auto"", **kwargs)
103 
104         super().__init__(draw)","100 
101         super().__init__(draw)
102 
103 
104 class HeatMapPlot(Plot):
105     DEFAULT_CMAP_FACTORY = lambda numPoints: LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/numPoints, (1, 0.96, 0.96)), (1, (0.7, 0, 0))), numPoints)
106 
107     def __init__(self, x, y, xLabel=None, yLabel=None, bins=60, cmap=None, commonRange=True, diagonal=False,
108             diagonalColor=""green"", **kwargs):
109         assert len(x) == len(y)
","Before: 92
After: 105, 106, 107, 108, 109, 110, 111, 112, 113",add scatter/heatmap support to plot.py,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,860,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 54, 'argument_list': 8, '(': 13, ')': 13, ':': 4, 'block': 4, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 28, 'default_parameter': 2, '=': 14, 'integer': 11, 'none': 2, 'dictionary_splat_pattern': 1, '**': 2, 'nonlocal_statement': 1, 'nonlocal': 1, 'expression_statement': 5, 'assignment': 5, 'list': 4, '[': 8, 'call': 7, ']': 8, 'pattern_list': 1, 'attribute': 4, '.': 4, 'keyword_argument': 7, 'subscript': 4, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'tuple': 3, 'float': 1, 'return_statement': 1, 'return': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.27891532849728873,0.25669802118751395,"(tensor([0.7781]), tensor([0.7788]), tensor([0.7785]), tensor([0.7788]))"
"89 
90 
91 class HeatMapPlot(Plot):
92     def __init__(self, x, y, bins=60, cmap=None, **kwargs):
93 
94         def draw():
95             nonlocal cmap
96             x_range = [min(x), max(x)]
97             y_range = [min(y), max(y)]
98             heatmap, _, _ = np.histogram2d(x, y, range=[x_range, y_range], bins=bins)
99             extent = [x_range[0], x_range[1], y_range[0], y_range[1]]
100             if cmap is None:
101                 cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
102             return plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, aspect=""auto"", **kwargs)
103 
104         super().__init__(draw)","104 class HeatMapPlot(Plot):
105     DEFAULT_CMAP_FACTORY = lambda numPoints: LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/numPoints, (1, 0.96, 0.96)), (1, (0.7, 0, 0))), numPoints)
106 
107     def __init__(self, x, y, xLabel=None, yLabel=None, bins=60, cmap=None, commonRange=True, diagonal=False,
108             diagonalColor=""green"", **kwargs):
109         assert len(x) == len(y)
110         if xLabel is None and hasattr(x, ""name""):
111             xLabel = x.name
112         if yLabel is None and hasattr(y, ""name""):
113             yLabel = y.name
114 
115         def draw():
116             nonlocal cmap
117             x_range = [min(x), max(x)]
118             y_range = [min(y), max(y)]
119             range = [min(x_range[0], y_range[0]), max(x_range[1], y_range[1])]
120             if diagonal:
121                 plt.plot(range, range, '-', lw=0.75, label=""_not in legend"", color=diagonalColor, zorder=2)
122             heatmap, _, _ = np.histogram2d(x, y, range=[x_range, y_range], bins=bins, density=False)
123             if commonRange:
124                 extent = [range[0], range[1], range[0], range[1]]
125             else:
126                 extent = [x_range[0], x_range[1], y_range[0], y_range[1]]
127             if cmap is None:
128                 cmap = HeatMapPlot.DEFAULT_CMAP_FACTORY(len(x))
129             if xLabel is not None:
130                 plt.xlabel(xLabel)
131             if yLabel is not None:
132                 plt.ylabel(yLabel)
133             return plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, aspect=""auto"", **kwargs)
134 
135         super().__init__(draw)","Before: 98, 99
After: 119, 120, 121, 122, 123, 124, 125, 126",add scatter/heatmap support to plot.py,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,946,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 54, 'argument_list': 8, '(': 13, ')': 13, ':': 4, 'block': 4, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 28, 'default_parameter': 2, '=': 14, 'integer': 11, 'none': 2, 'dictionary_splat_pattern': 1, '**': 2, 'nonlocal_statement': 1, 'nonlocal': 1, 'expression_statement': 5, 'assignment': 5, 'list': 4, '[': 8, 'call': 7, ']': 8, 'pattern_list': 1, 'attribute': 4, '.': 4, 'keyword_argument': 7, 'subscript': 4, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'tuple': 3, 'float': 1, 'return_statement': 1, 'return': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3563154999109227,0.3432409588034097,"(tensor([0.8056]), tensor([0.8918]), tensor([0.8465]), tensor([0.8824]))"
"89 
90 
91 class HeatMapPlot(Plot):
92     def __init__(self, x, y, bins=60, cmap=None, **kwargs):
93 
94         def draw():
95             nonlocal cmap
96             x_range = [min(x), max(x)]
97             y_range = [min(y), max(y)]
98             heatmap, _, _ = np.histogram2d(x, y, range=[x_range, y_range], bins=bins)
99             extent = [x_range[0], x_range[1], y_range[0], y_range[1]]
100             if cmap is None:
101                 cmap = LinearSegmentedColormap.from_list(""whiteToRed"", ((1, 1, 1), (0.7, 0, 0)))
102             return plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, aspect=""auto"", **kwargs)
103 
104         super().__init__(draw)","104 class HeatMapPlot(Plot):
105     DEFAULT_CMAP_FACTORY = lambda numPoints: LinearSegmentedColormap.from_list(""whiteToRed"", ((0, (1, 1, 1)), (1/numPoints, (1, 0.96, 0.96)), (1, (0.7, 0, 0))), numPoints)
106 
107     def __init__(self, x, y, xLabel=None, yLabel=None, bins=60, cmap=None, commonRange=True, diagonal=False,
108             diagonalColor=""green"", **kwargs):
109         assert len(x) == len(y)
110         if xLabel is None and hasattr(x, ""name""):
111             xLabel = x.name
112         if yLabel is None and hasattr(y, ""name""):
113             yLabel = y.name
114 
115         def draw():
116             nonlocal cmap
117             x_range = [min(x), max(x)]
118             y_range = [min(y), max(y)]
119             range = [min(x_range[0], y_range[0]), max(x_range[1], y_range[1])]
120             if diagonal:
121                 plt.plot(range, range, '-', lw=0.75, label=""_not in legend"", color=diagonalColor, zorder=2)
122             heatmap, _, _ = np.histogram2d(x, y, range=[x_range, y_range], bins=bins, density=False)
123             if commonRange:
124                 extent = [range[0], range[1], range[0], range[1]]
125             else:
126                 extent = [x_range[0], x_range[1], y_range[0], y_range[1]]
127             if cmap is None:
128                 cmap = HeatMapPlot.DEFAULT_CMAP_FACTORY(len(x))
129             if xLabel is not None:
130                 plt.xlabel(xLabel)
131             if yLabel is not None:
132                 plt.ylabel(yLabel)
133             return plt.imshow(heatmap.T, extent=extent, origin='lower', cmap=cmap, zorder=1, aspect=""auto"", **kwargs)
134 
135         super().__init__(draw)","Before: 101
After: 128, 129, 130, 131, 132",add scatter/heatmap support to plot.py,Sync a4lbs,https://github.com/opcode81/sensAI,src/sensai/util/plot.py,d50663748de70b4b8294a36616b0da11f093003e,de2d3e5fb8e113f22a6aadba76771b9229d04b64,0,1021,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 54, 'argument_list': 8, '(': 13, ')': 13, ':': 4, 'block': 4, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 28, 'default_parameter': 2, '=': 14, 'integer': 11, 'none': 2, 'dictionary_splat_pattern': 1, '**': 2, 'nonlocal_statement': 1, 'nonlocal': 1, 'expression_statement': 5, 'assignment': 5, 'list': 4, '[': 8, 'call': 7, ']': 8, 'pattern_list': 1, 'attribute': 4, '.': 4, 'keyword_argument': 7, 'subscript': 4, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'tuple': 3, 'float': 1, 'return_statement': 1, 'return': 1, 'dictionary_splat': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 53, 'name': 'plotMatrix', 'long_name': 'plotMatrix( matrix , title , xticklabels : Sequence [ str ] , yticklabels : Sequence [ str ] , xlabel : str , ylabel : str , normalize = True , figsize = ( 9 , 9 )', 'start_line': 13, 'end_line': 14, 'full_parameters': ['matrix', ' title', ' xticklabels : Sequence [ str ]', ' yticklabels : Sequence [ str ]', ' xlabel : str', ' ylabel : str', ' normalize = True', ' figsize = ( 9', ' 9'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/plot.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3563154999109227,0.3432409588034097,"(tensor([0.8056]), tensor([0.8918]), tensor([0.8465]), tensor([0.8824]))"
"13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31,
17             max_depth=-1, n_estimators=100, min_child_samples=20, importance_type=""gain"", **modelArgs):
18         """"""
19         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
20             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
21             need not be specified (should be inferred automatically).
22             In general, passing categorical features is preferable to using one-hot encoding, for example.
23         :param random_state: the random seed to use
24         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
25         :param max_depth: maximum tree depth for base learners, <=0 means no limit
26         :param n_estimators: number of boosted trees to fit
27         :param min_child_samples: minimum number of data needed in a child (leaf)
28         :param importance_type: the type of feature importance to be filled into feature_importances_.
29             If â€˜splitâ€™, result contains numbers of times the feature is used in a model.
30             If â€˜gainâ€™, result contains total gains of splits which use the feature.
31         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
32         """"""
33         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, importance_type=importance_type,
34             max_depth=max_depth, n_estimators=n_estimators, min_child_samples=min_child_samples,
35             **modelArgs)
36 
37         if type(categoricalFeatureNames) == str:
38             categoricalFeatureNameRegex = categoricalFeatureNames
39         else:
40             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
41                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
42             else:
43                 categoricalFeatureNameRegex = None
44         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
45 
46     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31,
17             max_depth=-1, n_estimators=100, min_child_samples=20, importance_type=""gain"", **modelArgs):
18         """"""
19         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
20             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
21             need not be specified (should be inferred automatically).
22             In general, passing categorical features is preferable to using one-hot encoding, for example.
23         :param random_state: the random seed to use
24         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
25         :param max_depth: maximum tree depth for base learners, <=0 means no limit
26         :param n_estimators: number of boosted trees to fit
27         :param min_child_samples: minimum number of data needed in a child (leaf)
28         :param importance_type: the type of feature importance to be set in the 'feature_importances_' property of the wrapped model.
29             If â€˜splitâ€™, result contains numbers of times the feature is used in a model.
30             If â€˜gainâ€™, result contains total gains of splits which use the feature.
31         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
32         """"""
33         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, importance_type=importance_type,
34             max_depth=max_depth, n_estimators=n_estimators, min_child_samples=min_child_samples,
35             **modelArgs)
36 
37         if type(categoricalFeatureNames) == str:
38             categoricalFeatureNameRegex = categoricalFeatureNames
39         else:
40             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
41                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
42             else:
43                 categoricalFeatureNameRegex = None
44         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
45 
46     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 28
After: 28",fix typo in lightgbm.py,Changed docstring to circumvent strange sphinx error,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,192744eabb201cffd1f7102fc97dc14346408171,af3d713b4d5c0fe09b04d0ec2a79839c9a8b4f8b,1,387,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 55, 'argument_list': 7, '(': 8, ')': 8, ':': 8, 'block': 6, 'expression_statement': 7, 'assignment': 5, '=': 18, 'call': 6, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 16, 'typed_default_parameter': 1, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 3, 'default_parameter': 6, 'integer': 6, 'unary_operator': 1, '-': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'dictionary_splat_pattern': 1, '**': 2, 'keyword_argument': 6, 'dictionary_splat': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '==': 1, 'else_clause': 2, 'else': 2, 'boolean_operator': 1, 'is not': 2, 'and': 1, '>': 1}","{'cyclomatic_complexity': 4, 'nloc': 13, 'token_count': 134, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , max_depth = - 1 , n_estimators = 100 , min_child_samples = 20 , importance_type = ""gain"" , ** modelArgs )', 'start_line': 16, 'end_line': 44, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' max_depth = - 1', ' n_estimators = 100', ' min_child_samples = 20', ' importance_type = ""gain""', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 13, 'token_count': 134, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , max_depth = - 1 , n_estimators = 100 , min_child_samples = 20 , importance_type = ""gain"" , ** modelArgs )', 'start_line': 16, 'end_line': 44, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' max_depth = - 1', ' n_estimators = 100', ' min_child_samples = 20', ' importance_type = ""gain""', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9682608186272064,0.9679310151580096,"(tensor([0.9918]), tensor([0.9948]), tensor([0.9933]), tensor([0.9945]))"
"13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31,
17             max_depth=-1, n_estimators=100, min_child_samples=20, importance_type=""gain"", **modelArgs):
18         """"""
19         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
20             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
21             need not be specified (should be inferred automatically).
22             In general, passing categorical features is preferable to using one-hot encoding, for example.
23         :param random_state: the random seed to use
24         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
25         :param max_depth: maximum tree depth for base learners, <=0 means no limit
26         :param n_estimators: number of boosted trees to fit
27         :param min_child_samples: minimum number of data needed in a child (leaf)
28         :param importance_type: the type of feature importance to be set in the 'feature_importances_' property of the wrapped model.
29             If â€˜splitâ€™, result contains numbers of times the feature is used in a model.
30             If â€˜gainâ€™, result contains total gains of splits which use the feature.
31         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
32         """"""
33         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, importance_type=importance_type,
34             max_depth=max_depth, n_estimators=n_estimators, min_child_samples=min_child_samples,
35             **modelArgs)
36 
37         if type(categoricalFeatureNames) == str:
38             categoricalFeatureNameRegex = categoricalFeatureNames
39         else:
40             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
41                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
42             else:
43                 categoricalFeatureNameRegex = None
44         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
45 
46     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","13 class LightGBMVectorRegressionModel(AbstractSkLearnMultipleOneDimVectorRegressionModel):
14     log = log.getChild(__qualname__)
15 
16     def __init__(self, categoricalFeatureNames: Optional[Union[Sequence[str], str]] = None, random_state=42, num_leaves=31,
17             max_depth=-1, n_estimators=100, min_child_samples=20, importance_type=""gain"", **modelArgs):
18         """"""
19         :param categoricalFeatureNames: sequence of feature names in the input data that are categorical.
20             Columns that have dtype 'category' (as will be the case for categorical columns created via FeatureGenerators)
21             need not be specified (should be inferred automatically).
22             In general, passing categorical features is preferable to using one-hot encoding, for example.
23         :param random_state: the random seed to use
24         :param num_leaves: the maximum number of leaves in one tree (original lightgbm default is 31)
25         :param max_depth: maximum tree depth for base learners, <=0 means no limit
26         :param n_estimators: number of boosted trees to fit
27         :param min_child_samples: minimum number of data needed in a child (leaf)
28         :param importance_type: the type of feature importance to be set in the respective property of the wrapped model.
29             If â€˜splitâ€™, result contains numbers of times the feature is used in a model.
30             If â€˜gainâ€™, result contains total gains of splits which use the feature.
31         :param modelArgs: see https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html
32         """"""
33         super().__init__(lightgbm.sklearn.LGBMRegressor, random_state=random_state, num_leaves=num_leaves, importance_type=importance_type,
34             max_depth=max_depth, n_estimators=n_estimators, min_child_samples=min_child_samples,
35             **modelArgs)
36 
37         if type(categoricalFeatureNames) == str:
38             categoricalFeatureNameRegex = categoricalFeatureNames
39         else:
40             if categoricalFeatureNames is not None and len(categoricalFeatureNames) > 0:
41                 categoricalFeatureNameRegex = orRegexGroup(categoricalFeatureNames)
42             else:
43                 categoricalFeatureNameRegex = None
44         self._categoricalFeatureNameRegex: str = categoricalFeatureNameRegex
45 
46     def _updateModelArgs(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 28
After: 28",fix typo in src/src/sensai/lightgbm.py,Second attempt to fix strange sphinx error,https://github.com/opcode81/sensAI,src/sensai/lightgbm.py,4d6f3c7b8fe5bffbc717332ad2771d680741d9aa,192744eabb201cffd1f7102fc97dc14346408171,1,397,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 55, 'argument_list': 7, '(': 8, ')': 8, ':': 8, 'block': 6, 'expression_statement': 7, 'assignment': 5, '=': 18, 'call': 6, 'attribute': 5, '.': 5, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 16, 'typed_default_parameter': 1, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'none': 3, 'default_parameter': 6, 'integer': 6, 'unary_operator': 1, '-': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'dictionary_splat_pattern': 1, '**': 2, 'keyword_argument': 6, 'dictionary_splat': 1, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '==': 1, 'else_clause': 2, 'else': 2, 'boolean_operator': 1, 'is not': 2, 'and': 1, '>': 1}","{'cyclomatic_complexity': 4, 'nloc': 13, 'token_count': 134, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , max_depth = - 1 , n_estimators = 100 , min_child_samples = 20 , importance_type = ""gain"" , ** modelArgs )', 'start_line': 16, 'end_line': 44, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' max_depth = - 1', ' n_estimators = 100', ' min_child_samples = 20', ' importance_type = ""gain""', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 4, 'nloc': 13, 'token_count': 134, 'name': '__init__', 'long_name': '__init__( self , categoricalFeatureNames : Optional [ Union [ Sequence [ str ] , str ] ] = None , random_state = 42 , num_leaves = 31 , max_depth = - 1 , n_estimators = 100 , min_child_samples = 20 , importance_type = ""gain"" , ** modelArgs )', 'start_line': 16, 'end_line': 44, 'full_parameters': ['self', ' categoricalFeatureNames : Optional [ Union [ Sequence [ str ]', ' str ] ] = None', ' random_state = 42', ' num_leaves = 31', ' max_depth = - 1', ' n_estimators = 100', ' min_child_samples = 20', ' importance_type = ""gain""', ' ** modelArgs'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/lightgbm.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9903378912685626,0.9899895159714672,"(tensor([0.9943]), tensor([0.9940]), tensor([0.9941]), tensor([0.9940]))"
"179         if modelBytes is not None:
180             self.setModuleBytes(modelBytes)
181 
182     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
183             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
184             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
185         """"""
186         Applies the model to the given input tensor and returns the result (normalized)
187 
188         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
189             (if the model accepts more than one input).
190             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
191         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
192         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
193         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
194         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
195         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
196         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
197 
198         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
199             containing standard deviations
200         """"""
201         def extract(z):
202             if scaleOutput:
203                 z = self.scaledOutput(z)
204             if self._isCudaEnabled():
205                 z = z.cpu()
206             z = z.detach()
207             if asNumpy:
208                 z = z.numpy()
209             return z
210 
211         model = self.getTorchModule()
212         model.eval()
213 
214         if isinstance(X, TorchDataSet):
215             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
216         elif isinstance(X, np.ndarray):
217             X = toFloatArray(X)
218             X = torch.from_numpy(X).float()
219 
220         if type(X) not in (list, tuple):
221             inputs = [X]
222         else:
223             inputs = X
224 
225         if self._isCudaEnabled():
226             torch.cuda.set_device(self._gpu)
227             inputs = [t.cuda() for t in inputs]
228         if scaleInput:
229             inputs = [self.inputScaler.normalise(t) for t in inputs]
230         if createBatch:
231             inputs = [t.view(1, *X.size()) for t in inputs]
232 
233         maxValue = max([t.max().item() for t in inputs])
234         if maxValue > 2:
235             log.warning(""Received input which is likely to not be correctly normalised: maximum value in input tensor is %f"" % maxValue)
236 
237         if mcDropoutSamples is None:
238             y = model(*inputs)
239             return extract(y)
240         else:
241             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
242             return extract(y), extract(stddev)
243 
244     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","181         if modelBytes is not None:
182             self.setModuleBytes(modelBytes)
183 
184     def apply(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], asNumpy: bool = True, createBatch: bool = False,
185             mcDropoutSamples: Optional[int] = None, mcDropoutProbability: Optional[float] = None, scaleOutput: bool = False,
186             scaleInput: bool = False) -> Union[torch.Tensor, np.ndarray, Tuple]:
187         """"""
188         Applies the model to the given input tensor and returns the result (normalized)
189 
190         :param X: the input tensor (either a batch or, if createBatch=True, a single data point), a data set or a tuple/list of tensors
191             (if the model accepts more than one input).
192             If it is a data set, it will be processed at once, so the data set must not be too large to be processed at once.
193         :param asNumpy: flag indicating whether to convert the result to a numpy.array (if False, return tensor)
194         :param createBatch: whether to add an additional tensor dimension for a batch containing just one data point
195         :param mcDropoutSamples: if not None, apply MC-Dropout-based inference with the respective number of samples; if None, apply regular inference
196         :param mcDropoutProbability: the probability with which to apply dropouts in MC-Dropout-based inference; if None, use model's default
197         :param scaleOutput: whether to scale the output that is produced by the underlying model (using this instance's output scaler)
198         :param scaleInput: whether to scale the input (using this instance's input scaler) before applying the underlying model
199 
200         :return: an output tensor or, if MC-Dropout is applied, a pair (y, sd) where y the mean output tensor and sd is a tensor of the same dimension
201             containing standard deviations
202         """"""
203         def extract(z):
204             if scaleOutput:
205                 z = self.scaledOutput(z)
206             if self._isCudaEnabled():
207                 z = z.cpu()
208             z = z.detach()
209             if asNumpy:
210                 z = z.numpy()
211             return z
212 
213         model = self.getTorchModule()
214         model.eval()
215 
216         if isinstance(X, TorchDataSet):
217             X = next(X.iterBatches(X.size(), inputOnly=True, shuffle=False))
218         elif isinstance(X, np.ndarray):
219             X = toFloatArray(X)
220             X = torch.from_numpy(X).float()
221 
222         if type(X) not in (list, tuple):
223             inputs = [X]
224         else:
225             inputs = X
226 
227         if self._isCudaEnabled():
228             torch.cuda.set_device(self._gpu)
229             inputs = [t.cuda() for t in inputs]
230         if scaleInput:
231             inputs = [self.inputScaler.normalise(t) for t in inputs]
232         if createBatch:
233             inputs = [t.view(1, *X.size()) for t in inputs]
234 
235         # check input normalisation
236         if self.NORMALISATION_CHECK_THRESHOLD is not None:
237             maxValue = 0.0
238             for t in inputs:
239                 if t.is_floating_point():  # ignore any integer tensors (which typically contain lengths)
240                     maxValue = max(t.abs().max().item(), maxValue)
241             if maxValue > self.NORMALISATION_CHECK_THRESHOLD:
242                 log.warning(""Received input which is likely to not be correctly normalised: maximum abs. value in input tensor is %f"" % maxValue)
243 
244         if mcDropoutSamples is None:
245             y = model(*inputs)
246             return extract(y)
247         else:
248             y, stddev = model.inferMCDropout(X, mcDropoutSamples, p=mcDropoutProbability)
249             return extract(y), extract(stddev)
250 
251     def applyScaled(self, X: Union[torch.Tensor, np.ndarray, TorchDataSet, Sequence[torch.Tensor]], **kwargs) -> Union[torch.Tensor, np.ndarray]:
","Before: 233, 234, 235
After: 235, 236, 237, 238, 239, 240, 241, 242",add a constant for normalisation_check_threshold,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,9e7a3694a32a1eb492c127bbca6922ae88f818fe,7980433604b75cde4c1c8e6b1dc916d7e6368a6a,0,2121,"{'module': 1, 'if_statement': 11, 'if': 11, 'comparison_operator': 4, 'identifier': 149, 'is not': 2, 'none': 4, ':': 23, 'block': 16, 'expression_statement': 21, 'call': 32, 'attribute': 31, '.': 31, 'argument_list': 32, '(': 35, ')': 35, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 22, 'typed_parameter': 1, 'type': 18, 'generic_type': 5, 'type_parameter': 5, '[': 10, ']': 10, 'typed_default_parameter': 6, '=': 25, 'true': 2, 'false': 4, '->': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'assignment': 16, 'return_statement': 3, 'return': 3, 'keyword_argument': 3, 'elif_clause': 1, 'elif': 1, 'not in': 2, 'tuple': 1, 'list': 1, 'else_clause': 2, 'else': 2, 'list_comprehension': 4, 'for_in_clause': 4, 'for': 4, 'in': 4, 'integer': 2, 'list_splat': 2, '*': 2, '>': 1, 'binary_operator': 1, '%': 1, 'is': 1, 'pattern_list': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.766061739189868,0.7644284287862791,"(tensor([0.9635]), tensor([0.9798]), tensor([0.9715]), tensor([0.9781]))"
"567 
568 
569 class DefaultClassificationTorchDataSetProviderFactory(TorchDataSetProviderFactory):
570     def createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame, model: TorchVectorClassificationModel,
571             trainingContext: TrainingContext, inputTensoriser: Optional[Tensoriser],
572             dataFrameSplitter: Optional[DataFrameSplitter]) -> TorchDataSetProvider:
573         dataUtil = ClassificationVectorDataUtil(inputs, outputs, model.model.cuda, len(model._labels),
574             normalisationMode=model.normalisationMode, inputTensoriser=inputTensoriser)
575         return TorchDataSetProviderFromDataUtil(dataUtil, model.model.cuda)
576 
577 
","574 
575 
576 class DefaultClassificationTorchDataSetProviderFactory(TorchDataSetProviderFactory):
577     def createDataSetProvider(self, inputs: pd.DataFrame, outputs: pd.DataFrame, model: TorchVectorClassificationModel,
578             trainingContext: TrainingContext, inputTensoriser: Optional[Tensoriser],
579             dataFrameSplitter: Optional[DataFrameSplitter]) -> TorchDataSetProvider:
580         dataUtil = ClassificationVectorDataUtil(inputs, outputs, model.model.cuda, len(model._labels),
581             normalisationMode=model.normalisationMode, inputTensoriser=inputTensoriser, dataFrameSplitter=dataFrameSplitter)
582         return TorchDataSetProviderFromDataUtil(dataUtil, model.model.cuda)
583 
584 
","Before: 574
After: 581",add a constant for normalisation_check_threshold,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/torch/torch_base.py,9e7a3694a32a1eb492c127bbca6922ae88f818fe,7980433604b75cde4c1c8e6b1dc916d7e6368a6a,0,5679,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 41, 'argument_list': 4, '(': 5, ')': 5, ':': 8, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 12, 'typed_parameter': 6, 'type': 9, 'attribute': 8, '.': 8, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, '->': 1, 'expression_statement': 1, 'assignment': 1, '=': 3, 'call': 3, 'keyword_argument': 2, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 24, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 32, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.727415276635878,0.7087930394465156,"(tensor([0.9760]), tensor([0.9736]), tensor([0.9748]), tensor([0.9738]))"
"1 from bisect import bisect_right, bisect_left
2 from typing import Sequence, Optional, TypeVar, Generic, Tuple
3 
4 from . import sequences as array_util
5 
6 TKey = TypeVar(""TKey"")
","1 from bisect import bisect_right, bisect_left
2 from enum import Enum
3 from typing import Sequence, Optional, TypeVar, Generic, Tuple, Dict, Any
4 
5 from . import sequences as array_util
6 
","Before: 2
After: 2, 3",add logic to find a unique key in the src/sensai/util/datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,9e7a3694a32a1eb492c127bbca6922ae88f818fe,7980433604b75cde4c1c8e6b1dc916d7e6368a6a,0,31,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 10, 'identifier': 11, 'import': 3, ',': 5, 'relative_import': 1, 'import_prefix': 1, '.': 1, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 17, 'name': '__init__', 'long_name': '__init__( self , sortedValues : Sequence [ TValue ] )', 'start_line': 14, 'end_line': 15, 'full_parameters': ['self', ' sortedValues : Sequence [ TValue ]'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6260616588471203,0.6180847223319859,"(tensor([0.9445]), tensor([0.9457]), tensor([0.9451]), tensor([0.9456]))"
"1 import time
2 
3 
4 class StopWatch:
5     """"""
6     A simple stopwatch singleton which can be used to determine execution times
7     """"""
8     _instance = None
9 
10     @classmethod
","1 import time
2 
3 
4 class StopWatch:
5     """"""
6     Represents a stop watch for timing an execution. Constructing an instance starts the stopwatch.
7     """"""
8     def __init__(self):
9         self.startTime = time.time()
10 
","Before: 6
After: 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21",add stopwatchmanager and stopwatch,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/logging.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,26,"{'module': 1, 'import_statement': 1, 'import': 1, 'dotted_name': 1, 'identifier': 3, 'class_definition': 1, 'class': 1, ':': 1, 'block': 1, 'expression_statement': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'assignment': 1, '=': 1, 'none': 1}","{'cyclomatic_complexity': 2, 'nloc': 4, 'token_count': 24, 'name': 'getInstance', 'long_name': 'getInstance( cls )', 'start_line': 11, 'end_line': 14, 'full_parameters': ['cls'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/logging.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 14, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/logging.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3612820074736043,0.3106950367889356,"(tensor([0.8446]), tensor([0.8576]), tensor([0.8510]), tensor([0.8562]))"
"8     _instance = None
9 
10     @classmethod
11     def getInstance(cls):
12         if cls._instance is None:
13             cls._instance = StopWatch(42)
14         return cls._instance
15 
16     def __init__(self, secret):
","23     _instance = None
24 
25     @classmethod
26     def getInstance(cls):
27         if cls._instance is None:
28             cls._instance = StopWatchManager(42)
29         return cls._instance
30 
31     def __init__(self, secret):
","Before: 13
After: 28",add stopwatchmanager and stopwatch,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/logging.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,58,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 11, '=': 2, 'none': 2, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ')': 2, ':': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'attribute': 3, '.': 3, 'is': 1, 'call': 1, 'argument_list': 1, 'integer': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 2, 'nloc': 4, 'token_count': 24, 'name': 'getInstance', 'long_name': 'getInstance( cls )', 'start_line': 11, 'end_line': 14, 'full_parameters': ['cls'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/logging.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 14, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 8, 'end_line': 9, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/logging.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.45800707241138094,0.41372621114248176,"(tensor([0.9464]), tensor([0.9502]), tensor([0.9483]), tensor([0.9498]))"
"1 import logging
2 import pickle
3 from typing import List
4 
5 log = logging.getLogger(__name__)
6 
7 
","1 import logging
2 import pickle
3 from typing import List, Dict, Any
4 
5 log = logging.getLogger(__name__)
6 
7 
","Before: 3
After: 3",add setstate and getstate helper functions,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/pickle.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,17,"{'module': 1, 'import_statement': 2, 'import': 3, 'dotted_name': 4, 'identifier': 8, 'import_from_statement': 1, 'from': 1, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'attribute': 1, '.': 1, 'argument_list': 1, '(': 1, ')': 1}","{'cyclomatic_complexity': 11, 'nloc': 29, 'token_count': 177, 'name': '_debugFailure', 'long_name': '_debugFailure( cls , obj , path , failures , handledObjectIds )', 'start_line': 16, 'end_line': 50, 'full_parameters': ['cls', ' obj', ' path', ' failures', ' handledObjectIds'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pickle.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 11, 'nloc': 29, 'token_count': 177, 'name': '_debugFailure', 'long_name': '_debugFailure( cls , obj , path , failures , handledObjectIds )', 'start_line': 16, 'end_line': 50, 'full_parameters': ['cls', ' obj', ' path', ' failures', ' handledObjectIds'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pickle.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.760856626273165,0.7249495673238483,"(tensor([0.9561]), tensor([0.9862]), tensor([0.9710]), tensor([0.9831]))"
"2 import re
3 
4 
5 def dictString(d: Dict):
6     return ', '.join([f'{k}={v}' for k, v in d.items()])
7 
8 
","2 import re
3 
4 
5 def dictString(d: Dict, brackets: Optional[str] = None):
6     s = ', '.join([f'{k}={toString(v)}' for k, v in d.items()])
7     if brackets is not None:
8         return brackets[:1] + s + brackets[-1:]
9     else:
10         return s
11 
12 
","Before: 5, 6
After: 5, 6, 7, 8, 9, 10",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,44,"{'module': 1, 'import_statement': 1, 'import': 1, 'dotted_name': 1, 'identifier': 11, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 3, 'typed_parameter': 1, ':': 2, 'type': 1, ')': 3, 'block': 1, 'return_statement': 1, 'return': 1, 'call': 2, 'attribute': 2, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, '.': 2, 'argument_list': 2, 'list_comprehension': 1, '[': 1, 'interpolation': 2, '{': 2, '}': 2, 'for_in_clause': 1, 'for': 1, 'pattern_list': 1, ',': 1, 'in': 1, ']': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.42111722312896654,0.40631425874147575,"(tensor([0.7949]), tensor([0.9498]), tensor([0.8655]), tensor([0.9317]))"
"6     return ', '.join([f'{k}={v}' for k, v in d.items()])
7 
8 
9 def listString(l: Iterable[Any]):
10     return ""["" + "", "".join((str(x) for x in l)) + ""]""
11 
12 
","10         return s
11 
12 
13 def listString(l: Iterable[Any], brackets=""[]""):
14     return brackets[:1] + "", "".join((toString(x) for x in l)) + brackets[-1:]
15 
16 
","Before: 9, 10
After: 13, 14",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,106,"{'module': 1, 'return_statement': 2, 'return': 2, 'call': 4, 'attribute': 3, 'string': 5, 'string_start': 5, 'string_content': 5, 'string_end': 5, '.': 3, 'identifier': 16, 'argument_list': 4, '(': 6, 'list_comprehension': 1, '[': 2, 'interpolation': 2, '{': 2, '}': 2, 'for_in_clause': 2, 'for': 2, 'pattern_list': 1, ',': 1, 'in': 2, ')': 6, ']': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 2, 'type': 2, 'generic_type': 1, 'type_parameter': 1, 'block': 1, 'binary_operator': 2, '+': 2, 'generator_expression': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.35300322955937785,0.3251709971503211,"(tensor([0.8511]), tensor([0.8725]), tensor([0.8617]), tensor([0.8703]))"
"10     return ""["" + "", "".join((str(x) for x in l)) + ""]""
11 
12 
13 def objectRepr(obj, memberNamesOrDict: Union[List[str], Dict[str, Any]]):
14     def toString(x):
15         if type(x) == dict:
16             return ""{"" + dictString(x) + ""}""
17         else:
18             return str(x)
19 
20     if type(memberNamesOrDict) == dict:
21         membersDict = memberNamesOrDict
22     else:
23         membersDict = {m: toString(getattr(obj, m)) for m in memberNamesOrDict}
24     return f""{obj.__class__.__name__}[{dictString(membersDict)}]""
25 
26 
","14     return brackets[:1] + "", "".join((toString(x) for x in l)) + brackets[-1:]
15 
16 
17 def toString(x):
18     if type(x) == list:
19         return listString(x)
20     elif type(x) == tuple:
21         return listString(x, brackets=""()"")
22     elif type(x) == dict:
23         return dictString(x, brackets=""{}"")
24     else:
25         return str(x)
26 
27 
","Before: 13, 14, 15, 16, 17, 18
After: 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,184,"{'module': 1, 'return_statement': 4, 'return': 4, 'binary_operator': 4, 'string': 6, 'string_start': 6, 'string_content': 7, 'string_end': 6, '+': 4, 'call': 9, 'attribute': 3, '.': 3, 'identifier': 41, 'argument_list': 9, '(': 12, 'generator_expression': 1, ')': 12, 'for_in_clause': 2, 'for': 2, 'in': 2, 'function_definition': 2, 'def': 2, 'parameters': 2, ',': 4, 'typed_parameter': 1, ':': 8, 'type': 6, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'block': 6, 'if_statement': 2, 'if': 2, 'comparison_operator': 2, '==': 2, 'else_clause': 2, 'else': 2, 'expression_statement': 2, 'assignment': 2, '=': 2, 'dictionary_comprehension': 1, '{': 3, 'pair': 1, '}': 3, 'interpolation': 2}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.2519764285073795,0.2320757558198107,"(tensor([0.8633]), tensor([0.7805]), tensor([0.8198]), tensor([0.7880]))"
"43     def _toStringClassName(self):
44         return type(self).__qualname__
45 
46     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
47             **additionalEntries) -> str:
48         """"""
49         Creates a string of the class attributes, with optional exclusions/inclusions/additions
50 
51         :param exclude: attributes to be excluded; can only be non-empty if include is empty
52         :param include: attributes to be included; can only be non-empty if exclude is empty
53         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
54         :return: a string containing all attribute names and values
55         """"""
56         def mklist(x):
57             if x is None:
58                 return []
59             if type(x) == str:
60                 return [x]
61             return x
62 
63         exclude = mklist(exclude)
64         include = mklist(include)
65         if len(exclude) > 0 and len(include) > 0:
66             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
67 
68         if len(include) == 0:
69             attributeDict = self.__dict__
70             if self._toStringExcludePrivate():
71                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
72         else:
73             attributeDict = {k: getattr(self, k) for k in include}
74         d = {k: v for k, v in attributeDict.items() if k not in exclude}
75         d.update(additionalEntries)
76         return dictString(d)
77 
78     def _toStringObjectInfo(self) -> str:
","52     def _toStringClassName(self):
53         return type(self).__qualname__
54 
55     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
56             **additionalEntries) -> str:
57         """"""
58         Creates a string of the class attributes, with optional exclusions/inclusions/additions.
59         Exclusions take precedence over inclusions.
60 
61         :param exclude: attributes to be excluded
62         :param include: attributes to be included; if None/empty, include all that are not excluded
63         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
64         :return: a string containing attribute names and values
65         """"""
66         def mklist(x):
67             if x is None:
68                 return []
69             if type(x) == str:
70                 return [x]
71             return x
72 
73         exclude = mklist(exclude)
74         include = mklist(include)
75 
76         if len(include) == 0:
77             attributeDict = self.__dict__
78             if self._toStringExcludePrivate():
79                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
80         else:
81             attributeDict = {k: getattr(self, k) for k in include if hasattr(self, k)}
82         d = {k: v for k, v in attributeDict.items() if k not in exclude}
83         d.update(additionalEntries)
84         return dictString(d)
85 
86     def _toStringObjectInfo(self) -> str:
","Before: 49
After: 58, 59",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,504,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 77, 'parameters': 3, '(': 18, ')': 18, ':': 14, 'block': 9, 'return_statement': 5, 'return': 5, 'attribute': 7, 'call': 15, 'argument_list': 15, '.': 7, ',': 8, 'typed_default_parameter': 2, 'type': 11, 'generic_type': 6, 'type_parameter': 6, '[': 8, ']': 8, '=': 8, 'none': 3, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 8, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 5, 'if': 7, 'comparison_operator': 6, 'is': 1, 'list': 2, '==': 2, 'assignment': 6, 'boolean_operator': 1, '>': 2, 'integer': 3, 'and': 1, 'raise_statement': 1, 'raise': 1, 'dictionary_comprehension': 3, '{': 3, 'pair': 3, 'for_in_clause': 3, 'for': 3, 'pattern_list': 2, 'in': 3, 'if_clause': 2, 'not_operator': 1, 'not': 1, '}': 3, 'else_clause': 1, 'else': 1, 'not in': 2}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6305277828983327,0.6257912919629509,"(tensor([0.9442]), tensor([0.9268]), tensor([0.9354]), tensor([0.9285]))"
"43     def _toStringClassName(self):
44         return type(self).__qualname__
45 
46     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
47             **additionalEntries) -> str:
48         """"""
49         Creates a string of the class attributes, with optional exclusions/inclusions/additions
50 
51         :param exclude: attributes to be excluded; can only be non-empty if include is empty
52         :param include: attributes to be included; can only be non-empty if exclude is empty
53         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
54         :return: a string containing all attribute names and values
55         """"""
56         def mklist(x):
57             if x is None:
58                 return []
59             if type(x) == str:
60                 return [x]
61             return x
62 
63         exclude = mklist(exclude)
64         include = mklist(include)
65         if len(exclude) > 0 and len(include) > 0:
66             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
67 
68         if len(include) == 0:
69             attributeDict = self.__dict__
70             if self._toStringExcludePrivate():
71                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
72         else:
73             attributeDict = {k: getattr(self, k) for k in include}
74         d = {k: v for k, v in attributeDict.items() if k not in exclude}
75         d.update(additionalEntries)
76         return dictString(d)
77 
78     def _toStringObjectInfo(self) -> str:
","52     def _toStringClassName(self):
53         return type(self).__qualname__
54 
55     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
56             **additionalEntries) -> str:
57         """"""
58         Creates a string of the class attributes, with optional exclusions/inclusions/additions.
59         Exclusions take precedence over inclusions.
60 
61         :param exclude: attributes to be excluded
62         :param include: attributes to be included; if None/empty, include all that are not excluded
63         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
64         :return: a string containing attribute names and values
65         """"""
66         def mklist(x):
67             if x is None:
68                 return []
69             if type(x) == str:
70                 return [x]
71             return x
72 
73         exclude = mklist(exclude)
74         include = mklist(include)
75 
76         if len(include) == 0:
77             attributeDict = self.__dict__
78             if self._toStringExcludePrivate():
79                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
80         else:
81             attributeDict = {k: getattr(self, k) for k in include if hasattr(self, k)}
82         d = {k: v for k, v in attributeDict.items() if k not in exclude}
83         d.update(additionalEntries)
84         return dictString(d)
85 
86     def _toStringObjectInfo(self) -> str:
","Before: 51, 52
After: 61, 62",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,531,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 77, 'parameters': 3, '(': 18, ')': 18, ':': 14, 'block': 9, 'return_statement': 5, 'return': 5, 'attribute': 7, 'call': 15, 'argument_list': 15, '.': 7, ',': 8, 'typed_default_parameter': 2, 'type': 11, 'generic_type': 6, 'type_parameter': 6, '[': 8, ']': 8, '=': 8, 'none': 3, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 8, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 5, 'if': 7, 'comparison_operator': 6, 'is': 1, 'list': 2, '==': 2, 'assignment': 6, 'boolean_operator': 1, '>': 2, 'integer': 3, 'and': 1, 'raise_statement': 1, 'raise': 1, 'dictionary_comprehension': 3, '{': 3, 'pair': 3, 'for_in_clause': 3, 'for': 3, 'pattern_list': 2, 'in': 3, 'if_clause': 2, 'not_operator': 1, 'not': 1, '}': 3, 'else_clause': 1, 'else': 1, 'not in': 2}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6305277828983327,0.6257912919629509,"(tensor([0.9442]), tensor([0.9268]), tensor([0.9354]), tensor([0.9285]))"
"43     def _toStringClassName(self):
44         return type(self).__qualname__
45 
46     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
47             **additionalEntries) -> str:
48         """"""
49         Creates a string of the class attributes, with optional exclusions/inclusions/additions
50 
51         :param exclude: attributes to be excluded; can only be non-empty if include is empty
52         :param include: attributes to be included; can only be non-empty if exclude is empty
53         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
54         :return: a string containing all attribute names and values
55         """"""
56         def mklist(x):
57             if x is None:
58                 return []
59             if type(x) == str:
60                 return [x]
61             return x
62 
63         exclude = mklist(exclude)
64         include = mklist(include)
65         if len(exclude) > 0 and len(include) > 0:
66             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
67 
68         if len(include) == 0:
69             attributeDict = self.__dict__
70             if self._toStringExcludePrivate():
71                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
72         else:
73             attributeDict = {k: getattr(self, k) for k in include}
74         d = {k: v for k, v in attributeDict.items() if k not in exclude}
75         d.update(additionalEntries)
76         return dictString(d)
77 
78     def _toStringObjectInfo(self) -> str:
","52     def _toStringClassName(self):
53         return type(self).__qualname__
54 
55     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
56             **additionalEntries) -> str:
57         """"""
58         Creates a string of the class attributes, with optional exclusions/inclusions/additions.
59         Exclusions take precedence over inclusions.
60 
61         :param exclude: attributes to be excluded
62         :param include: attributes to be included; if None/empty, include all that are not excluded
63         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
64         :return: a string containing attribute names and values
65         """"""
66         def mklist(x):
67             if x is None:
68                 return []
69             if type(x) == str:
70                 return [x]
71             return x
72 
73         exclude = mklist(exclude)
74         include = mklist(include)
75 
76         if len(include) == 0:
77             attributeDict = self.__dict__
78             if self._toStringExcludePrivate():
79                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
80         else:
81             attributeDict = {k: getattr(self, k) for k in include if hasattr(self, k)}
82         d = {k: v for k, v in attributeDict.items() if k not in exclude}
83         d.update(additionalEntries)
84         return dictString(d)
85 
86     def _toStringObjectInfo(self) -> str:
","Before: 54
After: 64",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,589,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 77, 'parameters': 3, '(': 18, ')': 18, ':': 14, 'block': 9, 'return_statement': 5, 'return': 5, 'attribute': 7, 'call': 15, 'argument_list': 15, '.': 7, ',': 8, 'typed_default_parameter': 2, 'type': 11, 'generic_type': 6, 'type_parameter': 6, '[': 8, ']': 8, '=': 8, 'none': 3, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 8, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 5, 'if': 7, 'comparison_operator': 6, 'is': 1, 'list': 2, '==': 2, 'assignment': 6, 'boolean_operator': 1, '>': 2, 'integer': 3, 'and': 1, 'raise_statement': 1, 'raise': 1, 'dictionary_comprehension': 3, '{': 3, 'pair': 3, 'for_in_clause': 3, 'for': 3, 'pattern_list': 2, 'in': 3, 'if_clause': 2, 'not_operator': 1, 'not': 1, '}': 3, 'else_clause': 1, 'else': 1, 'not in': 2}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6305277828983327,0.6257912919629509,"(tensor([0.9442]), tensor([0.9268]), tensor([0.9354]), tensor([0.9285]))"
"43     def _toStringClassName(self):
44         return type(self).__qualname__
45 
46     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
47             **additionalEntries) -> str:
48         """"""
49         Creates a string of the class attributes, with optional exclusions/inclusions/additions
50 
51         :param exclude: attributes to be excluded; can only be non-empty if include is empty
52         :param include: attributes to be included; can only be non-empty if exclude is empty
53         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
54         :return: a string containing all attribute names and values
55         """"""
56         def mklist(x):
57             if x is None:
58                 return []
59             if type(x) == str:
60                 return [x]
61             return x
62 
63         exclude = mklist(exclude)
64         include = mklist(include)
65         if len(exclude) > 0 and len(include) > 0:
66             raise ValueError(""Cannot provide inclusions and exclusions at the same time"")
67 
68         if len(include) == 0:
69             attributeDict = self.__dict__
70             if self._toStringExcludePrivate():
71                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
72         else:
73             attributeDict = {k: getattr(self, k) for k in include}
74         d = {k: v for k, v in attributeDict.items() if k not in exclude}
75         d.update(additionalEntries)
76         return dictString(d)
77 
78     def _toStringObjectInfo(self) -> str:
","52     def _toStringClassName(self):
53         return type(self).__qualname__
54 
55     def _toStringProperties(self, exclude: Optional[Union[str, Iterable[str]]] = None, include: Optional[Union[str, Iterable[str]]] = None,
56             **additionalEntries) -> str:
57         """"""
58         Creates a string of the class attributes, with optional exclusions/inclusions/additions.
59         Exclusions take precedence over inclusions.
60 
61         :param exclude: attributes to be excluded
62         :param include: attributes to be included; if None/empty, include all that are not excluded
63         :param additionalEntries: additional key-value-pairs which are added to the string just like the other attributes
64         :return: a string containing attribute names and values
65         """"""
66         def mklist(x):
67             if x is None:
68                 return []
69             if type(x) == str:
70                 return [x]
71             return x
72 
73         exclude = mklist(exclude)
74         include = mklist(include)
75 
76         if len(include) == 0:
77             attributeDict = self.__dict__
78             if self._toStringExcludePrivate():
79                 attributeDict = {k: v for k, v in attributeDict.items() if not k.startswith(""_"")}
80         else:
81             attributeDict = {k: getattr(self, k) for k in include if hasattr(self, k)}
82         d = {k: v for k, v in attributeDict.items() if k not in exclude}
83         d.update(additionalEntries)
84         return dictString(d)
85 
86     def _toStringObjectInfo(self) -> str:
","Before: 65, 66, 73
After: 81",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,576,"{'module': 1, 'function_definition': 3, 'def': 3, 'identifier': 77, 'parameters': 3, '(': 18, ')': 18, ':': 14, 'block': 9, 'return_statement': 5, 'return': 5, 'attribute': 7, 'call': 15, 'argument_list': 15, '.': 7, ',': 8, 'typed_default_parameter': 2, 'type': 11, 'generic_type': 6, 'type_parameter': 6, '[': 8, ']': 8, '=': 8, 'none': 3, 'dictionary_splat_pattern': 1, '**': 1, '->': 1, 'expression_statement': 8, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'if_statement': 5, 'if': 7, 'comparison_operator': 6, 'is': 1, 'list': 2, '==': 2, 'assignment': 6, 'boolean_operator': 1, '>': 2, 'integer': 3, 'and': 1, 'raise_statement': 1, 'raise': 1, 'dictionary_comprehension': 3, '{': 3, 'pair': 3, 'for_in_clause': 3, 'for': 3, 'pattern_list': 2, 'in': 3, 'if_clause': 2, 'not_operator': 1, 'not': 1, '}': 3, 'else_clause': 1, 'else': 1, 'not in': 2}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6305277828983327,0.6257912919629509,"(tensor([0.9442]), tensor([0.9268]), tensor([0.9354]), tensor([0.9285]))"
"85         """"""
86         return self._toStringProperties(exclude=self._toStringExcludes(), include=self._toStringIncludes(), **self._toStringAdditionalEntries())
87 
88     def _toStringExcludes(self) -> List[str]:
89         """"""
90         Returns a list of attribute names to be excluded from __str__ and __repr__. This method can be overwritten by
91         sub-classes which can call super to extend this list. This method will only have an effect if _toStringObjectInfo
92         is not overwritten by the sub class.
93 
94         :return: a list of attribute names
95         """"""
96         return []
97 
98     def _toStringIncludes(self) -> List[str]:
","93         """"""
94         return self._toStringProperties(exclude=self._toStringExcludes(), include=self._toStringIncludes(), **self._toStringAdditionalEntries())
95 
96     def _toStringExcludes(self) -> List[str]:
97         """"""
98         Returns a list of attribute names to be excluded from __str__ and __repr__. This method can be overwritten by
99         sub-classes which can call super and extend the list returned.
100         This method will only have no effect if _toStringObjectInfo is overridden to not use its result.
101 
102         :return: a list of attribute names
103         """"""
104         return []
105 
106     def _toStringIncludes(self) -> List[str]:
","Before: 91, 92
After: 99, 100",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,871,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 8, 'identifier': 45, 'boolean_operator': 1, 'and': 1, 'attribute': 3, '.': 3, 'binary_operator': 1, '-': 1, 'if': 1, 'is': 1, 'not': 1, 'class': 1, ':': 2, 'return': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6373573827088249,0.6264134366354682,"(tensor([0.9557]), tensor([0.9617]), tensor([0.9587]), tensor([0.9611]))"
"95         """"""
96         return []
97 
98     def _toStringIncludes(self) -> List[str]:
99         """"""
100         Returns a list of attribute names to be included in __str__ and __repr__. This method can be overwritten by
101         sub-classes which can call super to extend this list. This method will only have an effect if _toStringObjectInfo
102         is not overwritten by the sub class.
103 
104         :return: a list of attribute names; if empty, include all attributes (except the ones being excluded according to other methods)
105         """"""
106         return []
107 
108     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
","103         """"""
104         return []
105 
106     def _toStringIncludes(self) -> List[str]:
107         """"""
108         Returns a list of attribute names to be included in __str__ and __repr__. This method can be overwritten by
109         sub-classes which can call super and extend the list returned.
110         This method will only have no effect if _toStringObjectInfo is overridden to not use its result.
111 
112         :return: a list of attribute names; if empty, include all attributes (except the ones being excluded according to other methods)
113         """"""
114         return []
115 
116     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
","Before: 101, 102
After: 109, 110",update string.py for python 3,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,900,"{'module': 1, 'expression_statement': 4, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 10, 'identifier': 58, 'boolean_operator': 1, 'comparison_operator': 2, 'in': 1, 'and': 1, 'attribute': 3, '.': 3, 'binary_operator': 1, '-': 1, 'if': 1, 'is not': 2, 'class': 1, ':': 2, 'return': 2, ';': 1, ',': 1, 'call': 1, 'argument_list': 1, '(': 1, ')': 1, 'return_statement': 1, 'list': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 27, 'name': 'dictString', 'long_name': 'dictString( d : Dict )', 'start_line': 5, 'end_line': 6, 'full_parameters': ['d : Dict'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6555439081618226,0.6470322298551037,"(tensor([0.9524]), tensor([0.9574]), tensor([0.9549]), tensor([0.9569]))"
"275                 X = self._featureGenerator.fitGenerate(X, Y, self)
276         self._inputTransformerChain.fit(X)
277 
278     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
279         """"""
280         Fits the model using the given data
281 
282         :param X: a data frame containing input data
283         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
284             fitting, e.g. with rule-based models
285         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
286             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
287             an exception will be raised.
288         """"""
289         self._trainingContext = TrainingContext(X, Y)
290         try:
291             log.info(f""Training {self.__class__.__name__}"")
292             self._predictedVariableNames = list(Y.columns)
293             if not self._underlyingModelRequiresFitting():
294                 self._fitPreprocessors(X, Y=Y)
295             else:
296                 if Y is None:
297                     raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
298                 X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
299                 self._modelInputVariableNames = list(X.columns)
300                 inputsWithTypes = ', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])
301                 log.info(f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, ""
302                     f""inputs[{len(self._modelInputVariableNames)}]=[{inputsWithTypes}]; N={len(X)} data points"")
303                 self._fit(X, Y)
304                 self._isFitted = True
305         finally:
306             self._trainingContext = None
307 
308     def isBeingFitted(self) -> bool:
","276                 X = self._featureGenerator.fitGenerate(X, Y, self)
277         self._inputTransformerChain.fit(X)
278 
279     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
280         """"""
281         Fits the model using the given data
282 
283         :param X: a data frame containing input data
284         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
285             fitting, e.g. with rule-based models
286         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
287             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
288             an exception will be raised.
289         """"""
290         self._trainingContext = TrainingContext(X, Y)
291         try:
292             log.info(f""Fitting {self.__class__.__name__} instance"")
293             sw = StopWatch()
294             self._predictedVariableNames = list(Y.columns)
295             if not self._underlyingModelRequiresFitting():
296                 self._fitPreprocessors(X, Y=Y)
297             else:
298                 if Y is None:
299                     raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
300                 X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
301                 self._modelInputVariableNames = list(X.columns)
302                 inputsWithTypes = ', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])
303                 log.info(f""Fitting with outputs[{len(Y.columns)}]={list(Y.columns)}, ""
304                     f""inputs[{len(self._modelInputVariableNames)}]=[{inputsWithTypes}]; N={len(X)} data points"")
305                 self._fit(X, Y)
306                 self._isFitted = True
307             log.info(f""Fitting completed in {sw.getElapsedTimeSecs():.2f} seconds: {self}"")
308         finally:
309             self._trainingContext = None
310 
311     def isBeingFitted(self) -> bool:
","Before: 291
After: 292, 293",fix vector_model.py -- a/s/src/sensai/vector_model.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,1,1995,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'identifier': 90, '=': 12, 'call': 17, 'attribute': 28, '.': 28, 'argument_list': 17, '(': 18, ',': 10, ')': 18, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 1, 'true': 2, 'block': 6, 'string': 7, 'string_start': 7, 'string_content': 12, 'string_end': 7, 'try_statement': 1, 'try': 1, 'interpolation': 6, '{': 6, '}': 6, 'if_statement': 2, 'if': 2, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 1, 'else': 1, 'comparison_operator': 1, 'is': 1, 'none': 2, 'raise_statement': 1, 'raise': 1, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'concatenated_string': 1, 'finally_clause': 1, 'finally': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 34, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7123394841734854,0.7150628481103272,"(tensor([0.9586]), tensor([0.9816]), tensor([0.9700]), tensor([0.9792]))"
"275                 X = self._featureGenerator.fitGenerate(X, Y, self)
276         self._inputTransformerChain.fit(X)
277 
278     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
279         """"""
280         Fits the model using the given data
281 
282         :param X: a data frame containing input data
283         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
284             fitting, e.g. with rule-based models
285         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
286             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
287             an exception will be raised.
288         """"""
289         self._trainingContext = TrainingContext(X, Y)
290         try:
291             log.info(f""Training {self.__class__.__name__}"")
292             self._predictedVariableNames = list(Y.columns)
293             if not self._underlyingModelRequiresFitting():
294                 self._fitPreprocessors(X, Y=Y)
295             else:
296                 if Y is None:
297                     raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
298                 X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
299                 self._modelInputVariableNames = list(X.columns)
300                 inputsWithTypes = ', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])
301                 log.info(f""Training with outputs[{len(Y.columns)}]={list(Y.columns)}, ""
302                     f""inputs[{len(self._modelInputVariableNames)}]=[{inputsWithTypes}]; N={len(X)} data points"")
303                 self._fit(X, Y)
304                 self._isFitted = True
305         finally:
306             self._trainingContext = None
307 
308     def isBeingFitted(self) -> bool:
","276                 X = self._featureGenerator.fitGenerate(X, Y, self)
277         self._inputTransformerChain.fit(X)
278 
279     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
280         """"""
281         Fits the model using the given data
282 
283         :param X: a data frame containing input data
284         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
285             fitting, e.g. with rule-based models
286         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
287             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
288             an exception will be raised.
289         """"""
290         self._trainingContext = TrainingContext(X, Y)
291         try:
292             log.info(f""Fitting {self.__class__.__name__} instance"")
293             sw = StopWatch()
294             self._predictedVariableNames = list(Y.columns)
295             if not self._underlyingModelRequiresFitting():
296                 self._fitPreprocessors(X, Y=Y)
297             else:
298                 if Y is None:
299                     raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
300                 X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
301                 self._modelInputVariableNames = list(X.columns)
302                 inputsWithTypes = ', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])
303                 log.info(f""Fitting with outputs[{len(Y.columns)}]={list(Y.columns)}, ""
304                     f""inputs[{len(self._modelInputVariableNames)}]=[{inputsWithTypes}]; N={len(X)} data points"")
305                 self._fit(X, Y)
306                 self._isFitted = True
307             log.info(f""Fitting completed in {sw.getElapsedTimeSecs():.2f} seconds: {self}"")
308         finally:
309             self._trainingContext = None
310 
311     def isBeingFitted(self) -> bool:
","Before: 301
After: 303, 307",fix vector_model.py -- a/s/src/sensai/vector_model.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vector_model.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,1,2184,"{'module': 1, 'expression_statement': 14, 'assignment': 8, 'identifier': 90, '=': 12, 'call': 17, 'attribute': 28, '.': 28, 'argument_list': 17, '(': 18, ',': 10, ')': 18, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 8, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 1, 'true': 2, 'block': 6, 'string': 7, 'string_start': 7, 'string_content': 12, 'string_end': 7, 'try_statement': 1, 'try': 1, 'interpolation': 6, '{': 6, '}': 6, 'if_statement': 2, 'if': 2, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 1, 'else': 1, 'comparison_operator': 1, 'is': 1, 'none': 2, 'raise_statement': 1, 'raise': 1, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'concatenated_string': 1, 'finally_clause': 1, 'finally': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 34, 'end_line': 35, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 35, 'end_line': 36, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7123394841734854,0.7150628481103272,"(tensor([0.9586]), tensor([0.9816]), tensor([0.9700]), tensor([0.9792]))"
"1 from enum import Enum
2 from typing import Callable, Union, TypeVar, Generic, Sequence, List, Tuple, Iterable, Dict, Hashable
3 
4 import numpy as np
5 
6 T = TypeVar(""T"")
","1 from enum import Enum
2 from typing import Callable, Union, TypeVar, Generic, Sequence, List, Tuple, Iterable, Dict, Hashable, Optional
3 
4 import numpy as np
5 
6 from dcs.sensai.util.string import listString, ToStringMixin, dictString
","Before: 2
After: 2, 6, 7",add name and name to vectoriser classes,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,43,"{'module': 1, 'import_from_statement': 2, 'from': 2, 'dotted_name': 14, 'identifier': 15, 'import': 3, ',': 9, 'import_statement': 1, 'aliased_import': 1, 'as': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 14, 'end_line': 22, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.740099811048383,0.7322669646608678,"(tensor([0.9018]), tensor([0.9657]), tensor([0.9326]), tensor([0.9589]))"
"21         self.transformer = transformer
22         self._resultType = None
23 
24     def __setstate__(self, state):
25         if ""_resultType"" not in state:
26             state[""_resultType""] = None
27         self.__dict__ = state
28 
29     def fit(self, items: Iterable[T]):
","24         self._resultType = None
25         self.name = None
26 
27     def __setstate__(self, state):
28         for newOptionalProperty in [""_resultType"", ""name""]:
29             if newOptionalProperty not in state:
30                 state[newOptionalProperty] = None
31         self.__dict__ = state
32 
33     def setName(self, name):
","Before: 25, 26
After: 28, 29, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44",add name and name to vectoriser classes,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,180,"{'module': 1, 'expression_statement': 4, 'assignment': 4, 'attribute': 3, 'identifier': 13, '.': 3, '=': 4, 'none': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 1, ')': 1, ':': 2, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'not in': 2, 'subscript': 1, '[': 1, ']': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 14, 'end_line': 22, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.30837980535336307,0.29122225775132327,"(tensor([0.8214]), tensor([0.8463]), tensor([0.8337]), tensor([0.8438]))"
"85                 return cls.NUMPY_ARRAY
86             else:
87                 raise ValueError(f""Received unhandled value of type {type(y)}"")
88 
89 
90 class SequenceVectoriser(Generic[T]):
91     """"""
92     Supports the application of Vectorisers to sequences of objects of some type T, where each object of type T is
93     mapped to a vector (1D array) by the vectorisers.
94     A SequenceVectoriser is fitted by fitting the underlying Vectorisers. In order to obtain the instances of T that
","101                 return cls.NUMPY_ARRAY
102             else:
103                 raise ValueError(f""Received unhandled value of type {type(y)}"")
104 
105 
106 class SequenceVectoriser(Generic[T], ToStringMixin):
107     """"""
108     Supports the application of Vectorisers to sequences of objects of some type T, where each object of type T is
109     mapped to a vector (1D array) by the vectorisers.
110     A SequenceVectoriser is fitted by fitting the underlying Vectorisers. In order to obtain the instances of T that
","Before: 90
After: 106",add name and name to vectoriser classes,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,719,"{'module': 1, 'return_statement': 1, 'return': 1, 'attribute': 1, 'identifier': 38, '.': 2, 'expression_statement': 1, 'assignment': 1, ':': 2, 'ERROR': 5, 'type': 1, 'call': 3, 'argument_list': 4, '(': 4, 'string': 1, 'string_start': 2, 'string_content': 1, 'interpolation': 1, '{': 1, ')': 4, '}': 1, 'string_end': 1, 'class_definition': 1, 'class': 1, 'subscript': 1, '[': 1, ']': 1, ',': 1, 'is': 1, 'integer': 1, 'block': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 14, 'end_line': 22, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.753310953944511,0.7368392468086012,"(tensor([0.9545]), tensor([0.9704]), tensor([0.9624]), tensor([0.9688]))"
"202     def getAvailableVectorisers(self):
203         return list(self._factories.keys())
204 
205     def registerFactory(self, name: Hashable, factory: Callable[[Callable], Vectoriser]):
206         """"""
207         Registers a vectoriser factory which can subsequently be referenced via their name
208 
209         :param name: the name
210         :param factory: the factory, which takes the default transformer factory as an argument
211         """"""
212         if name in self._factories:
213             raise ValueError(f""Vectoriser factory for name '{name}' already registered"")
214         self._factories[name] = factory
215 
216     def getVectoriser(self, name: Hashable, defaultTransformerFactory: Callable) -> Vectoriser:
","219         self._factories: Dict[Hashable, Callable[[Callable], Vectoriser]] = {}
220 
221     def getAvailableVectorisers(self):
222         return list(self._factories.keys())
223 
224     @staticmethod
225     def _name(name: Hashable):
226         # for enums, which have .name, use the name only, because it is less problematic to persist
227         if hasattr(name, ""name""):
228             name = name.name
","Before: 205
After: 224, 225, 226, 227, 228, 229, 230, 231, 232",add name and name to vectoriser classes,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,1639,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 23, 'parameters': 2, '(': 5, ')': 5, ':': 5, 'block': 3, 'return_statement': 1, 'return': 1, 'call': 3, 'argument_list': 3, 'attribute': 4, '.': 4, ',': 3, 'typed_parameter': 2, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 3, 'list': 1, ']': 3, 'expression_statement': 2, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'in': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1, 'assignment': 1, 'subscript': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 14, 'end_line': 22, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.15193989892485824,0.1339006221652261,"(tensor([0.8147]), tensor([0.7848]), tensor([0.7995]), tensor([0.7877]))"
"202     def getAvailableVectorisers(self):
203         return list(self._factories.keys())
204 
205     def registerFactory(self, name: Hashable, factory: Callable[[Callable], Vectoriser]):
206         """"""
207         Registers a vectoriser factory which can subsequently be referenced via their name
208 
209         :param name: the name
210         :param factory: the factory, which takes the default transformer factory as an argument
211         """"""
212         if name in self._factories:
213             raise ValueError(f""Vectoriser factory for name '{name}' already registered"")
214         self._factories[name] = factory
215 
216     def getVectoriser(self, name: Hashable, defaultTransformerFactory: Callable) -> Vectoriser:
","228             name = name.name
229         return name
230 
231     def registerFactory(self, name: Hashable, factory: Callable[[Callable], Vectoriser],
232             additionalNames: Optional[Iterable[Hashable]] = None):
233         """"""
234         Registers a vectoriser factory which can subsequently be referenced via their name
235 
236         :param name: the name (which can, in particular, be a string or an enum item)
237         :param factory: the factory, which takes the default transformer factory as an argument
238         :param additionalNames: (optional) additional names under which to register the factory
239         """"""
240         self._registerFactory(name, factory)
241         if additionalNames is not None:
242             for n in additionalNames:
243                 self._registerFactory(n, factory)
244 
245     def _registerFactory(self, name: Hashable, factory):
","Before: 209
After: 236, 238, 240, 241, 242, 243, 244, 245, 246",add name and name to vectoriser classes,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,1662,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 23, 'parameters': 2, '(': 5, ')': 5, ':': 5, 'block': 3, 'return_statement': 1, 'return': 1, 'call': 3, 'argument_list': 3, 'attribute': 4, '.': 4, ',': 3, 'typed_parameter': 2, 'type': 4, 'generic_type': 1, 'type_parameter': 1, '[': 3, 'list': 1, ']': 3, 'expression_statement': 2, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'in': 1, 'raise_statement': 1, 'raise': 1, 'interpolation': 1, '{': 1, '}': 1, 'assignment': 1, 'subscript': 1, '=': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 14, 'end_line': 22, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.3810077701573318,0.34567949222142064,"(tensor([0.8341]), tensor([0.8580]), tensor([0.8459]), tensor([0.8555]))"
"213             raise ValueError(f""Vectoriser factory for name '{name}' already registered"")
214         self._factories[name] = factory
215 
216     def getVectoriser(self, name: Hashable, defaultTransformerFactory: Callable) -> Vectoriser:
217         """"""
218         Creates a vectoriser from a name, which must have been previously registered.
219 
220         :param name: the name of the generator
221         :param defaultTransformerFactory: the default transformer factory
222         :return: a new vectoriser instance
223         """"""
224         factory = self._factories.get(name)
225         if factory is None:
226             raise ValueError(f""No factory registered for name '{name}': known names: {list(self._factories.keys())}. Register the factory first."")
227         return factory(defaultTransformerFactory)
228 
229     def getVectorisers(self, names: List[Hashable], defaultTransformerFactory: Callable) -> List[Vectoriser]:
","248             raise ValueError(f""Vectoriser factory for name '{name}' already registered"")
249         self._factories[name] = factory
250 
251     def getVectoriser(self, name: Hashable, defaultTransformerFactory: Callable) -> Vectoriser:
252         """"""
253         Creates a vectoriser from a name, which must have been previously registered.
254 
255         :param name: the name (which can, in particular, be a string or an enum item)
256         :param defaultTransformerFactory: the default transformer factory
257         :return: a new vectoriser instance
258         """"""
259         name = self._name(name)
260         factory = self._factories.get(name)
261         if factory is None:
262             raise ValueError(f""No factory registered for name '{name}': known names: {listString(self._factories.keys())}. Register the factory first."")
263         instance = factory(defaultTransformerFactory)
264         instance.setName(name)
265         return instance
266 
267     def getVectorisers(self, names: List[Hashable], defaultTransformerFactory: Callable) -> List[Vectoriser]:
","Before: 220
After: 255, 259",add name and name to vectoriser classes,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,1735,"{'module': 1, 'raise_statement': 2, 'raise': 2, 'call': 6, 'identifier': 27, 'argument_list': 6, '(': 7, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 3, '{': 3, '}': 3, 'string_end': 3, ')': 7, 'expression_statement': 3, 'assignment': 2, 'subscript': 1, 'attribute': 5, '.': 5, '[': 1, ']': 1, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 2, ':': 4, 'type': 3, '->': 1, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 14, 'end_line': 22, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6451047836460868,0.6295374961206255,"(tensor([0.9253]), tensor([0.9516]), tensor([0.9383]), tensor([0.9489]))"
"213             raise ValueError(f""Vectoriser factory for name '{name}' already registered"")
214         self._factories[name] = factory
215 
216     def getVectoriser(self, name: Hashable, defaultTransformerFactory: Callable) -> Vectoriser:
217         """"""
218         Creates a vectoriser from a name, which must have been previously registered.
219 
220         :param name: the name of the generator
221         :param defaultTransformerFactory: the default transformer factory
222         :return: a new vectoriser instance
223         """"""
224         factory = self._factories.get(name)
225         if factory is None:
226             raise ValueError(f""No factory registered for name '{name}': known names: {list(self._factories.keys())}. Register the factory first."")
227         return factory(defaultTransformerFactory)
228 
229     def getVectorisers(self, names: List[Hashable], defaultTransformerFactory: Callable) -> List[Vectoriser]:
","248             raise ValueError(f""Vectoriser factory for name '{name}' already registered"")
249         self._factories[name] = factory
250 
251     def getVectoriser(self, name: Hashable, defaultTransformerFactory: Callable) -> Vectoriser:
252         """"""
253         Creates a vectoriser from a name, which must have been previously registered.
254 
255         :param name: the name (which can, in particular, be a string or an enum item)
256         :param defaultTransformerFactory: the default transformer factory
257         :return: a new vectoriser instance
258         """"""
259         name = self._name(name)
260         factory = self._factories.get(name)
261         if factory is None:
262             raise ValueError(f""No factory registered for name '{name}': known names: {listString(self._factories.keys())}. Register the factory first."")
263         instance = factory(defaultTransformerFactory)
264         instance.setName(name)
265         return instance
266 
267     def getVectorisers(self, names: List[Hashable], defaultTransformerFactory: Callable) -> List[Vectoriser]:
","Before: 226, 227
After: 262, 263, 264, 265",add name and name to vectoriser classes,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,836ced27e8953f149a5cc563e93da49831580be4,0e653548ff21decb0adf7599373cba380a2c4dab,0,1772,"{'module': 1, 'raise_statement': 2, 'raise': 2, 'call': 6, 'identifier': 27, 'argument_list': 6, '(': 7, 'string': 3, 'string_start': 3, 'string_content': 6, 'interpolation': 3, '{': 3, '}': 3, 'string_end': 3, ')': 7, 'expression_statement': 3, 'assignment': 2, 'subscript': 1, 'attribute': 5, '.': 5, '[': 1, ']': 1, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 2, 'typed_parameter': 2, ':': 4, 'type': 3, '->': 1, 'block': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'return_statement': 1, 'return': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 45, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 14, 'end_line': 22, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6451047836460868,0.6295374961206255,"(tensor([0.9253]), tensor([0.9516]), tensor([0.9383]), tensor([0.9489]))"
"1 from enum import Enum
2 from typing import Callable, Union, TypeVar, Generic, Sequence, List, Tuple, Iterable, Dict, Hashable, Optional
3 
4 import numpy as np
5 
6 from dcs.sensai.util.string import listString, ToStringMixin, dictString
7 
8 T = TypeVar(""T"")
9 
10 
","1 from enum import Enum
2 from typing import Callable, Union, TypeVar, Generic, Sequence, List, Tuple, Iterable, Dict, Hashable, Optional
3 
4 import numpy as np
5 
6 from .util.string import listString, ToStringMixin, dictString
7 
8 T = TypeVar(""T"")
9 
10 
","Before: 6
After: 6",fix typo in src/src/sensai/vectoriser.py,Fixed absolute import,https://github.com/opcode81/sensAI,src/sensai/vectoriser.py,430c9c69b244053dcbda5af35959507ad659e5ef,cce79e5151ded1c2a09ee4d158c9e0e04b63f205,0,72,"{'module': 1, 'import_from_statement': 3, 'from': 3, 'dotted_name': 19, 'identifier': 25, 'import': 4, ',': 12, 'import_statement': 1, 'aliased_import': 1, 'as': 1, '.': 3, 'expression_statement': 1, 'assignment': 1, '=': 1, 'call': 1, 'argument_list': 1, '(': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, ')': 1}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 5, 'token_count': 50, 'name': '__init__', 'long_name': '__init__( self , f : Callable [ [ T ] , Union [ float , np . ndarray , list ] ] , transformer = None )', 'start_line': 16, 'end_line': 25, 'full_parameters': ['self', ' f : Callable [ [ T ]', ' Union [ float', ' np . ndarray', ' list ] ]', ' transformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vectoriser.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9551190479308641,0.9511849641326403,"(tensor([0.9951]), tensor([0.9698]), tensor([0.9823]), tensor([0.9723]))"
"128         self.keys = keys
129         self.values = values
130 
131     def floorIndex(self, value) -> Optional[int]:
132         """"""
133         Finds the rightmost index where the value is less than or equal to the given value
134 
135         :param value: the value to search for
136         :return: the index or None if there is no such index
137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
","148         self.keys = keys
149         self.values = values
150 
151     def floorIndex(self, key) -> Optional[int]:
152         """"""
153         Finds the rightmost index where the key value is less than or equal to the given value
154 
155         :param key: the value to search for
156         :return: the index or None if there is no such index
157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
","Before: 131
After: 151",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,885,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 4, 'identifier': 16, '.': 4, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 2, ')': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5690289509945765,0.5629671473782727,"(tensor([0.9595]), tensor([0.9634]), tensor([0.9614]), tensor([0.9630]))"
"128         self.keys = keys
129         self.values = values
130 
131     def floorIndex(self, value) -> Optional[int]:
132         """"""
133         Finds the rightmost index where the value is less than or equal to the given value
134 
135         :param value: the value to search for
136         :return: the index or None if there is no such index
137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
","148         self.keys = keys
149         self.values = values
150 
151     def floorIndex(self, key) -> Optional[int]:
152         """"""
153         Finds the rightmost index where the key value is less than or equal to the given value
154 
155         :param key: the value to search for
156         :return: the index or None if there is no such index
157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
","Before: 133
After: 153",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,908,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 4, 'identifier': 16, '.': 4, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 2, ')': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5690289509945765,0.5629671473782727,"(tensor([0.9595]), tensor([0.9634]), tensor([0.9614]), tensor([0.9630]))"
"128         self.keys = keys
129         self.values = values
130 
131     def floorIndex(self, value) -> Optional[int]:
132         """"""
133         Finds the rightmost index where the value is less than or equal to the given value
134 
135         :param value: the value to search for
136         :return: the index or None if there is no such index
137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
","148         self.keys = keys
149         self.values = values
150 
151     def floorIndex(self, key) -> Optional[int]:
152         """"""
153         Finds the rightmost index where the key value is less than or equal to the given value
154 
155         :param key: the value to search for
156         :return: the index or None if there is no such index
157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
","Before: 135
After: 155",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,917,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 4, 'identifier': 16, '.': 4, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 2, ')': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5690289509945765,0.5629671473782727,"(tensor([0.9595]), tensor([0.9634]), tensor([0.9614]), tensor([0.9630]))"
"128         self.keys = keys
129         self.values = values
130 
131     def floorIndex(self, value) -> Optional[int]:
132         """"""
133         Finds the rightmost index where the value is less than or equal to the given value
134 
135         :param value: the value to search for
136         :return: the index or None if there is no such index
137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
","148         self.keys = keys
149         self.values = values
150 
151     def floorIndex(self, key) -> Optional[int]:
152         """"""
153         Finds the rightmost index where the key value is less than or equal to the given value
154 
155         :param key: the value to search for
156         :return: the index or None if there is no such index
157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
","Before: 138
After: 158",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,906,"{'module': 1, 'expression_statement': 3, 'assignment': 2, 'attribute': 4, 'identifier': 16, '.': 4, '=': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 2, ',': 2, ')': 2, '->': 1, 'type': 2, 'generic_type': 1, 'type_parameter': 1, '[': 1, ']': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'return_statement': 1, 'return': 1, 'call': 1, 'argument_list': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5690289509945765,0.5629671473782727,"(tensor([0.9595]), tensor([0.9634]), tensor([0.9614]), tensor([0.9630]))"
"137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
141         """"""
142         Finds the leftmost index where the value is greater than or equal to the given value
143 
144         :param value: the value to search for
145         :return: the index or None if there is no such index
146         """"""
147         return array_util.ceilIndex(self.values, value)
148 
149     def floorValue(self, key) -> Optional[TValue]:
","157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
161         """"""
162         Finds the leftmost index where the key value is greater than or equal to the given value
163 
164         :param key: the value to search for
165         :return: the index or None if there is no such index
166         """"""
167         return array_util.ceilIndex(self.keys, key)
168 
169     def closestIndex(self, key) -> Optional[int]:
","Before: 140
After: 160",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,926,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 29, 'boolean_operator': 2, 'comparison_operator': 1, 'is': 2, 'or': 2, ':': 4, 'type': 1, 'none': 1, 'if': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5347680091881887,0.5216182649459928,"(tensor([0.9496]), tensor([0.9403]), tensor([0.9449]), tensor([0.9412]))"
"137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
141         """"""
142         Finds the leftmost index where the value is greater than or equal to the given value
143 
144         :param value: the value to search for
145         :return: the index or None if there is no such index
146         """"""
147         return array_util.ceilIndex(self.values, value)
148 
149     def floorValue(self, key) -> Optional[TValue]:
","157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
161         """"""
162         Finds the leftmost index where the key value is greater than or equal to the given value
163 
164         :param key: the value to search for
165         :return: the index or None if there is no such index
166         """"""
167         return array_util.ceilIndex(self.keys, key)
168 
169     def closestIndex(self, key) -> Optional[int]:
","Before: 142
After: 162",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,949,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 29, 'boolean_operator': 2, 'comparison_operator': 1, 'is': 2, 'or': 2, ':': 4, 'type': 1, 'none': 1, 'if': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5347680091881887,0.5216182649459928,"(tensor([0.9496]), tensor([0.9403]), tensor([0.9449]), tensor([0.9412]))"
"137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
141         """"""
142         Finds the leftmost index where the value is greater than or equal to the given value
143 
144         :param value: the value to search for
145         :return: the index or None if there is no such index
146         """"""
147         return array_util.ceilIndex(self.values, value)
148 
149     def floorValue(self, key) -> Optional[TValue]:
","157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
161         """"""
162         Finds the leftmost index where the key value is greater than or equal to the given value
163 
164         :param key: the value to search for
165         :return: the index or None if there is no such index
166         """"""
167         return array_util.ceilIndex(self.keys, key)
168 
169     def closestIndex(self, key) -> Optional[int]:
","Before: 144
After: 164",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,958,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 29, 'boolean_operator': 2, 'comparison_operator': 1, 'is': 2, 'or': 2, ':': 4, 'type': 1, 'none': 1, 'if': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5347680091881887,0.5216182649459928,"(tensor([0.9496]), tensor([0.9403]), tensor([0.9449]), tensor([0.9412]))"
"137         """"""
138         return array_util.floorIndex(self.values, value)
139 
140     def ceilIndex(self, value) -> Optional[int]:
141         """"""
142         Finds the leftmost index where the value is greater than or equal to the given value
143 
144         :param value: the value to search for
145         :return: the index or None if there is no such index
146         """"""
147         return array_util.ceilIndex(self.values, value)
148 
149     def floorValue(self, key) -> Optional[TValue]:
","157         """"""
158         return array_util.floorIndex(self.keys, key)
159 
160     def ceilIndex(self, key) -> Optional[int]:
161         """"""
162         Finds the leftmost index where the key value is greater than or equal to the given value
163 
164         :param key: the value to search for
165         :return: the index or None if there is no such index
166         """"""
167         return array_util.ceilIndex(self.keys, key)
168 
169     def closestIndex(self, key) -> Optional[int]:
","Before: 147
After: 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177",fix bug in sorted_keys_and_values and datastruct.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/datastruct.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,947,"{'module': 1, 'expression_statement': 2, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'ERROR': 6, 'identifier': 29, 'boolean_operator': 2, 'comparison_operator': 1, 'is': 2, 'or': 2, ':': 4, 'type': 1, 'none': 1, 'if': 1}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 2, 'token_count': 19, 'name': 'fromBool', 'long_name': 'fromBool( cls , b : bool )', 'start_line': 17, 'end_line': 18, 'full_parameters': ['cls', ' b : bool'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/datastruct.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5347680091881887,0.5216182649459928,"(tensor([0.9496]), tensor([0.9403]), tensor([0.9449]), tensor([0.9412]))"
"34         os.makedirs(resultDir, exist_ok=True)
35         return ResultWriter(resultDir, filenamePrefix=self.filenamePrefix)
36 
37     def path(self, filenameSuffix: str, extensionToAdd=None, validOtherExtensions: Optional[Sequence[str]] = None):
38         """"""
39         :param filenameSuffix: the suffix to add (which may or may not already include the file extension ""txt"", which
40             will be added if it is not already present)
41         :param extensionToAdd: if not None, the file extension to add (without the leading ""."") unless
42             the extension to add or one of the extenions in validExtensions is already present
43         :param validOtherExtensions: a sequence of valid other extensions (without the "".""), only
44             relevant if extensionToAdd is specified
45         :return: the full path
46         """"""
47         if extensionToAdd is not None:
48             addExt = True
49             validExtensions = set(validOtherExtensions) if validOtherExtensions is not None else set()
50             validExtensions.add(extensionToAdd)
51             if validExtensions is not None:
52                 for ext in validExtensions:
53                     if filenameSuffix.endswith(""."" + ext):
54                         addExt = False
55                         break
56             if addExt:
57                 filenameSuffix += ""."" + extensionToAdd
58         path = os.path.join(self.resultDir, f""{self.filenamePrefix}{filenameSuffix}"")
59         return path
60 
61     def writeTextFile(self, filenameSuffix, content):
","34         os.makedirs(resultDir, exist_ok=True)
35         return ResultWriter(resultDir, filenamePrefix=self.filenamePrefix)
36 
37     def path(self, filenameSuffix: str, extensionToAdd=None, validOtherExtensions: Optional[Sequence[str]] = None):
38         """"""
39         :param filenameSuffix: the suffix to add (which may or may not already include a file extension)
40         :param extensionToAdd: if not None, the file extension to add (without the leading ""."") unless
41             the extension to add or one of the extenions in validExtensions is already present
42         :param validOtherExtensions: a sequence of valid other extensions (without the "".""), only
43             relevant if extensionToAdd is specified
44         :return: the full path
45         """"""
46         if extensionToAdd is not None:
47             addExt = True
48             validExtensions = set(validOtherExtensions) if validOtherExtensions is not None else set()
49             validExtensions.add(extensionToAdd)
50             if validExtensions is not None:
51                 for ext in validExtensions:
52                     if filenameSuffix.endswith(""."" + ext):
53                         addExt = False
54                         break
55             if addExt:
56                 filenameSuffix += ""."" + extensionToAdd
57         path = os.path.join(self.resultDir, f""{self.filenamePrefix}{filenameSuffix}"")
58         return path
59 
60     def writeTextFile(self, filenameSuffix, content):
","Before: 39, 40
After: 39",fix typo in src/sensai/util/io.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/io.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,296,"{'module': 1, 'expression_statement': 8, 'call': 7, 'attribute': 8, 'identifier': 48, '.': 8, 'argument_list': 7, '(': 8, ',': 6, 'keyword_argument': 2, '=': 8, 'true': 2, ')': 8, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, ':': 8, 'type': 4, 'default_parameter': 1, 'none': 5, 'typed_default_parameter': 1, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'block': 6, 'string': 4, 'string_start': 4, 'string_content': 3, 'string_end': 4, 'if_statement': 4, 'if': 5, 'comparison_operator': 3, 'is not': 6, 'assignment': 4, 'conditional_expression': 1, 'else': 1, 'for_statement': 1, 'for': 1, 'in': 1, 'binary_operator': 2, '+': 2, 'false': 1, 'break_statement': 1, 'break': 1, 'augmented_assignment': 1, '+=': 1, 'interpolation': 2, '{': 2, '}': 2}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 31, 'name': '__init__', 'long_name': '__init__( self , resultDir , filenamePrefix = """" )', 'start_line': 17, 'end_line': 20, 'full_parameters': ['self', ' resultDir', ' filenamePrefix = """"'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/io.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 31, 'name': '__init__', 'long_name': '__init__( self , resultDir , filenamePrefix = """" )', 'start_line': 17, 'end_line': 20, 'full_parameters': ['self', ' resultDir', ' filenamePrefix = """"'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/io.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7517085462402308,0.751262522748811,"(tensor([0.9808]), tensor([0.9728]), tensor([0.9768]), tensor([0.9736]))"
"126     def __str__(self):
127         return f""{self._toStringClassName()}[{self._toStringObjectInfo()}]""
128 
129     def __repr__(self):
130         info = f""id={id(self)}""
131         propertyInfo = self._toStringObjectInfo()
132         if len(propertyInfo) > 0:
133             info += "", "" + propertyInfo
134         return f""{self._toStringClassName()}[{info}]""","126     def __str__(self):
127         return f""{self._toStringClassName()}[{self._toStringObjectInfo()}]""
128 
129     def __repr__(self):
130         info = f""id={id(self)}""
131         propertyInfo = self._toStringObjectInfo()
132         if len(propertyInfo) > 0:
133             info += "", "" + propertyInfo
134         return f""{self._toStringClassName()}[{info}]""
135 
136 
","Before: 134
After: 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172",add prettystringrepr function,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/string.py,b3258a15728658aef33939aaf3fc348a69df94de,430c9c69b244053dcbda5af35959507ad659e5ef,0,1122,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 16, 'parameters': 2, '(': 7, ')': 7, ':': 3, 'block': 3, 'return_statement': 1, 'return': 1, 'string': 2, 'string_start': 2, 'interpolation': 3, '{': 3, 'call': 5, 'attribute': 3, '.': 3, 'argument_list': 5, '}': 3, 'string_content': 3, 'string_end': 2, 'expression_statement': 2, 'assignment': 2, '=': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>': 1, 'integer': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 62, 'name': 'dictString', 'long_name': 'dictString( d : Dict , brackets : Optional [ str ] = None )', 'start_line': 5, 'end_line': 10, 'full_parameters': ['d : Dict', ' brackets : Optional [ str ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/string.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9760435560183437,0.9698943973643359,"(tensor([0.9905]), tensor([0.9977]), tensor([0.9941]), tensor([0.9970]))"
"22         self.dimensionNames = list(dataFrame.columns)
23 
24     @classmethod
25     def _computeScalingParams(cls, rawArray: np.ndarray, normalisationMode: NormalisationMode):
26         """"""
27         :param rawArray: numpy array containing raw data
28         :param normalisationMode: the normalization mode (0=none, 1=by maximum in entire data set, 2=by separate maximum in each column)
29         """"""
30         translate = None
31         scale = None
32         if normalisationMode != NormalisationMode.NONE:
33             if len(rawArray.shape) != 2:
34                 raise ValueError(f""Only 2D arrays are supported by {cls.__name__} with mode {normalisationMode}"")
35             dim = rawArray.shape[1]
36             if normalisationMode == NormalisationMode.MAX_ALL:
37                 scale = np.ones(dim) * np.max(rawArray)
38             elif normalisationMode == NormalisationMode.MAX_BY_COLUMN:
39                 scale = np.ones(dim)
40                 for i in range(dim):
41                     scale[i] = np.max(np.abs(rawArray[:, i]))
42             elif normalisationMode == NormalisationMode.STANDARDISED:
43                 standardScaler = sklearn.preprocessing.StandardScaler()
44                 standardScaler.fit(rawArray)
45                 translate = standardScaler.mean_
46                 scale = standardScaler.scale_
47             else:
48                 raise Exception(""Unknown normalization mode"")
49         return scale, translate
50 
51     @staticmethod
","22         self.dimensionNames = list(dataFrame.columns)
23 
24     @classmethod
25     def _computeScalingParams(cls, rawArray: np.ndarray, normalisationMode: NormalisationMode):
26         """"""
27         :param rawArray: numpy array containing raw data
28         :param normalisationMode: the normalization mode (0=none, 1=by maximum in entire data set, 2=by separate maximum in each column)
29         """"""
30         translate = None
31         scale = None
32         if normalisationMode != NormalisationMode.NONE:
33             if len(rawArray.shape) != 2:
34                 raise ValueError(f""Only 2D arrays are supported by {cls.__name__} with mode {normalisationMode}"")
35             dim = rawArray.shape[1]
36             if normalisationMode == NormalisationMode.MAX_ALL:
37                 scale = np.ones(dim) * np.max(np.abs(rawArray))
38             elif normalisationMode == NormalisationMode.MAX_BY_COLUMN:
39                 scale = np.ones(dim)
40                 for i in range(dim):
41                     scale[i] = np.max(np.abs(rawArray[:, i]))
42             elif normalisationMode == NormalisationMode.STANDARDISED:
43                 standardScaler = sklearn.preprocessing.StandardScaler()
44                 standardScaler.fit(rawArray)
45                 translate = standardScaler.mean_
46                 scale = standardScaler.scale_
47             else:
48                 raise Exception(""Unknown normalization mode"")
49         return scale, translate
50 
51     @staticmethod
","Before: 37
After: 37",fix typo in src/src/sensai/normalisation.py,Fixed NormalisationMode.MAX_ALL not using absolute values,https://github.com/opcode81/sensAI,src/sensai/normalisation.py,f14b493c613720d964fd11517aa283dfae9225ee,d28d633e10d94425cd46477cd287b8978a987700,0,309,"{'module': 1, 'expression_statement': 12, 'assignment': 10, 'attribute': 20, 'identifier': 75, '.': 20, '=': 10, 'call': 12, 'argument_list': 12, '(': 13, ')': 13, 'decorated_definition': 1, 'decorator': 1, '@': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 4, 'typed_parameter': 2, ':': 11, 'type': 2, 'block': 8, 'string': 3, 'string_start': 3, 'string_content': 4, 'string_end': 3, 'none': 2, 'if_statement': 3, 'if': 3, 'comparison_operator': 5, '!=': 2, 'integer': 2, 'raise_statement': 2, 'raise': 2, 'interpolation': 2, '{': 2, '}': 2, 'subscript': 3, '[': 3, ']': 3, '==': 3, 'binary_operator': 1, '*': 1, 'elif_clause': 2, 'elif': 2, 'for_statement': 1, 'for': 1, 'in': 1, 'slice': 1, 'else_clause': 1, 'else': 1, 'return_statement': 1, 'return': 1, 'expression_list': 1}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 48, 'name': '__init__', 'long_name': '__init__( self , dataFrame : pd . DataFrame , normalisationMode : NormalisationMode )', 'start_line': 19, 'end_line': 22, 'full_parameters': ['self', ' dataFrame : pd . DataFrame', ' normalisationMode : NormalisationMode'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/normalisation.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 4, 'token_count': 48, 'name': '__init__', 'long_name': '__init__( self , dataFrame : pd . DataFrame , normalisationMode : NormalisationMode )', 'start_line': 19, 'end_line': 22, 'full_parameters': ['self', ' dataFrame : pd . DataFrame', ' normalisationMode : NormalisationMode'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/normalisation.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9803309472637173,0.9817661802433846,"(tensor([0.9939]), tensor([0.9944]), tensor([0.9942]), tensor([0.9943]))"
"273                 X = self._featureGenerator.fitGenerate(X, Y, self)
274         self._inputTransformerChain.fit(X)
275 
276     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
277         """"""
278         Fits the model using the given data
279 
280         :param X: a data frame containing input data
281         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
282             fitting, e.g. with rule-based models
283         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
284             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
285             an exception will be raised.
286         """"""
287         self._trainingContext = TrainingContext(X, Y)
288         try:
289             log.info(f""Fitting {self.__class__.__name__} instance"")
290             sw = StopWatch()
291             self._predictedVariableNames = list(Y.columns)
292             if not self._underlyingModelRequiresFitting():
293                 self._fitPreprocessors(X, Y=Y)
294             else:
295                 if Y is None:
296                     raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
297                 X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
298                 self._modelInputVariableNames = list(X.columns)
299                 inputsWithTypes = ', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])
300                 log.info(f""Fitting with outputs[{len(Y.columns)}]={list(Y.columns)}, ""
301                     f""inputs[{len(self._modelInputVariableNames)}]=[{inputsWithTypes}]; N={len(X)} data points"")
302                 self._fit(X, Y)
303                 self._isFitted = True
304             log.info(f""Fitting completed in {sw.getElapsedTimeSecs():.2f} seconds: {self}"")
305         finally:
306             self._trainingContext = None
307 
308     def isBeingFitted(self) -> bool:
","273                 X = self._featureGenerator.fitGenerate(X, Y, self)
274         self._inputTransformerChain.fit(X)
275 
276     def fit(self, X: pd.DataFrame, Y: Optional[pd.DataFrame], fitPreprocessors=True):
277         """"""
278         Fits the model using the given data
279 
280         :param X: a data frame containing input data
281         :param Y: a data frame containing output data. None may be passed if the underlying model does not require
282             fitting, e.g. with rule-based models
283         :param fitPreprocessors: if False, the model's feature generator and input transformers will not be fitted.
284             If a preprocessor requires fitting, was not separately fit before and this option is set to False,
285             an exception will be raised.
286         """"""
287         self._trainingContext = TrainingContext(X, Y)
288         try:
289             log.info(f""Fitting {self.__class__.__name__} instance"")
290             sw = StopWatch()
291             self._predictedVariableNames = list(Y.columns)
292             if not self._underlyingModelRequiresFitting():
293                 if fitPreprocessors:
294                     self._fitPreprocessors(X, Y=Y)
295                 self._modelInputVariableNames = None  # not known for rule-based models because the fitting process is optimised
296             else:
297                 if Y is None:
298                     raise Exception(f""The underlying model requires a data frame for fitting but Y=None was passed"")
299                 X = self._computeModelInputs(X, Y=Y, fit=fitPreprocessors)
300                 self._modelInputVariableNames = list(X.columns)
301                 inputsWithTypes = ', '.join([n + '/' + X[n].dtype.name for n in self._modelInputVariableNames])
302                 log.info(f""Fitting with outputs[{len(Y.columns)}]={list(Y.columns)}, ""
303                     f""inputs[{len(self._modelInputVariableNames)}]=[{inputsWithTypes}]; N={len(X)} data points"")
304                 self._fit(X, Y)
305                 self._isFitted = True
306             log.info(f""Fitting completed in {sw.getElapsedTimeSecs():.2f} seconds: {self}"")
307         finally:
308             self._trainingContext = None
309 
310     def isBeingFitted(self) -> bool:
","Before: 293
After: 293, 294, 295",fix vector_model and rule_based_vector_model,"Removed fit overrides in rule-based models, which changed the interface (and even semantics)",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,c90c075ac574becdb33f5e184c5d00f0c70b5d93,e0e87fbe0b154c3d5fa43716ff6657356ecff0c5,1,2064,"{'module': 1, 'expression_statement': 16, 'assignment': 9, 'identifier': 97, '=': 13, 'call': 20, 'attribute': 30, '.': 30, 'argument_list': 20, '(': 21, ',': 10, ')': 21, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 2, ':': 9, 'type': 3, 'generic_type': 1, 'type_parameter': 1, '[': 3, ']': 3, 'default_parameter': 1, 'true': 2, 'block': 6, 'string': 8, 'string_start': 8, 'string_content': 15, 'string_end': 8, 'try_statement': 1, 'try': 1, 'interpolation': 8, '{': 8, '}': 8, 'if_statement': 2, 'if': 2, 'not_operator': 1, 'not': 1, 'keyword_argument': 3, 'else_clause': 1, 'else': 1, 'comparison_operator': 1, 'is': 1, 'none': 2, 'raise_statement': 1, 'raise': 1, 'list_comprehension': 1, 'binary_operator': 2, '+': 2, 'subscript': 1, 'for_in_clause': 1, 'for': 1, 'in': 1, 'concatenated_string': 1, 'format_specifier': 1, 'finally_clause': 1, 'finally': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 36, 'end_line': 37, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 36, 'end_line': 37, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8645881637293629,0.8600669450590982,"(tensor([0.9672]), tensor([0.9759]), tensor([0.9715]), tensor([0.9750]))"
"315     def getPredictedVariableNames(self):
316         return self._predictedVariableNames
317 
318     def getModelInputVariableNames(self) -> Optional[List[str]]:
319         """"""
320         :return: the list of variable names required by the model as input (after feature generation and data frame transformation)
321             or None if the model has not been fitted.
322         """"""
323         return self._modelInputVariableNames
324 
325     def getInputTransformer(self, cls: Type[DataFrameTransformer]):
","317     def getPredictedVariableNames(self):
318         return self._predictedVariableNames
319 
320     def getModelInputVariableNames(self) -> Optional[List[str]]:
321         """"""
322         :return: the list of variable names required by the underlying model as input (after feature generation and data frame
323             transformation) or None if the model has not been fitted (or is a rule-based model which does not determine the variable names).
324         """"""
325         return self._modelInputVariableNames
326 
327     def getInputTransformer(self, cls: Type[DataFrameTransformer]):
","Before: 320, 321
After: 322, 323",fix vector_model and rule_based_vector_model,"Removed fit overrides in rule-based models, which changed the interface (and even semantics)",https://github.com/opcode81/sensAI,src/sensai/vector_model.py,c90c075ac574becdb33f5e184c5d00f0c70b5d93,e0e87fbe0b154c3d5fa43716ff6657356ecff0c5,1,2439,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 11, 'parameters': 2, '(': 2, ')': 2, ':': 2, 'block': 2, 'return_statement': 2, 'return': 2, 'attribute': 2, '.': 2, '->': 1, 'type': 3, 'generic_type': 2, 'type_parameter': 2, '[': 2, ']': 2, 'expression_statement': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 36, 'end_line': 37, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 10, 'name': '__init__', 'long_name': '__init__( self )', 'start_line': 36, 'end_line': 37, 'full_parameters': ['self'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/vector_model.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6014522546940454,0.5915389665172852,"(tensor([0.9446]), tensor([0.9759]), tensor([0.9600]), tensor([0.9727]))"
"108     def _toStringExcludes(self) -> List[str]:
109         return super()._toStringExcludes() + [""models""]
110 
111     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
112         d = super()._toStringAdditionalEntries()
113         if len(self.models) > 0:
114             d[""model[0]""] = str(next(iter(self.models.values())))
115         else:
116             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
117         return d
118 
119     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","119     def _toStringExcludes(self) -> List[str]:
120         return super()._toStringExcludes() + [""models""]
121 
122     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
123         d = super()._toStringAdditionalEntries()
124         if len(self.models) > 0:
125             d[""model[0]""] = strSkLearnModel(next(iter(self.models.values())))
126         else:
127             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
128         return d
129 
130     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 114
After: 125",update sklearn_base.py to use re.sub() instead of str(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,894,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 31, 'parameters': 2, '(': 11, ')': 11, '->': 2, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 5, ']': 5, ':': 4, 'block': 4, 'return_statement': 2, 'return': 2, 'binary_operator': 1, 'call': 9, 'attribute': 8, 'argument_list': 9, '.': 8, '+': 1, 'list': 1, 'string': 4, 'string_start': 4, 'string_content': 3, 'string_end': 4, ',': 1, 'expression_statement': 3, 'assignment': 3, '=': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '>': 1, 'integer': 1, 'subscript': 2, 'else_clause': 1, 'else': 1, 'interpolation': 2, '{': 2, '}': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 16, 'end_line': 20, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7714366999566914,0.7503447546190476,"(tensor([0.9623]), tensor([0.9677]), tensor([0.9650]), tensor([0.9672]))"
"143     def _toStringExcludes(self) -> List[str]:
144         return super()._toStringExcludes() + [""model""]
145 
146     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
147         d = super()._toStringAdditionalEntries()
148         if self.model is not None:
149             d[""model""] = str(self.model)
150         else:
151             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
152         return d
153 
154     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","154     def _toStringExcludes(self) -> List[str]:
155         return super()._toStringExcludes() + [""model""]
156 
157     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
158         d = super()._toStringAdditionalEntries()
159         if self.model is not None:
160             d[""model""] = strSkLearnModel(self.model)
161         else:
162             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
163         return d
164 
165     def _fitSkLearn(self, inputs: pd.DataFrame, outputs: pd.DataFrame):
","Before: 149
After: 160",update sklearn_base.py to use re.sub() instead of str(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,1292,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 27, 'parameters': 2, '(': 7, ')': 7, '->': 2, 'type': 5, 'generic_type': 2, 'type_parameter': 2, '[': 5, ']': 5, ':': 4, 'block': 4, 'return_statement': 2, 'return': 2, 'binary_operator': 1, 'call': 5, 'attribute': 7, 'argument_list': 5, '.': 7, '+': 1, 'list': 1, 'string': 4, 'string_start': 4, 'string_content': 3, 'string_end': 4, ',': 1, 'expression_statement': 3, 'assignment': 3, '=': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is not': 2, 'none': 1, 'subscript': 2, 'else_clause': 1, 'else': 1, 'interpolation': 2, '{': 2, '}': 2}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 16, 'end_line': 20, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7429010167666413,0.7228799737610347,"(tensor([0.9604]), tensor([0.9661]), tensor([0.9633]), tensor([0.9656]))"
"182         return super()._toStringExcludes() + [""modelConstructor"", ""sklearnInputTransformer"", ""sklearnOutputTransformer"",
183                                               ""modelArgs"", ""model""]
184 
185     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
186         d = super()._toStringAdditionalEntries()
187         if self.model is None:
188             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
189         else:
190             d[""model""] = str(self.model)
191         return d
192 
193     def withSkLearnInputTransformer(self, sklearnInputTransformer) -> __qualname__:
","193         return super()._toStringExcludes() + [""modelConstructor"", ""sklearnInputTransformer"", ""sklearnOutputTransformer"",
194                                               ""modelArgs"", ""model""]
195 
196     def _toStringAdditionalEntries(self) -> Dict[str, Any]:
197         d = super()._toStringAdditionalEntries()
198         if self.model is None:
199             d[""modelConstructor""] = f""{self.modelConstructor.__name__}{self.modelArgs}""
200         else:
201             d[""model""] = strSkLearnModel(self.model)
202         return d
203 
204     def withSkLearnInputTransformer(self, sklearnInputTransformer) -> __qualname__:
","Before: 190
After: 201",update sklearn_base.py to use re.sub() instead of str(),Sync dcs,https://github.com/opcode81/sensAI,src/sensai/sklearn/sklearn_base.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,1791,"{'module': 1, 'return_statement': 2, 'return': 2, 'binary_operator': 1, 'call': 5, 'attribute': 7, 'identifier': 23, 'argument_list': 5, '(': 6, ')': 6, '.': 7, '+': 1, 'list': 1, '[': 4, 'string': 8, 'string_start': 8, 'string_content': 7, 'string_end': 8, ',': 5, ']': 4, 'function_definition': 1, 'def': 1, 'parameters': 1, '->': 1, 'type': 3, 'generic_type': 1, 'type_parameter': 1, ':': 3, 'block': 3, 'expression_statement': 3, 'assignment': 3, '=': 3, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 1, 'subscript': 2, 'interpolation': 2, '{': 2, '}': 2, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 15, 'end_line': 19, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 2, 'nloc': 5, 'token_count': 40, 'name': 'createSkLearnModel', 'long_name': 'createSkLearnModel( modelConstructor , modelArgs , outputTransformer = None )', 'start_line': 16, 'end_line': 20, 'full_parameters': ['modelConstructor', ' modelArgs', ' outputTransformer = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/sklearn/sklearn_base.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7407569799336474,0.7176918497034148,"(tensor([0.9680]), tensor([0.9706]), tensor([0.9693]), tensor([0.9703]))"
"564         pass
565 
566 
567 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
568         backend=""pickle"") -> T:
569     """"""
570     :param fn: the function whose result is to be cached
571     :param picklePath: the path in which to store the cached result
572     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
573         informative)
574     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
575         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
576         the function fn is called to compute the result and the cached result is updated.
577     :param backend: pickle or joblib
578     :return: the res (either obtained from the cache or the function)
579     """"""
580     if functionName is None:
581         functionName = fn.__name__
582 
583     def callFnAndCacheResult():
584         res = fn()
585         log.info(f""Saving cached result in {picklePath}"")
586         dumpPickle(res, picklePath, backend=backend)
587         return res
588 
589     if os.path.exists(picklePath):
590         log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
591         result = loadPickle(picklePath, backend=backend)
592         if validityCheckFn is not None:
593             if not validityCheckFn(result):
594                 log.info(f""Cached result is no longer valid, recomputing ..."")
595                 result = callFnAndCacheResult()
596         return result
597     else:
598         log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
599         return callFnAndCacheResult()
600 
601 
","564         pass
565 
566 
567 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
568         backend=""pickle"", protocol=pickle.HIGHEST_PROTOCOL) -> T:
569     """"""
570     :param fn: the function whose result is to be cached
571     :param picklePath: the path in which to store the cached result
572     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
573         informative)
574     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
575         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
576         the function fn is called to compute the result and the cached result is updated.
577     :param backend: pickle or joblib
578     :param protocol: the pickle protocol version
579     :return: the res (either obtained from the cache or the function)
580     """"""
581     if functionName is None:
582         functionName = fn.__name__
583 
584     def callFnAndCacheResult():
585         res = fn()
586         log.info(f""Saving cached result in {picklePath}"")
587         dumpPickle(res, picklePath, backend=backend, protocol=protocol)
588         return res
589 
590     if os.path.exists(picklePath):
591         log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
592         result = loadPickle(picklePath, backend=backend)
593         if validityCheckFn is not None:
594             if not validityCheckFn(result):
595                 log.info(f""Cached result is no longer valid, recomputing ..."")
596                 result = callFnAndCacheResult()
597         return result
598     else:
599         log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
600         return callFnAndCacheResult()
601 
602 
","Before: 568
After: 568",add pickle protocol support,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,4135,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 2, 'def': 2, 'identifier': 55, 'parameters': 2, '(': 13, 'typed_parameter': 1, ':': 9, 'type': 8, 'generic_type': 3, 'type_parameter': 3, '[': 5, 'list': 2, ']': 5, ',': 9, 'default_parameter': 2, '=': 9, 'none': 4, 'typed_default_parameter': 1, 'string': 6, 'string_start': 6, 'string_content': 9, 'string_end': 6, ')': 13, '->': 1, 'block': 7, 'expression_statement': 10, 'if_statement': 4, 'if': 4, 'comparison_operator': 2, 'is': 1, 'assignment': 4, 'attribute': 7, '.': 7, 'call': 11, 'argument_list': 11, 'interpolation': 5, '{': 5, '}': 5, 'keyword_argument': 2, 'return_statement': 3, 'return': 3, 'is not': 2, 'not_operator': 1, 'not': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8332808657125746,0.8334189951028145,"(tensor([0.9637]), tensor([0.9804]), tensor([0.9720]), tensor([0.9787]))"
"564         pass
565 
566 
567 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
568         backend=""pickle"") -> T:
569     """"""
570     :param fn: the function whose result is to be cached
571     :param picklePath: the path in which to store the cached result
572     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
573         informative)
574     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
575         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
576         the function fn is called to compute the result and the cached result is updated.
577     :param backend: pickle or joblib
578     :return: the res (either obtained from the cache or the function)
579     """"""
580     if functionName is None:
581         functionName = fn.__name__
582 
583     def callFnAndCacheResult():
584         res = fn()
585         log.info(f""Saving cached result in {picklePath}"")
586         dumpPickle(res, picklePath, backend=backend)
587         return res
588 
589     if os.path.exists(picklePath):
590         log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
591         result = loadPickle(picklePath, backend=backend)
592         if validityCheckFn is not None:
593             if not validityCheckFn(result):
594                 log.info(f""Cached result is no longer valid, recomputing ..."")
595                 result = callFnAndCacheResult()
596         return result
597     else:
598         log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
599         return callFnAndCacheResult()
600 
601 
","564         pass
565 
566 
567 def cached(fn: Callable[[], T], picklePath, functionName=None, validityCheckFn: Optional[Callable[[T], bool]] = None,
568         backend=""pickle"", protocol=pickle.HIGHEST_PROTOCOL) -> T:
569     """"""
570     :param fn: the function whose result is to be cached
571     :param picklePath: the path in which to store the cached result
572     :param functionName: the name of the function fn (for the case where its __name__ attribute is not
573         informative)
574     :param validityCheckFn: an optional function to call in order to check whether a cached result is still valid;
575         the function shall return True if the res is still valid and false otherwise. If a cached result is invalid,
576         the function fn is called to compute the result and the cached result is updated.
577     :param backend: pickle or joblib
578     :param protocol: the pickle protocol version
579     :return: the res (either obtained from the cache or the function)
580     """"""
581     if functionName is None:
582         functionName = fn.__name__
583 
584     def callFnAndCacheResult():
585         res = fn()
586         log.info(f""Saving cached result in {picklePath}"")
587         dumpPickle(res, picklePath, backend=backend, protocol=protocol)
588         return res
589 
590     if os.path.exists(picklePath):
591         log.info(f""Loading cached result of function '{functionName}' from {picklePath}"")
592         result = loadPickle(picklePath, backend=backend)
593         if validityCheckFn is not None:
594             if not validityCheckFn(result):
595                 log.info(f""Cached result is no longer valid, recomputing ..."")
596                 result = callFnAndCacheResult()
597         return result
598     else:
599         log.info(f""No cached result found in {picklePath}, calling function '{functionName}' ..."")
600         return callFnAndCacheResult()
601 
602 
","Before: 586
After: 587",add pickle protocol support,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,4204,"{'module': 1, 'pass_statement': 1, 'pass': 1, 'function_definition': 2, 'def': 2, 'identifier': 55, 'parameters': 2, '(': 13, 'typed_parameter': 1, ':': 9, 'type': 8, 'generic_type': 3, 'type_parameter': 3, '[': 5, 'list': 2, ']': 5, ',': 9, 'default_parameter': 2, '=': 9, 'none': 4, 'typed_default_parameter': 1, 'string': 6, 'string_start': 6, 'string_content': 9, 'string_end': 6, ')': 13, '->': 1, 'block': 7, 'expression_statement': 10, 'if_statement': 4, 'if': 4, 'comparison_operator': 2, 'is': 1, 'assignment': 4, 'attribute': 7, '.': 7, 'call': 11, 'argument_list': 11, 'interpolation': 5, '{': 5, '}': 5, 'keyword_argument': 2, 'return_statement': 3, 'return': 3, 'is not': 2, 'not_operator': 1, 'not': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.8332808657125746,0.8334189951028145,"(tensor([0.9637]), tensor([0.9804]), tensor([0.9720]), tensor([0.9787]))"
"603     """"""
604     Function decorator for caching function results via pickle
605     """"""
606     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None, backend=""pickle""):
607         """"""
608         :param cacheBasePath: the directory where the pickle cache file will be stored
609         :param filenamePrefix: a prefix of the name of the cache file to be created, to which the function name and, where applicable,
610             a hash code of the function arguments will be appended and "".cache.pickle"" will be appended; if None, use """" (if filename
611             has not been provided)
612         :param filename: the full file name of the cache file to be created; this is admissible only if the function has no arguments
613         """"""
614         self.filename = filename
615         self.cacheBasePath = cacheBasePath
616         self.filenamePrefix = filenamePrefix
617         self.backend = backend
618 
619         if self.filenamePrefix is None:
620             self.filenamePrefix = """"
621         else:
622             self.filenamePrefix += ""-""
623 
624     def __call__(self, fn: Callable, *_args, **_kwargs):
","604     """"""
605     Function decorator for caching function results via pickle
606     """"""
607     def __init__(self, cacheBasePath: str, filenamePrefix: str = None, filename: str = None, backend=""pickle"",
608             protocol=pickle.HIGHEST_PROTOCOL):
609         """"""
610         :param cacheBasePath: the directory where the pickle cache file will be stored
611         :param filenamePrefix: a prefix of the name of the cache file to be created, to which the function name and, where applicable,
612             a hash code of the function arguments will be appended and "".cache.pickle"" will be appended; if None, use """" (if filename
613             has not been provided)
614         :param filename: the full file name of the cache file to be created; this is admissible only if the function has no arguments
615         :param backend: the serialisation backend to use (see dumpPickle)
616         :param protocol: the pickle protocol version to use
617         """"""
618         self.filename = filename
619         self.cacheBasePath = cacheBasePath
620         self.filenamePrefix = filenamePrefix
621         self.backend = backend
622         self.protocol = protocol
623 
624         if self.filenamePrefix is None:
625             self.filenamePrefix = """"
626         else:
627             self.filenamePrefix += ""-""
628 
629     def __call__(self, fn: Callable, *_args, **_kwargs):
","Before: 606
After: 607, 608, 615, 616, 622",add pickle protocol support,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/cache.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,4393,"{'module': 1, 'expression_statement': 8, 'string': 5, 'string_start': 5, 'string_content': 4, 'string_end': 5, 'function_definition': 1, 'def': 1, 'identifier': 27, 'parameters': 1, '(': 1, ',': 4, 'typed_parameter': 1, ':': 6, 'type': 3, 'typed_default_parameter': 2, '=': 8, 'none': 3, 'default_parameter': 1, ')': 1, 'block': 3, 'assignment': 5, 'attribute': 7, '.': 7, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'else_clause': 1, 'else': 1, 'augmented_assignment': 1, '+=': 1}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 2, 'token_count': 11, 'name': 'set', 'long_name': 'set( self , key , value )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' key', ' value'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/cache.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6706259310110319,0.6648356262993206,"(tensor([0.9279]), tensor([0.9655]), tensor([0.9463]), tensor([0.9616]))"
"82     def start(self):
83         self.startTime = time.time()
84 
85     def stop(self):
86         log.info(f""{self.name} completed in {time.time()-self.startTime} seconds"")
87 
88     def __exit__(self, exc_type, exc_value, traceback):
","82     def start(self):
83         self.startTime = time.time()
84 
85     def stop(self):
86         log.info(f""{self.name} completed in {time.time()-self.startTime:.3f} seconds"")
87 
88     def __exit__(self, exc_type, exc_value, traceback):
","Before: 86
After: 86",fix flake8 error,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/logging.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,626,"{'module': 1, 'function_definition': 2, 'def': 2, 'identifier': 16, 'parameters': 2, '(': 5, ')': 5, ':': 2, 'block': 2, 'expression_statement': 2, 'assignment': 1, 'attribute': 6, '.': 6, '=': 1, 'call': 3, 'argument_list': 3, 'string': 1, 'string_start': 1, 'interpolation': 2, '{': 2, '}': 2, 'string_content': 2, 'binary_operator': 1, '-': 1, 'string_end': 1}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 70, 'name': 'configureLogging', 'long_name': 'configureLogging( format = LOG_DEFAULT_FORMAT , level = lg . DEBUG )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['format = LOG_DEFAULT_FORMAT', ' level = lg . DEBUG'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/logging.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 1, 'nloc': 6, 'token_count': 70, 'name': 'configureLogging', 'long_name': 'configureLogging( format = LOG_DEFAULT_FORMAT , level = lg . DEBUG )', 'start_line': 14, 'end_line': 19, 'full_parameters': ['format = LOG_DEFAULT_FORMAT', ' level = lg . DEBUG'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/logging.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.9350761925543661,0.933100102077391,"(tensor([0.9785]), tensor([0.9974]), tensor([0.9879]), tensor([0.9955]))"
"18             raise ValueError(f""Unknown backend '{backend}'"")
19 
20 
21 def dumpPickle(obj, picklePath, backend=""pickle""):
22     dirName = os.path.dirname(picklePath)
23     if dirName != """":
24         os.makedirs(dirName, exist_ok=True)
25     with open(picklePath, ""wb"") as f:
26         if backend == ""pickle"":
27             try:
28                 pickle.dump(obj, f)
29             except AttributeError as e:
30                 failingPaths = PickleFailureDebugger.debugFailure(obj)
31                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
32         elif backend == ""joblib"":
33             joblib.dump(obj, f)
34         else:
35             raise ValueError(f""Unknown backend '{backend}'"")
36 
37 
","18             raise ValueError(f""Unknown backend '{backend}'"")
19 
20 
21 def dumpPickle(obj, picklePath, backend=""pickle"", protocol=pickle.HIGHEST_PROTOCOL):
22     dirName = os.path.dirname(picklePath)
23     if dirName != """":
24         os.makedirs(dirName, exist_ok=True)
25     with open(picklePath, ""wb"") as f:
26         if backend == ""pickle"":
27             try:
28                 pickle.dump(obj, f, protocol=protocol)
29             except AttributeError as e:
30                 failingPaths = PickleFailureDebugger.debugFailure(obj)
31                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
32         elif backend == ""joblib"":
33             joblib.dump(obj, f, protocol=protocol)
34         else:
35             raise ValueError(f""Unknown backend '{backend}'"")
36 
37 
","Before: 21
After: 21",add protocol argument to pickle.dump,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/pickle.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,165,"{'module': 1, 'raise_statement': 3, 'raise': 3, 'call': 10, 'identifier': 42, 'argument_list': 10, '(': 11, 'string': 8, 'string_start': 8, 'string_content': 11, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 8, ')': 11, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'default_parameter': 1, '=': 4, ':': 8, 'block': 8, 'expression_statement': 5, 'assignment': 2, 'attribute': 6, '.': 6, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '!=': 1, 'keyword_argument': 1, 'true': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 2, 'as': 2, 'as_pattern_target': 2, '==': 2, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'elif_clause': 1, 'elif': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 51, 'name': 'loadPickle', 'long_name': 'loadPickle( path , backend = ""pickle"" )', 'start_line': 11, 'end_line': 18, 'full_parameters': ['path', ' backend = ""pickle""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pickle.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 51, 'name': 'loadPickle', 'long_name': 'loadPickle( path , backend = ""pickle"" )', 'start_line': 11, 'end_line': 18, 'full_parameters': ['path', ' backend = ""pickle""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pickle.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.935001687950287,0.9353148223157823,"(tensor([0.9554]), tensor([0.9914]), tensor([0.9731]), tensor([0.9877]))"
"18             raise ValueError(f""Unknown backend '{backend}'"")
19 
20 
21 def dumpPickle(obj, picklePath, backend=""pickle""):
22     dirName = os.path.dirname(picklePath)
23     if dirName != """":
24         os.makedirs(dirName, exist_ok=True)
25     with open(picklePath, ""wb"") as f:
26         if backend == ""pickle"":
27             try:
28                 pickle.dump(obj, f)
29             except AttributeError as e:
30                 failingPaths = PickleFailureDebugger.debugFailure(obj)
31                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
32         elif backend == ""joblib"":
33             joblib.dump(obj, f)
34         else:
35             raise ValueError(f""Unknown backend '{backend}'"")
36 
37 
","18             raise ValueError(f""Unknown backend '{backend}'"")
19 
20 
21 def dumpPickle(obj, picklePath, backend=""pickle"", protocol=pickle.HIGHEST_PROTOCOL):
22     dirName = os.path.dirname(picklePath)
23     if dirName != """":
24         os.makedirs(dirName, exist_ok=True)
25     with open(picklePath, ""wb"") as f:
26         if backend == ""pickle"":
27             try:
28                 pickle.dump(obj, f, protocol=protocol)
29             except AttributeError as e:
30                 failingPaths = PickleFailureDebugger.debugFailure(obj)
31                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
32         elif backend == ""joblib"":
33             joblib.dump(obj, f, protocol=protocol)
34         else:
35             raise ValueError(f""Unknown backend '{backend}'"")
36 
37 
","Before: 28
After: 28",add protocol argument to pickle.dump,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/pickle.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,256,"{'module': 1, 'raise_statement': 3, 'raise': 3, 'call': 10, 'identifier': 42, 'argument_list': 10, '(': 11, 'string': 8, 'string_start': 8, 'string_content': 11, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 8, ')': 11, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'default_parameter': 1, '=': 4, ':': 8, 'block': 8, 'expression_statement': 5, 'assignment': 2, 'attribute': 6, '.': 6, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '!=': 1, 'keyword_argument': 1, 'true': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 2, 'as': 2, 'as_pattern_target': 2, '==': 2, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'elif_clause': 1, 'elif': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 51, 'name': 'loadPickle', 'long_name': 'loadPickle( path , backend = ""pickle"" )', 'start_line': 11, 'end_line': 18, 'full_parameters': ['path', ' backend = ""pickle""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pickle.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 51, 'name': 'loadPickle', 'long_name': 'loadPickle( path , backend = ""pickle"" )', 'start_line': 11, 'end_line': 18, 'full_parameters': ['path', ' backend = ""pickle""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pickle.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.935001687950287,0.9353148223157823,"(tensor([0.9554]), tensor([0.9914]), tensor([0.9731]), tensor([0.9877]))"
"18             raise ValueError(f""Unknown backend '{backend}'"")
19 
20 
21 def dumpPickle(obj, picklePath, backend=""pickle""):
22     dirName = os.path.dirname(picklePath)
23     if dirName != """":
24         os.makedirs(dirName, exist_ok=True)
25     with open(picklePath, ""wb"") as f:
26         if backend == ""pickle"":
27             try:
28                 pickle.dump(obj, f)
29             except AttributeError as e:
30                 failingPaths = PickleFailureDebugger.debugFailure(obj)
31                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
32         elif backend == ""joblib"":
33             joblib.dump(obj, f)
34         else:
35             raise ValueError(f""Unknown backend '{backend}'"")
36 
37 
","18             raise ValueError(f""Unknown backend '{backend}'"")
19 
20 
21 def dumpPickle(obj, picklePath, backend=""pickle"", protocol=pickle.HIGHEST_PROTOCOL):
22     dirName = os.path.dirname(picklePath)
23     if dirName != """":
24         os.makedirs(dirName, exist_ok=True)
25     with open(picklePath, ""wb"") as f:
26         if backend == ""pickle"":
27             try:
28                 pickle.dump(obj, f, protocol=protocol)
29             except AttributeError as e:
30                 failingPaths = PickleFailureDebugger.debugFailure(obj)
31                 raise AttributeError(f""Cannot pickle paths {failingPaths} of {obj}: {str(e)}"")
32         elif backend == ""joblib"":
33             joblib.dump(obj, f, protocol=protocol)
34         else:
35             raise ValueError(f""Unknown backend '{backend}'"")
36 
37 
","Before: 33
After: 33",add protocol argument to pickle.dump,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/util/pickle.py,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,8bba83ee5e27a0d113cd0032e6bda7620f63d8d7,0,329,"{'module': 1, 'raise_statement': 3, 'raise': 3, 'call': 10, 'identifier': 42, 'argument_list': 10, '(': 11, 'string': 8, 'string_start': 8, 'string_content': 11, 'interpolation': 5, '{': 5, '}': 5, 'string_end': 8, ')': 11, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 6, 'default_parameter': 1, '=': 4, ':': 8, 'block': 8, 'expression_statement': 5, 'assignment': 2, 'attribute': 6, '.': 6, 'if_statement': 2, 'if': 2, 'comparison_operator': 3, '!=': 1, 'keyword_argument': 1, 'true': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1, 'as_pattern': 2, 'as': 2, 'as_pattern_target': 2, '==': 2, 'try_statement': 1, 'try': 1, 'except_clause': 1, 'except': 1, 'elif_clause': 1, 'elif': 1, 'else_clause': 1, 'else': 1}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 51, 'name': 'loadPickle', 'long_name': 'loadPickle( path , backend = ""pickle"" )', 'start_line': 11, 'end_line': 18, 'full_parameters': ['path', ' backend = ""pickle""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/util/pickle.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 8, 'token_count': 51, 'name': 'loadPickle', 'long_name': 'loadPickle( path , backend = ""pickle"" )', 'start_line': 11, 'end_line': 18, 'full_parameters': ['path', ' backend = ""pickle""'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/util/pickle.py', 'top_nesting_level': 0, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.935001687950287,0.9353148223157823,"(tensor([0.9554]), tensor([0.9914]), tensor([0.9731]), tensor([0.9877]))"
"164             :return: an ordered dictionary with validation metrics
165             """"""
166             pass
167 
168 
169 class NNLossEvaluatorRegression(NNLossEvaluator):
170     """"""A loss evaluator for (multi-variate) regression.""""""
171 
172     class LossFunction(Enum):
173         L1LOSS = ""L1Loss""
","280             :return: an ordered dictionary with validation metrics
281             """"""
282             pass
283 
284 
285 class NNLossEvaluatorRegression(NNLossEvaluatorFixedDim):
286     """"""A loss evaluator for (multi-variate) regression.""""""
287 
288     class LossFunction(Enum):
289         L1LOSS = ""L1Loss""
","Before: 169
After: 285",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,1037,"{'module': 1, 'expression_statement': 2, 'assignment': 1, 'identifier': 16, ':': 3, 'type': 3, 'constrained_type': 1, 'ERROR': 3, 'call': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, 'for': 1, 'argument_list': 2, '(': 2, 'binary_operator': 1, '-': 1, ')': 2, '.': 1, 'class_definition': 1, 'class': 1, 'block': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5993925451224453,0.5530277183095058,"(tensor([0.9500]), tensor([0.9579]), tensor([0.9539]), tensor([0.9571]))"
"198             criterion = nn.SmoothL1Loss(reduction='sum')
199         else:
200             raise AssertionError(f""Loss function {self.lossFn} defined but instantiation not implemented."")
201         return criterion
202 
203     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
204         def __init__(self, cuda: bool):
205             self.total_loss_l1 = None
206             self.total_loss_l2 = None
207             self.outputDims = None
","314             criterion = nn.SmoothL1Loss(reduction='sum')
315         else:
316             raise AssertionError(f""Loss function {self.lossFn} defined but instantiation not implemented."")
317         return criterion
318 
319     class ValidationLossEvaluator(NNLossEvaluatorFixedDim.ValidationLossEvaluator):
320         def __init__(self, cuda: bool):
321             self.total_loss_l1 = None
322             self.total_loss_l2 = None
323             self.outputDims = None
","Before: 203
After: 319",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,1406,"{'module': 1, 'expression_statement': 4, 'assignment': 4, 'identifier': 21, '=': 4, 'call': 2, 'attribute': 5, '.': 5, 'argument_list': 3, '(': 4, 'keyword_argument': 1, 'string': 2, 'string_start': 2, 'string_content': 3, 'string_end': 2, ')': 4, ':': 4, 'ERROR': 1, 'type': 2, 'interpolation': 1, '{': 1, '}': 1, 'return_statement': 1, 'return': 1, 'class_definition': 1, 'class': 1, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, 'none': 2}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5984080005195894,0.5729475190976063,"(tensor([0.9642]), tensor([0.9694]), tensor([0.9668]), tensor([0.9689]))"
"264             return ""MSE""
265         else:
266             raise AssertionError(f""No selection criterion defined for loss function {self.lossFn}"")
267 
268 
269 class NNLossEvaluatorClassification(NNLossEvaluator):
270     """"""A loss evaluator for classification""""""
271 
272     class LossFunction(Enum):
273         CROSSENTROPY = ""CrossEntropy""
","380             return ""MSE""
381         else:
382             raise AssertionError(f""No selection criterion defined for loss function {self.lossFn}"")
383 
384 
385 class NNLossEvaluatorClassification(NNLossEvaluatorFixedDim):
386     """"""A loss evaluator for classification""""""
387 
388     class LossFunction(Enum):
389         CROSSENTROPY = ""CrossEntropy""
","Before: 269
After: 385",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,2314,"{'module': 1, 'return_statement': 1, 'return': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, 'expression_statement': 2, 'assignment': 1, 'identifier': 9, ':': 3, 'ERROR': 1, 'type': 1, 'call': 1, 'argument_list': 3, '(': 3, 'interpolation': 1, '{': 1, 'attribute': 1, '.': 1, '}': 1, ')': 3, 'class_definition': 2, 'class': 2, 'block': 2}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6320825314737704,0.5965010660718121,"(tensor([0.9589]), tensor([0.9670]), tensor([0.9629]), tensor([0.9662]))"
"306         return self.ValidationLossEvaluator(cuda, self.lossFn)
307 
308     def getTrainingCriterion(self):
309         return self.lossFn.createCriterion()
310 
311     class ValidationLossEvaluator(NNLossEvaluator.ValidationLossEvaluator):
312         def __init__(self, cuda: bool, lossFn: ""NNLossEvaluatorClassification.LossFunction""):
313             self.lossFn = lossFn
314             self.totalLoss = None
315             self.numValidationSamples = None
","422         return self.ValidationLossEvaluator(cuda, self.lossFn)
423 
424     def getTrainingCriterion(self):
425         return self.lossFn.createCriterion()
426 
427     class ValidationLossEvaluator(NNLossEvaluatorFixedDim.ValidationLossEvaluator):
428         def __init__(self, cuda: bool, lossFn: ""NNLossEvaluatorClassification.LossFunction""):
429             self.lossFn = lossFn
430             self.totalLoss = None
431             self.numValidationSamples = None
","Before: 311
After: 427",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,2696,"{'module': 1, 'return_statement': 2, 'return': 2, 'call': 2, 'attribute': 7, 'identifier': 23, '.': 7, 'argument_list': 3, '(': 5, ',': 3, ')': 5, 'function_definition': 2, 'def': 2, 'parameters': 2, ':': 5, 'block': 3, 'class_definition': 1, 'class': 1, 'typed_parameter': 2, 'type': 2, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'expression_statement': 2, 'assignment': 2, '=': 2, 'none': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.571690921139729,0.5310725349788701,"(tensor([0.9678]), tensor([0.9739]), tensor([0.9708]), tensor([0.9732]))"
"425 class NNOptimiser:
426     log = log.getChild(__qualname__)
427 
428     def __init__(self, params: NNOptimiserParams):
429         """"""
430         :param cuda: whether to use CUDA
431         :param lossEvaluator: the loss evaluator to use
432         :param gpu: index of the gpu to be used (if CUDA is enabled in the model to be trained)
433         :param optimiser: the optimizer to be used; defaults to ""adam""
434         :param optimiserClip: the maximum gradient norm beyond which to apply shrinkage (if useShrinkage is True)
435         :param optimiserLR: the optimiser's learning rate
436         :param batchSize: the batch size to use; for algorithms L-BFGS (optimiser='lbfgs'), which do not use batches, leave this at None.
437             If the algorithm uses batches and None is specified, batch size 64 will be used by default.
438         :param trainFraction: the fraction of the data used for training (with the remainder being used for validation).
439             If no validation is to be performed, pass 1.0.
440         :param scaledOutputs: whether to scale all outputs, resulting in computations of the loss function based on scaled values rather than normalised values.
441             Enabling scaling may not be appropriate in cases where there are multiple outputs on different scales/with completely different units.
442         :param useShrinkage: whether to apply shrinkage to gradients whose norm exceeds optimiserClip
443         :param optimiserArgs: keyword arguments to be passed on to the actual torch optimiser
444         """"""
445         if params.lossEvaluator is None:
446             raise ValueError(""Must provide a loss evaluator"")
447 
448         self.params = params
449         self.lossEvaluatorState = None
450         self.cuda = None
451 
452     def __str__(self):
","567     def __str__(self):
568         return f""{self.__class__.__name__}[params={self.params}]""
569 
570     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
571                                                    TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
572             createTorchModule=True) -> ""TrainingInfo"":
573         """"""
574         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
575 
576             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
577               the `trainFraction` parameter of this object)
578             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
579             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
580               the validation set
581 
582         :param model: the model to be fitted
583         :param data: the data to use (see variants above)
584         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
585             If False, (re-)train the existing module.
586         """"""
587         self.cuda = model.cuda
588         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
589 
590         useValidation = self.params.trainFraction != 1.0
591 
592         def toDataSetProvider(d) -> TorchDataSetProvider:
593             if isinstance(d, TorchDataSetProvider):
594                 return d
595             elif isinstance(d, DataUtil):
596                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
597             else:
598                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
599 
600         trainingLogEntries = []
601 
602         def trainingLog(s):
603             self.log.info(s)
604             trainingLogEntries.append(s)
605 
606         self._init_cuda()
607 
608         # Set the random seed manually for reproducibility.
609         seed = 42
610         torch.manual_seed(seed)
611         if self.cuda:
612             torchcuda.manual_seed_all(seed)
613         torch.backends.cudnn.benchmark = False
614         torch.backends.cudnn.deterministic = True
615 
616         # obtain data, splitting it into training and validation set(s)
617         validationSets = []
618         trainingSets = []
619         outputScalers = []
620         if type(data) != list:
621             data = [data]
622         self.log.info(""Obtaining input/output training instances"")
623         for idxDataItem, dataItem in enumerate(data):
624             if isinstance(dataItem, TorchDataSet):
625                 if useValidation:
626                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
627                 trainingSet = dataItem
628                 validationSet = None
629                 outputScaler = TensorScalerIdentity()
630             elif type(dataItem) == tuple:
631                 trainingSet, validationSet = dataItem
632                 outputScaler = TensorScalerIdentity()
633             else:
634                 dataSetProvider = toDataSetProvider(dataItem)
635                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
636                 outputScaler = dataSetProvider.getOutputTensorScaler()
637             trainingSets.append(trainingSet)
638             if validationSet is not None:
639                 validationSets.append(validationSet)
640             outputScalers.append(outputScaler)
641             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
642         trainingLog(""Number of validation sets: %d"" % len(validationSets))
643 
644         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
645         if self.cuda:
646             torchModel.cuda()
647         model.setTorchModule(torchModel)
648 
649         nParams = sum([p.nelement() for p in torchModel.parameters()])
650         self.log.info(f""Learning parameters of {model}"")
651         trainingLog('Number of parameters: %d' % nParams)
652         trainingLog(f""Starting training process via {self}"")
653 
654         lossEvaluator = self.params.lossEvaluator
655 
656         totalEpochs = None
657         best_val = 1e9
658         best_epoch = 0
659         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
660             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
661 
662         bestModelBytes = model.getModuleBytes()
663         lossEvaluation = lossEvaluator.startEvaluation(self.cuda)
664         validationMetricName = lossEvaluator.getValidationMetricName()
665         trainingLossValues = []
666         validationMetricValues = []
667         try:
668             self.log.info(f'Begin training with cuda={self.cuda}')
669             self.log.info('Press Ctrl+C to end training early')
670             for epoch in range(1, self.params.epochs + 1):
671                 lossEvaluation.startEpoch()
672                 epoch_start_time = time.time()
673 
674                 # perform training step, processing all the training data once
675                 train_loss = self._train(trainingSets, torchModel, optim, lossEvaluation, self.params.batchSize, outputScalers)
676                 trainingLossValues.append(train_loss)
677 
678                 # perform validation, computing the mean metrics across all validation sets (if more than one),
679                 # and check for new best result according to validation results
680                 isNewBest = False
681                 if useValidation:
682                     metricsSum = None
683                     metricsKeys = None
684                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
685                         metrics = self._evaluate(validationSet, torchModel, lossEvaluation, outputScaler)
686                         metricsArray = np.array(list(metrics.values()))
687                         if i == 0:
688                             metricsSum = metricsArray
689                             metricsKeys = metrics.keys()
690                         else:
691                             metricsSum += metricsArray
692                     metricsSum /= len(validationSets)  # mean results
693                     metrics = dict(zip(metricsKeys, metricsSum))
694                     current_val = metrics[lossEvaluator.getValidationMetricName()]
695                     validationMetricValues.append(current_val)
696                     isNewBest = current_val < best_val
697                     if isNewBest:
698                         best_val = current_val
699                         best_epoch = epoch
700                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
701                     else:
702                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
703                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
704                 else:
705                     valStr = """"
706                 trainingLog(
707                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
708                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
709                 totalEpochs = epoch
710                 if useValidation:
711                     if isNewBest:
712                         bestModelBytes = model.getModuleBytes()
713 
714                     # check for early stopping
715                     numEpochsWithoutImprovement = epoch - best_epoch
716                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
717                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
718                         break
719 
720             trainingLog(""Training complete"")
721         except KeyboardInterrupt:
722             trainingLog('Exiting from training early because of keyboard interrupt')
723 
724         # reload best model according to validation results
725         if useValidation:
726             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
727             self.bestEpoch = best_epoch
728             model.setModuleBytes(bestModelBytes)
729 
730         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
731             trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
732 
733     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","Before: 449, 456
After: 571",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,3731,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 19, ':': 4, 'block': 3, 'expression_statement': 5, 'assignment': 4, '=': 4, 'call': 2, 'attribute': 5, '.': 5, 'argument_list': 2, '(': 3, ')': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, ',': 1, 'typed_parameter': 1, 'type': 1, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'none': 3, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.01936957738446246,0.01810994475973774,"(tensor([0.7158]), tensor([0.8007]), tensor([0.7559]), tensor([0.7913]))"
"452     def __str__(self):
453         return f""{self.__class__.__name__}[params={self.params}]""
454 
455     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
456             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
457             createTorchModule=True) -> ""TrainingInfo"":
458         """"""
459         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
460 
461             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
462               the `trainFraction` parameter of this object)
463             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
464             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
465               the validation set
466 
467         :param model: the model to be fitted
468         :param data: the data to use (see variants above)
469         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
470             If False, (re-)train the existing module.
471         """"""
472         self.cuda = model.cuda
473         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
474 
475         useValidation = self.params.trainFraction != 1.0
476 
477         def toDataSetProvider(d) -> TorchDataSetProvider:
478             if isinstance(d, TorchDataSetProvider):
479                 return d
480             elif isinstance(d, DataUtil):
481                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
482             else:
483                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
484 
485         trainingLogEntries = []
486 
487         def trainingLog(s):
488             self.log.info(s)
489             trainingLogEntries.append(s)
490 
491         self._init_cuda()
492 
493         # Set the random seed manually for reproducibility.
494         seed = 42
495         torch.manual_seed(seed)
496         if self.cuda:
497             torchcuda.manual_seed_all(seed)
498         torch.backends.cudnn.benchmark = False
499         torch.backends.cudnn.deterministic = True
500 
501         # obtain data, splitting it into training and validation set(s)
502         validationSets = []
503         trainingSets = []
504         outputScalers = []
505         if type(data) != list:
506             data = [data]
507         self.log.info(""Obtaining input/output training instances"")
508         for idxDataItem, dataItem in enumerate(data):
509             if isinstance(dataItem, TorchDataSet):
510                 if useValidation:
511                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
512                 trainingSet = dataItem
513                 validationSet = None
514                 outputScaler = TensorScalerIdentity()
515             elif type(dataItem) == tuple:
516                 trainingSet, validationSet = dataItem
517                 outputScaler = TensorScalerIdentity()
518             else:
519                 dataSetProvider = toDataSetProvider(dataItem)
520                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
521                 outputScaler = dataSetProvider.getOutputTensorScaler()
522             trainingSets.append(trainingSet)
523             if validationSet is not None:
524                 validationSets.append(validationSet)
525             outputScalers.append(outputScaler)
526             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
527         trainingLog(""Number of validation sets: %d"" % len(validationSets))
528 
529         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
530         if self.cuda:
531             torchModel.cuda()
532         model.setTorchModule(torchModel)
533 
534         nParams = sum([p.nelement() for p in torchModel.parameters()])
535         self.log.info(f""Learning parameters of {model}"")
536         trainingLog('Number of parameters: %d' % nParams)
537         trainingLog(f""Starting training process via {self}"")
538 
539         lossEvaluator = self.params.lossEvaluator
540         criterion = lossEvaluator.getTrainingCriterion()
541 
542         if self.cuda:
543             criterion = criterion.cuda()
544 
545         totalEpochs = None
546         best_val = 1e9
547         best_epoch = 0
548         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
549             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
550 
551         bestModelBytes = model.getModuleBytes()
552         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
553         validationMetricName = lossEvaluator.getValidationMetricName()
554         trainingLossValues = []
555         validationMetricValues = []
556         try:
557             self.log.info(f'Begin training with cuda={self.cuda}')
558             self.log.info('Press Ctrl+C to end training early')
559             for epoch in range(1, self.params.epochs + 1):
560                 epoch_start_time = time.time()
561 
562                 # perform training step, processing all the training data once
563                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
564                 trainingLossValues.append(train_loss)
565 
566                 # perform validation, computing the mean metrics across all validation sets (if more than one),
567                 # and check for new best result according to validation results
568                 isNewBest = False
569                 if useValidation:
570                     metricsSum = None
571                     metricsKeys = None
572                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
573                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
574                         metricsArray = np.array(list(metrics.values()))
575                         if i == 0:
576                             metricsSum = metricsArray
577                             metricsKeys = metrics.keys()
578                         else:
579                             metricsSum += metricsArray
580                     metricsSum /= len(validationSets)  # mean results
581                     metrics = dict(zip(metricsKeys, metricsSum))
582                     current_val = metrics[lossEvaluator.getValidationMetricName()]
583                     validationMetricValues.append(current_val)
584                     isNewBest = current_val < best_val
585                     if isNewBest:
586                         best_val = current_val
587                         best_epoch = epoch
588                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
589                     else:
590                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
591                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
592                 else:
593                     valStr = """"
594                 trainingLog(
595                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
596                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
597                 totalEpochs = epoch
598                 if useValidation:
599                     if isNewBest:
600                         bestModelBytes = model.getModuleBytes()
601 
602                     # check for early stopping
603                     numEpochsWithoutImprovement = epoch - best_epoch
604                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
605                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
606                         break
607 
608             trainingLog(""Training complete"")
609         except KeyboardInterrupt:
610             trainingLog('Exiting from training early because of keyboard interrupt')
611 
612         # reload best model according to validation results
613         if useValidation:
614             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
615             self.bestEpoch = best_epoch
616             model.setModuleBytes(bestModelBytes)
617 
618         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
619                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
620 
621     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","567     def __str__(self):
568         return f""{self.__class__.__name__}[params={self.params}]""
569 
570     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
571                                                    TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
572             createTorchModule=True) -> ""TrainingInfo"":
573         """"""
574         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
575 
576             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
577               the `trainFraction` parameter of this object)
578             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
579             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
580               the validation set
581 
582         :param model: the model to be fitted
583         :param data: the data to use (see variants above)
584         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
585             If False, (re-)train the existing module.
586         """"""
587         self.cuda = model.cuda
588         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
589 
590         useValidation = self.params.trainFraction != 1.0
591 
592         def toDataSetProvider(d) -> TorchDataSetProvider:
593             if isinstance(d, TorchDataSetProvider):
594                 return d
595             elif isinstance(d, DataUtil):
596                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
597             else:
598                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
599 
600         trainingLogEntries = []
601 
602         def trainingLog(s):
603             self.log.info(s)
604             trainingLogEntries.append(s)
605 
606         self._init_cuda()
607 
608         # Set the random seed manually for reproducibility.
609         seed = 42
610         torch.manual_seed(seed)
611         if self.cuda:
612             torchcuda.manual_seed_all(seed)
613         torch.backends.cudnn.benchmark = False
614         torch.backends.cudnn.deterministic = True
615 
616         # obtain data, splitting it into training and validation set(s)
617         validationSets = []
618         trainingSets = []
619         outputScalers = []
620         if type(data) != list:
621             data = [data]
622         self.log.info(""Obtaining input/output training instances"")
623         for idxDataItem, dataItem in enumerate(data):
624             if isinstance(dataItem, TorchDataSet):
625                 if useValidation:
626                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
627                 trainingSet = dataItem
628                 validationSet = None
629                 outputScaler = TensorScalerIdentity()
630             elif type(dataItem) == tuple:
631                 trainingSet, validationSet = dataItem
632                 outputScaler = TensorScalerIdentity()
633             else:
634                 dataSetProvider = toDataSetProvider(dataItem)
635                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
636                 outputScaler = dataSetProvider.getOutputTensorScaler()
637             trainingSets.append(trainingSet)
638             if validationSet is not None:
639                 validationSets.append(validationSet)
640             outputScalers.append(outputScaler)
641             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
642         trainingLog(""Number of validation sets: %d"" % len(validationSets))
643 
644         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
645         if self.cuda:
646             torchModel.cuda()
647         model.setTorchModule(torchModel)
648 
649         nParams = sum([p.nelement() for p in torchModel.parameters()])
650         self.log.info(f""Learning parameters of {model}"")
651         trainingLog('Number of parameters: %d' % nParams)
652         trainingLog(f""Starting training process via {self}"")
653 
654         lossEvaluator = self.params.lossEvaluator
655 
656         totalEpochs = None
657         best_val = 1e9
658         best_epoch = 0
659         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
660             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
661 
662         bestModelBytes = model.getModuleBytes()
663         lossEvaluation = lossEvaluator.startEvaluation(self.cuda)
664         validationMetricName = lossEvaluator.getValidationMetricName()
665         trainingLossValues = []
666         validationMetricValues = []
667         try:
668             self.log.info(f'Begin training with cuda={self.cuda}')
669             self.log.info('Press Ctrl+C to end training early')
670             for epoch in range(1, self.params.epochs + 1):
671                 lossEvaluation.startEpoch()
672                 epoch_start_time = time.time()
673 
674                 # perform training step, processing all the training data once
675                 train_loss = self._train(trainingSets, torchModel, optim, lossEvaluation, self.params.batchSize, outputScalers)
676                 trainingLossValues.append(train_loss)
677 
678                 # perform validation, computing the mean metrics across all validation sets (if more than one),
679                 # and check for new best result according to validation results
680                 isNewBest = False
681                 if useValidation:
682                     metricsSum = None
683                     metricsKeys = None
684                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
685                         metrics = self._evaluate(validationSet, torchModel, lossEvaluation, outputScaler)
686                         metricsArray = np.array(list(metrics.values()))
687                         if i == 0:
688                             metricsSum = metricsArray
689                             metricsKeys = metrics.keys()
690                         else:
691                             metricsSum += metricsArray
692                     metricsSum /= len(validationSets)  # mean results
693                     metrics = dict(zip(metricsKeys, metricsSum))
694                     current_val = metrics[lossEvaluator.getValidationMetricName()]
695                     validationMetricValues.append(current_val)
696                     isNewBest = current_val < best_val
697                     if isNewBest:
698                         best_val = current_val
699                         best_epoch = epoch
700                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
701                     else:
702                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
703                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
704                 else:
705                     valStr = """"
706                 trainingLog(
707                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
708                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
709                 totalEpochs = epoch
710                 if useValidation:
711                     if isNewBest:
712                         bestModelBytes = model.getModuleBytes()
713 
714                     # check for early stopping
715                     numEpochsWithoutImprovement = epoch - best_epoch
716                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
717                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
718                         break
719 
720             trainingLog(""Training complete"")
721         except KeyboardInterrupt:
722             trainingLog('Exiting from training early because of keyboard interrupt')
723 
724         # reload best model according to validation results
725         if useValidation:
726             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
727             self.bestEpoch = best_epoch
728             model.setModuleBytes(bestModelBytes)
729 
730         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
731             trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
732 
733     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","Before: 552
After: 663",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,4729,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 396, 'parameters': 4, '(': 85, ')': 85, ':': 33, 'block': 31, 'return_statement': 4, 'return': 4, 'string': 27, 'string_start': 27, 'interpolation': 19, '{': 19, 'attribute': 99, '.': 99, '}': 19, 'string_content': 37, 'string_end': 27, ',': 48, 'typed_parameter': 2, 'type': 20, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 64, 'true': 2, '->': 2, 'expression_statement': 84, 'assignment': 54, 'call': 79, 'argument_list': 79, 'comparison_operator': 9, '!=': 2, 'float': 2, 'if_statement': 15, 'if': 18, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 8, 'raise_statement': 2, 'raise': 2, 'list': 7, 'comment': 8, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, 'none': 8, '==': 2, 'is not': 6, 'binary_operator': 7, '+': 2, 'conditional_expression': 3, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 9, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 2, 'boolean_operator': 1, 'and': 1, '>=': 1, 'break_statement': 1, 'break': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7203820842364708,0.7119131566015755,"(tensor([0.9570]), tensor([0.9576]), tensor([0.9573]), tensor([0.9576]))"
"452     def __str__(self):
453         return f""{self.__class__.__name__}[params={self.params}]""
454 
455     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
456             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
457             createTorchModule=True) -> ""TrainingInfo"":
458         """"""
459         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
460 
461             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
462               the `trainFraction` parameter of this object)
463             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
464             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
465               the validation set
466 
467         :param model: the model to be fitted
468         :param data: the data to use (see variants above)
469         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
470             If False, (re-)train the existing module.
471         """"""
472         self.cuda = model.cuda
473         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
474 
475         useValidation = self.params.trainFraction != 1.0
476 
477         def toDataSetProvider(d) -> TorchDataSetProvider:
478             if isinstance(d, TorchDataSetProvider):
479                 return d
480             elif isinstance(d, DataUtil):
481                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
482             else:
483                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
484 
485         trainingLogEntries = []
486 
487         def trainingLog(s):
488             self.log.info(s)
489             trainingLogEntries.append(s)
490 
491         self._init_cuda()
492 
493         # Set the random seed manually for reproducibility.
494         seed = 42
495         torch.manual_seed(seed)
496         if self.cuda:
497             torchcuda.manual_seed_all(seed)
498         torch.backends.cudnn.benchmark = False
499         torch.backends.cudnn.deterministic = True
500 
501         # obtain data, splitting it into training and validation set(s)
502         validationSets = []
503         trainingSets = []
504         outputScalers = []
505         if type(data) != list:
506             data = [data]
507         self.log.info(""Obtaining input/output training instances"")
508         for idxDataItem, dataItem in enumerate(data):
509             if isinstance(dataItem, TorchDataSet):
510                 if useValidation:
511                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
512                 trainingSet = dataItem
513                 validationSet = None
514                 outputScaler = TensorScalerIdentity()
515             elif type(dataItem) == tuple:
516                 trainingSet, validationSet = dataItem
517                 outputScaler = TensorScalerIdentity()
518             else:
519                 dataSetProvider = toDataSetProvider(dataItem)
520                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
521                 outputScaler = dataSetProvider.getOutputTensorScaler()
522             trainingSets.append(trainingSet)
523             if validationSet is not None:
524                 validationSets.append(validationSet)
525             outputScalers.append(outputScaler)
526             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
527         trainingLog(""Number of validation sets: %d"" % len(validationSets))
528 
529         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
530         if self.cuda:
531             torchModel.cuda()
532         model.setTorchModule(torchModel)
533 
534         nParams = sum([p.nelement() for p in torchModel.parameters()])
535         self.log.info(f""Learning parameters of {model}"")
536         trainingLog('Number of parameters: %d' % nParams)
537         trainingLog(f""Starting training process via {self}"")
538 
539         lossEvaluator = self.params.lossEvaluator
540         criterion = lossEvaluator.getTrainingCriterion()
541 
542         if self.cuda:
543             criterion = criterion.cuda()
544 
545         totalEpochs = None
546         best_val = 1e9
547         best_epoch = 0
548         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
549             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
550 
551         bestModelBytes = model.getModuleBytes()
552         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
553         validationMetricName = lossEvaluator.getValidationMetricName()
554         trainingLossValues = []
555         validationMetricValues = []
556         try:
557             self.log.info(f'Begin training with cuda={self.cuda}')
558             self.log.info('Press Ctrl+C to end training early')
559             for epoch in range(1, self.params.epochs + 1):
560                 epoch_start_time = time.time()
561 
562                 # perform training step, processing all the training data once
563                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
564                 trainingLossValues.append(train_loss)
565 
566                 # perform validation, computing the mean metrics across all validation sets (if more than one),
567                 # and check for new best result according to validation results
568                 isNewBest = False
569                 if useValidation:
570                     metricsSum = None
571                     metricsKeys = None
572                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
573                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
574                         metricsArray = np.array(list(metrics.values()))
575                         if i == 0:
576                             metricsSum = metricsArray
577                             metricsKeys = metrics.keys()
578                         else:
579                             metricsSum += metricsArray
580                     metricsSum /= len(validationSets)  # mean results
581                     metrics = dict(zip(metricsKeys, metricsSum))
582                     current_val = metrics[lossEvaluator.getValidationMetricName()]
583                     validationMetricValues.append(current_val)
584                     isNewBest = current_val < best_val
585                     if isNewBest:
586                         best_val = current_val
587                         best_epoch = epoch
588                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
589                     else:
590                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
591                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
592                 else:
593                     valStr = """"
594                 trainingLog(
595                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
596                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
597                 totalEpochs = epoch
598                 if useValidation:
599                     if isNewBest:
600                         bestModelBytes = model.getModuleBytes()
601 
602                     # check for early stopping
603                     numEpochsWithoutImprovement = epoch - best_epoch
604                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
605                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
606                         break
607 
608             trainingLog(""Training complete"")
609         except KeyboardInterrupt:
610             trainingLog('Exiting from training early because of keyboard interrupt')
611 
612         # reload best model according to validation results
613         if useValidation:
614             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
615             self.bestEpoch = best_epoch
616             model.setModuleBytes(bestModelBytes)
617 
618         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
619                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
620 
621     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","567     def __str__(self):
568         return f""{self.__class__.__name__}[params={self.params}]""
569 
570     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
571                                                    TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
572             createTorchModule=True) -> ""TrainingInfo"":
573         """"""
574         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
575 
576             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
577               the `trainFraction` parameter of this object)
578             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
579             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
580               the validation set
581 
582         :param model: the model to be fitted
583         :param data: the data to use (see variants above)
584         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
585             If False, (re-)train the existing module.
586         """"""
587         self.cuda = model.cuda
588         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
589 
590         useValidation = self.params.trainFraction != 1.0
591 
592         def toDataSetProvider(d) -> TorchDataSetProvider:
593             if isinstance(d, TorchDataSetProvider):
594                 return d
595             elif isinstance(d, DataUtil):
596                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
597             else:
598                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
599 
600         trainingLogEntries = []
601 
602         def trainingLog(s):
603             self.log.info(s)
604             trainingLogEntries.append(s)
605 
606         self._init_cuda()
607 
608         # Set the random seed manually for reproducibility.
609         seed = 42
610         torch.manual_seed(seed)
611         if self.cuda:
612             torchcuda.manual_seed_all(seed)
613         torch.backends.cudnn.benchmark = False
614         torch.backends.cudnn.deterministic = True
615 
616         # obtain data, splitting it into training and validation set(s)
617         validationSets = []
618         trainingSets = []
619         outputScalers = []
620         if type(data) != list:
621             data = [data]
622         self.log.info(""Obtaining input/output training instances"")
623         for idxDataItem, dataItem in enumerate(data):
624             if isinstance(dataItem, TorchDataSet):
625                 if useValidation:
626                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
627                 trainingSet = dataItem
628                 validationSet = None
629                 outputScaler = TensorScalerIdentity()
630             elif type(dataItem) == tuple:
631                 trainingSet, validationSet = dataItem
632                 outputScaler = TensorScalerIdentity()
633             else:
634                 dataSetProvider = toDataSetProvider(dataItem)
635                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
636                 outputScaler = dataSetProvider.getOutputTensorScaler()
637             trainingSets.append(trainingSet)
638             if validationSet is not None:
639                 validationSets.append(validationSet)
640             outputScalers.append(outputScaler)
641             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
642         trainingLog(""Number of validation sets: %d"" % len(validationSets))
643 
644         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
645         if self.cuda:
646             torchModel.cuda()
647         model.setTorchModule(torchModel)
648 
649         nParams = sum([p.nelement() for p in torchModel.parameters()])
650         self.log.info(f""Learning parameters of {model}"")
651         trainingLog('Number of parameters: %d' % nParams)
652         trainingLog(f""Starting training process via {self}"")
653 
654         lossEvaluator = self.params.lossEvaluator
655 
656         totalEpochs = None
657         best_val = 1e9
658         best_epoch = 0
659         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
660             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
661 
662         bestModelBytes = model.getModuleBytes()
663         lossEvaluation = lossEvaluator.startEvaluation(self.cuda)
664         validationMetricName = lossEvaluator.getValidationMetricName()
665         trainingLossValues = []
666         validationMetricValues = []
667         try:
668             self.log.info(f'Begin training with cuda={self.cuda}')
669             self.log.info('Press Ctrl+C to end training early')
670             for epoch in range(1, self.params.epochs + 1):
671                 lossEvaluation.startEpoch()
672                 epoch_start_time = time.time()
673 
674                 # perform training step, processing all the training data once
675                 train_loss = self._train(trainingSets, torchModel, optim, lossEvaluation, self.params.batchSize, outputScalers)
676                 trainingLossValues.append(train_loss)
677 
678                 # perform validation, computing the mean metrics across all validation sets (if more than one),
679                 # and check for new best result according to validation results
680                 isNewBest = False
681                 if useValidation:
682                     metricsSum = None
683                     metricsKeys = None
684                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
685                         metrics = self._evaluate(validationSet, torchModel, lossEvaluation, outputScaler)
686                         metricsArray = np.array(list(metrics.values()))
687                         if i == 0:
688                             metricsSum = metricsArray
689                             metricsKeys = metrics.keys()
690                         else:
691                             metricsSum += metricsArray
692                     metricsSum /= len(validationSets)  # mean results
693                     metrics = dict(zip(metricsKeys, metricsSum))
694                     current_val = metrics[lossEvaluator.getValidationMetricName()]
695                     validationMetricValues.append(current_val)
696                     isNewBest = current_val < best_val
697                     if isNewBest:
698                         best_val = current_val
699                         best_epoch = epoch
700                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
701                     else:
702                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
703                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
704                 else:
705                     valStr = """"
706                 trainingLog(
707                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
708                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
709                 totalEpochs = epoch
710                 if useValidation:
711                     if isNewBest:
712                         bestModelBytes = model.getModuleBytes()
713 
714                     # check for early stopping
715                     numEpochsWithoutImprovement = epoch - best_epoch
716                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
717                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
718                         break
719 
720             trainingLog(""Training complete"")
721         except KeyboardInterrupt:
722             trainingLog('Exiting from training early because of keyboard interrupt')
723 
724         # reload best model according to validation results
725         if useValidation:
726             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
727             self.bestEpoch = best_epoch
728             model.setModuleBytes(bestModelBytes)
729 
730         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
731             trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
732 
733     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","Before: 563
After: 675",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,4870,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 396, 'parameters': 4, '(': 85, ')': 85, ':': 33, 'block': 31, 'return_statement': 4, 'return': 4, 'string': 27, 'string_start': 27, 'interpolation': 19, '{': 19, 'attribute': 99, '.': 99, '}': 19, 'string_content': 37, 'string_end': 27, ',': 48, 'typed_parameter': 2, 'type': 20, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 64, 'true': 2, '->': 2, 'expression_statement': 84, 'assignment': 54, 'call': 79, 'argument_list': 79, 'comparison_operator': 9, '!=': 2, 'float': 2, 'if_statement': 15, 'if': 18, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 8, 'raise_statement': 2, 'raise': 2, 'list': 7, 'comment': 8, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, 'none': 8, '==': 2, 'is not': 6, 'binary_operator': 7, '+': 2, 'conditional_expression': 3, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 9, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 2, 'boolean_operator': 1, 'and': 1, '>=': 1, 'break_statement': 1, 'break': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7203820842364708,0.7119131566015755,"(tensor([0.9570]), tensor([0.9576]), tensor([0.9573]), tensor([0.9576]))"
"452     def __str__(self):
453         return f""{self.__class__.__name__}[params={self.params}]""
454 
455     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
456             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
457             createTorchModule=True) -> ""TrainingInfo"":
458         """"""
459         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
460 
461             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
462               the `trainFraction` parameter of this object)
463             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
464             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
465               the validation set
466 
467         :param model: the model to be fitted
468         :param data: the data to use (see variants above)
469         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
470             If False, (re-)train the existing module.
471         """"""
472         self.cuda = model.cuda
473         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
474 
475         useValidation = self.params.trainFraction != 1.0
476 
477         def toDataSetProvider(d) -> TorchDataSetProvider:
478             if isinstance(d, TorchDataSetProvider):
479                 return d
480             elif isinstance(d, DataUtil):
481                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
482             else:
483                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
484 
485         trainingLogEntries = []
486 
487         def trainingLog(s):
488             self.log.info(s)
489             trainingLogEntries.append(s)
490 
491         self._init_cuda()
492 
493         # Set the random seed manually for reproducibility.
494         seed = 42
495         torch.manual_seed(seed)
496         if self.cuda:
497             torchcuda.manual_seed_all(seed)
498         torch.backends.cudnn.benchmark = False
499         torch.backends.cudnn.deterministic = True
500 
501         # obtain data, splitting it into training and validation set(s)
502         validationSets = []
503         trainingSets = []
504         outputScalers = []
505         if type(data) != list:
506             data = [data]
507         self.log.info(""Obtaining input/output training instances"")
508         for idxDataItem, dataItem in enumerate(data):
509             if isinstance(dataItem, TorchDataSet):
510                 if useValidation:
511                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
512                 trainingSet = dataItem
513                 validationSet = None
514                 outputScaler = TensorScalerIdentity()
515             elif type(dataItem) == tuple:
516                 trainingSet, validationSet = dataItem
517                 outputScaler = TensorScalerIdentity()
518             else:
519                 dataSetProvider = toDataSetProvider(dataItem)
520                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
521                 outputScaler = dataSetProvider.getOutputTensorScaler()
522             trainingSets.append(trainingSet)
523             if validationSet is not None:
524                 validationSets.append(validationSet)
525             outputScalers.append(outputScaler)
526             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
527         trainingLog(""Number of validation sets: %d"" % len(validationSets))
528 
529         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
530         if self.cuda:
531             torchModel.cuda()
532         model.setTorchModule(torchModel)
533 
534         nParams = sum([p.nelement() for p in torchModel.parameters()])
535         self.log.info(f""Learning parameters of {model}"")
536         trainingLog('Number of parameters: %d' % nParams)
537         trainingLog(f""Starting training process via {self}"")
538 
539         lossEvaluator = self.params.lossEvaluator
540         criterion = lossEvaluator.getTrainingCriterion()
541 
542         if self.cuda:
543             criterion = criterion.cuda()
544 
545         totalEpochs = None
546         best_val = 1e9
547         best_epoch = 0
548         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
549             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
550 
551         bestModelBytes = model.getModuleBytes()
552         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
553         validationMetricName = lossEvaluator.getValidationMetricName()
554         trainingLossValues = []
555         validationMetricValues = []
556         try:
557             self.log.info(f'Begin training with cuda={self.cuda}')
558             self.log.info('Press Ctrl+C to end training early')
559             for epoch in range(1, self.params.epochs + 1):
560                 epoch_start_time = time.time()
561 
562                 # perform training step, processing all the training data once
563                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
564                 trainingLossValues.append(train_loss)
565 
566                 # perform validation, computing the mean metrics across all validation sets (if more than one),
567                 # and check for new best result according to validation results
568                 isNewBest = False
569                 if useValidation:
570                     metricsSum = None
571                     metricsKeys = None
572                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
573                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
574                         metricsArray = np.array(list(metrics.values()))
575                         if i == 0:
576                             metricsSum = metricsArray
577                             metricsKeys = metrics.keys()
578                         else:
579                             metricsSum += metricsArray
580                     metricsSum /= len(validationSets)  # mean results
581                     metrics = dict(zip(metricsKeys, metricsSum))
582                     current_val = metrics[lossEvaluator.getValidationMetricName()]
583                     validationMetricValues.append(current_val)
584                     isNewBest = current_val < best_val
585                     if isNewBest:
586                         best_val = current_val
587                         best_epoch = epoch
588                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
589                     else:
590                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
591                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
592                 else:
593                     valStr = """"
594                 trainingLog(
595                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
596                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
597                 totalEpochs = epoch
598                 if useValidation:
599                     if isNewBest:
600                         bestModelBytes = model.getModuleBytes()
601 
602                     # check for early stopping
603                     numEpochsWithoutImprovement = epoch - best_epoch
604                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
605                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
606                         break
607 
608             trainingLog(""Training complete"")
609         except KeyboardInterrupt:
610             trainingLog('Exiting from training early because of keyboard interrupt')
611 
612         # reload best model according to validation results
613         if useValidation:
614             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
615             self.bestEpoch = best_epoch
616             model.setModuleBytes(bestModelBytes)
617 
618         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
619                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
620 
621     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","567     def __str__(self):
568         return f""{self.__class__.__name__}[params={self.params}]""
569 
570     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
571                                                    TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
572             createTorchModule=True) -> ""TrainingInfo"":
573         """"""
574         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
575 
576             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
577               the `trainFraction` parameter of this object)
578             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
579             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
580               the validation set
581 
582         :param model: the model to be fitted
583         :param data: the data to use (see variants above)
584         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
585             If False, (re-)train the existing module.
586         """"""
587         self.cuda = model.cuda
588         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
589 
590         useValidation = self.params.trainFraction != 1.0
591 
592         def toDataSetProvider(d) -> TorchDataSetProvider:
593             if isinstance(d, TorchDataSetProvider):
594                 return d
595             elif isinstance(d, DataUtil):
596                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
597             else:
598                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
599 
600         trainingLogEntries = []
601 
602         def trainingLog(s):
603             self.log.info(s)
604             trainingLogEntries.append(s)
605 
606         self._init_cuda()
607 
608         # Set the random seed manually for reproducibility.
609         seed = 42
610         torch.manual_seed(seed)
611         if self.cuda:
612             torchcuda.manual_seed_all(seed)
613         torch.backends.cudnn.benchmark = False
614         torch.backends.cudnn.deterministic = True
615 
616         # obtain data, splitting it into training and validation set(s)
617         validationSets = []
618         trainingSets = []
619         outputScalers = []
620         if type(data) != list:
621             data = [data]
622         self.log.info(""Obtaining input/output training instances"")
623         for idxDataItem, dataItem in enumerate(data):
624             if isinstance(dataItem, TorchDataSet):
625                 if useValidation:
626                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
627                 trainingSet = dataItem
628                 validationSet = None
629                 outputScaler = TensorScalerIdentity()
630             elif type(dataItem) == tuple:
631                 trainingSet, validationSet = dataItem
632                 outputScaler = TensorScalerIdentity()
633             else:
634                 dataSetProvider = toDataSetProvider(dataItem)
635                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
636                 outputScaler = dataSetProvider.getOutputTensorScaler()
637             trainingSets.append(trainingSet)
638             if validationSet is not None:
639                 validationSets.append(validationSet)
640             outputScalers.append(outputScaler)
641             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
642         trainingLog(""Number of validation sets: %d"" % len(validationSets))
643 
644         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
645         if self.cuda:
646             torchModel.cuda()
647         model.setTorchModule(torchModel)
648 
649         nParams = sum([p.nelement() for p in torchModel.parameters()])
650         self.log.info(f""Learning parameters of {model}"")
651         trainingLog('Number of parameters: %d' % nParams)
652         trainingLog(f""Starting training process via {self}"")
653 
654         lossEvaluator = self.params.lossEvaluator
655 
656         totalEpochs = None
657         best_val = 1e9
658         best_epoch = 0
659         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
660             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
661 
662         bestModelBytes = model.getModuleBytes()
663         lossEvaluation = lossEvaluator.startEvaluation(self.cuda)
664         validationMetricName = lossEvaluator.getValidationMetricName()
665         trainingLossValues = []
666         validationMetricValues = []
667         try:
668             self.log.info(f'Begin training with cuda={self.cuda}')
669             self.log.info('Press Ctrl+C to end training early')
670             for epoch in range(1, self.params.epochs + 1):
671                 lossEvaluation.startEpoch()
672                 epoch_start_time = time.time()
673 
674                 # perform training step, processing all the training data once
675                 train_loss = self._train(trainingSets, torchModel, optim, lossEvaluation, self.params.batchSize, outputScalers)
676                 trainingLossValues.append(train_loss)
677 
678                 # perform validation, computing the mean metrics across all validation sets (if more than one),
679                 # and check for new best result according to validation results
680                 isNewBest = False
681                 if useValidation:
682                     metricsSum = None
683                     metricsKeys = None
684                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
685                         metrics = self._evaluate(validationSet, torchModel, lossEvaluation, outputScaler)
686                         metricsArray = np.array(list(metrics.values()))
687                         if i == 0:
688                             metricsSum = metricsArray
689                             metricsKeys = metrics.keys()
690                         else:
691                             metricsSum += metricsArray
692                     metricsSum /= len(validationSets)  # mean results
693                     metrics = dict(zip(metricsKeys, metricsSum))
694                     current_val = metrics[lossEvaluator.getValidationMetricName()]
695                     validationMetricValues.append(current_val)
696                     isNewBest = current_val < best_val
697                     if isNewBest:
698                         best_val = current_val
699                         best_epoch = epoch
700                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
701                     else:
702                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
703                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
704                 else:
705                     valStr = """"
706                 trainingLog(
707                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
708                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
709                 totalEpochs = epoch
710                 if useValidation:
711                     if isNewBest:
712                         bestModelBytes = model.getModuleBytes()
713 
714                     # check for early stopping
715                     numEpochsWithoutImprovement = epoch - best_epoch
716                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
717                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
718                         break
719 
720             trainingLog(""Training complete"")
721         except KeyboardInterrupt:
722             trainingLog('Exiting from training early because of keyboard interrupt')
723 
724         # reload best model according to validation results
725         if useValidation:
726             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
727             self.bestEpoch = best_epoch
728             model.setModuleBytes(bestModelBytes)
729 
730         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
731             trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
732 
733     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","Before: 573
After: 685",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,4946,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 396, 'parameters': 4, '(': 85, ')': 85, ':': 33, 'block': 31, 'return_statement': 4, 'return': 4, 'string': 27, 'string_start': 27, 'interpolation': 19, '{': 19, 'attribute': 99, '.': 99, '}': 19, 'string_content': 37, 'string_end': 27, ',': 48, 'typed_parameter': 2, 'type': 20, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 64, 'true': 2, '->': 2, 'expression_statement': 84, 'assignment': 54, 'call': 79, 'argument_list': 79, 'comparison_operator': 9, '!=': 2, 'float': 2, 'if_statement': 15, 'if': 18, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 8, 'raise_statement': 2, 'raise': 2, 'list': 7, 'comment': 8, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, 'none': 8, '==': 2, 'is not': 6, 'binary_operator': 7, '+': 2, 'conditional_expression': 3, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 9, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 2, 'boolean_operator': 1, 'and': 1, '>=': 1, 'break_statement': 1, 'break': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7203820842364708,0.7119131566015755,"(tensor([0.9570]), tensor([0.9576]), tensor([0.9573]), tensor([0.9576]))"
"452     def __str__(self):
453         return f""{self.__class__.__name__}[params={self.params}]""
454 
455     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
456             TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
457             createTorchModule=True) -> ""TrainingInfo"":
458         """"""
459         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
460 
461             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
462               the `trainFraction` parameter of this object)
463             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
464             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
465               the validation set
466 
467         :param model: the model to be fitted
468         :param data: the data to use (see variants above)
469         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
470             If False, (re-)train the existing module.
471         """"""
472         self.cuda = model.cuda
473         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
474 
475         useValidation = self.params.trainFraction != 1.0
476 
477         def toDataSetProvider(d) -> TorchDataSetProvider:
478             if isinstance(d, TorchDataSetProvider):
479                 return d
480             elif isinstance(d, DataUtil):
481                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
482             else:
483                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
484 
485         trainingLogEntries = []
486 
487         def trainingLog(s):
488             self.log.info(s)
489             trainingLogEntries.append(s)
490 
491         self._init_cuda()
492 
493         # Set the random seed manually for reproducibility.
494         seed = 42
495         torch.manual_seed(seed)
496         if self.cuda:
497             torchcuda.manual_seed_all(seed)
498         torch.backends.cudnn.benchmark = False
499         torch.backends.cudnn.deterministic = True
500 
501         # obtain data, splitting it into training and validation set(s)
502         validationSets = []
503         trainingSets = []
504         outputScalers = []
505         if type(data) != list:
506             data = [data]
507         self.log.info(""Obtaining input/output training instances"")
508         for idxDataItem, dataItem in enumerate(data):
509             if isinstance(dataItem, TorchDataSet):
510                 if useValidation:
511                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
512                 trainingSet = dataItem
513                 validationSet = None
514                 outputScaler = TensorScalerIdentity()
515             elif type(dataItem) == tuple:
516                 trainingSet, validationSet = dataItem
517                 outputScaler = TensorScalerIdentity()
518             else:
519                 dataSetProvider = toDataSetProvider(dataItem)
520                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
521                 outputScaler = dataSetProvider.getOutputTensorScaler()
522             trainingSets.append(trainingSet)
523             if validationSet is not None:
524                 validationSets.append(validationSet)
525             outputScalers.append(outputScaler)
526             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
527         trainingLog(""Number of validation sets: %d"" % len(validationSets))
528 
529         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
530         if self.cuda:
531             torchModel.cuda()
532         model.setTorchModule(torchModel)
533 
534         nParams = sum([p.nelement() for p in torchModel.parameters()])
535         self.log.info(f""Learning parameters of {model}"")
536         trainingLog('Number of parameters: %d' % nParams)
537         trainingLog(f""Starting training process via {self}"")
538 
539         lossEvaluator = self.params.lossEvaluator
540         criterion = lossEvaluator.getTrainingCriterion()
541 
542         if self.cuda:
543             criterion = criterion.cuda()
544 
545         totalEpochs = None
546         best_val = 1e9
547         best_epoch = 0
548         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
549             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
550 
551         bestModelBytes = model.getModuleBytes()
552         self.lossEvaluatorState = lossEvaluator.createValidationLossEvaluator(self.cuda)
553         validationMetricName = lossEvaluator.getValidationMetricName()
554         trainingLossValues = []
555         validationMetricValues = []
556         try:
557             self.log.info(f'Begin training with cuda={self.cuda}')
558             self.log.info('Press Ctrl+C to end training early')
559             for epoch in range(1, self.params.epochs + 1):
560                 epoch_start_time = time.time()
561 
562                 # perform training step, processing all the training data once
563                 train_loss = self._train(trainingSets, torchModel, criterion, optim, self.params.batchSize, self.cuda, outputScalers)
564                 trainingLossValues.append(train_loss)
565 
566                 # perform validation, computing the mean metrics across all validation sets (if more than one),
567                 # and check for new best result according to validation results
568                 isNewBest = False
569                 if useValidation:
570                     metricsSum = None
571                     metricsKeys = None
572                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
573                         metrics = self._evaluate(validationSet, torchModel, outputScaler)
574                         metricsArray = np.array(list(metrics.values()))
575                         if i == 0:
576                             metricsSum = metricsArray
577                             metricsKeys = metrics.keys()
578                         else:
579                             metricsSum += metricsArray
580                     metricsSum /= len(validationSets)  # mean results
581                     metrics = dict(zip(metricsKeys, metricsSum))
582                     current_val = metrics[lossEvaluator.getValidationMetricName()]
583                     validationMetricValues.append(current_val)
584                     isNewBest = current_val < best_val
585                     if isNewBest:
586                         best_val = current_val
587                         best_epoch = epoch
588                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
589                     else:
590                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
591                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
592                 else:
593                     valStr = """"
594                 trainingLog(
595                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
596                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
597                 totalEpochs = epoch
598                 if useValidation:
599                     if isNewBest:
600                         bestModelBytes = model.getModuleBytes()
601 
602                     # check for early stopping
603                     numEpochsWithoutImprovement = epoch - best_epoch
604                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
605                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
606                         break
607 
608             trainingLog(""Training complete"")
609         except KeyboardInterrupt:
610             trainingLog('Exiting from training early because of keyboard interrupt')
611 
612         # reload best model according to validation results
613         if useValidation:
614             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
615             self.bestEpoch = best_epoch
616             model.setModuleBytes(bestModelBytes)
617 
618         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
619                 trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
620 
621     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","567     def __str__(self):
568         return f""{self.__class__.__name__}[params={self.params}]""
569 
570     def fit(self, model: ""TorchModel"", data: Union[DataUtil, List[DataUtil], TorchDataSetProvider, List[TorchDataSetProvider],
571                                                    TorchDataSet, List[TorchDataSet], Tuple[TorchDataSet, TorchDataSet], List[Tuple[TorchDataSet, TorchDataSet]]],
572             createTorchModule=True) -> ""TrainingInfo"":
573         """"""
574         Fits the parameters of the given model to the given data, which can be a list of or single instance of one of the following:
575 
576             * a `DataUtil` or `TorchDataSetProvider` (from which a training set and validation set will be obtained according to
577               the `trainFraction` parameter of this object)
578             * a `TorchDataSet` which shall be used as the training set (for the case where no validation set shall be used)
579             * a tuple with two `TorchDataSet` instances, where the first shall be used as the training set and the second as
580               the validation set
581 
582         :param model: the model to be fitted
583         :param data: the data to use (see variants above)
584         :param createTorchModule: whether to newly create the torch module that is to be trained from the model's factory.
585             If False, (re-)train the existing module.
586         """"""
587         self.cuda = model.cuda
588         self.log.info(f""Preparing parameter learning of {model} via {self} with cuda={self.cuda}"")
589 
590         useValidation = self.params.trainFraction != 1.0
591 
592         def toDataSetProvider(d) -> TorchDataSetProvider:
593             if isinstance(d, TorchDataSetProvider):
594                 return d
595             elif isinstance(d, DataUtil):
596                 return TorchDataSetProviderFromDataUtil(d, self.cuda)
597             else:
598                 raise ValueError(f""Cannot create a TorchDataSetProvider from {d}"")
599 
600         trainingLogEntries = []
601 
602         def trainingLog(s):
603             self.log.info(s)
604             trainingLogEntries.append(s)
605 
606         self._init_cuda()
607 
608         # Set the random seed manually for reproducibility.
609         seed = 42
610         torch.manual_seed(seed)
611         if self.cuda:
612             torchcuda.manual_seed_all(seed)
613         torch.backends.cudnn.benchmark = False
614         torch.backends.cudnn.deterministic = True
615 
616         # obtain data, splitting it into training and validation set(s)
617         validationSets = []
618         trainingSets = []
619         outputScalers = []
620         if type(data) != list:
621             data = [data]
622         self.log.info(""Obtaining input/output training instances"")
623         for idxDataItem, dataItem in enumerate(data):
624             if isinstance(dataItem, TorchDataSet):
625                 if useValidation:
626                     raise ValueError(""Passing a TorchDataSet instance is not admissible when validation is enabled (trainFraction != 1.0). Pass a TorchDataSetProvider or another representation that supports validation instead."")
627                 trainingSet = dataItem
628                 validationSet = None
629                 outputScaler = TensorScalerIdentity()
630             elif type(dataItem) == tuple:
631                 trainingSet, validationSet = dataItem
632                 outputScaler = TensorScalerIdentity()
633             else:
634                 dataSetProvider = toDataSetProvider(dataItem)
635                 trainingSet, validationSet = dataSetProvider.provideSplit(self.params.trainFraction)
636                 outputScaler = dataSetProvider.getOutputTensorScaler()
637             trainingSets.append(trainingSet)
638             if validationSet is not None:
639                 validationSets.append(validationSet)
640             outputScalers.append(outputScaler)
641             trainingLog(f""Data set {idxDataItem+1}/{len(data)}: #train={trainingSet.size()}, #validation={validationSet.size() if validationSet is not None else 'None'}"")
642         trainingLog(""Number of validation sets: %d"" % len(validationSets))
643 
644         torchModel = model.createTorchModule() if createTorchModule else model.getTorchModule()
645         if self.cuda:
646             torchModel.cuda()
647         model.setTorchModule(torchModel)
648 
649         nParams = sum([p.nelement() for p in torchModel.parameters()])
650         self.log.info(f""Learning parameters of {model}"")
651         trainingLog('Number of parameters: %d' % nParams)
652         trainingLog(f""Starting training process via {self}"")
653 
654         lossEvaluator = self.params.lossEvaluator
655 
656         totalEpochs = None
657         best_val = 1e9
658         best_epoch = 0
659         optim = _Optimiser(torchModel.parameters(), method=self.params.optimiser, lr=self.params.optimiserLR,
660             max_grad_norm=self.params.shrinkageClip, use_shrinkage=self.params.useShrinkage, **self.params.optimiserArgs)
661 
662         bestModelBytes = model.getModuleBytes()
663         lossEvaluation = lossEvaluator.startEvaluation(self.cuda)
664         validationMetricName = lossEvaluator.getValidationMetricName()
665         trainingLossValues = []
666         validationMetricValues = []
667         try:
668             self.log.info(f'Begin training with cuda={self.cuda}')
669             self.log.info('Press Ctrl+C to end training early')
670             for epoch in range(1, self.params.epochs + 1):
671                 lossEvaluation.startEpoch()
672                 epoch_start_time = time.time()
673 
674                 # perform training step, processing all the training data once
675                 train_loss = self._train(trainingSets, torchModel, optim, lossEvaluation, self.params.batchSize, outputScalers)
676                 trainingLossValues.append(train_loss)
677 
678                 # perform validation, computing the mean metrics across all validation sets (if more than one),
679                 # and check for new best result according to validation results
680                 isNewBest = False
681                 if useValidation:
682                     metricsSum = None
683                     metricsKeys = None
684                     for i, (validationSet, outputScaler) in enumerate(zip(validationSets, outputScalers)):
685                         metrics = self._evaluate(validationSet, torchModel, lossEvaluation, outputScaler)
686                         metricsArray = np.array(list(metrics.values()))
687                         if i == 0:
688                             metricsSum = metricsArray
689                             metricsKeys = metrics.keys()
690                         else:
691                             metricsSum += metricsArray
692                     metricsSum /= len(validationSets)  # mean results
693                     metrics = dict(zip(metricsKeys, metricsSum))
694                     current_val = metrics[lossEvaluator.getValidationMetricName()]
695                     validationMetricValues.append(current_val)
696                     isNewBest = current_val < best_val
697                     if isNewBest:
698                         best_val = current_val
699                         best_epoch = epoch
700                         bestStr = ""best {:s} {:5.6f} from this epoch"".format(validationMetricName, best_val)
701                     else:
702                         bestStr = ""best {:s} {:5.6f} from epoch {:d}"".format(validationMetricName, best_val, best_epoch)
703                     valStr = f' | validation {"", "".join([""%s %5.4f"" % e for e in metrics.items()])} | {bestStr}'
704                 else:
705                     valStr = """"
706                 trainingLog(
707                     'Epoch {:3d}/{} completed in {:5.2f}s | train loss {:5.4f}{:s}'.format(
708                         epoch, self.params.epochs, (time.time() - epoch_start_time), train_loss, valStr))
709                 totalEpochs = epoch
710                 if useValidation:
711                     if isNewBest:
712                         bestModelBytes = model.getModuleBytes()
713 
714                     # check for early stopping
715                     numEpochsWithoutImprovement = epoch - best_epoch
716                     if self.params.earlyStoppingEpochs is not None and numEpochsWithoutImprovement >= self.params.earlyStoppingEpochs:
717                         trainingLog(f""Stopping early: {numEpochsWithoutImprovement} epochs without validation metric improvement"")
718                         break
719 
720             trainingLog(""Training complete"")
721         except KeyboardInterrupt:
722             trainingLog('Exiting from training early because of keyboard interrupt')
723 
724         # reload best model according to validation results
725         if useValidation:
726             trainingLog(f'Best model is from epoch {best_epoch} with {validationMetricName} {best_val} on validation set')
727             self.bestEpoch = best_epoch
728             model.setModuleBytes(bestModelBytes)
729 
730         return TrainingInfo(bestEpoch=best_epoch if useValidation else None, log=trainingLogEntries, totalEpochs=totalEpochs,
731             trainingLossSequence=trainingLossValues, validationMetricSequence=validationMetricValues)
732 
733     def _applyModel(self, model, input: Union[torch.Tensor, Sequence[torch.Tensor]], groundTruth, outputScaler: TensorScaler):
","Before: 619
After: 731",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,5417,"{'module': 1, 'function_definition': 4, 'def': 4, 'identifier': 396, 'parameters': 4, '(': 85, ')': 85, ':': 33, 'block': 31, 'return_statement': 4, 'return': 4, 'string': 27, 'string_start': 27, 'interpolation': 19, '{': 19, 'attribute': 99, '.': 99, '}': 19, 'string_content': 37, 'string_end': 27, ',': 48, 'typed_parameter': 2, 'type': 20, 'generic_type': 7, 'type_parameter': 7, '[': 17, ']': 17, 'default_parameter': 1, '=': 64, 'true': 2, '->': 2, 'expression_statement': 84, 'assignment': 54, 'call': 79, 'argument_list': 79, 'comparison_operator': 9, '!=': 2, 'float': 2, 'if_statement': 15, 'if': 18, 'elif_clause': 2, 'elif': 2, 'else_clause': 5, 'else': 8, 'raise_statement': 2, 'raise': 2, 'list': 7, 'comment': 8, 'integer': 6, 'false': 2, 'for_statement': 3, 'for': 5, 'pattern_list': 4, 'in': 5, 'none': 8, '==': 2, 'is not': 6, 'binary_operator': 7, '+': 2, 'conditional_expression': 3, '%': 3, 'list_comprehension': 2, 'for_in_clause': 2, 'keyword_argument': 9, 'dictionary_splat': 1, '**': 1, 'try_statement': 1, 'try': 1, 'tuple_pattern': 1, 'augmented_assignment': 2, '+=': 1, '/=': 1, 'subscript': 1, '<': 1, 'parenthesized_expression': 1, '-': 2, 'boolean_operator': 1, 'and': 1, '>=': 1, 'break_statement': 1, 'break': 1, 'except_clause': 1, 'except': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7203820842364708,0.7119131566015755,"(tensor([0.9570]), tensor([0.9576]), tensor([0.9573]), tensor([0.9576]))"
"633         scaledTruth = outputScaler.denormalise(groundTruth)
634         return scaledOutput, scaledTruth
635 
636     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, criterion: nn.modules.loss._Loss,
637             optim: _Optimiser, batch_size: int, cuda: bool, outputScalers: Sequence[TensorScaler]):
638         """"""Performs one training epoch""""""
639         model.train()
640         total_loss = 0
641         n_samples = 0
642         numOutputsPerDataPoint = None
643         for dataSet, outputScaler in zip(dataSets, outputScalers):
644             for X, Y in dataSet.iterBatches(batch_size, shuffle=self.params.shuffle):
645                 if numOutputsPerDataPoint is None:
646                     outputShape = Y.shape[1:]
647                     numOutputsPerDataPoint = functools.reduce(lambda x, y: x * y, outputShape, 1)
648 
649                 def closure():
650                     model.zero_grad()
651                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
652                     loss = criterion(output, groundTruth)
653                     loss.backward()
654                     return loss
655 
656                 loss = optim.step(closure)
657                 total_loss += loss.item()
658                 numDataPointsInBatch = Y.size(0)
659                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
660         return total_loss / n_samples
661 
662     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
","745         scaledTruth = outputScaler.denormalise(groundTruth)
746         return scaledOutput, scaledTruth
747 
748     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, optim: _Optimiser,
749             lossEvaluation: NNLossEvaluator.Evaluation, batch_size: int, outputScalers: Sequence[TensorScaler]):
750         """"""Performs one training epoch""""""
751         model.train()
752         for dataSet, outputScaler in zip(dataSets, outputScalers):
753             for X, Y in dataSet.iterBatches(batch_size, shuffle=self.params.shuffle):
754                 def closure():
755                     model.zero_grad()
756                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
757                     loss = lossEvaluation.computeTrainBatchLoss(output, groundTruth, X, Y)
758                     loss.backward()
759                     return loss
760 
761                 optim.step(closure)
762         return lossEvaluation.getEpochTrainLoss()
763 
764     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, lossEvaluation: NNLossEvaluator.Evaluation,
","Before: 636, 637
After: 748, 749",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,5634,"{'module': 1, 'expression_statement': 16, 'assignment': 10, 'identifier': 92, '=': 11, 'call': 12, 'attribute': 17, '.': 17, 'argument_list': 12, '(': 14, ')': 14, 'return_statement': 3, 'return': 3, 'expression_list': 1, ',': 20, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 7, ':': 14, 'type': 9, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'integer': 5, 'none': 2, 'for_statement': 2, 'for': 2, 'pattern_list': 3, 'in': 2, 'keyword_argument': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, 'slice': 1, 'lambda': 2, 'lambda_parameters': 1, 'binary_operator': 3, '*': 2, 'augmented_assignment': 2, '+=': 2, '/': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.41526129345636503,0.39481650977376204,"(tensor([0.8753]), tensor([0.8370]), tensor([0.8557]), tensor([0.8407]))"
"633         scaledTruth = outputScaler.denormalise(groundTruth)
634         return scaledOutput, scaledTruth
635 
636     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, criterion: nn.modules.loss._Loss,
637             optim: _Optimiser, batch_size: int, cuda: bool, outputScalers: Sequence[TensorScaler]):
638         """"""Performs one training epoch""""""
639         model.train()
640         total_loss = 0
641         n_samples = 0
642         numOutputsPerDataPoint = None
643         for dataSet, outputScaler in zip(dataSets, outputScalers):
644             for X, Y in dataSet.iterBatches(batch_size, shuffle=self.params.shuffle):
645                 if numOutputsPerDataPoint is None:
646                     outputShape = Y.shape[1:]
647                     numOutputsPerDataPoint = functools.reduce(lambda x, y: x * y, outputShape, 1)
648 
649                 def closure():
650                     model.zero_grad()
651                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
652                     loss = criterion(output, groundTruth)
653                     loss.backward()
654                     return loss
655 
656                 loss = optim.step(closure)
657                 total_loss += loss.item()
658                 numDataPointsInBatch = Y.size(0)
659                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
660         return total_loss / n_samples
661 
662     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
","745         scaledTruth = outputScaler.denormalise(groundTruth)
746         return scaledOutput, scaledTruth
747 
748     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, optim: _Optimiser,
749             lossEvaluation: NNLossEvaluator.Evaluation, batch_size: int, outputScalers: Sequence[TensorScaler]):
750         """"""Performs one training epoch""""""
751         model.train()
752         for dataSet, outputScaler in zip(dataSets, outputScalers):
753             for X, Y in dataSet.iterBatches(batch_size, shuffle=self.params.shuffle):
754                 def closure():
755                     model.zero_grad()
756                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
757                     loss = lossEvaluation.computeTrainBatchLoss(output, groundTruth, X, Y)
758                     loss.backward()
759                     return loss
760 
761                 optim.step(closure)
762         return lossEvaluation.getEpochTrainLoss()
763 
764     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, lossEvaluation: NNLossEvaluator.Evaluation,
","Before: 640, 641, 642, 645, 646, 647, 648, 652
After: 757",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,5686,"{'module': 1, 'expression_statement': 16, 'assignment': 10, 'identifier': 92, '=': 11, 'call': 12, 'attribute': 17, '.': 17, 'argument_list': 12, '(': 14, ')': 14, 'return_statement': 3, 'return': 3, 'expression_list': 1, ',': 20, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 7, ':': 14, 'type': 9, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'integer': 5, 'none': 2, 'for_statement': 2, 'for': 2, 'pattern_list': 3, 'in': 2, 'keyword_argument': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, 'slice': 1, 'lambda': 2, 'lambda_parameters': 1, 'binary_operator': 3, '*': 2, 'augmented_assignment': 2, '+=': 2, '/': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.41526129345636503,0.39481650977376204,"(tensor([0.8753]), tensor([0.8370]), tensor([0.8557]), tensor([0.8407]))"
"633         scaledTruth = outputScaler.denormalise(groundTruth)
634         return scaledOutput, scaledTruth
635 
636     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, criterion: nn.modules.loss._Loss,
637             optim: _Optimiser, batch_size: int, cuda: bool, outputScalers: Sequence[TensorScaler]):
638         """"""Performs one training epoch""""""
639         model.train()
640         total_loss = 0
641         n_samples = 0
642         numOutputsPerDataPoint = None
643         for dataSet, outputScaler in zip(dataSets, outputScalers):
644             for X, Y in dataSet.iterBatches(batch_size, shuffle=self.params.shuffle):
645                 if numOutputsPerDataPoint is None:
646                     outputShape = Y.shape[1:]
647                     numOutputsPerDataPoint = functools.reduce(lambda x, y: x * y, outputShape, 1)
648 
649                 def closure():
650                     model.zero_grad()
651                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
652                     loss = criterion(output, groundTruth)
653                     loss.backward()
654                     return loss
655 
656                 loss = optim.step(closure)
657                 total_loss += loss.item()
658                 numDataPointsInBatch = Y.size(0)
659                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
660         return total_loss / n_samples
661 
662     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
","745         scaledTruth = outputScaler.denormalise(groundTruth)
746         return scaledOutput, scaledTruth
747 
748     def _train(self, dataSets: Sequence[TorchDataSet], model: nn.Module, optim: _Optimiser,
749             lossEvaluation: NNLossEvaluator.Evaluation, batch_size: int, outputScalers: Sequence[TensorScaler]):
750         """"""Performs one training epoch""""""
751         model.train()
752         for dataSet, outputScaler in zip(dataSets, outputScalers):
753             for X, Y in dataSet.iterBatches(batch_size, shuffle=self.params.shuffle):
754                 def closure():
755                     model.zero_grad()
756                     output, groundTruth = self._applyModel(model, X, Y, outputScaler)
757                     loss = lossEvaluation.computeTrainBatchLoss(output, groundTruth, X, Y)
758                     loss.backward()
759                     return loss
760 
761                 optim.step(closure)
762         return lossEvaluation.getEpochTrainLoss()
763 
764     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, lossEvaluation: NNLossEvaluator.Evaluation,
","Before: 656, 657, 658, 659, 660
After: 761, 762",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,5867,"{'module': 1, 'expression_statement': 16, 'assignment': 10, 'identifier': 92, '=': 11, 'call': 12, 'attribute': 17, '.': 17, 'argument_list': 12, '(': 14, ')': 14, 'return_statement': 3, 'return': 3, 'expression_list': 1, ',': 20, 'function_definition': 2, 'def': 2, 'parameters': 2, 'typed_parameter': 7, ':': 14, 'type': 9, 'generic_type': 2, 'type_parameter': 2, '[': 3, ']': 3, 'block': 5, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'integer': 5, 'none': 2, 'for_statement': 2, 'for': 2, 'pattern_list': 3, 'in': 2, 'keyword_argument': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, 'slice': 1, 'lambda': 2, 'lambda_parameters': 1, 'binary_operator': 3, '*': 2, 'augmented_assignment': 2, '+=': 2, '/': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.41526129345636503,0.39481650977376204,"(tensor([0.8753]), tensor([0.8370]), tensor([0.8557]), tensor([0.8407]))"
"659                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
660         return total_loss / n_samples
661 
662     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
663         """"""Evaluates the model on the given data set (a validation set)""""""
664         model.eval()
665 
666         groundTruthShape = None
667         for X, Y in dataSet.iterBatches(self.params.batchSize, shuffle=False):
668             if groundTruthShape is None:
669                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
670                 self.lossEvaluatorState.startValidationCollection(groundTruthShape)
671             with torch.no_grad():
672                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
673             self.lossEvaluatorState.processValidationResultBatch(output, groundTruth)
674 
675         return self.lossEvaluatorState.endValidationCollection()
676 
677     def _init_cuda(self):
","761                 optim.step(closure)
762         return lossEvaluation.getEpochTrainLoss()
763 
764     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, lossEvaluation: NNLossEvaluator.Evaluation,
765             outputScaler: TensorScaler):
766         """"""Evaluates the model on the given data set (a validation set)""""""
767         model.eval()
768         for X, Y in dataSet.iterBatches(self.params.batchSize, shuffle=False):
769             with torch.no_grad():
770                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
771             lossEvaluation.processValidationBatch(output, groundTruth, X, Y)
772         return lossEvaluation.getValidationMetrics()
773 
774     def _init_cuda(self):
","Before: 662
After: 764, 765",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,5936,"{'module': 1, 'expression_statement': 8, 'augmented_assignment': 1, 'identifier': 51, '+=': 1, 'binary_operator': 2, '*': 1, 'return_statement': 2, 'return': 2, '/': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 10, 'typed_parameter': 3, ':': 8, 'type': 3, 'attribute': 14, '.': 14, ')': 8, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 7, 'argument_list': 7, 'assignment': 3, '=': 4, 'none': 2, 'for_statement': 1, 'for': 1, 'pattern_list': 2, 'in': 1, 'keyword_argument': 1, 'false': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, '[': 1, 'slice': 1, 'integer': 1, ']': 1, 'comment': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.46732222543480945,0.44237234130966413,"(tensor([0.8855]), tensor([0.8533]), tensor([0.8691]), tensor([0.8565]))"
"659                 n_samples += numDataPointsInBatch * numOutputsPerDataPoint
660         return total_loss / n_samples
661 
662     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, outputScaler: TensorScaler):
663         """"""Evaluates the model on the given data set (a validation set)""""""
664         model.eval()
665 
666         groundTruthShape = None
667         for X, Y in dataSet.iterBatches(self.params.batchSize, shuffle=False):
668             if groundTruthShape is None:
669                 groundTruthShape = Y.shape[1:]  # the shape of the output of a single model application
670                 self.lossEvaluatorState.startValidationCollection(groundTruthShape)
671             with torch.no_grad():
672                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
673             self.lossEvaluatorState.processValidationResultBatch(output, groundTruth)
674 
675         return self.lossEvaluatorState.endValidationCollection()
676 
677     def _init_cuda(self):
","761                 optim.step(closure)
762         return lossEvaluation.getEpochTrainLoss()
763 
764     def _evaluate(self, dataSet: TorchDataSet, model: nn.Module, lossEvaluation: NNLossEvaluator.Evaluation,
765             outputScaler: TensorScaler):
766         """"""Evaluates the model on the given data set (a validation set)""""""
767         model.eval()
768         for X, Y in dataSet.iterBatches(self.params.batchSize, shuffle=False):
769             with torch.no_grad():
770                 output, groundTruth = self._applyModel(model, X, Y, outputScaler)
771             lossEvaluation.processValidationBatch(output, groundTruth, X, Y)
772         return lossEvaluation.getValidationMetrics()
773 
774     def _init_cuda(self):
","Before: 665, 666, 668, 669, 670, 673, 674, 675
After: 771, 772",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,5950,"{'module': 1, 'expression_statement': 8, 'augmented_assignment': 1, 'identifier': 51, '+=': 1, 'binary_operator': 2, '*': 1, 'return_statement': 2, 'return': 2, '/': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 8, ',': 10, 'typed_parameter': 3, ':': 8, 'type': 3, 'attribute': 14, '.': 14, ')': 8, 'block': 4, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1, 'call': 7, 'argument_list': 7, 'assignment': 3, '=': 4, 'none': 2, 'for_statement': 1, 'for': 1, 'pattern_list': 2, 'in': 1, 'keyword_argument': 1, 'false': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, 'is': 1, 'subscript': 1, '[': 1, 'slice': 1, 'integer': 1, ']': 1, 'comment': 1, 'with_statement': 1, 'with': 1, 'with_clause': 1, 'with_item': 1}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.46732222543480945,0.44237234130966413,"(tensor([0.8855]), tensor([0.8533]), tensor([0.8691]), tensor([0.8565]))"
"692 
693 
694 class TrainingInfo:
695     def __init__(self, bestEpoch: int = None, log: List[str] = None, trainingLossSequence: Sequence[float] = None, validationMetricSequence:
696             Sequence[float] = None, totalEpochs=None):
697         self.validationMetricSequence = validationMetricSequence
698         self.trainingLossSequence = trainingLossSequence
699         self.log = log
700         self.bestEpoch = bestEpoch
701         self.totalEpochs = totalEpochs
702 
703     def __setstate__(self, state):
","789 
790 
791 class TrainingInfo:
792     def __init__(self, bestEpoch: int = None, log: List[str] = None, trainingLossSequence: Sequence[float] = None, validationMetricSequence:
793     Sequence[float] = None, totalEpochs=None):
794         self.validationMetricSequence = validationMetricSequence
795         self.trainingLossSequence = trainingLossSequence
796         self.log = log
797         self.bestEpoch = bestEpoch
798         self.totalEpochs = totalEpochs
799 
800     def __setstate__(self, state):
","Before: 696
After: 793",fix typos in src/sensai/torch_opt.py,Generalised NNLossEvaluator's interface to make far fewer assumptions (e.g. pertaining,https://github.com/opcode81/sensAI,src/sensai/torch/torch_opt.py,5250455170eabeedb023d8527583fa9b3036303c,f4d886c2eb9d6cb71541c4b74ad5c726beb990fc,0,6299,"{'module': 1, 'class_definition': 1, 'class': 1, 'identifier': 30, ':': 6, 'block': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, '(': 1, ',': 5, 'typed_default_parameter': 4, 'type': 7, '=': 10, 'none': 5, 'generic_type': 3, 'type_parameter': 3, '[': 3, ']': 3, 'default_parameter': 1, ')': 1, 'expression_statement': 5, 'assignment': 5, 'attribute': 5, '.': 5}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 6, 'token_count': 39, 'name': 'fromName', 'long_name': 'fromName( cls , name : str )', 'start_line': 44, 'end_line': 49, 'full_parameters': ['cls', ' name : str'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/torch/torch_opt.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6779274417857676,0.6416265458296206,"(tensor([0.9614]), tensor([0.9659]), tensor([0.9636]), tensor([0.9655]))"
"1 import numpy as np
2 import pandas as pd
3 import seaborn as sns
4 from abc import ABC, abstractmethod
5 from matplotlib import pyplot as plt
6 from typing import Generic, TypeVar, List, Union, Dict, Sequence
7 
8 from ...util.tracking import timed
9 from ...vector_model import VectorModel
10 
","1 import numpy as np
2 import pandas as pd
3 import seaborn as sns
4 from abc import ABC, abstractmethod
5 from matplotlib import pyplot as plt
6 from typing import Generic, TypeVar, List, Union, Dict, Sequence, Optional
7 
8 from ...util.string import ToStringMixin, dictString
9 from ...util.tracking import timed
10 from ...vector_model import VectorModel
","Before: 6
After: 6, 8",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,65,"{'module': 1, 'import_statement': 3, 'import': 8, 'aliased_import': 4, 'dotted_name': 19, 'identifier': 24, 'as': 4, 'import_from_statement': 5, 'from': 5, ',': 6, 'relative_import': 2, 'import_prefix': 2, '.': 7}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7949858946314581,0.7997336727261947,"(tensor([0.9601]), tensor([0.9907]), tensor([0.9751]), tensor([0.9875]))"
"18 TVectorModel = TypeVar(""TVectorModel"", bound=VectorModel)
19 
20 PredictionArray = Union[np.ndarray, pd.Series, pd.DataFrame, list]
21 
22 
23 class EvalStats(Generic[TMetric]):
24     def __init__(self, metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
25         if len(metrics) == 0:
26             raise ValueError(""No metrics provided"")
27         self.metrics = metrics
","19 TVectorModel = TypeVar(""TVectorModel"", bound=VectorModel)
20 
21 PredictionArray = Union[np.ndarray, pd.Series, pd.DataFrame, list]
22 
23 
24 class EvalStats(Generic[TMetric], ToStringMixin):
25     def __init__(self, metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
26         if len(metrics) == 0:
27             raise ValueError(""No metrics provided"")
28         self.metrics = metrics
","Before: 23
After: 24",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,173,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'identifier': 27, '=': 4, 'call': 3, 'argument_list': 4, '(': 5, 'string': 2, 'string_start': 2, 'string_content': 2, 'string_end': 2, ',': 6, 'keyword_argument': 1, ')': 5, 'subscript': 2, '[': 4, 'attribute': 3, '.': 3, ']': 4, 'class_definition': 1, 'class': 1, ':': 5, 'block': 3, 'function_definition': 1, 'def': 1, 'parameters': 1, 'typed_parameter': 1, 'type': 4, 'generic_type': 2, 'type_parameter': 2, 'typed_default_parameter': 1, 'none': 1, 'if_statement': 1, 'if': 1, 'comparison_operator': 1, '==': 1, 'integer': 1, 'raise_statement': 1, 'raise': 1}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7595085227401726,0.7558450116429853,"(tensor([0.9652]), tensor([0.9853]), tensor([0.9751]), tensor([0.9832]))"
"48             d[metric.name] = self.computeMetricValue(metric)
49         return d
50 
51     def __str__(self):
52         d = self.getAll()
53         return ""EvalStats[%s]"" % "", "".join([f""{k}={v:4f}"" for (k, v) in d.items()])
54 
55 
","48             d[metric.name] = self.computeMetricValue(metric)
49         return d
50 
51     def _toStringObjectInfo(self) -> str:
52         return dictString(self.getAll())
53 
54 
","Before: 51, 52, 53
After: 51, 52",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,432,"{'module': 1, 'expression_statement': 2, 'assignment': 2, 'subscript': 1, 'identifier': 19, '[': 2, 'attribute': 5, '.': 5, ']': 2, '=': 2, 'call': 4, 'argument_list': 4, '(': 6, ')': 6, 'return_statement': 2, 'return': 2, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 2, 'block': 1, 'binary_operator': 1, 'string': 3, 'string_start': 3, 'string_content': 3, 'string_end': 3, '%': 1, 'list_comprehension': 1, 'interpolation': 2, '{': 2, '}': 2, 'format_specifier': 1, 'for_in_clause': 1, 'for': 1, 'tuple_pattern': 1, ',': 1, 'in': 1}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.22005077874553072,0.21119343500511598,"(tensor([0.9070]), tensor([0.8528]), tensor([0.8791]), tensor([0.8580]))"
"119                "", "".join([f""{key}={self.aggStats()[key]:.4f}"" for key in self.metrics]) + ""]""
120 
121 
122 class PredictionEvalStats(EvalStats[TMetric], ABC):
123     """"""
124     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
125     and computes corresponding metrics
126     """"""
127     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
128                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
","118                "", "".join([f""{key}={self.aggStats()[key]:.4f}"" for key in self.metrics]) + ""]""
119 
120 
121 class PredictionEvalStats(EvalStats[TMetric], ABC):
122     """"""
123     Collects data for the evaluation of predicted values (including multi-dimensional predictions)
124     and computes corresponding metrics
125     """"""
126     def __init__(self, y_predicted: Optional[PredictionArray], y_true: Optional[PredictionArray],
127                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
","Before: 124
After: 123",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,1234,"{'module': 1, 'expression_statement': 2, 'binary_operator': 1, 'call': 2, 'attribute': 3, 'string': 4, 'string_start': 4, 'string_content': 4, 'string_end': 4, '.': 3, 'identifier': 18, 'argument_list': 3, '(': 4, 'list_comprehension': 1, '[': 3, 'interpolation': 2, '{': 2, '}': 2, 'subscript': 2, ')': 3, ']': 3, 'format_specifier': 1, ':': 4, 'for_in_clause': 1, 'for': 1, 'in': 1, '+': 1, 'class_definition': 1, 'class': 1, ',': 4, 'block': 1, 'ERROR': 1, 'def': 1, 'typed_parameter': 2, 'type': 2}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.7157806704451586,0.7159002362507895,"(tensor([0.9742]), tensor([0.9834]), tensor([0.9788]), tensor([0.9825]))"
"124     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
125     and computes corresponding metrics
126     """"""
127     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
128                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
129         """"""
130         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
131             one column per dimension should be passed
132         :param y_true: sequence of ground truth labels of same shape as y_predicted
133         :param metrics: list of metrics to be computed on the provided data
134         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
135         """"""
136         self.y_true = []
137         self.y_predicted = []
138         self.y_true_multidim = None
139         self.y_predicted_multidim = None
140         if y_predicted is not None:
141             self._addAll(y_predicted, y_true)
142         super().__init__(metrics, additionalMetrics=additionalMetrics)
143 
144     def _add(self, y_predicted, y_true):
","123     Collects data for the evaluation of predicted values (including multi-dimensional predictions)
124     and computes corresponding metrics
125     """"""
126     def __init__(self, y_predicted: Optional[PredictionArray], y_true: Optional[PredictionArray],
127                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
128         """"""
129         :param y_predicted: sequence of predicted values, or, in case of multi-dimensional predictions, either a data frame with
130             one column per dimension or a nested sequence of values
131         :param y_true: sequence of ground truth labels of same shape as y_predicted
132         :param metrics: list of metrics to be computed on the provided data
133         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
134         """"""
135         self.y_true = []
136         self.y_predicted = []
137         self.y_true_multidim = None
138         self.y_predicted_multidim = None
139         if y_predicted is not None:
140             self.addAll(y_predicted, y_true)
141         super().__init__(metrics, additionalMetrics=additionalMetrics)
142 
143     def add(self, y_predicted, y_true):
","Before: 127
After: 126",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,1234,"{'module': 1, 'ERROR': 5, 'identifier': 74, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, 'binary_operator': 2, '-': 2, ')': 1, 'and': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 8, 'attribute': 2, '.': 2, ',': 1, 'with': 1, 'as': 1, 'none': 1}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6275765572656147,0.6261278767503988,"(tensor([0.9671]), tensor([0.9782]), tensor([0.9726]), tensor([0.9771]))"
"124     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
125     and computes corresponding metrics
126     """"""
127     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
128                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
129         """"""
130         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
131             one column per dimension should be passed
132         :param y_true: sequence of ground truth labels of same shape as y_predicted
133         :param metrics: list of metrics to be computed on the provided data
134         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
135         """"""
136         self.y_true = []
137         self.y_predicted = []
138         self.y_true_multidim = None
139         self.y_predicted_multidim = None
140         if y_predicted is not None:
141             self._addAll(y_predicted, y_true)
142         super().__init__(metrics, additionalMetrics=additionalMetrics)
143 
144     def _add(self, y_predicted, y_true):
","123     Collects data for the evaluation of predicted values (including multi-dimensional predictions)
124     and computes corresponding metrics
125     """"""
126     def __init__(self, y_predicted: Optional[PredictionArray], y_true: Optional[PredictionArray],
127                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
128         """"""
129         :param y_predicted: sequence of predicted values, or, in case of multi-dimensional predictions, either a data frame with
130             one column per dimension or a nested sequence of values
131         :param y_true: sequence of ground truth labels of same shape as y_predicted
132         :param metrics: list of metrics to be computed on the provided data
133         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
134         """"""
135         self.y_true = []
136         self.y_predicted = []
137         self.y_true_multidim = None
138         self.y_predicted_multidim = None
139         if y_predicted is not None:
140             self.addAll(y_predicted, y_true)
141         super().__init__(metrics, additionalMetrics=additionalMetrics)
142 
143     def add(self, y_predicted, y_true):
","Before: 130, 131
After: 129, 130",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,1294,"{'module': 1, 'ERROR': 5, 'identifier': 74, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, 'binary_operator': 2, '-': 2, ')': 1, 'and': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 8, 'attribute': 2, '.': 2, ',': 1, 'with': 1, 'as': 1, 'none': 1}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6275765572656147,0.6261278767503988,"(tensor([0.9671]), tensor([0.9782]), tensor([0.9726]), tensor([0.9771]))"
"124     Collects data for the evaluation of predicted labels (including multi-dimensional predictions)
125     and computes corresponding metrics
126     """"""
127     def __init__(self, y_predicted: PredictionArray, y_true: PredictionArray,
128                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
129         """"""
130         :param y_predicted: sequence of predicted labels. In case of multi-dimensional predictions, a data frame with
131             one column per dimension should be passed
132         :param y_true: sequence of ground truth labels of same shape as y_predicted
133         :param metrics: list of metrics to be computed on the provided data
134         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
135         """"""
136         self.y_true = []
137         self.y_predicted = []
138         self.y_true_multidim = None
139         self.y_predicted_multidim = None
140         if y_predicted is not None:
141             self._addAll(y_predicted, y_true)
142         super().__init__(metrics, additionalMetrics=additionalMetrics)
143 
144     def _add(self, y_predicted, y_true):
","123     Collects data for the evaluation of predicted values (including multi-dimensional predictions)
124     and computes corresponding metrics
125     """"""
126     def __init__(self, y_predicted: Optional[PredictionArray], y_true: Optional[PredictionArray],
127                  metrics: List[TMetric], additionalMetrics: List[TMetric] = None):
128         """"""
129         :param y_predicted: sequence of predicted values, or, in case of multi-dimensional predictions, either a data frame with
130             one column per dimension or a nested sequence of values
131         :param y_true: sequence of ground truth labels of same shape as y_predicted
132         :param metrics: list of metrics to be computed on the provided data
133         :param additionalMetrics: the metrics to additionally compute. This should only be provided if metrics is None
134         """"""
135         self.y_true = []
136         self.y_predicted = []
137         self.y_true_multidim = None
138         self.y_predicted_multidim = None
139         if y_predicted is not None:
140             self.addAll(y_predicted, y_true)
141         super().__init__(metrics, additionalMetrics=additionalMetrics)
142 
143     def add(self, y_predicted, y_true):
","Before: 141
After: 140",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,1325,"{'module': 1, 'ERROR': 5, 'identifier': 74, 'for': 1, 'call': 1, 'argument_list': 1, '(': 1, 'binary_operator': 2, '-': 2, ')': 1, 'and': 1, 'string': 1, 'string_start': 2, 'string_content': 1, 'string_end': 1, ':': 8, 'attribute': 2, '.': 2, ',': 1, 'with': 1, 'as': 1, 'none': 1}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.6275765572656147,0.6261278767503988,"(tensor([0.9671]), tensor([0.9782]), tensor([0.9726]), tensor([0.9771]))"
"141             self._addAll(y_predicted, y_true)
142         super().__init__(metrics, additionalMetrics=additionalMetrics)
143 
144     def _add(self, y_predicted, y_true):
145         """"""
146         Adds a single pair of values to the evaluation
147         Parameters:
148             y_predicted: the value predicted by the model
149             y_true: the true value
150         """"""
151         self.y_true.append(y_true)
152         self.y_predicted.append(y_predicted)
153 
154     def _addAll(self, y_predicted, y_true):
","140             self.addAll(y_predicted, y_true)
141         super().__init__(metrics, additionalMetrics=additionalMetrics)
142 
143     def add(self, y_predicted, y_true):
144         """"""
145         Adds a single pair of values to the evaluation
146         Parameters:
147             y_predicted: the value predicted by the model
148             y_true: the true value
149         """"""
150         self.y_true.append(y_true)
151         self.y_predicted.append(y_predicted)
152 
153     def addAll(self, y_predicted: PredictionArray, y_true: PredictionArray):
","Before: 144
After: 143",fix eval_stats_base.py -- a/src/sensai/evaluation/eval_stats_base.py,Sync dcs,https://github.com/opcode81/sensAI,src/sensai/evaluation/eval_stats/eval_stats_base.py,0b967bd1478ddb96f9cc28abc294e470e77be4de,5250455170eabeedb023d8527583fa9b3036303c,0,1357,"{'module': 1, 'expression_statement': 5, 'call': 5, 'attribute': 6, 'identifier': 21, '.': 6, 'argument_list': 5, '(': 6, ',': 4, ')': 6, 'keyword_argument': 1, '=': 1, 'function_definition': 1, 'def': 1, 'parameters': 1, ':': 1, 'block': 1, 'string': 1, 'string_start': 1, 'string_content': 1, 'string_end': 1}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 24, 'end_line': 32, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/prev/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}","{'cyclomatic_complexity': 3, 'nloc': 7, 'token_count': 59, 'name': '__init__', 'long_name': '__init__( self , metrics : List [ TMetric ] , additionalMetrics : List [ TMetric ] = None )', 'start_line': 25, 'end_line': 33, 'full_parameters': ['self', ' metrics : List [ TMetric ]', ' additionalMetrics : List [ TMetric ] = None'], 'filename': '/home/student/.local/lib/python3.8/site-packages/Minecpp/sensAI/curr/src/sensai/evaluation/eval_stats/eval_stats_base.py', 'top_nesting_level': 1, 'fan_in': 0, 'fan_out': 0, 'general_fan_out': 0}",0.5384456615109445,0.5261923708924484,"(tensor([0.9689]), tensor([0.9825]), tensor([0.9757]), tensor([0.9811]))"
